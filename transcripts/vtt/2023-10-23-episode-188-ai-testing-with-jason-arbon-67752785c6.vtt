WEBVTT

00:00:05.070 --> 00:00:11.350
Alan here jumping in off the top to tell you that our best friend of software in the world

00:00:11.350 --> 00:00:15.070
Zencaster kind of shit the bed on this podcast.

00:00:15.070 --> 00:00:20.130
I have spent countless hours this weekend stitching together the fragments of audio

00:00:20.130 --> 00:00:21.590
that it gave me.

00:00:21.590 --> 00:00:24.430
In the end we lost a big chunk of our guest's audio.

00:00:24.430 --> 00:00:29.490
I think I was able to string this together a little bit in a way that gives you the gist

00:00:29.490 --> 00:00:33.630
of what we were talking about and then we're going to follow up and have Jason on a future

00:00:33.630 --> 00:00:35.110
episode of the podcast.

00:00:35.110 --> 00:00:38.510
But I think it's enough here to get some conversations going.

00:00:38.510 --> 00:00:45.710
Hope you enjoy and we'll try it again next time with a different audio software.

00:00:45.710 --> 00:00:46.710
Welcome to A.B.

00:00:46.710 --> 00:00:50.230
Testing Podcast, your modern testing podcast.

00:00:50.230 --> 00:00:56.350
Your hosts Alan and Brent will be here to guide you through topics on testing, leadership,

00:00:56.350 --> 00:00:58.990
agile, and anything else that comes to mind.

00:00:58.990 --> 00:01:00.910
Now on with the show.

00:01:00.910 --> 00:01:02.310
Hey everybody.

00:01:02.310 --> 00:01:03.310
Welcome to the A.B.

00:01:03.310 --> 00:01:04.310
Testing Podcast.

00:01:04.310 --> 00:01:06.830
This is my happy jingle.

00:01:06.830 --> 00:01:07.830
Just made it up.

00:01:07.830 --> 00:01:08.830
I'm Alan.

00:01:08.830 --> 00:01:10.190
I'm Brent.

00:01:10.190 --> 00:01:11.270
And we have a guest.

00:01:11.270 --> 00:01:15.190
I'd like to welcome, before he speaks, I'd like to welcome President Joe Biden to the

00:01:15.190 --> 00:01:16.190
podcast.

00:01:16.190 --> 00:01:18.950
Joe, would you like to say something?

00:01:18.950 --> 00:01:19.950
Umm...

00:01:19.950 --> 00:01:23.270
Oh, actually Joe had to step out.

00:01:23.270 --> 00:01:24.390
So we have a better guest.

00:01:24.390 --> 00:01:27.150
We have, well, why don't you introduce yourself Jason?

00:01:27.150 --> 00:01:30.550
Yeah, I'm Jason Arbin and I'm a test nerd.

00:01:30.550 --> 00:01:36.190
And I listen to this podcast religiously and I get upset when I have to wait more than

00:01:36.190 --> 00:01:38.070
two weeks for a new episode.

00:01:38.070 --> 00:01:39.070
That's our fault.

00:01:39.070 --> 00:01:40.430
Now that's right away.

00:01:40.430 --> 00:01:45.830
We are 42 seconds in minus the theme song and we have a dig on us already.

00:01:45.830 --> 00:01:46.830
So I feel good.

00:01:46.830 --> 00:01:47.830
I feel good.

00:01:47.830 --> 00:01:48.830
Yeah.

00:01:48.830 --> 00:01:51.990
We're all about self-deprecation here Jason.

00:01:51.990 --> 00:01:52.990
So knock yourself out.

00:01:52.990 --> 00:01:53.990
Yeah.

00:01:53.990 --> 00:01:56.630
And there's a topic we already discussed we're not going to go into.

00:01:56.630 --> 00:02:01.630
The one thing I really like about Jason is he's not afraid to, he's to the point, he

00:02:01.630 --> 00:02:03.110
will say what's on his mind.

00:02:03.110 --> 00:02:10.620
I think I titled our last Cod, our Cod past, our podcast, all we talk about is AI, which

00:02:10.620 --> 00:02:13.540
is the new name, which, oh, you know what?

00:02:13.540 --> 00:02:18.780
Brent change your name to Ian and we can be the AI testing podcast.

00:02:18.780 --> 00:02:24.900
Or I could just get rid of you and it could be the AI and Brent podcast.

00:02:24.900 --> 00:02:28.860
Brent you couldn't find the recording button as it was biting you in the pants.

00:02:28.860 --> 00:02:30.820
Oh yeah.

00:02:30.820 --> 00:02:32.180
No one wants that.

00:02:32.180 --> 00:02:37.580
The quality, I mean people already complain about the quality of our editing staff.

00:02:37.580 --> 00:02:41.260
It will just go straight downhill without you controlling.

00:02:41.260 --> 00:02:42.260
Here's what I want to do.

00:02:42.260 --> 00:02:45.540
I would like to get Jason involved and kind of hear his thoughts because he has great thoughts.

00:02:45.540 --> 00:02:48.460
Jason's been in, it's isn't the Jason plugging.

00:02:48.460 --> 00:02:50.980
We don't do like, you know, like here, let's interview you about everything you've done

00:02:50.980 --> 00:02:52.060
in your life.

00:02:52.140 --> 00:02:57.060
But lately, the last couple of years, you have been deep into something that's become very

00:02:57.060 --> 00:03:03.300
popular, AI, and voicing thoughts on how AI can help testing.

00:03:03.300 --> 00:03:08.700
I think we're in a weird spot in the industry and, well, let me get back to that in a second.

00:03:08.700 --> 00:03:13.700
But I guess the question of the day is, and we can all chime in on this, is what

00:03:13.700 --> 00:03:17.780
is going to be the impact of AI on software testing?

00:03:17.780 --> 00:03:20.620
So I guess that's a direct for me.

00:03:20.940 --> 00:03:25.060
So I'm used to just listening to you guys and not interacting.

00:03:25.060 --> 00:03:28.060
Although I do yell at the radio or the car radio.

00:03:28.060 --> 00:03:29.870
Here's our first tangent.

00:03:29.870 --> 00:03:33.790
One of the things that used to exist when I was studying music was these records called

00:03:33.790 --> 00:03:36.020
Music Minus One.

00:03:36.020 --> 00:03:39.340
And what it was, it's like musician karaoke before karaoke was a thing.

00:03:39.340 --> 00:03:41.500
It was like, it could be a jazz thing.

00:03:41.500 --> 00:03:44.180
It could be an orchestral piece.

00:03:44.180 --> 00:03:47.100
And they just leave out one part so you could play along.

00:03:47.100 --> 00:03:53.940
And we should do a podcast like that, Brent, where we just leave space for people to add

00:03:53.940 --> 00:03:57.260
themselves and they can just make their own podcast out of us.

00:03:57.260 --> 00:04:00.100
But yeah, for this one, you're actually live in here.

00:04:00.100 --> 00:04:01.780
They could do that already, right?

00:04:01.780 --> 00:04:02.780
They could just...

00:04:02.780 --> 00:04:03.780
Pause.

00:04:03.780 --> 00:04:09.860
They could just skip the first 50% of the podcast, which is all the tangential bullshit.

00:04:09.860 --> 00:04:16.580
Why don't people make mashups of our podcast and mix it in with somebody else's?

00:04:16.780 --> 00:04:21.460
Anyway, let's rewind the stack and get back to how AI is going to impact software testing.

00:04:21.460 --> 00:04:23.380
Yeah, so I think there's really...

00:04:23.380 --> 00:04:24.380
It's a very broad question.

00:04:24.380 --> 00:04:29.780
I think there's really three cohorts of people that will be impacted differently.

00:04:29.780 --> 00:04:32.740
One are the folks that are just putting their head in the sand.

00:04:32.740 --> 00:04:35.340
They don't want to know it exists.

00:04:35.340 --> 00:04:36.340
They're threatened by it.

00:04:36.340 --> 00:04:37.340
They're concerned.

00:04:37.340 --> 00:04:38.340
They don't like change.

00:04:38.340 --> 00:04:42.380
I think we'll just hear less from them and about them over the coming years.

00:04:42.380 --> 00:04:46.100
I think it's not that interesting, actually.

00:04:47.100 --> 00:04:48.100
There's always people that...

00:04:48.100 --> 00:04:49.100
There's people still red horses around, right?

00:04:49.100 --> 00:04:50.100
It's all good.

00:04:50.100 --> 00:04:55.740
The second cohort is a primary set of people that are traditional software testers or people

00:04:55.740 --> 00:04:57.020
that care about quality.

00:04:57.020 --> 00:04:59.730
I think that what's going to happen...

00:04:59.730 --> 00:05:02.170
People are not really ready for...

00:05:02.170 --> 00:05:06.010
Those people want to add AI as a superpower to make their testing and their work better,

00:05:06.010 --> 00:05:07.850
faster, smarter, or cheaper.

00:05:07.850 --> 00:05:15.890
But I think what they don't realize is that this generation of AI is generative by definition.

00:05:15.930 --> 00:05:19.250
It generates things like test cases and bugs.

00:05:19.250 --> 00:05:20.250
People are...

00:05:20.250 --> 00:05:21.250
I don't think...

00:05:21.250 --> 00:05:22.610
Because I wasn't ready for it either.

00:05:22.610 --> 00:05:25.530
You get overwhelmed by the amount of data that comes out of these systems.

00:05:25.530 --> 00:05:30.250
You used to manually create them by writing a bunch of Java code or Python or typing into

00:05:30.250 --> 00:05:31.250
a test case manager.

00:05:31.250 --> 00:05:35.850
Now, they're just generated automatically by a machine.

00:05:35.850 --> 00:05:39.610
Testors will have to deal with this deluge of information to triage it.

00:05:39.610 --> 00:05:41.370
I think they'll have two big impacts, though.

00:05:41.370 --> 00:05:42.370
Bear with me.

00:05:42.370 --> 00:05:44.210
This is just kind of a highlight of stuff.

00:05:44.210 --> 00:05:48.580
I think there'll be two big impacts on software testing and testers.

00:05:48.580 --> 00:05:54.660
One is that there'll be more demand for testers that are experienced that can triage issues

00:05:54.660 --> 00:05:57.420
and tell the robot, yes, no, yes, no, yes, no.

00:05:57.420 --> 00:06:01.220
But there'll be less demand for basic software testers, like newbies, freshies, and stuff

00:06:01.220 --> 00:06:04.020
like that, because the machines will just do that work.

00:06:04.020 --> 00:06:05.780
And they can't really do the triage work.

00:06:05.780 --> 00:06:09.420
And then there's a third class of folks where teams will...

00:06:09.420 --> 00:06:13.900
New teams, you start a new team, you start a new app, or something like that, you will

00:06:13.900 --> 00:06:18.700
just start to use AI first before you hire that human tester.

00:06:18.700 --> 00:06:25.140
And the real interesting extrapolation and race here is, will the AI get better faster

00:06:25.140 --> 00:06:30.460
than the demand of those teams that delayed hiring a tester over time?

00:06:30.460 --> 00:06:31.460
But I think that that's...

00:06:31.460 --> 00:06:34.330
Those are the major trend lines I'm seeing.

00:06:34.330 --> 00:06:39.770
So you hit on some things that are very, very interesting and something that you know

00:06:39.770 --> 00:06:43.890
if you listen to the podcast, but I want to repeat in case anybody's listening to this,

00:06:43.890 --> 00:06:48.210
is that Brent and I started this podcast because we were seeing changes happening

00:06:48.210 --> 00:06:52.970
and testing with teams living to more, I don't even call it agile, more adaptive and faster

00:06:52.970 --> 00:06:54.800
delivery.

00:06:54.800 --> 00:06:56.880
And what we saw was sort of the same thing.

00:06:56.880 --> 00:07:01.640
A lot of people just put their heads in the sand and said, this isn't happening.

00:07:01.640 --> 00:07:07.080
A lot of people were overwhelmed by it and didn't understand how it would affect their

00:07:07.080 --> 00:07:09.400
job or could it affect their job?

00:07:09.400 --> 00:07:11.880
What could they do to not make it affect their job?

00:07:11.880 --> 00:07:15.800
And some people were like, yeah, we're in, we're going to go make this work.

00:07:15.800 --> 00:07:20.600
And we started the podcast because we wanted to help people navigate what that change was

00:07:20.600 --> 00:07:21.600
like.

00:07:21.600 --> 00:07:26.000
And I feel like eight years later, I think we're kind of in the same place in a way.

00:07:26.000 --> 00:07:28.560
AI is changing.

00:07:28.560 --> 00:07:31.920
Whether you like it or not, you can put your head in the sand, you can be overwhelmed

00:07:31.920 --> 00:07:38.040
by it, not sure what it's doing, or you can ride the wave, but it is going to make

00:07:38.680 --> 00:07:43.250
difference in the way we deliver software, like it or not.

00:07:43.250 --> 00:07:45.320
And now Brent's gone.

00:07:45.320 --> 00:07:47.080
So good. It's the.

00:07:47.080 --> 00:07:53.840
So now it's the AJ testing podcast because Brent puked live on screen.

00:07:53.840 --> 00:07:56.780
Okay, Brent's back. This is fun.

00:07:56.780 --> 00:08:01.300
So I'm curious in this case, well, Brent figures out his stuff.

00:08:01.300 --> 00:08:04.700
I fully agree with those kind of three camps where folks are.

00:08:04.700 --> 00:08:09.700
And based on those three camps, is it similar to what Brent and I have been giving a few

00:08:09.700 --> 00:08:18.060
years ago or like what advice do you have for testers right now who are either ignoring

00:08:18.060 --> 00:08:21.940
or watching or actually, if they're listening to this, they're probably not ignoring AI,

00:08:21.940 --> 00:08:24.380
but they may not be sure what it's going to do.

00:08:24.380 --> 00:08:25.660
What advice do you have for them?

00:08:25.660 --> 00:08:30.780
How do you, how do they, how do they ride the wave versus get beaten down by the

00:08:30.780 --> 00:08:32.220
wave?

00:08:33.180 --> 00:08:34.260
It depends on the person.

00:08:34.260 --> 00:08:36.700
I think they're going to self-select into one of those camps.

00:08:37.580 --> 00:08:42.140
And I don't think you or I or Brent can do much about it, actually.

00:08:43.180 --> 00:08:48.340
I think that's probably why I relate so much to modern testing and you guys banter here

00:08:48.340 --> 00:08:52.500
on on the podcast is that you're trying to help you, trying to talk through it, trying

00:08:52.500 --> 00:08:56.780
to be honest and vulnerable and trying to get through it with everybody.

00:08:56.780 --> 00:09:03.260
But I don't think that it helps all that much.

00:09:04.260 --> 00:09:08.180
I think the people that really need to listen to us don't listen to us.

00:09:08.700 --> 00:09:09.460
Right, right.

00:09:09.460 --> 00:09:12.780
And the people, there are people that can, there's a middle, there's a middle ground

00:09:12.780 --> 00:09:16.780
there where people will listen and go, they get confirmation bias, confirmation bias,

00:09:16.780 --> 00:09:19.300
or, you know, they know that they're not alone.

00:09:19.420 --> 00:09:22.420
I think it helps a lot emotionally, but I think people will just automatically

00:09:22.420 --> 00:09:25.140
self-select into different groups.

00:09:25.220 --> 00:09:28.540
And I, but I think like you guys have done, you at least give people an

00:09:28.540 --> 00:09:29.860
opportunity to hear the other side.

00:09:30.220 --> 00:09:31.900
Um, but I worry about, this is the funny thing.

00:09:32.140 --> 00:09:35.030
I worry about pushing too hard.

00:09:35.030 --> 00:09:38.750
Like if people just don't want to do it, you don't want to drag them into a fight

00:09:38.750 --> 00:09:41.870
that they don't have the energy to deal with.

00:09:41.910 --> 00:09:42.270
Right.

00:09:42.310 --> 00:09:44.150
And so it's almost cruel in a way.

00:09:44.550 --> 00:09:48.310
So I kind of feel like people just self share the information, um, and let

00:09:48.310 --> 00:09:52.030
people self-select in or out, but at least they should see the, hear the,

00:09:52.070 --> 00:09:53.510
have a chance to get the information.

00:09:53.870 --> 00:09:59.450
I think the single most important thing, um, it is very similar to then.

00:10:00.860 --> 00:10:08.340
Is to as much as possible, get as many people to realize that they have a choice.

00:10:08.820 --> 00:10:09.100
Right.

00:10:09.100 --> 00:10:11.980
They can be the butterfly or they can be the wind.

00:10:12.180 --> 00:10:13.620
They have that choice.

00:10:13.820 --> 00:10:15.900
Now, if they self-select, they self-select.

00:10:16.420 --> 00:10:16.780
Right.

00:10:16.980 --> 00:10:21.380
Um, in my view, if they, the, this is a bit of wisdom.

00:10:21.380 --> 00:10:27.420
I, I tell my children it's refusing to make a decision is a decision that

00:10:27.420 --> 00:10:28.900
you've chosen to be the wind.

00:10:28.900 --> 00:10:33.820
You're going to be guided in whatever direction, the powers that be in your

00:10:33.820 --> 00:10:35.980
life, uh, take you.

00:10:36.340 --> 00:10:37.380
Is it the heart?

00:10:37.420 --> 00:10:40.180
If you choose not to decide, you still made a choice.

00:10:40.220 --> 00:10:46.590
I, um, uh, got a D in my philosophy course in college.

00:10:46.590 --> 00:10:49.310
So I probably had not the person asked on that one.

00:10:49.790 --> 00:10:51.990
I took, I took it 75 years ago.

00:10:51.990 --> 00:10:56.630
So, um, but that's the important thing to me is making sure that people

00:10:56.630 --> 00:11:00.550
understand they, they at minimum have that choice.

00:11:03.480 --> 00:11:06.440
Well, it's, I want to jump in because you're, you're totally right, Jason.

00:11:06.440 --> 00:11:10.480
It's a weird place where we're doing a podcast and generally the people that

00:11:10.480 --> 00:11:14.880
are sticking in their head in the sand, they are not actively looking to

00:11:14.880 --> 00:11:17.400
discover new knowledge anywhere.

00:11:18.270 --> 00:11:19.790
They're not reading articles.

00:11:19.790 --> 00:11:24.190
They're not interacting on social media platforms around technology.

00:11:24.830 --> 00:11:27.870
These, uh, they're just, they're not going to conferences.

00:11:28.310 --> 00:11:29.390
Uh, they're just not.

00:11:29.790 --> 00:11:34.350
So you're at those people, the people that we probably need, you know, the most

00:11:34.350 --> 00:11:38.150
advice and kind of on how to deal with this stuff, aren't listening.

00:11:38.150 --> 00:11:40.830
And that's always, that's kind of always been our place with our three

00:11:40.830 --> 00:11:46.230
listeners there largely in camp with what we're doing, or they're

00:11:46.230 --> 00:11:47.550
wanting to move that direction.

00:11:47.870 --> 00:11:49.220
So, right.

00:11:49.220 --> 00:11:50.620
And you mentioned conferences too.

00:11:50.620 --> 00:11:54.860
Like I, I showed up at two conferences in the last two weeks, the last minute.

00:11:54.860 --> 00:11:56.780
And it was interesting.

00:11:56.900 --> 00:12:00.260
Almost everybody was talking or wanting to see something about AI.

00:12:00.660 --> 00:12:04.860
Then where, you know, a few years ago, I couldn't drag anybody into a talk about it.

00:12:05.380 --> 00:12:09.220
Now everyone in every vendor has applied AI to the end of their software name.

00:12:09.420 --> 00:12:10.620
It's kind of like going to Microsoft.

00:12:10.620 --> 00:12:13.620
Remember when they added.net to the end of everything in the early 2000s?

00:12:15.300 --> 00:12:15.780
Yes.

00:12:16.140 --> 00:12:17.060
I worked on windows.

00:12:17.060 --> 00:12:22.460
So he.net not what about not an ounce of managed code in that thing.

00:12:22.860 --> 00:12:25.540
And don't forget about active X before that.

00:12:25.980 --> 00:12:26.500
Oh God.

00:12:27.140 --> 00:12:27.380
All right.

00:12:28.450 --> 00:12:31.130
I worked on active X on windows CE.

00:12:33.450 --> 00:12:36.070
Enterprise for, oh, for business.

00:12:36.070 --> 00:12:36.470
Sorry.

00:12:36.750 --> 00:12:41.630
Windows CE.net for business, active X for windows CE.net for business.

00:12:41.710 --> 00:12:44.230
That's the product killer marketing.

00:12:44.910 --> 00:12:45.230
Yeah.

00:12:45.230 --> 00:12:49.630
So I think everybody's wanting, they're kind of curious, but the funny thing

00:12:49.630 --> 00:12:52.670
is there's like, I'm in different rooms where the focus is on AI.

00:12:52.990 --> 00:12:58.190
And then somebody, not me actually, would ask kept asking like on these panels

00:12:58.190 --> 00:13:00.750
and stuff, who here has tried chat GPT?

00:13:01.600 --> 00:13:03.800
And like, it was like a third of the room and raise her hand.

00:13:04.000 --> 00:13:05.600
Like my mom has used it.

00:13:06.000 --> 00:13:06.400
Right.

00:13:06.440 --> 00:13:07.640
When they know offense to my mother.

00:13:08.000 --> 00:13:12.280
Um, but she uses it and she asks me about AI stuff sometimes.

00:13:12.800 --> 00:13:16.520
So if you've got two thirds of a room of people that are in the technology

00:13:16.520 --> 00:13:21.080
world, right in software, even, um, and they haven't even tried it in the,

00:13:21.080 --> 00:13:25.760
in the excuse is what is too difficult because it's a chat window.

00:13:26.500 --> 00:13:27.340
I don't, I don't get it.

00:13:27.580 --> 00:13:30.620
So I really, I really do worry about trying to drag those types of

00:13:30.620 --> 00:13:32.140
folks into the conversation.

00:13:32.460 --> 00:13:36.260
It does come down to that fixed versus growth mindset in a way.

00:13:36.860 --> 00:13:39.740
And they're just, like I said before, they're, they're just not seeking

00:13:39.740 --> 00:13:41.940
knowledge for a lot of those test conferences.

00:13:41.940 --> 00:13:46.060
The vast majority of people have been in testing for less than a year.

00:13:46.380 --> 00:13:49.620
And their employers sent them, they're saying, please go learn something

00:13:49.620 --> 00:13:52.820
to make yourself more effective because the employers don't understand

00:13:52.820 --> 00:13:53.660
testing either.

00:13:54.020 --> 00:13:57.980
So they're just inherently not curious people.

00:13:57.980 --> 00:14:05.860
I think I, I'm not willing to, to, I am to pass such judgment, but, but.

00:14:06.380 --> 00:14:09.700
I mean, that does that same question totally befuddles me.

00:14:09.740 --> 00:14:13.860
One third is shockingly surprising.

00:14:14.220 --> 00:14:17.940
Like when you said that I'm like, okay, so my next follow up question

00:14:17.940 --> 00:14:22.340
to Jason is, Hey, when you went to these conferences, are you finding

00:14:22.340 --> 00:14:27.540
that they're now sort of old school echo chambers, or is it now more what

00:14:27.620 --> 00:14:28.620
Alan would just say?

00:14:28.700 --> 00:14:30.540
And no, it's young people who don't know better.

00:14:31.220 --> 00:14:32.260
I think it's, it's a barbell.

00:14:32.260 --> 00:14:33.900
I think you've got the old folks like me.

00:14:33.900 --> 00:14:35.140
And then you've got the young ones.

00:14:35.940 --> 00:14:37.900
Middle people are just busy doing actual testing.

00:14:38.500 --> 00:14:40.340
Um, they're the quiet silent majority.

00:14:41.060 --> 00:14:41.260
Yeah.

00:14:41.260 --> 00:14:45.100
But old, old people like you have used Chet GPT.

00:14:45.980 --> 00:14:47.260
Yeah, but I'm a little, yeah, yeah.

00:14:47.260 --> 00:14:49.580
No, and, and yeah, yeah.

00:14:49.620 --> 00:14:50.380
A couple of times.

00:14:50.420 --> 00:14:50.740
Yeah.

00:14:51.300 --> 00:14:56.620
Well, I think there are, my, my LinkedIn Chets beg to differ.

00:14:56.980 --> 00:15:02.860
Uh, I think, you know, my experience is, is that, you know, the other end of the

00:15:02.860 --> 00:15:07.300
barbell from the noobs, they are often there to try and peddle their wares to

00:15:07.300 --> 00:15:09.220
the noobs note, not picking on you.

00:15:09.220 --> 00:15:12.740
I'm picking on the folks trying to sell their, um, consulting skills because

00:15:12.740 --> 00:15:13.900
they, they prey on noobs.

00:15:13.940 --> 00:15:14.100
Yeah.

00:15:14.220 --> 00:15:14.860
I just had a robot.

00:15:14.860 --> 00:15:17.660
Um, but super interesting.

00:15:17.780 --> 00:15:21.820
I like, I use Chet GPT probably daily.

00:15:22.140 --> 00:15:26.500
I'm, I'm far, I'm far smarter with Chet GPT than without.

00:15:27.420 --> 00:15:27.660
Yeah.

00:15:27.860 --> 00:15:28.060
Yeah.

00:15:28.060 --> 00:15:31.460
But that's, that's a, that's a low, it's a low one mark though.

00:15:34.540 --> 00:15:39.900
Um, dude, you know, what's crazy is that, uh, there's just today, like there

00:15:39.900 --> 00:15:42.300
was this, you can look on find it on LinkedIn if you really want to, I'll

00:15:42.300 --> 00:15:43.940
protect the innocent, named innocent.

00:15:44.100 --> 00:15:48.460
But someone that heads one of the software testing organizations, like the

00:15:48.460 --> 00:15:51.660
association stuff that, you know, that think big thoughts all the time on

00:15:51.660 --> 00:15:56.660
testing, um, they were asking, and I, I, I like him as a human being, but he's

00:15:56.660 --> 00:15:59.660
asking, has anyone tried Dolly three?

00:16:00.340 --> 00:16:01.260
I'm interested.

00:16:01.260 --> 00:16:04.300
I haven't used it yet, but I'm interested in what people think about it.

00:16:05.100 --> 00:16:06.860
What, what kind of a question is that?

00:16:07.580 --> 00:16:09.380
Like it takes four seconds to try it.

00:16:09.580 --> 00:16:11.860
I, my, my kids can use it.

00:16:12.260 --> 00:16:12.820
Um,

00:16:13.740 --> 00:16:15.620
and like you just wasted.

00:16:15.660 --> 00:16:20.020
And then the people that reply that say, I haven't yet either, but will soon,

00:16:20.300 --> 00:16:24.340
it takes longer to write that response than it does.

00:16:24.340 --> 00:16:26.260
It would be to go generate a fix.

00:16:26.260 --> 00:16:30.660
Like, but, but that's why, like, but so bringing them into the, into the

00:16:30.780 --> 00:16:35.140
presence, not even the future is, is maybe a dis you know what, I think

00:16:35.180 --> 00:16:38.420
that I've, something I've discovered is we haven't talked about AI internally

00:16:38.420 --> 00:16:42.540
at my company is I think, I think they're a little daunted by it because

00:16:42.540 --> 00:16:45.500
it seems hard or it seems like it's magic.

00:16:46.060 --> 00:16:50.700
And I was thinking that the Clark quote, any sufficiently advanced technology

00:16:50.700 --> 00:16:52.100
is indistinguishable from magic.

00:16:52.460 --> 00:16:55.820
Then I go back and Brent, do you remember, I brought this up maybe on

00:16:55.820 --> 00:16:59.540
the podcast before, but I bring this up once in a while, but 10, how

00:16:59.540 --> 00:17:00.180
what year is it?

00:17:00.260 --> 00:17:04.300
Maybe 10 years ago, everything was data science, Brent's a data scientist now.

00:17:04.300 --> 00:17:07.820
But remember when, it's like, Oh, we'll solve that problem with data science.

00:17:08.060 --> 00:17:09.100
Everything was data science.

00:17:09.100 --> 00:17:11.180
It was just the new, it was the new magic.

00:17:11.500 --> 00:17:12.660
We are, Oh, you know what?

00:17:12.700 --> 00:17:13.740
We'll get a data scientist.

00:17:13.740 --> 00:17:14.900
They'll figure this out for us.

00:17:15.300 --> 00:17:17.340
And now today, 10 years later, it's AI.

00:17:17.500 --> 00:17:19.100
Let's use AI to solve that problem.

00:17:19.140 --> 00:17:20.100
We can do this AI.

00:17:20.500 --> 00:17:20.700
Yeah.

00:17:20.700 --> 00:17:20.900
Yeah.

00:17:21.060 --> 00:17:24.060
Actually you, you're buried the perfect summary right there.

00:17:24.060 --> 00:17:28.340
It was that, yeah, it it's intimidating because I think of the last 10 years.

00:17:28.780 --> 00:17:31.900
What's there's been a huge inflection point last October, right?

00:17:32.340 --> 00:17:33.380
October or November.

00:17:33.460 --> 00:17:38.380
Um, because, and I, I spent tons of Google money, uh, and tons of my

00:17:38.380 --> 00:17:43.900
life, my actual life on this planet, labeling, uh, you know, finding as many

00:17:43.900 --> 00:17:48.020
pictures of login buttons and search text boxes as I could, and then labeling them.

00:17:48.380 --> 00:17:50.780
It was very tedious, very expensive, very intimidating.

00:17:50.980 --> 00:17:54.500
And the stuff would work, you know, 90% of the time when you're happy.

00:17:54.820 --> 00:17:55.100
Right?

00:17:55.140 --> 00:17:56.700
Like it was very brutal.

00:17:57.060 --> 00:18:01.700
Um, but with my fingernails off, but go on with generative AI, it is, it,

00:18:01.980 --> 00:18:03.620
things have dramatically changed.

00:18:03.940 --> 00:18:07.140
Like I'm literally probably, I'm rebuilding some of the stuff I've

00:18:07.140 --> 00:18:08.300
done before right now.

00:18:08.460 --> 00:18:14.540
And it's with, with no team, like it's crazy, but I'm partnering with, with AI.

00:18:14.820 --> 00:18:18.100
And so it's, it's, it's, but it, because it's generative.

00:18:18.100 --> 00:18:21.340
And so it's weird cause you have this huge angst that only this, these super

00:18:21.340 --> 00:18:25.740
smart people in a closed room at AI at Microsoft or something can, can do this

00:18:25.740 --> 00:18:27.460
AI stuff in this magic, right?

00:18:27.740 --> 00:18:31.380
But it's been democratized in the last year so that anyone can get access to

00:18:31.380 --> 00:18:35.220
it and it's smarter than the even fathom to guess that it is.

00:18:35.460 --> 00:18:37.900
And they're worried that it's even smarter than they think it is.

00:18:38.140 --> 00:18:40.420
And so it's just a huge inflection point.

00:18:40.420 --> 00:18:44.260
So with both, like you said, it's both magical and it's intimidating.

00:18:44.260 --> 00:18:47.460
I think those things are still conflated with people in their mind.

00:18:47.900 --> 00:18:50.140
So yeah, I basically just did a long version of what you

00:18:50.140 --> 00:18:51.660
said eloquently and succinctly.

00:18:52.180 --> 00:18:52.780
No, no, no.

00:18:53.020 --> 00:18:58.860
I think what we've said before is I think people, they want, again, I'm not

00:18:58.860 --> 00:19:03.220
going to throw, try to throw people out of the bus here, but they want it to be

00:19:03.220 --> 00:19:04.980
something it's not sometimes too.

00:19:04.980 --> 00:19:07.060
They want it to answer their questions.

00:19:07.380 --> 00:19:10.940
And for me, it is my constant collaborator.

00:19:11.420 --> 00:19:18.380
Now, and honestly, right, there was a lot of that back and forth, right?

00:19:18.420 --> 00:19:24.380
I do see, there are times where I had to sit down even with data scientists

00:19:24.380 --> 00:19:27.740
and explain to them how LLM works.

00:19:27.740 --> 00:19:33.060
And there's a particular problem that I now actually even straight up call the genie

00:19:33.060 --> 00:19:36.680
problem. I think I've talked about it on the podcast.

00:19:37.280 --> 00:19:39.040
And there's this whole language.

00:19:39.040 --> 00:19:41.560
So people are getting more and more familiar with it.

00:19:41.560 --> 00:19:43.200
They have enough exposure to it.

00:19:43.200 --> 00:19:46.320
They it's being demystified.

00:19:46.840 --> 00:19:55.080
That said, right, machine and human working together, quite honestly, I think

00:19:55.120 --> 00:20:01.500
is always going to beat the separate component pieces.

00:20:01.980 --> 00:20:05.500
Finally, we have an argument or discussion.

00:20:06.220 --> 00:20:11.580
I actually I'm in my experience, it could be the human operator that's partnering

00:20:11.580 --> 00:20:18.240
with AI when I like the funny thing about about chat GPT is it's not in a loop,

00:20:18.240 --> 00:20:20.200
right? It just waits for you.

00:20:20.200 --> 00:20:21.880
I am the slow part of that loop.

00:20:21.880 --> 00:20:26.000
I am. And in fact, the way I used I've changed how I use it dramatically

00:20:26.000 --> 00:20:29.480
over the last six months, I used to think about my existing problems, break them

00:20:29.480 --> 00:20:33.040
down into pieces like we're told to do in software engineering and then have

00:20:33.160 --> 00:20:36.400
each little piece implemented by some AI prompt magic thing, right?

00:20:36.400 --> 00:20:37.480
With enough data.

00:20:37.480 --> 00:20:42.080
And the funniest thing is I've realized is that if you start with just

00:20:42.080 --> 00:20:47.360
the top level problem and then have GPT break it down for you, like the human

00:20:47.360 --> 00:20:49.080
in the loop is the dumb part.

00:20:49.080 --> 00:20:54.400
Like I like the GPT is unleashed only when I remove myself from more

00:20:54.400 --> 00:20:56.800
and resume myself from the equation.

00:20:56.800 --> 00:21:00.200
GPT is more powerful and I'm more efficient in what I'm trying to accomplish.

00:21:00.600 --> 00:21:02.880
If that sounds too convoluted, it is.

00:21:02.880 --> 00:21:07.320
But and people don't realize that it's also want to say that, you know,

00:21:07.320 --> 00:21:11.800
talk about data science or, you know, it was, you know, cloud or whatever

00:21:11.800 --> 00:21:13.520
these past revolutions have been.

00:21:14.240 --> 00:21:17.040
And they change the world and people like go, oh, yeah, or test automation.

00:21:17.040 --> 00:21:20.080
Right. Oh, yeah, there's there's now somebody trying to automate the things

00:21:20.080 --> 00:21:22.360
that I'm doing. OK, get it.

00:21:22.360 --> 00:21:24.280
But they have to linearly create them and stuff.

00:21:24.280 --> 00:21:26.270
But this is a sea change.

00:21:26.270 --> 00:21:29.950
Like people don't realize or want to realize like, oh, yeah,

00:21:29.950 --> 00:21:31.950
I can do some software testing maybe. Right.

00:21:32.390 --> 00:21:35.910
But Congress has hearings because they're worried

00:21:36.310 --> 00:21:41.350
that it may take over the world and kill everybody or destroy all jobs.

00:21:41.590 --> 00:21:44.110
And it's a national security risk.

00:21:44.110 --> 00:21:46.630
And we just had their past legislation to block.

00:21:47.350 --> 00:21:50.350
Well, it's unfortunately not where we put our smartest people.

00:21:50.670 --> 00:21:53.150
Right. But but guess who showed up at the hearing, though?

00:21:53.190 --> 00:21:55.470
The people making these models. Right.

00:21:55.870 --> 00:22:00.750
So and maybe it's in their self interest and regulated interest to do so.

00:22:00.950 --> 00:22:02.950
But for the protective modes.

00:22:02.950 --> 00:22:05.830
But but this said about data science.

00:22:06.030 --> 00:22:08.070
So it is a significantly different.

00:22:08.470 --> 00:22:11.750
It is. I think it freaked it freaks people out.

00:22:11.910 --> 00:22:13.870
The magic of it freaks people out.

00:22:13.870 --> 00:22:16.430
But people are too afraid of Skynet.

00:22:16.430 --> 00:22:19.790
And we are so many iterations of AI away from Skynet.

00:22:20.030 --> 00:22:23.150
Can I can I we could just agree.

00:22:23.710 --> 00:22:25.910
But by ones don't get exponential growth.

00:22:25.910 --> 00:22:27.950
Like, Brent, explain the exponential growth.

00:22:28.070 --> 00:22:30.230
Like, so I don't seem like I'm overly biased.

00:22:31.250 --> 00:22:33.850
Explain exponential growth by Jason.

00:22:34.410 --> 00:22:35.410
That's exponential.

00:22:36.930 --> 00:22:38.890
As always, it's

00:22:40.050 --> 00:22:41.210
inarguable.

00:22:41.210 --> 00:22:46.450
No, no, no. So so now I would want to get into sort of discussion

00:22:46.450 --> 00:22:50.050
around the distinction between an AI and AGI and ASI.

00:22:51.620 --> 00:22:53.660
AI is how do I put this?

00:22:53.940 --> 00:22:59.120
They know everything but can do nothing.

00:22:59.680 --> 00:23:05.400
That is why the human in the loop process or the human and machine

00:23:05.400 --> 00:23:08.280
working together is always going to outperform.

00:23:08.680 --> 00:23:14.040
The issue is AI, the AI, like, as you know, is generative.

00:23:14.160 --> 00:23:18.960
Like, I tell people over and over, you think of a generative model.

00:23:19.400 --> 00:23:21.040
Think parent.

00:23:21.040 --> 00:23:24.760
You go to someone's house and you hear the pair go, hello, hello.

00:23:25.120 --> 00:23:27.040
Right. You think of it.

00:23:27.040 --> 00:23:28.560
Oh, oh, they're greeting me.

00:23:29.300 --> 00:23:31.980
No, but from the parents point of view, that's just the noise.

00:23:31.980 --> 00:23:33.380
It's heard repeatedly.

00:23:33.380 --> 00:23:34.620
It doesn't know what it's saying.

00:23:34.620 --> 00:23:36.860
It's the same thing with the generative AI.

00:23:37.180 --> 00:23:42.300
I have a process, for example, in my team where where I've created

00:23:42.300 --> 00:23:45.700
a Python tool and I've actually created two agents that actually

00:23:45.700 --> 00:23:47.700
have a conversation towards a goal.

00:23:49.020 --> 00:23:52.860
So that aspect of it that you're talking about where where

00:23:53.820 --> 00:23:56.900
you could have LM work it out with itself.

00:23:57.460 --> 00:24:01.580
Right. The challenge is, is right now it doesn't have that thinking

00:24:01.580 --> 00:24:05.940
process around what's the next thing I should do with this.

00:24:06.340 --> 00:24:07.340
It does now, though.

00:24:07.340 --> 00:24:11.340
So like there's auto GPT and Microsoft released one a couple

00:24:11.340 --> 00:24:13.380
of weeks ago. I forget the name of it.

00:24:13.380 --> 00:24:14.820
It was Microsoft people.

00:24:14.820 --> 00:24:16.180
They're untrustworthy.

00:24:16.180 --> 00:24:17.860
That's for sure.

00:24:17.860 --> 00:24:18.820
We know too much.

00:24:18.820 --> 00:24:22.220
Could you trust the people that make PowerPoint to do anything constructive?

00:24:22.380 --> 00:24:24.740
If they put AI in it and then the AI generates the slides.

00:24:25.580 --> 00:24:29.820
Now, and to be clear, like you just said, auto GPD does this.

00:24:29.820 --> 00:24:33.100
And I'm just going to say up front, it doesn't.

00:24:33.420 --> 00:24:36.100
But the way they built it is an over.

00:24:37.300 --> 00:24:43.060
No, the way they built that is quite smart.

00:24:43.500 --> 00:24:50.000
So it is it the code that runs out of GPT

00:24:50.640 --> 00:24:55.040
knows how to drive LLM and take advantage of LLM.

00:24:55.320 --> 00:24:59.920
To even further make this sort of magical experience where all you need to do

00:24:59.920 --> 00:25:04.960
is add another little widget and suddenly its capabilities have grown.

00:25:04.960 --> 00:25:06.600
OK, so

00:25:09.380 --> 00:25:12.220
I think the key thing about AI is not its capabilities today.

00:25:12.660 --> 00:25:15.300
So say you grant you everything you just said, just granted all that.

00:25:16.420 --> 00:25:19.780
The people making these things and finding correlations between the amount

00:25:19.780 --> 00:25:22.900
of data and compute they put into it in the orders of 100 million dollars

00:25:22.900 --> 00:25:24.340
per model right now.

00:25:24.340 --> 00:25:28.220
And I think they think the cost per model might actually go up to 500 million

00:25:28.220 --> 00:25:30.140
or a billion in two years.

00:25:30.140 --> 00:25:36.100
But they're seeing an exponential increase in the capabilities of these

00:25:36.500 --> 00:25:39.140
AIs of even just LLMs.

00:25:40.380 --> 00:25:41.900
Their growth is accelerating.

00:25:41.900 --> 00:25:43.980
This is my talking about exponential growth thing.

00:25:44.300 --> 00:25:46.180
Right. Never get exponential growth.

00:25:46.180 --> 00:25:50.220
Like we couldn't have dreamed of auto GPT in all of its horribleness

00:25:50.900 --> 00:25:52.340
two years ago.

00:25:52.340 --> 00:25:53.380
It wasn't on anyone's radar.

00:25:53.380 --> 00:25:56.700
If someone thinks it was on their radar, they're lying because then

00:25:57.900 --> 00:25:59.740
they were just nuts because nobody saw this coming.

00:25:59.740 --> 00:26:06.540
Not even opening eyes saw the promise of LLMs until it started just doing things.

00:26:07.540 --> 00:26:08.020
Right.

00:26:08.020 --> 00:26:12.880
And so I think what people are factoring, I think what you may be

00:26:13.480 --> 00:26:15.880
the difference between you and I and thinking on this topic is

00:26:15.880 --> 00:26:17.480
is how fast it's getting better.

00:26:17.480 --> 00:26:19.480
Like most of these people that want to put their head in the sand,

00:26:19.480 --> 00:26:21.920
like, you know, Alan was talking about, you're talking about these people

00:26:21.920 --> 00:26:23.920
that want to put their head in the sand or ignore it or whatever.

00:26:23.920 --> 00:26:27.920
They find things that cannot do all the time.

00:26:27.920 --> 00:26:28.440
Right.

00:26:28.440 --> 00:26:29.240
Oh, it can't do math.

00:26:29.240 --> 00:26:32.840
It can't add to 20 digit numbers or it can't do whatever.

00:26:33.240 --> 00:26:34.880
Do you know what it can do now?

00:26:34.880 --> 00:26:39.080
It can actually you can write an academic formula from a paper,

00:26:39.080 --> 00:26:41.240
put it on a whiteboard or take a picture of it.

00:26:41.240 --> 00:26:44.120
It will generate the Python and you can execute the Python.

00:26:44.120 --> 00:26:47.040
It's an implementation of that that algebraic equation,

00:26:47.040 --> 00:26:48.480
not just adding two numbers.

00:26:48.480 --> 00:26:51.920
But these things, these barriers are falling faster than people,

00:26:51.920 --> 00:26:56.000
like people, people are being proven wrong within like 20, 30 days.

00:26:56.000 --> 00:27:00.600
Sometimes even like the I saw this.

00:27:00.600 --> 00:27:08.080
I saw this same phenomenon way back in the day when Google was full on Agile

00:27:08.080 --> 00:27:11.800
and Microsoft was still doing sort of waterfall big design up front.

00:27:11.800 --> 00:27:12.200
Right.

00:27:12.200 --> 00:27:15.360
It's well, real quick, but that was a linear process.

00:27:15.360 --> 00:27:18.640
I worked on both sides of that phone.

00:27:18.640 --> 00:27:19.640
Google Google.

00:27:19.640 --> 00:27:22.500
No, I get that Google improvement.

00:27:22.500 --> 00:27:24.740
It was a I don't think it was a linear.

00:27:24.740 --> 00:27:30.740
It was a it was an exponential improvement with a lower exponent.

00:27:30.740 --> 00:27:36.740
What I do agree with you, what you're seeing right now is a massive growth curve.

00:27:36.740 --> 00:27:41.940
And the one thing like I think you want to push back on whether or not

00:27:41.940 --> 00:27:44.740
humans plus the AI.

00:27:44.740 --> 00:27:45.740
Here's the thing.

00:27:45.740 --> 00:27:46.740
Here's the thing.

00:27:46.740 --> 00:27:52.740
I think there's going to be an agreement on the humans that do pay attention

00:27:52.740 --> 00:27:57.740
and learn to work with the AI and stay abreast of it.

00:27:57.740 --> 00:27:58.260
Right.

00:27:58.260 --> 00:28:03.260
Just as you called out where you are successfully building through projects

00:28:03.260 --> 00:28:08.260
without a team and you were surprised by the fact that you didn't have a team.

00:28:08.260 --> 00:28:11.260
That's going to be increasingly the case.

00:28:11.260 --> 00:28:12.260
Yes.

00:28:12.260 --> 00:28:16.260
And I want to this up my I'm going to I'm going to this isn't going to be a tangent.

00:28:16.260 --> 00:28:19.260
It's is I'm going to build this into a segue.

00:28:19.260 --> 00:28:25.260
So AI, I think we can agree AI is going to make software development faster.

00:28:25.260 --> 00:28:30.260
We are seeing that with co-pilot all over the frickin place.

00:28:30.260 --> 00:28:31.260
Yes.

00:28:31.260 --> 00:28:32.260
And again, don't check in code.

00:28:32.260 --> 00:28:35.260
You don't understand, but it's going to make you faster.

00:28:35.260 --> 00:28:38.260
It's like remember when like I've written code in notepad.

00:28:38.260 --> 00:28:42.260
I wrote a lot of Windows code notepad, but think how great autocomplete is.

00:28:42.260 --> 00:28:46.260
But now we have like autocomplete plus plus.

00:28:46.260 --> 00:28:50.260
It's it's it's it's a whole new world a million years ago.

00:28:50.260 --> 00:28:54.320
It's talking to James Whittaker and who we all know.

00:28:54.320 --> 00:28:59.320
And we're talking about heads up displays and testing and like let's get

00:28:59.320 --> 00:29:01.320
let's what's the overlay we can get.

00:29:01.320 --> 00:29:06.320
And let's get let's what's the overlay we can use to to let people know what kind

00:29:06.320 --> 00:29:11.320
of like we talked about doing it for coverage or for you know flagging bugs

00:29:11.320 --> 00:29:17.320
and things and and then he went to another small company called Google.

00:29:17.320 --> 00:29:20.320
And I don't know what he actually did with it there,

00:29:20.320 --> 00:29:25.320
but I believe he did talk about it at least in the Google Google software book.

00:29:25.320 --> 00:29:26.320
So thank you.

00:29:26.320 --> 00:29:27.320
So James is really good.

00:29:27.320 --> 00:29:28.320
We love James.

00:29:28.320 --> 00:29:29.320
He's really good.

00:29:29.320 --> 00:29:32.320
About taking ideas and get other people to do the actual work.

00:29:32.320 --> 00:29:35.320
So didn't he make you write most of his book?

00:29:35.320 --> 00:29:37.320
I don't claim any of it.

00:29:37.320 --> 00:29:38.320
Okay. All right.

00:29:38.320 --> 00:29:42.320
So the real quick I don't know if that's the answer.

00:29:42.320 --> 00:29:45.320
I wanted to bring that up as maybe a possible answer,

00:29:45.320 --> 00:29:50.320
but what is the testers when what is it and when will we see the testers

00:29:50.320 --> 00:29:52.320
version of Co-Pilot?

00:29:52.320 --> 00:29:56.320
What is let's let's let's stream for a minute based on your experience

00:29:56.320 --> 00:29:57.320
or Jason.

00:29:57.320 --> 00:29:59.320
What is Co-Pilot for testers?

00:29:59.320 --> 00:30:04.320
The human has to be in the loop because I'm a human and that's my priority.

00:30:04.320 --> 00:30:07.320
Somehow I put myself in the loop here.

00:30:07.320 --> 00:30:08.320
No, I'm kidding.

00:30:08.320 --> 00:30:12.320
A GitHub owned by another company you might have heard of called Microsoft.

00:30:12.320 --> 00:30:15.320
They have a thing called test pilot already.

00:30:15.320 --> 00:30:17.320
It's in it's in beta like literally.

00:30:17.320 --> 00:30:18.320
Look at me.

00:30:18.320 --> 00:30:20.320
Who's asking dumb questions here.

00:30:20.320 --> 00:30:23.320
That was not a leading question, but now I want to hear more about test pilot.

00:30:23.320 --> 00:30:24.320
Tell me.

00:30:24.320 --> 00:30:27.320
Oh, well, no, it's but this is the thing that nobody can keep up.

00:30:27.320 --> 00:30:28.320
You know what I mean?

00:30:28.320 --> 00:30:32.320
Especially when you're wandering around Mount Rainier with a with a beer.

00:30:32.320 --> 00:30:35.320
I got to say I'm going to interrupt now, but I got I worry because remember

00:30:35.320 --> 00:30:40.320
when remember when Microsoft added the exploratory testing plug-in to

00:30:40.320 --> 00:30:45.320
Visual Studio and they called it XT like these are industry calls it ET.

00:30:45.320 --> 00:30:47.320
They said, Oh, we'll call it XT.

00:30:47.320 --> 00:30:50.320
So basically, you know, we're going to be doing a test.

00:30:50.320 --> 00:30:51.320
And they call it XT.

00:30:51.320 --> 00:30:56.320
So based on that experience, I'm not confident in test pilot,

00:30:56.320 --> 00:30:57.320
but I do want to hear more.

00:30:57.320 --> 00:30:58.380
And that's okay.

00:30:58.380 --> 00:31:01.380
Like again, even in the more medicines, I'm not trying to convince anybody.

00:31:01.380 --> 00:31:02.380
I'm good.

00:31:02.380 --> 00:31:03.380
I'm good with it to be clear.

00:31:03.380 --> 00:31:07.380
Like if people don't want to think about exponential growth in these

00:31:07.380 --> 00:31:08.380
things.

00:31:08.380 --> 00:31:09.380
So the test pilot does it just it's right.

00:31:09.380 --> 00:31:12.380
It's test cases for you and there's a web page that demos it like,

00:31:12.380 --> 00:31:14.380
but it'll get better.

00:31:14.380 --> 00:31:15.380
That's the thing.

00:31:15.380 --> 00:31:16.380
I think people don't realize about AI.

00:31:16.380 --> 00:31:19.980
Like it will get not just better, but far better.

00:31:20.220 --> 00:31:22.700
Like in, in, in, in, in very quickly.

00:31:22.740 --> 00:31:23.180
Yeah.

00:31:23.180 --> 00:31:25.140
And let's, let's, let's bring disturb a little bit.

00:31:25.140 --> 00:31:29.980
Cause I think generating test cases, I suppose that's interesting if you care

00:31:29.980 --> 00:31:35.780
about a bunch of test cases, but I wanted to generate, like I want it to prioritize

00:31:35.780 --> 00:31:38.060
it, like I don't want to know all of them.

00:31:38.060 --> 00:31:40.340
I want to know, okay, so do these, you're good.

00:31:40.380 --> 00:31:42.020
Guess who prioritizes all that stuff?

00:31:42.740 --> 00:31:45.180
The human human has to be a human, you know, right?

00:31:45.620 --> 00:31:45.940
Right.

00:31:46.260 --> 00:31:50.700
Doesn't have to be, I was arguing with Wayne, Wayne, the, you know, Wayne.

00:31:50.940 --> 00:31:51.140
Yeah.

00:31:51.140 --> 00:31:52.180
But we call him Wayne.

00:31:52.300 --> 00:31:57.700
You saw him on the love Wayne cause Wayne again, he's super freaking smart and

00:31:58.260 --> 00:31:59.980
bet, and he is not afraid to challenge.

00:31:59.980 --> 00:32:00.980
We just hear it.

00:32:00.980 --> 00:32:03.980
He lives in like the past, the present and the future.

00:32:04.220 --> 00:32:07.380
And they all can say, um, yeah, exactly.

00:32:08.100 --> 00:32:08.580
Exactly.

00:32:08.940 --> 00:32:12.300
Well, when I long term the mystery about, about like, um, because one of my

00:32:12.300 --> 00:32:14.340
screenshots, I shared something where there was a,

00:32:20.940 --> 00:32:22.860
I'm willing to pay $18 a month.

00:32:22.900 --> 00:32:27.660
Cause that's the price of Zencaster, by the way, for a podcast recording platform.

00:32:27.660 --> 00:32:31.700
Let me record separate audio Tyrax and it actually works every time.

00:32:31.980 --> 00:32:32.900
I don't ask for a lot.

00:32:33.260 --> 00:32:34.020
All right, everybody.

00:32:34.020 --> 00:32:36.500
This has been the AB testing podcast episode 188.

00:32:36.500 --> 00:32:37.540
I am Alan.

00:32:37.940 --> 00:32:38.620
I'm Brit.

