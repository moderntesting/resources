WEBVTT

00:00:00.000 --> 00:00:02.560
Can we just do no, we can't.

00:00:02.800 --> 00:00:05.120
But couldn't we just know we can't.

00:00:05.120 --> 00:00:06.480
Oh, my God. Why do you?

00:00:06.480 --> 00:00:09.320
Why did you just set up a daily one on one with me?

00:00:09.320 --> 00:00:11.800
Oh, my God. No, stop. Stop.

00:00:15.490 --> 00:00:16.610
Welcome to A.B.

00:00:16.610 --> 00:00:19.650
Testing podcast, your modern testing podcast.

00:00:19.970 --> 00:00:24.530
Your hosts, Alan and Brent will be here to guide you through topics

00:00:24.530 --> 00:00:28.650
on testing, leadership, agile and anything else that comes to mind.

00:00:28.850 --> 00:00:30.450
Now on with the show.

00:00:30.450 --> 00:00:34.330
I worry that we've already had all of the interesting discussions

00:00:34.330 --> 00:00:35.330
in the afternoon.

00:00:35.330 --> 00:00:37.690
We have if we're going to get out.

00:00:38.130 --> 00:00:41.490
Yeah, it's it wasn't that I forgot to press record.

00:00:41.570 --> 00:00:43.850
We were just talking and it was interesting.

00:00:43.850 --> 00:00:48.010
And then I thought, let's just press record and continue the conversation

00:00:48.010 --> 00:00:51.250
in the middle, because I don't know what I said five minutes ago.

00:00:51.250 --> 00:00:53.730
As anybody who works with me will remind you.

00:00:53.970 --> 00:00:56.210
I'm Alan and he's Brent.

00:00:56.210 --> 00:00:59.690
And together we make up the Alan Brent testing podcast,

00:00:59.770 --> 00:01:04.010
where we rarely, if ever, talk about testing, but we do talk about stuff.

00:01:04.800 --> 00:01:08.760
Yeah, we do talk about Alan and Brent quite often.

00:01:09.040 --> 00:01:13.680
Yes. Mostly Alan talking about Alan and Brent talking about Brent,

00:01:13.680 --> 00:01:17.000
because it's our podcast and we're the centers of attention.

00:01:18.480 --> 00:01:20.200
I was going to go a different angle.

00:01:20.200 --> 00:01:24.480
It's more normally Alan bitching about Brent and vice versa.

00:01:24.640 --> 00:01:26.840
Oh, I can fire you.

00:01:27.240 --> 00:01:29.520
A B could be Alan bitching.

00:01:29.520 --> 00:01:31.490
Bye bye. All right.

00:01:31.490 --> 00:01:32.290
Talk to you.

00:01:32.730 --> 00:01:34.010
Brent has hung up.

00:01:34.010 --> 00:01:37.450
I'm leaving.

00:01:38.890 --> 00:01:41.170
I mean, boy, oh boy.

00:01:41.170 --> 00:01:44.650
One of my favorite things to do when I'm just done with a telephone

00:01:44.650 --> 00:01:46.850
conversation, it reminds me of it.

00:01:46.850 --> 00:01:49.450
I will go, oh, hey, Alan.

00:01:50.490 --> 00:01:52.090
Do you know what I have?

00:01:52.090 --> 00:01:53.770
I have a pop quiz for you.

00:01:53.770 --> 00:01:56.210
And if I do it right, you'll go, what?

00:01:57.020 --> 00:02:00.900
And I'm like, can you guess what is this sound?

00:02:01.180 --> 00:02:03.640
And then I just hang up.

00:02:03.640 --> 00:02:04.760
Sound of silence.

00:02:04.760 --> 00:02:06.360
Did we talk about the last of us already?

00:02:06.360 --> 00:02:08.560
You've been watching that? No.

00:02:08.560 --> 00:02:10.200
Oh, it's good. You got to watch it.

00:02:10.200 --> 00:02:14.000
Yeah, I am up to date on Doom Patrol.

00:02:14.000 --> 00:02:15.320
I am up to date.

00:02:15.320 --> 00:02:17.120
I mean, and none of this really matters.

00:02:17.120 --> 00:02:19.880
No, it doesn't. Anyway, I got to I got to get back on Doom Patrol.

00:02:19.880 --> 00:02:22.240
I watched a bunch of that when I was stranded in Buffalo.

00:02:22.640 --> 00:02:23.600
But I have to get back on.

00:02:23.600 --> 00:02:26.440
And then I got distracted by White Lotus and and it was better.

00:02:26.600 --> 00:02:28.080
But I want to get back on that.

00:02:28.080 --> 00:02:30.080
What is last of us on with streaming?

00:02:30.080 --> 00:02:31.640
HBO, HBO.

00:02:31.640 --> 00:02:33.360
OK, maybe I'll check that.

00:02:33.360 --> 00:02:34.160
Yeah, I have that one.

00:02:34.160 --> 00:02:37.160
And I think I maybe didn't say it's on the podcast before.

00:02:37.440 --> 00:02:41.280
But I normally do not like well, because they're not very good.

00:02:41.760 --> 00:02:47.190
Usually the movies or series that are modeled up your video games

00:02:47.190 --> 00:02:49.460
are just not good.

00:02:49.460 --> 00:02:51.260
Yeah, I don't have to list them all off.

00:02:51.260 --> 00:02:52.980
But I watched it anyway.

00:02:52.980 --> 00:02:55.820
And I was surprised. It's very good. Very, very good.

00:02:56.420 --> 00:03:00.300
I am hopeful that the movie that they're going to do for fallout

00:03:00.300 --> 00:03:03.770
will not follow that path.

00:03:04.610 --> 00:03:06.330
Oh, that's right. We did talk about this a little bit.

00:03:06.330 --> 00:03:08.810
OK, let's we are forgetful old men.

00:03:09.210 --> 00:03:12.930
I don't know how old I am, but it's old. It's super old.

00:03:13.710 --> 00:03:17.430
All right. I want to talk about I mentioned it on the three of my five for Friday.

00:03:17.670 --> 00:03:21.190
I mentioned this invention

00:03:21.930 --> 00:03:23.890
and we've talked about it before on the podcast.

00:03:23.890 --> 00:03:26.170
Everyone drink. I have coffee. I'm not brand has.

00:03:26.770 --> 00:03:28.010
I have a coke.

00:03:28.010 --> 00:03:31.370
Let's talk about again, yet again,

00:03:32.330 --> 00:03:35.250
because it never goes away and it's getting more interesting.

00:03:35.250 --> 00:03:39.090
And actually, it takes a lot to make Microsoft interesting.

00:03:39.090 --> 00:03:41.890
Satya has fixed a lot of things wrong with Microsoft.

00:03:42.330 --> 00:03:44.130
Oh, by the way, glad to see you're still employed.

00:03:44.130 --> 00:03:46.530
I see another round launch today.

00:03:46.530 --> 00:03:48.850
Yesterday. Oh, my God.

00:03:49.370 --> 00:03:52.170
He's done some stuff. He has really made Microsoft interesting.

00:03:52.530 --> 00:03:55.250
Microsoft caters to corporations.

00:03:55.250 --> 00:03:59.610
People buying tens of thousands of licenses of office and windows.

00:03:59.930 --> 00:04:02.690
And although there are consumers use that stuff, it's for corporations.

00:04:02.690 --> 00:04:06.810
And it's boring. But Microsoft is getting less boring.

00:04:06.810 --> 00:04:11.370
You know why? And in my eyes, as a non Microsofty, former Microsofty,

00:04:11.370 --> 00:04:15.010
it's getting more interesting because I'm seeing our old friend chat

00:04:15.010 --> 00:04:19.410
GPT pop up all over the place and press releases from Microsoft.

00:04:19.410 --> 00:04:21.880
We know about the Bing one.

00:04:21.880 --> 00:04:27.200
And we've also seen chat GPT integrated with Microsoft Teams.

00:04:27.720 --> 00:04:29.040
And I'm going to ask you a question.

00:04:29.040 --> 00:04:31.520
I know you want to ask us to talk about chat GPT,

00:04:31.520 --> 00:04:34.720
but I think it'll generate some interest.

00:04:34.720 --> 00:04:38.600
So Microsoft invests heavily in chat GPT, right?

00:04:39.280 --> 00:04:41.680
But I know that that part you can answer.

00:04:41.680 --> 00:04:47.120
So to be clear, there will be no accidental release of Microsoft

00:04:47.120 --> 00:04:50.240
IP today. OK, no, no, no.

00:04:50.360 --> 00:04:55.400
But I do have a request and I'm almost afraid to ask because if it's

00:04:55.840 --> 00:04:59.480
if it's something that you can't answer either, then I'll tell you,

00:04:59.480 --> 00:05:01.040
I can't answer. All right.

00:05:01.040 --> 00:05:02.440
But here's my suggestion for you.

00:05:02.440 --> 00:05:05.040
Here's my here's my top notch suggestion for you.

00:05:05.640 --> 00:05:11.120
Can I just get myself rather than go through chat GPT?

00:05:12.290 --> 00:05:17.170
Can I just get my own Azure VM or Azure cluster

00:05:17.570 --> 00:05:22.450
with GPT through technology so I can just run my own language processing service

00:05:23.010 --> 00:05:25.530
so you can without GPT?

00:05:25.570 --> 00:05:29.970
No, well, with GPT, like what I'm saying is here's my tip is

00:05:30.290 --> 00:05:35.610
build in GPT through technology into pre build it into some Azure VM

00:05:35.610 --> 00:05:39.130
so I can just huck training material at something that I check out

00:05:39.130 --> 00:05:41.490
in his mind and I pay you for and everybody's happy.

00:05:41.490 --> 00:05:45.450
Well, so that's that is officially.

00:05:46.130 --> 00:05:48.010
Well, I guess I don't understand the distinction.

00:05:48.010 --> 00:05:51.450
So maybe I should double check if this is public.

00:05:51.730 --> 00:05:55.370
So maybe maybe maybe I'm just going to say that because it's something

00:05:55.370 --> 00:06:00.450
that should be available. I think it's it's Microsoft will absolutely

00:06:00.890 --> 00:06:07.490
enable or directly enable a scenario similar to what you describe.

00:06:07.570 --> 00:06:12.170
OK, so really all I'm saying is is Microsoft is all of a sudden

00:06:12.170 --> 00:06:17.250
in an interesting place in making by making this technology accessible,

00:06:17.250 --> 00:06:23.530
it makes some of its old stodgy business, the applications and and Bing,

00:06:23.530 --> 00:06:28.650
which I will soon probably have to I will have to stop reminding people

00:06:28.650 --> 00:06:31.930
that being as a search engine like Google and that you can use it

00:06:31.930 --> 00:06:34.370
to Google stuff because I think it's going to become relevant.

00:06:34.370 --> 00:06:37.210
I think a lot of interesting things are happening.

00:06:37.450 --> 00:06:38.690
And then you have stuff you want to say.

00:06:38.690 --> 00:06:42.450
But the other thing for my side is I posted an article today

00:06:42.450 --> 00:06:46.290
showing how chat GPT was helping people.

00:06:46.290 --> 00:06:49.410
And I like that. I think a lot of the photo, a lot of the comments

00:06:49.410 --> 00:06:54.130
I see on social media and articles are people showing, oh, look,

00:06:54.130 --> 00:06:57.570
chat GPT sucks for this. It sucks for this because they're using it

00:06:57.570 --> 00:07:00.010
in things it's not designed for.

00:07:00.010 --> 00:07:03.530
Or someone pointed out you can gaslight chat GPT

00:07:04.480 --> 00:07:06.920
as you can with any AI.

00:07:06.920 --> 00:07:10.320
They're trying to rather than trying to use it for good.

00:07:10.320 --> 00:07:12.720
They're trying to find ways that it won't work, which are endless.

00:07:12.720 --> 00:07:14.560
It's endless. It's infallible.

00:07:14.560 --> 00:07:19.840
I think it is not Skynet, nor does it claim to be.

00:07:20.160 --> 00:07:24.600
I would say it's probably closer to a very advanced version of that thing

00:07:24.600 --> 00:07:28.000
in your text app as Brent is texting and in Gmail

00:07:28.000 --> 00:07:30.200
that tries to predict what your next word is going to be.

00:07:30.440 --> 00:07:34.160
No, I'm trying to quickly scan the article that you sent out.

00:07:34.160 --> 00:07:36.680
Oh, just why do you look for it?

00:07:36.680 --> 00:07:39.920
I'll talk about how I have ended up using chat GPT.

00:07:40.200 --> 00:07:42.420
I use it for brainstorming.

00:07:42.420 --> 00:07:44.500
I know that's interesting. It's interesting.

00:07:44.500 --> 00:07:47.220
I use it as a creative tool. Walk me through it.

00:07:47.260 --> 00:07:48.860
I use it to help me figure out.

00:07:48.860 --> 00:07:52.300
Well, walk me through a scenario if you can. Sure.

00:07:52.300 --> 00:07:54.060
Let me just look at my history real quick.

00:07:54.060 --> 00:07:58.140
Well, I tried to figure I didn't help me directly.

00:07:58.140 --> 00:08:00.340
Well, one, I was just trying to think of a new name for my org.

00:08:01.470 --> 00:08:03.230
I described what it did. It gave me some ideas.

00:08:03.230 --> 00:08:04.470
I said, those are horrible.

00:08:04.470 --> 00:08:05.670
How about something more like this?

00:08:05.670 --> 00:08:09.470
We kind of go back and forth as a can you make sure you use this word,

00:08:09.470 --> 00:08:13.430
but don't use any words like this as kind of a brainstorming partner.

00:08:14.590 --> 00:08:21.070
I use it for I use it for summarizing articles for for having

00:08:21.070 --> 00:08:23.030
having to explain complex things to me.

00:08:23.790 --> 00:08:26.430
There's a medical article I read once I had it explained.

00:08:26.670 --> 00:08:28.710
Oh, OK, I get it. I understand it now.

00:08:28.710 --> 00:08:33.540
Well, of course, of course, on that one, there is a risk.

00:08:34.140 --> 00:08:38.380
You're aware that there's a risk when it summarizes things that you don't understand.

00:08:38.900 --> 00:08:40.860
It could it could summarize wrong.

00:08:40.860 --> 00:08:45.630
Right. Of course, it could it could present itself as truth,

00:08:46.310 --> 00:08:48.590
something that is false, and then you walk away.

00:08:49.070 --> 00:08:52.030
Yeah. And usually what I do, because the way I work,

00:08:52.070 --> 00:08:55.870
I work around that is I don't I paste in a chunk of an article

00:08:56.720 --> 00:08:59.560
and the odds of it giving me something and then maybe another chunk later

00:08:59.560 --> 00:09:02.080
and the odds of it giving me the wrong thing twice or two.

00:09:02.440 --> 00:09:05.000
If those things don't jive, I have to go look up more.

00:09:05.000 --> 00:09:08.520
But again, I do know to take it with a grain of salt because it's not

00:09:09.380 --> 00:09:12.060
it's it's it's not a language processing engine.

00:09:12.060 --> 00:09:14.980
It's not it's not self-aware.

00:09:15.500 --> 00:09:18.100
It is not even a language processing engine.

00:09:18.140 --> 00:09:19.740
Yeah, you would know you would know the right words.

00:09:19.740 --> 00:09:22.820
But but yeah, I use it for OK.

00:09:22.820 --> 00:09:25.260
I'm going to cut this one out because I don't want the secret to get out.

00:09:25.380 --> 00:09:30.260
So somebody will figure it out someday.

00:09:30.580 --> 00:09:34.740
So anyway, I use it. You have a you have a I mean, you have a clear hint.

00:09:35.300 --> 00:09:36.020
All right.

00:09:36.020 --> 00:09:38.300
I might leave little bits of that in there, but I'm just

00:09:38.580 --> 00:09:40.380
because somebody, if they thought about it, would figure it out.

00:09:40.380 --> 00:09:43.060
They just they ask chat GPT what these names have in common.

00:09:43.460 --> 00:09:45.420
Breadcrumbs. All right.

00:09:45.420 --> 00:09:49.820
So but you and I want to mark it today is

00:09:50.020 --> 00:09:55.060
we're recording this on Friday, February 10th, episode 175, A.B.

00:09:55.060 --> 00:09:59.220
testing, and this is the first time in three, four, maybe five years.

00:09:59.220 --> 00:10:03.580
Brent has topics for discussion.

00:10:04.580 --> 00:10:07.620
And I probably stole some of this thunder because he wanted to talk about

00:10:07.620 --> 00:10:09.980
chat GPT and I'm going to join in the conversation.

00:10:09.980 --> 00:10:14.860
But I think he's going to lay down some some insights.

00:10:15.860 --> 00:10:18.660
Is that true or or or did I read the wrong agenda?

00:10:18.900 --> 00:10:22.700
I don't know if it's going to be insights like I am.

00:10:22.940 --> 00:10:28.460
Part of my brain is is trying to convince another part of my brain

00:10:28.860 --> 00:10:34.060
that I am just in conspiracy theory mode, and therefore I should ignore that.

00:10:34.460 --> 00:10:37.380
The part of my brain that's generating all of these conclusions.

00:10:37.580 --> 00:10:41.940
Well, I want to pause and remind people that who hopefully this isn't

00:10:41.940 --> 00:10:45.340
their first episode, but back in our, you know, well, I don't know

00:10:45.340 --> 00:10:50.060
what number it was, 170 something probably our prediction episode.

00:10:50.420 --> 00:10:54.580
Really, a month and a half ago, Brent predicted that chat GPT

00:10:54.580 --> 00:10:57.620
was going to be a multi billion dollar business and grow super fast.

00:10:57.620 --> 00:10:59.820
And here it is already.

00:11:00.670 --> 00:11:04.070
So I think anything you want to say about chat GPT

00:11:04.070 --> 00:11:05.470
probably has a good chance of happening.

00:11:05.470 --> 00:11:07.630
So how fucked are we?

00:11:08.990 --> 00:11:15.860
We're doomed the but before I go into that story, I don't know if we are,

00:11:15.860 --> 00:11:18.580
but I absolutely believe our grandchildren are.

00:11:18.980 --> 00:11:24.100
And in terms of short term stuff like you remember when we were at micro

00:11:24.100 --> 00:11:30.060
when when you were at Microsoft's do or as well, and we knew way in advance

00:11:30.580 --> 00:11:32.940
that test was disappearing.

00:11:33.500 --> 00:11:35.380
Yes. OK.

00:11:35.380 --> 00:11:38.020
And I don't know if you had these arguments, but I did.

00:11:38.420 --> 00:11:41.140
Right. People saying, what are you talking about?

00:11:41.140 --> 00:11:43.820
You're so full of crap and blah, blah, blah, blah, blah, blah, blah, blah.

00:11:44.500 --> 00:11:50.780
The way in in particular today, because I got even more information on on this topic

00:11:51.980 --> 00:11:56.140
because I run a data science team that specializes in NLP.

00:11:56.460 --> 00:12:00.460
And it has a lot of it has a high reputation

00:12:00.460 --> 00:12:02.460
and a lot of people throughout Microsoft know it.

00:12:03.360 --> 00:12:08.800
So I keep everybody in their mother is coming to me around chat GPT.

00:12:09.000 --> 00:12:14.320
OK, and I will say if another goddamn PM comes to me

00:12:14.800 --> 00:12:19.360
talking about chat GPT and cannot distinguish the difference between magic

00:12:19.360 --> 00:12:22.880
and data science, I'm going to explode.

00:12:24.360 --> 00:12:29.080
Any sufficiently advanced technology is indistinguishable from magic.

00:12:29.080 --> 00:12:30.960
We know this. Yeah.

00:12:31.520 --> 00:12:34.120
But they're like, well, can we go like you helps also?

00:12:34.960 --> 00:12:37.120
Can we just do? No, we can't.

00:12:37.440 --> 00:12:39.760
Well, well, couldn't we just no, we can't.

00:12:39.760 --> 00:12:43.960
Oh, my God. Why do you why did you just set up a daily one on one with me?

00:12:43.960 --> 00:12:46.760
Oh, my God. No, stop, stop.

00:12:46.760 --> 00:12:50.000
That's kind of my life the last two weeks.

00:12:50.600 --> 00:12:51.760
So what do they what?

00:12:51.760 --> 00:12:52.920
Give me an example.

00:12:52.920 --> 00:12:54.120
What do they want to do?

00:12:54.120 --> 00:12:59.040
What is their what is the odd Brent them chat GPT threesome they want to set up?

00:12:59.740 --> 00:13:05.620
Oh, in this particular case, they they want to use.

00:13:05.940 --> 00:13:10.300
I'll just say, can't we just use chat GPT to just eliminate our entire

00:13:10.300 --> 00:13:12.860
support organization, right? Things like that.

00:13:13.580 --> 00:13:14.980
I'm using hyperbole.

00:13:14.980 --> 00:13:16.900
That doesn't even fall into.

00:13:16.900 --> 00:13:20.300
Well, actually, I mean, I was going to say the answer to any sufficiently

00:13:20.300 --> 00:13:22.260
complex question is it depends.

00:13:23.420 --> 00:13:25.820
And there I would basically say, you know what?

00:13:26.100 --> 00:13:28.980
Bing team is probably already ahead of us on that one.

00:13:28.980 --> 00:13:32.300
We probably don't need to do anything because customers,

00:13:32.300 --> 00:13:34.620
when that's once they realize that

00:13:35.180 --> 00:13:40.540
and Bing is very focused on these data science, particularly on the issues,

00:13:41.180 --> 00:13:45.740
they're going to get they're going to get their well ahead of

00:13:46.340 --> 00:13:51.700
certainly my freaking team of 10 people of which for focus on NLP.

00:13:52.460 --> 00:13:54.820
Why are you talking to me anyway?

00:13:54.820 --> 00:13:56.620
I got I got to ask, because is this

00:13:57.220 --> 00:13:59.900
is this testers going away all over again in a way?

00:13:59.900 --> 00:14:02.340
I mean, not this is what it feels like to me.

00:14:02.340 --> 00:14:07.220
My my emotion right now around how I feel

00:14:07.860 --> 00:14:09.940
about a prediction I'm going to.

00:14:09.940 --> 00:14:11.380
Well, I've already made.

00:14:11.380 --> 00:14:16.700
I feel it as strong as you and I did then.

00:14:17.340 --> 00:14:19.340
And it's not because I'm predicting.

00:14:19.340 --> 00:14:22.740
It's because I see the direction the momentum is going.

00:14:23.500 --> 00:14:25.740
OK, you follow. You follow.

00:14:25.740 --> 00:14:28.140
I have I have a thread in my head.

00:14:28.140 --> 00:14:30.140
I want to have been all you. OK.

00:14:30.140 --> 00:14:32.460
So today I woke up in a bad mood.

00:14:33.100 --> 00:14:34.900
I wake up in a bad mood every day these days.

00:14:34.900 --> 00:14:37.660
And it was it was fully thinking of this because I'm very

00:14:38.020 --> 00:14:40.460
I feel very responsible to my team.

00:14:40.500 --> 00:14:42.860
And as you did when you were a test manager.

00:14:43.140 --> 00:14:45.980
Correct. Before you went and joined Bing as a dev manager.

00:14:46.660 --> 00:14:48.660
And what did I do?

00:14:48.660 --> 00:14:51.060
I went to go and learn.

00:14:51.060 --> 00:14:54.380
I had a very clear sense of what's coming next.

00:14:54.700 --> 00:14:57.060
I went to go learn it so I could broadcast it.

00:14:57.620 --> 00:15:01.740
OK. And and help those who don't

00:15:02.180 --> 00:15:05.780
who who weren't believers, who didn't understand, be prepared.

00:15:05.780 --> 00:15:11.940
Right. That whole process led to ultimately things like modern testing principles.

00:15:12.140 --> 00:15:17.940
Right. It's OK. Over the years, we see more and more evidence of this and et cetera.

00:15:18.380 --> 00:15:22.260
Right now, though, I'm at a point.

00:15:22.820 --> 00:15:24.380
I don't know.

00:15:24.380 --> 00:15:27.660
I run a team with data scientists and NLP.

00:15:27.660 --> 00:15:31.580
And I'm not certain what direction to invest them in.

00:15:31.580 --> 00:15:33.420
Where they should train.

00:15:33.420 --> 00:15:38.830
OK. And and it certainly doesn't help

00:15:39.030 --> 00:15:43.350
where when we're in an environment where we've been told, yeah,

00:15:43.550 --> 00:15:46.910
layoffs going to keep coming until March 31st. Right.

00:15:48.110 --> 00:15:51.830
But there's that's a shorter term problem that will get resolved.

00:15:51.830 --> 00:15:55.230
I'm pretty certain that like just before our podcast,

00:15:55.230 --> 00:15:59.350
I was having a similar conversation with one of my key partners on this.

00:16:00.230 --> 00:16:04.630
And I basically told him, look, I'm seriously thinking about getting out of NLP.

00:16:05.590 --> 00:16:07.350
And he's like, what?

00:16:07.350 --> 00:16:08.950
Oh, my God, Britt, I need you.

00:16:08.950 --> 00:16:09.830
What's going to happen?

00:16:09.830 --> 00:16:10.990
Bubba, right?

00:16:10.990 --> 00:16:13.630
So things will converge and we'll work out the business thing.

00:16:13.910 --> 00:16:16.510
What I wanted to talk about today.

00:16:16.710 --> 00:16:20.630
And hopefully I don't take too long to do it because I'm hoping we can have

00:16:20.630 --> 00:16:24.430
a discussion around what are options to deal with it.

00:16:25.200 --> 00:16:27.880
I mentioned on on our Slack channel

00:16:27.960 --> 00:16:31.560
that there was a new invention that came out.

00:16:31.920 --> 00:16:34.400
Apple glasses. Are you familiar with Apple glasses?

00:16:34.400 --> 00:16:36.920
Have you done any research into this thing?

00:16:37.200 --> 00:16:39.200
I wouldn't say I've done research, no.

00:16:39.600 --> 00:16:41.400
OK, there is a feature.

00:16:41.680 --> 00:16:45.960
You go look at the marketing thing and there is a feature that's being discussed.

00:16:46.280 --> 00:16:51.130
OK, and it says there is a camera

00:16:51.770 --> 00:16:56.060
on each lens to track your eyes

00:16:56.660 --> 00:17:02.140
so that the glasses can be more confident in what the user is looking at.

00:17:02.660 --> 00:17:05.660
How do you feel about how do you feel about that sentence?

00:17:06.420 --> 00:17:08.660
Well, doesn't bug you yet.

00:17:09.380 --> 00:17:11.340
It bugs me a little.

00:17:11.340 --> 00:17:14.420
I just remember to take my glasses off when I don't want it to know where I'm looking.

00:17:15.020 --> 00:17:17.650
OK, what if I remind you

00:17:18.450 --> 00:17:22.450
that roughly 80 percent of your eye movements are subconscious?

00:17:23.430 --> 00:17:25.670
Yeah, I would never wear them.

00:17:26.510 --> 00:17:31.600
OK, the only thing about Apple glasses here

00:17:32.120 --> 00:17:35.360
that I can come up with as a positive

00:17:36.220 --> 00:17:41.380
is that they are not their competitor Facebook, who's also building a similar thing.

00:17:41.780 --> 00:17:45.380
Yeah, Apple has has has shown very publicly

00:17:45.380 --> 00:17:49.860
a very willingness to tell the government to F off if they ask for private data.

00:17:49.940 --> 00:17:53.500
But I will tell you the wares of these glasses,

00:17:54.660 --> 00:17:57.980
that company will have the data.

00:17:58.820 --> 00:18:02.660
They will have the data to know everything about you,

00:18:03.420 --> 00:18:07.500
including things that you don't know in about a month.

00:18:08.380 --> 00:18:10.660
All of it. They will know.

00:18:11.780 --> 00:18:13.660
Hey, are you gay?

00:18:13.660 --> 00:18:18.100
They will know what religion you're in or believe in.

00:18:18.540 --> 00:18:21.260
They will know the perfect

00:18:21.980 --> 00:18:25.660
the the image of the perfect mate for you.

00:18:25.700 --> 00:18:30.220
They will know everything or that data will contain

00:18:30.700 --> 00:18:33.540
all they need to know to know everything about you.

00:18:34.470 --> 00:18:38.500
Now, there is one thing about human behavior that I

00:18:39.140 --> 00:18:42.160
I have seen over and over again.

00:18:42.160 --> 00:18:46.080
And it's this old phrase, the road to hell is paid with good intentions.

00:18:46.560 --> 00:18:49.840
And so now let's imagine Apple glasses

00:18:50.480 --> 00:18:53.480
and someone comes up with the idea of, hey,

00:18:54.400 --> 00:18:59.200
since it is so effective at knowing what they're looking at,

00:18:59.520 --> 00:19:01.520
could we not put them on infants?

00:19:01.520 --> 00:19:05.520
Could we not make like Apple infant goggles or something

00:19:06.000 --> 00:19:10.800
where where we can start getting additional insight into

00:19:10.960 --> 00:19:12.720
what their thought process is?

00:19:12.720 --> 00:19:14.120
They can't communicate yet.

00:19:14.120 --> 00:19:15.920
So that's harder. This could help.

00:19:15.920 --> 00:19:17.160
Oh, and even better.

00:19:17.160 --> 00:19:20.800
Could we use that to sort of understand the difference between babies

00:19:20.800 --> 00:19:25.240
with autism and normal babies?

00:19:25.960 --> 00:19:29.560
Right. This would then start to build a process.

00:19:29.880 --> 00:19:32.920
By the way, just just to interrupt the the

00:19:33.520 --> 00:19:36.480
for those listening air quotes around normal.

00:19:36.720 --> 00:19:38.720
Brent is not saying anyway.

00:19:38.720 --> 00:19:42.160
Just just want to clarify that for for that statement. Yeah.

00:19:43.000 --> 00:19:44.520
No, actually.

00:19:44.520 --> 00:19:46.480
So I understand what you're saying there.

00:19:46.480 --> 00:19:50.440
Okay. But that pushback is exactly what I'm afraid of.

00:19:51.200 --> 00:19:53.660
Who gets to define normal?

00:19:53.660 --> 00:19:55.300
Right. Right.

00:19:55.300 --> 00:19:57.300
No, we're on the we're on the same pager. Right.

00:19:57.300 --> 00:19:58.900
Because yeah, I don't.

00:19:58.980 --> 00:20:00.900
That's why the air quotes are there because you don't.

00:20:01.540 --> 00:20:03.460
This is very possible.

00:20:05.260 --> 00:20:07.860
I would say almost inevitable.

00:20:08.620 --> 00:20:10.500
Oh, it's absolutely inevitable.

00:20:10.500 --> 00:20:11.900
This is where I'm going.

00:20:11.900 --> 00:20:15.740
But even if you don't wear glasses, there are going to be ways to.

00:20:16.460 --> 00:20:19.380
But we've seen glimpses of the future in films

00:20:19.980 --> 00:20:24.380
where the ads you see, not just in your browser,

00:20:24.380 --> 00:20:28.500
but the ads you see walking around are tailored to you.

00:20:28.900 --> 00:20:33.380
Your world tailors around because there is no so much information about you.

00:20:33.860 --> 00:20:37.260
Right. If some today, like 20 years ago,

00:20:38.260 --> 00:20:42.180
if someone were to ask you some fact about something,

00:20:42.180 --> 00:20:45.260
you know, how deep is the deepest part of the ocean?

00:20:45.820 --> 00:20:49.580
Maybe you have a guess, but you you wouldn't know and you'd be fine with it.

00:20:49.620 --> 00:20:51.780
You're OK to estimate it today.

00:20:52.060 --> 00:20:54.540
We look up those things immediately on our phones,

00:20:54.540 --> 00:20:56.820
on Wikipedia or the Internet somewhere.

00:20:56.820 --> 00:20:59.540
But that's a lot of wasted effort to get that phone out

00:20:59.540 --> 00:21:02.620
and either type it in or voice recognition that query.

00:21:03.100 --> 00:21:08.600
Mm hmm. Why don't our why doesn't something connected to our bodies?

00:21:08.600 --> 00:21:10.760
And eventually maybe that we're going way off the deep end here.

00:21:10.760 --> 00:21:13.320
But why maybe even something implanted?

00:21:14.250 --> 00:21:15.210
Recognize that.

00:21:15.210 --> 00:21:18.170
And then with a thought or an action,

00:21:18.170 --> 00:21:21.490
I can hear a conversation and ask for more clarification.

00:21:22.090 --> 00:21:24.090
Like this is to keep it to glasses for now.

00:21:24.090 --> 00:21:26.330
My glasses obviously have microphones, too.

00:21:26.850 --> 00:21:29.690
And someone asked me a question and I can say, I don't know.

00:21:29.690 --> 00:21:31.610
And I give a special blink blink.

00:21:31.610 --> 00:21:36.410
And I get a little little thing in my ear telling me the answer to this thing.

00:21:36.810 --> 00:21:39.970
Oh, my God. Schools are irrelevant because I can cheat so easily.

00:21:40.170 --> 00:21:42.090
Everybody has to take the glasses off for their test.

00:21:42.090 --> 00:21:43.090
Oh, my God.

00:21:43.090 --> 00:21:44.450
Yeah, right.

00:21:45.490 --> 00:21:48.250
So I when I was in high school,

00:21:48.890 --> 00:21:52.090
I guess it was a period where there was a transition where some

00:21:52.450 --> 00:21:55.850
some math teachers allowed calculator, some didn't.

00:21:56.290 --> 00:21:57.290
Now it's common.

00:21:57.290 --> 00:22:00.090
Every every class has a high school. Right.

00:22:00.450 --> 00:22:04.090
Going back to the baby example, the other thing that that would do

00:22:04.690 --> 00:22:06.690
is it then normalizes the glasses.

00:22:07.410 --> 00:22:09.530
These are new human beings that don't.

00:22:10.210 --> 00:22:12.650
Yeah. They don't know a different world.

00:22:12.650 --> 00:22:14.010
Yeah. It may be glass.

00:22:14.010 --> 00:22:15.010
It'd be something else.

00:22:15.010 --> 00:22:16.770
I think there are.

00:22:16.770 --> 00:22:20.450
I remember this from I used to show this video was made by Microsoft

00:22:20.450 --> 00:22:23.490
when I used to give a talk at our new employee orientation.

00:22:24.090 --> 00:22:27.090
And it was a view into the future of health care

00:22:27.930 --> 00:22:33.290
and how through a wristwatch or a smart ring or some bit of jewelry,

00:22:33.850 --> 00:22:37.290
you could get a lot more constant feedback on your health,

00:22:37.930 --> 00:22:40.130
which I think, you know, there's some good benefit there, too.

00:22:40.130 --> 00:22:44.350
But just the same case, these glasses, there is potential for

00:22:44.990 --> 00:22:47.190
all of this data to be used in different ways.

00:22:47.590 --> 00:22:48.750
Raise your insurance.

00:22:48.750 --> 00:22:51.110
Understand when you're drinking too much.

00:22:51.110 --> 00:22:54.350
Understand if you've what you're if and what you're smoking.

00:22:54.910 --> 00:22:57.670
I think there again, all of this is inevitable.

00:22:57.990 --> 00:22:59.510
It's how the data is used.

00:22:59.510 --> 00:23:04.220
There's some ethical discussion, massive, massive ethical things.

00:23:04.220 --> 00:23:06.900
And here's and here is actually the crux of it.

00:23:07.140 --> 00:23:08.380
OK, and then I want to go back.

00:23:08.380 --> 00:23:13.540
Crux me. The dangerous part of this is at what point in time

00:23:14.260 --> 00:23:17.820
is this data being used to make decisions on your part,

00:23:18.100 --> 00:23:20.180
whether it be good or bad.

00:23:20.860 --> 00:23:24.060
Right. For example, I drive an F-150

00:23:24.730 --> 00:23:26.610
big gas guzzler right now.

00:23:26.610 --> 00:23:28.970
It's telling me I get 11 miles per gallon.

00:23:29.700 --> 00:23:32.860
You drive a Tesla purely electrical

00:23:33.750 --> 00:23:36.830
and you know this is going to be true.

00:23:36.830 --> 00:23:40.430
I don't know if it's in your opinion, but in terms of the people

00:23:40.430 --> 00:23:45.370
who are concerned about the climate, hey, wouldn't it be cool

00:23:45.370 --> 00:23:49.370
if we could get access to this data and force people

00:23:49.370 --> 00:23:51.210
to make eco friendly decisions?

00:23:52.540 --> 00:23:56.220
It's where this data becomes dangerous

00:23:56.780 --> 00:24:01.900
is when decisions are made for you and you're not involved.

00:24:02.500 --> 00:24:04.380
I think that's very Philip K.

00:24:04.380 --> 00:24:09.700
Dick yet, which is sort of in the adjacent possible of future possibilities.

00:24:10.020 --> 00:24:13.140
Well, no, that's not only a future possibility.

00:24:13.500 --> 00:24:17.940
People are going to willingly let the system make those decisions for it.

00:24:18.500 --> 00:24:20.420
I want to go into that, but I want to back up.

00:24:20.420 --> 00:24:23.980
This is do some magic editing put this way back, but I'm not going to

00:24:23.980 --> 00:24:25.260
because I'm far too lazy for that.

00:24:25.260 --> 00:24:29.140
But very early in the conversation, you were talking about

00:24:29.900 --> 00:24:31.100
grow way off in the future now.

00:24:31.100 --> 00:24:35.980
Talk about the now you're said you're feeling much now with this data science

00:24:36.460 --> 00:24:41.500
like you were with test, however many years ago, was 15 years ago,

00:24:41.500 --> 00:24:45.740
10 years ago, actually 10 years ago when we started realizing that,

00:24:45.740 --> 00:24:48.340
you know what, in the way we're shipping software, we're not going to

00:24:48.340 --> 00:24:49.820
test as much anymore.

00:24:49.820 --> 00:24:52.740
And now you're saying with the way with the tools available,

00:24:53.060 --> 00:24:55.140
we don't need as much data science anymore.

00:24:55.140 --> 00:24:58.340
Is that a clear is that a fair summary?

00:24:59.060 --> 00:25:02.180
We're not only not going to need as much data science, it's going to go

00:25:02.180 --> 00:25:03.500
into other places as well.

00:25:03.780 --> 00:25:04.340
Yeah. Okay.

00:25:04.340 --> 00:25:07.500
So let me let me talk about let me set the stage for people that

00:25:07.500 --> 00:25:12.740
remember and remember also, as I discussed when I posted my made my

00:25:12.740 --> 00:25:17.580
post on why most teams don't need dedicated software testers, the

00:25:17.580 --> 00:25:22.460
uproar of illogical really rebuttals.

00:25:23.100 --> 00:25:26.420
There are lots of teams out there still employing testers who are

00:25:26.420 --> 00:25:31.980
convinced, who are convinced they are absolutely essential to

00:25:31.980 --> 00:25:35.900
delivering software even on fast moving stuff.

00:25:36.770 --> 00:25:37.410
Can't change that.

00:25:37.410 --> 00:25:38.130
They're still around.

00:25:38.410 --> 00:25:38.770
Yep.

00:25:38.970 --> 00:25:40.930
Thus data science still be around.

00:25:41.210 --> 00:25:45.290
I think when we first saw this, we didn't know that we didn't

00:25:45.290 --> 00:25:47.210
know about the rise of data science yet.

00:25:47.210 --> 00:25:49.450
I think partway through we were podcasting with that.

00:25:49.450 --> 00:25:51.930
I can't find the, I don't know when the date was.

00:25:51.930 --> 00:25:56.420
I can't find the presentation, but there was a Microsoft

00:25:57.180 --> 00:25:58.580
internal event.

00:25:59.460 --> 00:26:02.660
God, I wish I could remember when it was, but Seth, Elliot and I,

00:26:02.700 --> 00:26:04.860
Seth, Elliot and I sat there.

00:26:04.860 --> 00:26:07.300
If you're listening, she does sometimes not very often.

00:26:07.300 --> 00:26:07.860
Hello.

00:26:08.730 --> 00:26:09.290
Hey, Seth.

00:26:09.850 --> 00:26:15.250
We were, we give a presentation on data science and I told the, the

00:26:15.250 --> 00:26:23.090
famous target story of how, of how target knew this girl was pregnant

00:26:23.090 --> 00:26:25.650
before her parents did and very famous.

00:26:25.650 --> 00:26:26.210
Go look it up.

00:26:26.210 --> 00:26:30.050
I can tell the whole story again through data science and analysis of data.

00:26:30.050 --> 00:26:34.410
What you're talking about seeing for the future is this turn to 11 or 11,000.

00:26:34.970 --> 00:26:35.250
Yeah.

00:26:35.370 --> 00:26:41.810
But what are the graphs I showed was a Google trends graph showing the data

00:26:41.810 --> 00:26:45.890
science, the phrase as it was showing up.

00:26:45.930 --> 00:26:46.930
It didn't exist.

00:26:48.020 --> 00:26:50.100
It really didn't exist at all.

00:26:50.620 --> 00:26:53.540
Other than, you know, insignificantly it existed 12 years ago,

00:26:53.540 --> 00:26:54.540
whatever the timeline was.

00:26:54.820 --> 00:26:56.740
It certainly wasn't really a title.

00:26:57.020 --> 00:26:59.740
And now it's now it's a huge thing.

00:27:01.000 --> 00:27:07.160
So I'm wondering if the thing that's next and whether it's a new role,

00:27:07.160 --> 00:27:08.920
you know, data, what's the, what's the joke?

00:27:09.280 --> 00:27:10.440
You know, what's the data scientists?

00:27:10.440 --> 00:27:14.360
It's a, it's a, uh, statistician that works in Silicon Valley.

00:27:14.440 --> 00:27:14.960
Ha ha.

00:27:15.480 --> 00:27:18.160
But I think the creative parts missing.

00:27:19.120 --> 00:27:21.360
So let me, let me, let me just finish my thought here.

00:27:21.360 --> 00:27:22.360
I'll ask you a question and go on.

00:27:22.640 --> 00:27:28.200
So you still have to figure out, you can't just give GPT three,

00:27:28.200 --> 00:27:29.160
here's a bunch of stuff.

00:27:29.160 --> 00:27:30.240
Tell me what I should do.

00:27:30.960 --> 00:27:36.330
Someone has to figure out what questions to ask, how to dig in there.

00:27:36.330 --> 00:27:38.810
But one thing, here's an observation I want you to comment on, and then

00:27:38.810 --> 00:27:39.770
I'll show you for a while.

00:27:39.970 --> 00:27:40.290
All right.

00:27:40.690 --> 00:27:44.650
One of the things that data scientists have told me for as long as I've

00:27:44.650 --> 00:27:49.650
known what a data scientist is, is how much of their time they spend cleaning

00:27:49.650 --> 00:27:54.150
up the data so that they can actually do something meaningful with it.

00:27:54.900 --> 00:27:55.260
Yep.

00:27:55.780 --> 00:27:59.940
And you can tell me, does GPT through technology, it seems to me,

00:27:59.980 --> 00:28:02.940
it works fine on dirty data.

00:28:03.620 --> 00:28:04.180
It is.

00:28:04.300 --> 00:28:09.100
That is not a limit, a limitation to, to the tech behind GPD.

00:28:09.460 --> 00:28:09.660
Yeah.

00:28:09.660 --> 00:28:10.580
I just realized this.

00:28:10.580 --> 00:28:14.340
So, uh, anyway, just a real little relevation while we talk, but, uh,

00:28:14.380 --> 00:28:15.140
I've talked long enough.

00:28:15.140 --> 00:28:18.220
So anyway, I made my point to continue on your stories.

00:28:18.340 --> 00:28:19.340
Tell me all kinds of things.

00:28:19.660 --> 00:28:21.980
Tell me how doomed we are and we'll make a movie.

00:28:22.340 --> 00:28:24.180
I don't know how doomed we are.

00:28:24.420 --> 00:28:24.740
Right.

00:28:24.980 --> 00:28:31.110
I just know we, I know our grandchildren from the, the context in which we

00:28:31.110 --> 00:28:36.070
judge the world today, our grandchildren are doomed, but it'll be normal.

00:28:36.270 --> 00:28:39.470
I guess the positive thing is that it'll be normalized for them.

00:28:40.190 --> 00:28:41.150
What that means.

00:28:41.150 --> 00:28:41.790
I don't know.

00:28:42.230 --> 00:28:47.590
People will go and say, but chat GPT can't do this and chat GPT can't do that.

00:28:47.590 --> 00:28:48.110
Chat GPT.

00:28:48.590 --> 00:28:48.830
Yeah.

00:28:48.830 --> 00:28:50.550
They, that's all true.

00:28:50.550 --> 00:28:51.430
So the hell what?

00:28:52.390 --> 00:28:52.630
Right.

00:28:52.630 --> 00:28:58.430
It's it's, to me, it reminds me it even further, it further solidifies my point

00:28:58.430 --> 00:29:04.350
of view on this prediction is so happening because the immediate thing is people to

00:29:04.350 --> 00:29:14.060
go and, and nitpick on what it can't do as sort of a self-defense mechanism.

00:29:14.220 --> 00:29:15.420
We saw this in tests.

00:29:15.460 --> 00:29:18.220
Oh, but who's going to find bugs?

00:29:18.260 --> 00:29:21.980
Devs don't want to, oh, that's a stupid argument.

00:29:22.620 --> 00:29:23.860
But your ship it to cut.

00:29:24.020 --> 00:29:25.980
Oh, that's a dumb argument too.

00:29:26.100 --> 00:29:30.100
Uh, it's eventually they lose the grip on the white knuckles.

00:29:30.500 --> 00:29:30.700
Right.

00:29:30.700 --> 00:29:32.540
Cause progress keeps moving forward.

00:29:33.060 --> 00:29:34.780
I'll give you a great example right now.

00:29:34.780 --> 00:29:38.770
Chat GPT does facts like ass.

00:29:39.370 --> 00:29:39.570
Okay.

00:29:39.570 --> 00:29:40.090
You ask.

00:29:40.690 --> 00:29:41.010
Okay.

00:29:41.570 --> 00:29:45.210
But people are like, Oh, chat GPT will never do facts.

00:29:45.650 --> 00:29:46.250
Okay.

00:29:46.770 --> 00:29:51.930
Well, we will wear the, the, the news article that came out this week where

00:29:51.970 --> 00:30:00.010
Steven Wolfram, the head of Wolfram alpha is offering to, to work with chat GPT

00:30:00.010 --> 00:30:03.300
to fix that problem, like stop.

00:30:03.660 --> 00:30:08.660
Google, Google has come out with an algorithm that you say, Hey, generate

00:30:08.660 --> 00:30:11.300
a song for me that feels like this.

00:30:11.860 --> 00:30:12.700
And guess what?

00:30:13.020 --> 00:30:14.460
It's pretty damn good.

00:30:15.020 --> 00:30:15.260
Right.

00:30:15.260 --> 00:30:15.900
Here's the thing.

00:30:16.860 --> 00:30:22.100
When you say creativity, like I'm going to be nothing but doom for the next five

00:30:22.100 --> 00:30:27.390
minutes, I'm just telling you, when you bring up creativity, what we view as

00:30:27.390 --> 00:30:32.790
creativity is the generation of new ideas and where do new ideas come from?

00:30:32.790 --> 00:30:33.190
Allen.

00:30:33.950 --> 00:30:34.110
My.

00:30:35.430 --> 00:30:36.030
Okay.

00:30:37.830 --> 00:30:45.470
Well, all of those ideas generally are documented someplace and this

00:30:45.470 --> 00:30:49.230
system is really good at pulling those in.

00:30:49.750 --> 00:30:52.870
Now, will it come up with random directions initially?

00:30:53.150 --> 00:30:53.950
Probably.

00:30:54.390 --> 00:30:59.110
But when you say, okay, generate a new idea for me in this direction, it'll get

00:30:59.110 --> 00:31:00.830
better and better and better and better.

00:31:01.310 --> 00:31:07.060
The last one that I, because I've been sharing this with, let me pause right

00:31:07.060 --> 00:31:10.740
there, cause this is the creativity brainstorming I was talking about.

00:31:10.740 --> 00:31:11.740
I can be writing a song.

00:31:11.740 --> 00:31:15.540
I could say, help me find a, help me find a word that rounds with orange.

00:31:15.820 --> 00:31:19.300
I could just say, what would the next line, what should the next line of the song be?

00:31:19.460 --> 00:31:24.580
And it may not be what I want or use, but it gives me an idea that I can work from.

00:31:24.620 --> 00:31:27.860
Ideas come from other ideas, which is what you wanted me to say earlier.

00:31:28.140 --> 00:31:28.500
Right.

00:31:28.700 --> 00:31:30.540
And that's what I use chat GPT for.

00:31:30.540 --> 00:31:32.500
I use it to get ideas.

00:31:32.820 --> 00:31:36.380
I might not use its ideas, but I may use, I may use its ideas come up with

00:31:36.380 --> 00:31:38.460
a new idea that I couldn't have come up with on my own.

00:31:38.820 --> 00:31:40.340
It's great because I don't like people.

00:31:40.660 --> 00:31:42.260
And now I have someone to brainstorm with.

00:31:43.260 --> 00:31:49.720
One of my employees today, we brought up some data and one of the things I realized.

00:31:50.160 --> 00:31:53.400
So my team documents all of its work in one note.

00:31:54.000 --> 00:31:57.320
And I'm like, huh, if I was a word I haven't heard in a long time,

00:31:57.320 --> 00:31:58.120
I forgot about that app.

00:31:58.520 --> 00:31:58.840
Right.

00:31:58.840 --> 00:32:03.720
And we do very tactical work, like two, two weeks at a time.

00:32:03.720 --> 00:32:03.920
Right.

00:32:04.120 --> 00:32:09.400
And I'm like, huh, if I were to copy paste that into the chat, GPT, every

00:32:09.720 --> 00:32:15.960
six months for each employee, and then ask chest GPT, Hey, um, given that these

00:32:15.960 --> 00:32:22.120
are the principles for me as a manager that are important, write a professional

00:32:22.120 --> 00:32:25.240
performance review based off of this work.

00:32:25.680 --> 00:32:26.000
Right.

00:32:26.000 --> 00:32:32.880
And I'm like, Oh, I, I now have just gone down the path of, of automating

00:32:33.320 --> 00:32:39.400
management and then my employee said, Oh, but I could do the same

00:32:39.400 --> 00:32:43.560
and create essentially an employee bot.

00:32:44.640 --> 00:32:46.720
And, and, and that just set me off.

00:32:46.720 --> 00:32:52.580
I'm like, Oh my God, we could Alan could create a, uh, an Alan.

00:32:53.300 --> 00:32:58.260
But we could, we could go and put these glasses on for a week so that our

00:32:58.260 --> 00:33:03.900
bots could really understand us and create a remarkably train it with all the,

00:33:04.060 --> 00:33:09.340
the, the code we've ever written, the bugs we filed and create a very

00:33:09.340 --> 00:33:14.180
realistic simulation of us in that particular persona.

00:33:14.460 --> 00:33:17.820
And then let's say you and I, there's something we disagree with.

00:33:18.340 --> 00:33:21.900
We get just instead of you, you and I could go get beers and just have the

00:33:21.900 --> 00:33:25.260
stupid bots argue it out until they come to the conclusion.

00:33:25.460 --> 00:33:31.540
And, and I'm like, uh, I even think I know how I could write that.

00:33:32.390 --> 00:33:36.430
And if I can do it, these experts building chat GPT absolutely can't.

00:33:37.230 --> 00:33:38.110
I'm not entirely sure.

00:33:38.150 --> 00:33:39.710
I could do that one.

00:33:39.950 --> 00:33:44.190
The only defense, by the way, there's only two things I've seen in terms of sort of

00:33:45.170 --> 00:33:51.390
defending against decisions being made on my behalf without my permission.

00:33:51.950 --> 00:33:56.470
There's only two things I've been able to is number one, come up with a new service

00:33:56.990 --> 00:34:00.310
and, and change regulations that makes it very clear.

00:34:00.390 --> 00:34:02.790
My data is mine.

00:34:03.630 --> 00:34:09.830
And I want the ability, a first ability is I, I want to know any time, even

00:34:09.830 --> 00:34:17.930
a single bit of my data, as in there are eight bits in a bite, even, even bit of

00:34:17.930 --> 00:34:21.410
my data was used in some decision-making process.

00:34:21.690 --> 00:34:26.010
I want to know the decisions being made off of my data or the other ways.

00:34:26.010 --> 00:34:30.130
So the whole new service that, that basically like the data gets moved

00:34:30.130 --> 00:34:31.770
to one of my vaults.

00:34:31.970 --> 00:34:33.250
I can delete it anytime.

00:34:33.250 --> 00:34:35.970
It's not, I have to ask Google to delete the data.

00:34:35.970 --> 00:34:37.890
No, it's stored in my location.

00:34:37.890 --> 00:34:38.730
It's mine.

00:34:38.890 --> 00:34:41.890
The fine line here is I get exactly what you're saying.

00:34:42.130 --> 00:34:42.450
Yep.

00:34:42.690 --> 00:34:47.610
And it should give, AI should give us suggestions, not decisions.

00:34:48.530 --> 00:34:50.920
But here's the deal.

00:34:51.740 --> 00:34:53.220
Let me go back to Apple glasses.

00:34:53.220 --> 00:34:56.980
Let's say could do some things like just something simple.

00:34:56.980 --> 00:35:03.270
Like today, you know, a lot of people use apps like my fitness pal or similar

00:35:03.270 --> 00:35:05.870
things to track their calories and exercise throughout the day.

00:35:06.270 --> 00:35:09.870
They're trying to get the right number of not only calories, but macros

00:35:09.870 --> 00:35:11.630
between fats, carbs, and protein.

00:35:12.270 --> 00:35:16.510
And they're painstakingly looking up things to get the breakdown and guessing

00:35:16.510 --> 00:35:19.310
here and there and trying to find the best diet for them in order to

00:35:19.310 --> 00:35:22.870
optimize the kind of body they want to have with their, with their

00:35:22.870 --> 00:35:24.150
exercise routine and everything.

00:35:24.470 --> 00:35:28.340
Now, theoretically, Apple glasses, they can automate all that.

00:35:28.340 --> 00:35:32.100
They can look at the food and look at a database in the background AI,

00:35:32.100 --> 00:35:32.820
figure it all out.

00:35:33.260 --> 00:35:34.340
It could just be automatic.

00:35:34.340 --> 00:35:38.500
It could tell me what I should and shouldn't eat from a plate or from a buffet

00:35:38.500 --> 00:35:42.780
or even from a menu I'm reading, which I think a lot of people, a lot of

00:35:42.780 --> 00:35:48.580
fitness and health conscious people would actually look at that as a plus,

00:35:48.620 --> 00:35:49.420
a positive.

00:35:49.700 --> 00:35:50.060
Yeah.

00:35:50.620 --> 00:35:52.100
And they be right.

00:35:53.060 --> 00:35:59.180
But also, you know, it's on the edge of when does, when does good for you

00:35:59.420 --> 00:36:02.340
become creepy and then bad?

00:36:02.580 --> 00:36:04.060
The road, that's what you're worried about.

00:36:04.100 --> 00:36:07.620
The road to hell is paved with good intentions.

00:36:07.620 --> 00:36:13.900
So there are some good intentions in how our lives will change.

00:36:13.900 --> 00:36:16.780
And you can even think, I mean, I can, we can tell the old fart story

00:36:16.780 --> 00:36:18.140
about how we didn't have the internet.

00:36:18.860 --> 00:36:21.580
By the way, in your 2000, you and I both had internet.

00:36:21.700 --> 00:36:22.660
I used to dial up.

00:36:22.660 --> 00:36:23.660
I have an ISDN.

00:36:23.740 --> 00:36:26.900
No, by 2000, I might have even had, I can't remember what I got cable

00:36:26.900 --> 00:36:28.500
for the first time, but I had dial up.

00:36:28.860 --> 00:36:30.460
I saw this interesting stat.

00:36:31.460 --> 00:36:36.620
400,000 people worldwide had access to the internet estimated in 2000.

00:36:37.370 --> 00:36:42.130
And today it's again, think the whole world, even third world countries.

00:36:42.850 --> 00:36:45.850
I don't, I just had a percentage, it's like 65%.

00:36:46.570 --> 00:36:49.210
But 65% of 7 billion, 7 million or eight.

00:36:49.730 --> 00:36:53.250
It's a big number, a big number of people have access to the internet

00:36:53.250 --> 00:36:54.370
to get their questions answered.

00:36:55.220 --> 00:37:00.140
What is that we used to talk about, I'm going to say Murfrees law, Moore's law

00:37:00.180 --> 00:37:05.460
in that computers were, computer power was doubling every, like three years,

00:37:05.460 --> 00:37:07.100
every number of years and getting all the numbers wrong.

00:37:07.880 --> 00:37:11.560
But it's this ease of access to information.

00:37:12.400 --> 00:37:15.280
It's going through the same sort of exponential growth.

00:37:15.880 --> 00:37:20.560
And the way that information and data is acting on is now going

00:37:20.560 --> 00:37:22.640
through exponential growth.

00:37:23.240 --> 00:37:23.760
Yes.

00:37:24.710 --> 00:37:28.630
And it's going to enable a lot of cool things.

00:37:28.710 --> 00:37:29.790
The good intentions.

00:37:30.680 --> 00:37:35.760
It's also going to enable, as we've seen already with Facebook and other places,

00:37:36.040 --> 00:37:37.320
a whole bunch of bad shit.

00:37:37.320 --> 00:37:39.040
We do not want to happen.

00:37:41.230 --> 00:37:41.590
Yep.

00:37:43.720 --> 00:37:46.840
And I'm wondering just really twist this thing in a different direction.

00:37:46.840 --> 00:37:53.920
Here's a run at a time is we have in the US at least we have a government made up

00:37:53.960 --> 00:37:59.400
of 99% of people who do not understand any of this because they were born

00:37:59.560 --> 00:38:03.120
a hundred years before Brent and I also, I'm worried.

00:38:03.120 --> 00:38:06.520
I like this need, this needs some regulation and some people

00:38:06.520 --> 00:38:07.880
thinking about how to solve it.

00:38:08.160 --> 00:38:10.080
But I don't know where those people come from.

00:38:10.480 --> 00:38:14.440
So now I'm on the Brent train and maybe we are all doomed because there's

00:38:14.440 --> 00:38:20.520
no way to even protect ourselves from the evil we're making even with good intentions.

00:38:20.800 --> 00:38:30.360
There's only one, one, it's either lock it away and make it such that lock it away in a way that even

00:38:31.080 --> 00:38:36.160
the slow assholes running the government don't have access to it.

00:38:36.980 --> 00:38:37.220
Right.

00:38:37.220 --> 00:38:40.820
But then right now with the laws, that's just a regulation away.

00:38:41.020 --> 00:38:43.340
I don't know if you're paying attention to crypto coin.

00:38:44.300 --> 00:38:50.100
That's kind of what I view the government's doing on crypto coin and doing it way too slow.

00:38:50.660 --> 00:38:50.860
Right.

00:38:50.860 --> 00:38:58.820
By the time they realize they need to get regulations in place, it's not only going to be too late,

00:38:58.820 --> 00:39:01.180
it's going to be way too late.

00:39:01.380 --> 00:39:03.300
So now I'm wondering.

00:39:03.300 --> 00:39:06.420
And here's the thing, I'm not even certain as something they can do.

00:39:07.230 --> 00:39:08.110
Potentially not.

00:39:08.230 --> 00:39:12.670
Because there's going to be other governments that will do the exact opposite.

00:39:13.230 --> 00:39:16.670
They're like, no, I am a dictator of my country.

00:39:17.310 --> 00:39:26.310
I want this because this allows me not only that, but I want the ability from my office to be able to

00:39:26.310 --> 00:39:28.670
control the decision making from my populace.

00:39:28.670 --> 00:39:31.270
Free apple glasses for everyone in North Korea.

00:39:31.980 --> 00:39:32.860
Right. Exactly.

00:39:33.100 --> 00:39:39.820
The only other thing I can think about is of how to prevent this is if a company comes up with

00:39:40.260 --> 00:39:43.260
something like it's basically fight fire with fire.

00:39:43.740 --> 00:39:51.940
There needs to be a technology where you can have a chat GPT type thing that that's whole role in

00:39:51.940 --> 00:39:54.980
life is to defend you.

00:39:55.500 --> 00:39:56.100
Yeah.

00:39:56.660 --> 00:39:58.940
If I was smart enough, I'd go work for that company.

00:39:59.220 --> 00:40:00.900
Hopefully that company exists.

00:40:00.940 --> 00:40:01.340
Maybe.

00:40:01.980 --> 00:40:03.260
Why can't it be Microsoft?

00:40:03.260 --> 00:40:06.540
How come you can't be the defenders of because Google do no evil.

00:40:06.540 --> 00:40:07.180
Google could do it.

00:40:07.580 --> 00:40:08.340
They're not going to do it.

00:40:09.020 --> 00:40:09.460
Yeah.

00:40:10.140 --> 00:40:16.780
I keep hearing that motto and I have concluded that that evil is an acronym, but I don't know what

00:40:16.780 --> 00:40:17.500
it stands for.

00:40:18.100 --> 00:40:21.740
So one other thing I thought of, we're going to go a little bit over time because I'm sure you

00:40:21.740 --> 00:40:22.820
have some closing thoughts here.

00:40:23.420 --> 00:40:25.460
Not again, you're wondering, we started late.

00:40:25.500 --> 00:40:25.780
All right.

00:40:25.820 --> 00:40:26.740
We started late.

00:40:26.860 --> 00:40:27.940
It's not an hour podcast.

00:40:27.940 --> 00:40:29.260
It's as long as we want it to be.

00:40:29.820 --> 00:40:30.460
That was for the list.

00:40:31.380 --> 00:40:32.260
I think you know this.

00:40:32.300 --> 00:40:37.620
I go backpacking two or three times as much as I could get out of the house during the

00:40:37.620 --> 00:40:37.980
summer.

00:40:38.810 --> 00:40:46.010
I like to be out in the middle of nowhere with no people around with no cell service

00:40:46.010 --> 00:40:50.610
because damn it, it's just too easy to glance at my phone all the time when I'm home or

00:40:50.610 --> 00:40:53.290
look something up or stream a video.

00:40:54.050 --> 00:41:01.510
And I think a lot of people have forgotten what it's like to be a little bored or to be

00:41:01.590 --> 00:41:03.550
a little undistracted.

00:41:04.190 --> 00:41:06.190
And every time I'm out there, this story is going somewhere.

00:41:06.230 --> 00:41:06.790
Don't worry about it.

00:41:07.030 --> 00:41:10.950
Every time I'm out there, I think, you know, this is why people live off the grid.

00:41:10.950 --> 00:41:16.950
There's a lot of intrinsic value you get from just being off the grid.

00:41:17.190 --> 00:41:21.790
And we've all known or at least known of people who know people that have gone off

00:41:21.790 --> 00:41:23.070
the grid, no social media.

00:41:23.470 --> 00:41:28.070
Maybe they use a flip phone and they just do not go online at all.

00:41:28.830 --> 00:41:30.630
They would never buy Apple glasses.

00:41:31.070 --> 00:41:39.310
I wonder if this, I mean, in the dystopic science fiction thriller I'm viewing as

00:41:39.310 --> 00:41:43.750
this conversation, I think it's a growing number of people and maybe even

00:41:43.750 --> 00:41:51.470
communities in our doomsday future who consciously stay off of every single

00:41:51.470 --> 00:41:52.910
thing that can track them.

00:41:53.510 --> 00:41:55.830
Oh, I know of multiple people.

00:41:55.830 --> 00:42:00.230
Like, but I think there will be more as this track, as not only this

00:42:00.230 --> 00:42:05.590
tracking becomes more ubiquitous, but suggestions are as you worry decisions

00:42:05.590 --> 00:42:10.150
are made on your behalf, it would encourage more people just to say no to it all.

00:42:10.430 --> 00:42:11.310
Uh, yeah.

00:42:11.510 --> 00:42:17.630
And in there, I'm like now I know I'm going into conspiracy theory, but, but

00:42:18.030 --> 00:42:21.660
there are some on the, on the crypto coin thing.

00:42:22.260 --> 00:42:26.140
There are some that believe that a boot, a push towards a pure digital

00:42:26.140 --> 00:42:30.910
currency is blocks that as a solution.

00:42:31.940 --> 00:42:32.180
Right.

00:42:32.180 --> 00:42:34.480
It's essentially, right.

00:42:34.480 --> 00:42:34.800
Yeah.

00:42:35.520 --> 00:42:41.280
If, if your option is to then go back to God, what is it?

00:42:41.280 --> 00:42:47.840
600 BC and, and, and do trading of goods and services, right?

00:42:47.840 --> 00:42:52.920
The whole point of money was to, to simplify you, you go off the grid,

00:42:52.920 --> 00:42:55.160
you start raising sheep, you want eggs.

00:42:55.160 --> 00:42:58.320
You're like, okay, I'll take a hundred eggs for my one sheep.

00:42:58.320 --> 00:42:59.120
Bartering work.

00:42:59.120 --> 00:42:59.520
Sure.

00:42:59.520 --> 00:42:59.800
Right.

00:43:00.160 --> 00:43:05.680
And so if we move to, to, to pure cryptocurrency, well, that can't happen.

00:43:06.280 --> 00:43:10.080
Then, then the other thing that's going to happen, of course, I think Burt, yeah,

00:43:10.080 --> 00:43:12.680
I don't happen to burning man becomes a year round thing.

00:43:12.920 --> 00:43:13.600
Exactly.

00:43:13.960 --> 00:43:18.200
It becomes, there will be, that's exactly right.

00:43:19.200 --> 00:43:19.800
And I'm nodding.

00:43:19.800 --> 00:43:21.440
I'm not against this feature.

00:43:21.640 --> 00:43:24.800
Bernie, man, I don't think has enough space to fit all.

00:43:25.160 --> 00:43:27.240
No, we need several of these, all these entities.

00:43:27.600 --> 00:43:28.080
All right.

00:43:28.400 --> 00:43:29.800
Uh, I got to go, man.

00:43:30.120 --> 00:43:30.440
Yeah.

00:43:30.640 --> 00:43:33.240
Um, and you're eating, so what's being the podcast is over.

00:43:33.600 --> 00:43:34.160
It's over.

00:43:36.100 --> 00:43:36.740
All right, everybody.

00:43:36.740 --> 00:43:37.620
Thanks for listening.

00:43:37.660 --> 00:43:40.340
Um, if we're, if the world still exists in a few weeks, we'll

00:43:40.340 --> 00:43:41.460
be back for another podcast.

00:43:41.700 --> 00:43:42.460
I'm Alan.

00:43:42.620 --> 00:43:43.300
I'm Brent.

00:43:43.660 --> 00:43:45.580
And we'll see you another time.

