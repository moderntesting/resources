WEBVTT

00:00:00.000 --> 00:00:06.280
I believe she said the Romans invented the unicorn and the Greeks invented the

00:00:06.280 --> 00:00:11.160
Pegasus. And then she's like, I have no idea who came up with the aliquot.

00:00:14.800 --> 00:00:18.960
Welcome to AB testing podcast, your modern testing podcast.

00:00:19.280 --> 00:00:24.720
Your hosts, Alan and Brent, will be here to guide you through topics on testing,

00:00:24.920 --> 00:00:27.920
leadership, agile and anything else that comes to mind.

00:00:28.160 --> 00:00:29.760
Now on with the show.

00:00:30.570 --> 00:00:31.530
Hello, everyone.

00:00:32.170 --> 00:00:36.650
It's Alan and Brent and nobody else except for my dog.

00:00:36.650 --> 00:00:40.010
We're here for episode 195 of the AB testing podcast. Welcome back.

00:00:40.010 --> 00:00:41.610
Thanks for listening. How you doing, Brent?

00:00:42.010 --> 00:00:43.990
I'm doing super.

00:00:43.990 --> 00:00:45.070
He's doing so super.

00:00:45.070 --> 00:00:55.140
He is wearing a San Francisco 49ers shirt and they are a sports ball team in the US.

00:00:55.180 --> 00:00:58.260
They play football, but not the good kind of football.

00:00:58.500 --> 00:01:06.420
The other kind that has gained massive popularity with the rise of the Taylor

00:01:06.420 --> 00:01:08.340
Swift Travis Kelsey relationship.

00:01:08.380 --> 00:01:10.180
I don't know how the Florida understand a chance.

00:01:10.180 --> 00:01:11.740
Tell me what your thoughts, Brent.

00:01:12.140 --> 00:01:17.180
I yeah, it was actually one of one of my employees had a one on one today.

00:01:17.180 --> 00:01:20.540
She's like, I never watched football, but I'm totally watching this time.

00:01:20.540 --> 00:01:21.420
I'm like, oh, really?

00:01:21.420 --> 00:01:21.860
Why?

00:01:22.580 --> 00:01:23.940
And she's like, because Taylor Swift.

00:01:24.260 --> 00:01:29.500
Because you can see like five seconds of Taylor Swift in the box being

00:01:29.500 --> 00:01:30.220
Swiftie.

00:01:30.620 --> 00:01:31.340
Yeah, it's wild.

00:01:31.500 --> 00:01:32.420
It's a wild time.

00:01:32.820 --> 00:01:34.500
I'm just like, okay.

00:01:35.380 --> 00:01:35.660
Right.

00:01:35.700 --> 00:01:39.500
The Niners have a pretty good chance against the Chiefs, but can

00:01:39.500 --> 00:01:41.940
the Niners also tackle Swiftism?

00:01:42.220 --> 00:01:43.020
I don't know.

00:01:43.020 --> 00:01:45.180
It's going to be wild.

00:01:45.860 --> 00:01:47.300
I will watch every year.

00:01:47.300 --> 00:01:53.740
I love watching all of the new commercials because there's like three times a year.

00:01:53.940 --> 00:01:58.220
I watch actual commercials for more than those little 30 second blurbs on

00:01:58.260 --> 00:02:00.820
on YouTube or now Amazon prime.

00:02:01.260 --> 00:02:06.820
That is the Oscars, the Grammys and the Superbowl.

00:02:07.220 --> 00:02:07.820
It's about it.

00:02:08.480 --> 00:02:11.000
Because soccer, there's no commercials during the match.

00:02:11.000 --> 00:02:17.840
And I use, I use halftime to go get food or make something or whatever.

00:02:17.840 --> 00:02:19.120
Cause I know how long it's going to be.

00:02:19.120 --> 00:02:20.800
It's very predictable sport.

00:02:21.200 --> 00:02:21.720
Great.

00:02:21.880 --> 00:02:22.360
Great.

00:02:23.080 --> 00:02:25.280
Your brakes are more predictable.

00:02:25.640 --> 00:02:28.560
Like I would say football is fairly predictable as well.

00:02:28.760 --> 00:02:29.080
No.

00:02:29.160 --> 00:02:31.360
Well, here's the deal.

00:02:31.640 --> 00:02:35.800
Like if I'm actually, I will watch the CX play sometime, but I have to have a

00:02:35.800 --> 00:02:40.440
project while I'm watching football because there's not very much football

00:02:40.440 --> 00:02:41.960
in the three and a half hour football game.

00:02:42.900 --> 00:02:43.500
Right.

00:02:43.620 --> 00:02:44.020
Right.

00:02:44.060 --> 00:02:47.460
It's, it's, it's a great social sport.

00:02:47.460 --> 00:02:51.180
I can see why, like when I was younger, I go, I watch like football

00:02:51.180 --> 00:02:52.140
games with friends and things.

00:02:52.140 --> 00:02:55.820
Cause it's, it's, it's a social sport because there's plenty of time to

00:02:55.820 --> 00:02:58.100
talk between plays and things.

00:02:58.820 --> 00:02:59.500
Oh, for sure.

00:03:00.020 --> 00:03:00.500
For sure.

00:03:00.820 --> 00:03:01.980
And I don't like people.

00:03:01.980 --> 00:03:03.460
So social sports are not for me.

00:03:03.900 --> 00:03:09.300
I, I, um, I once, uh, a buddy of mine years ago once actually used

00:03:09.580 --> 00:03:14.080
football as sort of a metaphor for waterfall.

00:03:14.990 --> 00:03:15.670
Interesting.

00:03:16.030 --> 00:03:17.550
It's like, oh yeah.

00:03:17.670 --> 00:03:17.950
Right.

00:03:17.990 --> 00:03:21.190
And NFL game, they plan for an hour.

00:03:21.940 --> 00:03:27.320
The game's only going to take an hour, but yet it consistently never does.

00:03:27.880 --> 00:03:30.440
That's interesting, uh, metaphor.

00:03:30.440 --> 00:03:32.120
And it's one of the reasons why.

00:03:32.600 --> 00:03:32.840
Right.

00:03:32.840 --> 00:03:38.000
With, with the variant of agile, I never gave it the name, anything related to NFL.

00:03:38.480 --> 00:03:41.200
Cause NFL is absolutely in comparison to soccer.

00:03:41.280 --> 00:03:44.040
NFL is way more command and control.

00:03:44.680 --> 00:03:46.280
So we are by ourselves.

00:03:46.360 --> 00:03:51.080
We've had a couple of guests, uh, over the last few weeks, uh, split up,

00:03:51.120 --> 00:03:54.080
uh, Jason into two, a big thing on AI.

00:03:54.080 --> 00:03:56.280
And then we had Brian before that.

00:03:56.280 --> 00:03:59.320
I want to do a little retro reflection on some of the things I learned there and

00:03:59.320 --> 00:04:01.880
go a little bit deeper into those thoughts I have, but then I was also

00:04:01.880 --> 00:04:05.160
thinking like, I like having guests on the podcast and it's fun.

00:04:05.160 --> 00:04:10.240
People, we get feedback that people like getting new ideas and, and all these

00:04:10.240 --> 00:04:13.800
things, and then my boss told me this week, he says, I skipped past your past

00:04:13.920 --> 00:04:14.360
episodes.

00:04:14.360 --> 00:04:16.480
I only want to hear you and Brent rant about things.

00:04:16.480 --> 00:04:17.680
I don't care about your guests.

00:04:18.240 --> 00:04:19.000
Your boss said that.

00:04:20.920 --> 00:04:24.720
And I thought he's a total AI geek and he's listening right now.

00:04:24.720 --> 00:04:28.000
Well, I mean, right now as the moment he hears us, but he would, didn't listen

00:04:28.000 --> 00:04:29.280
to Jason geek out about AI.

00:04:29.280 --> 00:04:34.520
So whatever I was, I don't, I don't know how I feel about that feedback.

00:04:34.720 --> 00:04:35.400
Well, whatever.

00:04:35.480 --> 00:04:36.360
Well, just, you know what?

00:04:36.520 --> 00:04:37.280
Here's the deal.

00:04:37.280 --> 00:04:42.840
Here is 195 episodes in and people know by now, look, we are the

00:04:42.840 --> 00:04:44.200
AB testing podcast.

00:04:44.400 --> 00:04:46.320
We do whatever the fuck we want.

00:04:46.480 --> 00:04:47.800
I ain't fucking men.

00:04:47.800 --> 00:04:52.000
And one thing that's really important that I think you just click me in on.

00:04:52.000 --> 00:04:56.920
So you're saying that there's like a 95% chance that your manager is

00:04:56.920 --> 00:04:58.880
going to listen to this episode.

00:04:58.960 --> 00:05:00.360
Yes, I think so.

00:05:01.120 --> 00:05:04.960
And he will tell me, in fact, he will send me a message the

00:05:04.960 --> 00:05:06.280
moment he gets to that part.

00:05:06.320 --> 00:05:08.600
It says, Hey, heard the call out sup.

00:05:08.920 --> 00:05:12.040
I don't know your manager well, but I, I'm going to think of, I'm going

00:05:12.040 --> 00:05:15.280
to think a bit about the chaos I might be able to cause you.

00:05:15.440 --> 00:05:18.920
So I'm going to read you a blog post and our blog post, a LinkedIn post from today.

00:05:19.320 --> 00:05:19.560
Okay.

00:05:19.600 --> 00:05:22.600
And I want you to hold your comments and any facial expression, actually

00:05:22.600 --> 00:05:25.160
facial expressions are fine because nobody could see them until I'm done.

00:05:25.980 --> 00:05:29.180
This is, uh, from a fellow named Mike Thornton.

00:05:30.000 --> 00:05:31.920
Developers shouldn't test their own code.

00:05:32.730 --> 00:05:34.250
Developers have a blind spot.

00:05:34.290 --> 00:05:35.970
Their focus is problem solving.

00:05:36.090 --> 00:05:39.570
They are so solution oriented that they can't see education.

00:05:39.710 --> 00:05:41.690
AY, so they will only test the happier path.

00:05:41.810 --> 00:05:43.290
Only tester should test.

00:05:43.610 --> 00:05:45.810
Developers shouldn't design their own software.

00:05:45.890 --> 00:05:47.690
They will only design the happy path.

00:05:47.770 --> 00:05:48.850
Only designers.

00:05:48.890 --> 00:05:53.080
Should design developer should not deploy their own software.

00:05:53.120 --> 00:05:55.000
They will only deploy the happy path.

00:05:55.320 --> 00:05:57.560
Only deployers should deploy.

00:05:57.880 --> 00:06:00.360
Developers shouldn't code their own software.

00:06:00.560 --> 00:06:02.440
The will only code the happy path.

00:06:02.560 --> 00:06:04.240
Only coders should code.

00:06:05.570 --> 00:06:06.530
Last time, wait.

00:06:06.610 --> 00:06:07.170
I'm not done yet.

00:06:07.450 --> 00:06:09.650
Follow me for more career ending advice.

00:06:10.210 --> 00:06:17.610
Okay.

00:06:17.610 --> 00:06:22.800
I took Brent on a journey.

00:06:22.800 --> 00:06:24.760
That was a roller coaster.

00:06:24.760 --> 00:06:27.040
Oh my God.

00:06:27.040 --> 00:06:29.880
It started off bad and got more absurd and then got good.

00:06:29.880 --> 00:06:32.080
So I'm like, I'm like, what?

00:06:32.080 --> 00:06:35.320
Like the last one was like, what?

00:06:35.320 --> 00:06:40.400
What was cool is I saw this because Brian Finster commented on it and I followed Brian

00:06:40.400 --> 00:06:41.400
Finster on LinkedIn.

00:06:41.400 --> 00:06:45.400
So it showed up in my feed and it was really cool because what I'm finding is, and Brian

00:06:45.400 --> 00:06:52.480
was on episode 192 and there's a line from that or a concept from that podcast I've been

00:06:52.480 --> 00:06:58.400
talking about a lot lately where he says something to the effect of the way to really highlight

00:06:58.400 --> 00:07:01.960
where the problems are in your delivery is to try and do CD.

00:07:01.960 --> 00:07:02.960
Yeah.

00:07:02.960 --> 00:07:08.400
And I have been thinking about that a lot because it's true and it's really about we want to

00:07:08.400 --> 00:07:10.480
help people go super, super fast.

00:07:10.480 --> 00:07:14.240
I've been talking to my team a little bit about that and there's a lot to that and

00:07:14.240 --> 00:07:15.840
see what breaks.

00:07:15.840 --> 00:07:17.160
So I want to reflect a little bit on that.

00:07:17.160 --> 00:07:23.920
I have, I want to tie that into a longer topic, but also that was it for the Brian thing.

00:07:23.920 --> 00:07:25.200
I thought that was pretty funny.

00:07:25.200 --> 00:07:28.240
And of course the comments don't in the internet never read the comments.

00:07:28.240 --> 00:07:32.280
The comments are like, actually I do want to finish this, this topic because the comments

00:07:32.280 --> 00:07:34.400
are you're an idiot.

00:07:34.400 --> 00:07:35.800
People said, I don't agree with you.

00:07:35.800 --> 00:07:37.520
These are wrongs like, oh my God.

00:07:37.520 --> 00:07:43.800
But what I'm happy to see is we have people that don't even know what modern testing

00:07:43.800 --> 00:07:48.520
is doing modern testing principles, exactly what we knew was going on, but nobody believed

00:07:48.520 --> 00:07:49.520
us at first.

00:07:49.520 --> 00:07:56.240
And we're just seeing more and more examples of the fact that a whole lot of companies deliver.

00:07:56.240 --> 00:08:01.400
There's anything we saw coming on that we, we pay attention to sort of trends that seem

00:08:01.400 --> 00:08:02.400
to be successful.

00:08:02.400 --> 00:08:05.000
Like we didn't have this.

00:08:05.000 --> 00:08:06.000
We just observed it.

00:08:06.000 --> 00:08:13.080
And the momentum of that or the momentum of that sort of initiative, that just kept

00:08:13.080 --> 00:08:17.800
on going, but it, I think it's well beyond early adopter phase.

00:08:17.800 --> 00:08:22.160
And that's the thing we, and the thing I keep on reordering, we didn't invent anything.

00:08:22.160 --> 00:08:25.440
We talked about what we were seeing and we put some labels on things just to try and

00:08:25.440 --> 00:08:26.440
explain it better.

00:08:26.440 --> 00:08:30.520
So interesting now, like getting just that one connection with Brian brought a little

00:08:30.520 --> 00:08:37.200
bit expanded my network a little bit into more people that get how modern software delivery

00:08:37.200 --> 00:08:38.200
works.

00:08:38.200 --> 00:08:39.200
So cool stuff.

00:08:39.920 --> 00:08:44.680
I'm actually wondering how well, send me that link later.

00:08:44.680 --> 00:08:48.560
I'm actually wondering how well that would fit on a t-shirt.

00:08:48.560 --> 00:08:57.750
It's not just like that is the type of witty things that I would often get t-shirts.

00:08:57.750 --> 00:08:59.510
It might be a little too verbal.

00:08:59.510 --> 00:09:01.030
Yeah, it's a lot on there.

00:09:01.030 --> 00:09:05.230
I also wanted to reflect a little bit on, we had Jason and you and I geeked out a

00:09:05.230 --> 00:09:07.030
lot about AI.

00:09:07.030 --> 00:09:09.110
I know you've been thinking about that a little bit.

00:09:09.150 --> 00:09:12.070
You mentioned a little bit when we were first getting on the call and talking, but

00:09:12.070 --> 00:09:14.830
something about, please don't say AI ops.

00:09:14.830 --> 00:09:16.270
Tell me more where you were going.

00:09:16.270 --> 00:09:19.270
Like, what are you even thinking about since that, about AI since that call?

00:09:19.270 --> 00:09:27.870
Well, so one of the things, how do I do this without being, let me do a brief version

00:09:27.870 --> 00:09:31.980
of it, a longer version of it might be worthwhile.

00:09:31.980 --> 00:09:37.300
So been thinking around, I'll just invent a term.

00:09:37.300 --> 00:09:41.700
And I'm not inventing a term because this is a conversation I had with my manager

00:09:41.700 --> 00:09:47.060
just the other day, but let's just call it LLM ops.

00:09:47.060 --> 00:09:50.020
Yeah, I feel a little sick already, but go on.

00:09:50.020 --> 00:09:50.260
Right.

00:09:50.260 --> 00:09:55.300
It's essentially, so if we think about the progression and there's a couple phases

00:09:55.300 --> 00:09:58.940
in between that I'm, I'm forgetting the names of, right?

00:09:58.940 --> 00:10:04.900
There was start off with sort of a dedicated SRE team.

00:10:04.900 --> 00:10:10.940
Then the idea of DevOps and then probably a couple more.

00:10:10.940 --> 00:10:19.180
Then there's AI ops where, well, so the big part of DevOps is now, now actually

00:10:19.180 --> 00:10:22.620
the developer owns their own life side operations.

00:10:22.620 --> 00:10:28.500
The idea behind DevOps was to get rid of the wall between the development team

00:10:28.500 --> 00:10:32.900
and the operations team or the development team and the deployers, the

00:10:32.900 --> 00:10:35.340
devs and deployers never shall meet.

00:10:35.340 --> 00:10:37.140
Unicorn projects all around us.

00:10:37.140 --> 00:10:38.780
Cognitive distance.

00:10:38.780 --> 00:10:39.260
Yeah.

00:10:39.260 --> 00:10:40.300
Don't need it.

00:10:40.300 --> 00:10:42.100
So the idea is just get rid of the walls.

00:10:42.100 --> 00:10:43.140
It's faster.

00:10:43.140 --> 00:10:44.020
Get rid of the handoffs.

00:10:44.020 --> 00:10:45.020
Just get rid of the handoff.

00:10:45.020 --> 00:10:48.100
So to me, in a way, everything's DevOps.

00:10:48.100 --> 00:10:52.420
We're trying to get rid of handoffs between teams, but I'll let you go with your story.

00:10:52.420 --> 00:10:52.740
Right.

00:10:52.740 --> 00:10:59.300
Well, well, so AI ops is again, trying to speed things up, essentially get

00:10:59.300 --> 00:11:05.140
rid of the risk inherent in human decision making, right?

00:11:05.140 --> 00:11:09.450
Have the AI make the decisions.

00:11:09.450 --> 00:11:12.290
So we're not breaking down the wall between AI and ops.

00:11:12.290 --> 00:11:15.250
We're breaking down the wall between humans and their decisions.

00:11:15.250 --> 00:11:25.780
Well, so my absolute belief, AI, every AI I've encountered, and I think this is true

00:11:25.780 --> 00:11:27.500
period for all AI.

00:11:27.500 --> 00:11:31.460
AI's whole purpose in life is to automate decision making.

00:11:31.460 --> 00:11:33.180
That's what it does.

00:11:33.180 --> 00:11:40.180
It's certain AIs, certain AIs can only automate, you know, simple decision.

00:11:40.180 --> 00:11:44.500
Even very complex models can only automate certain decisions.

00:11:44.500 --> 00:11:57.490
But the thing around LLM that's attractive is it can, if you leverage it right,

00:11:57.490 --> 00:12:08.570
I suppose hand wave, hand wave, it can make decisions of unstructured data of lots of forms.

00:12:08.570 --> 00:12:14.330
Obviously, it can't, it has the weakness where it can't use numbers in a way, say,

00:12:14.330 --> 00:12:19.170
traditional AI, every number to an LLM is a strain.

00:12:19.170 --> 00:12:20.890
Everything is a strain.

00:12:20.890 --> 00:12:29.740
But to me, it kind of feels like the next logical progression of speeding things up.

00:12:29.740 --> 00:12:32.980
Of course, massive risk with it.

00:12:32.980 --> 00:12:41.190
I just wanted to flip that topic to see the potential for your brain serving there.

00:12:41.190 --> 00:12:47.430
Just to interject there, I've talked about the way I use chat GPT is to help me collaborate

00:12:47.430 --> 00:12:52.590
and really to make decisions like, I do this or this, it is a form of decisions.

00:12:52.590 --> 00:12:58.230
But then I was thinking also, like, one of the things that's always, that attracts like,

00:12:58.230 --> 00:13:04.390
the leaders I've liked in my career, one attribute they all had was the ability for

00:13:04.390 --> 00:13:09.950
them to make a quick and confident decision, whether it was based on a little data or

00:13:09.950 --> 00:13:13.150
a lot of data, they just they were good at decision making.

00:13:13.150 --> 00:13:16.390
And their track record was super accurate.

00:13:16.390 --> 00:13:24.830
So if we can try to figure out if we use AI to help us make decisions, or AI is there

00:13:24.830 --> 00:13:30.030
to make decisions for us or help us make decisions, I'll say, does that accelerate?

00:13:30.030 --> 00:13:33.510
Does that improve leadership or replace leadership?

00:13:33.510 --> 00:13:39.580
Certainly the risk of it to do both is quite high.

00:13:39.580 --> 00:13:43.260
I think it's the former, by the way, at least for the short term, I think it's just

00:13:43.260 --> 00:13:46.900
like the thing we always say, AI isn't taking your job away.

00:13:46.900 --> 00:13:49.380
People who know how to use it effectively are.

00:13:49.380 --> 00:13:58.980
Now, and the people who's here's my problem with with the folks knowledge, we've talked

00:13:58.980 --> 00:14:01.900
about knowledge and where ideas come from, etc.

00:14:01.900 --> 00:14:03.140
We like knowledge.

00:14:03.140 --> 00:14:07.740
Yes, there's this concept of the adjacent possible.

00:14:07.740 --> 00:14:10.100
I think I think I learned that from Johnson.

00:14:10.100 --> 00:14:11.820
Yep, you did learn that from Stephen Johnson.

00:14:12.020 --> 00:14:12.540
Yeah.

00:14:12.540 --> 00:14:19.820
And so if everyone was to view like your personal knowledge as like, as like a literally

00:14:19.820 --> 00:14:23.940
like a bubble, like when you go blow bubbles with children, right, a bubble inside of your

00:14:23.940 --> 00:14:27.860
head, it's a sphere that has a surface area.

00:14:27.860 --> 00:14:35.100
And anytime you gain knowledge, you're basically blowing more air into that bubble.

00:14:35.100 --> 00:14:39.340
So it grows bigger, it has a better surface area.

00:14:39.340 --> 00:14:47.340
And so there are more things that are now possible just out of the reach of that bubble.

00:14:47.340 --> 00:14:55.780
The question is that I think about, okay, these these AI is coming to to to take our

00:14:55.780 --> 00:14:56.820
jobs.

00:14:56.820 --> 00:14:57.820
They will.

00:14:57.820 --> 00:15:02.620
But will they take the portion of our job that we like or the portion of our job that

00:15:02.620 --> 00:15:03.740
we hate?

00:15:03.740 --> 00:15:16.200
Will will it be able to to what degree will it be able to go accelerate us such that each

00:15:16.200 --> 00:15:22.130
of us as human beings are are able to access more of the adjacent possible?

00:15:22.130 --> 00:15:28.850
I think when I think about a couple of things like automation, why did we build automation?

00:15:28.850 --> 00:15:29.850
Right.

00:15:29.850 --> 00:15:32.810
It's to get rid of sucky, repetitive things, the things that we kind of don't want to

00:15:32.810 --> 00:15:33.810
do.

00:15:34.490 --> 00:15:38.850
It's also to do things that scale that we can't do.

00:15:38.850 --> 00:15:45.690
Like I do not have the ability to run manually a thousand test cases in parallel.

00:15:45.690 --> 00:15:51.300
Like I'm pretty certain you don't either with automation I can, but not manually.

00:15:51.300 --> 00:15:56.180
So there is that risk of anytime we automate, because we can do it at parallel in that

00:15:56.180 --> 00:16:01.660
scale that that we're creating the ability to do something that we can't do manually.

00:16:01.660 --> 00:16:04.340
It's certainly possible with with AI.

00:16:04.340 --> 00:16:11.340
But a lot of it is is going to be based off of I don't need to make these decisions anymore.

00:16:11.340 --> 00:16:16.180
It can it like I'm perfectly fine with something else.

00:16:16.180 --> 00:16:20.540
Making those decisions, the lightweight ones, the ones where you're sitting with your wife

00:16:20.540 --> 00:16:22.180
and they're like, hey, where do you want to go eat?

00:16:22.180 --> 00:16:23.180
Oh, I don't know.

00:16:23.180 --> 00:16:24.180
Where do you want to go eat?

00:16:24.180 --> 00:16:25.180
Like, LLM.

00:16:25.180 --> 00:16:26.700
Tell us where to go eat.

00:16:26.700 --> 00:16:27.700
Great.

00:16:27.700 --> 00:16:28.700
For sure.

00:16:28.700 --> 00:16:29.700
So it's interesting you bring that up.

00:16:29.740 --> 00:16:33.020
I'll talk a little bit more about the adjacent possible here because I was talking to

00:16:33.220 --> 00:16:35.140
someone I forget who.

00:16:35.820 --> 00:16:36.740
It's a long story.

00:16:37.220 --> 00:16:39.220
Tech people, tech people I don't work with.

00:16:39.380 --> 00:16:40.140
I'll leave it there.

00:16:40.260 --> 00:16:41.580
And they asked the question.

00:16:41.620 --> 00:16:45.500
Every person asks me when we talk about tech.

00:16:45.860 --> 00:16:48.500
Alan, what do you think about AI?

00:16:50.240 --> 00:16:51.960
I said, really, do we have all night?

00:16:52.120 --> 00:16:55.200
But what it boils down to is I brought up the adjacent possible.

00:16:55.200 --> 00:17:00.080
I think, you know, everybody is excited about AI and AI is now a buzzword.

00:17:00.280 --> 00:17:04.120
Do you remember when Microsoft put dotnet on the on the end of every project?

00:17:04.400 --> 00:17:05.920
I figure people are doing that with AI.

00:17:05.920 --> 00:17:06.680
Everything's AI.

00:17:06.680 --> 00:17:10.840
It's not probably 90 percent of the things out there now that says powered by

00:17:10.840 --> 00:17:12.840
AI are not powered by AI.

00:17:12.840 --> 00:17:13.840
It's dumb.

00:17:13.840 --> 00:17:16.120
But that wasn't my answer.

00:17:16.120 --> 00:17:19.000
My answer was all the stuff we've talked about.

00:17:19.280 --> 00:17:21.360
Chat GPT is a great example.

00:17:21.360 --> 00:17:24.920
I talk about how I collaborate with it, how it's enabling a lot of things.

00:17:24.920 --> 00:17:29.040
It's really as exciting as chat GPT is.

00:17:29.680 --> 00:17:34.720
What it has done is brought us to and what the adjacent possible is.

00:17:34.720 --> 00:17:38.800
I forget Steven's definition, Steven Johnson, but it's like the adjacent

00:17:38.800 --> 00:17:42.760
possible are the things that are possible to get done at our current

00:17:42.760 --> 00:17:45.040
evolution of tech biology, whatever.

00:17:45.280 --> 00:17:45.520
Right.

00:17:45.520 --> 00:17:53.320
What chat GPT and LLMs and generative AI have done is it's we've taken a step

00:17:53.320 --> 00:17:59.000
forward in what's possible, but I really believe the big invention.

00:17:59.160 --> 00:18:02.480
The things that are really going to go, oh, shit about and go, wow, this is

00:18:02.600 --> 00:18:06.480
amazing. This is accelerating or it's doing A, it's doing B, it's doing C.

00:18:06.960 --> 00:18:10.960
They are things that are going that we haven't thought of yet, but are now

00:18:10.960 --> 00:18:15.280
the new adjacent possible because of the existence of generative AI.

00:18:15.440 --> 00:18:15.800
Right.

00:18:15.800 --> 00:18:17.480
And then and the new forms of AI coming out.

00:18:17.480 --> 00:18:19.360
There's something they say, what are you most excited about?

00:18:19.360 --> 00:18:22.920
I'm excited about the thing I haven't heard about yet that actually builds

00:18:22.920 --> 00:18:25.560
on this and takes us to a brand new place never been before.

00:18:25.880 --> 00:18:27.040
That's what I'm most excited about.

00:18:27.200 --> 00:18:33.000
I am I'll share with you how I'm thinking about it in the AI world.

00:18:33.040 --> 00:18:35.320
Are you familiar with the concept of a centaur?

00:18:36.080 --> 00:18:38.760
Oh, we've talked about this briefly before.

00:18:38.760 --> 00:18:39.280
Yes.

00:18:39.520 --> 00:18:40.880
An old professor.

00:18:41.120 --> 00:18:44.520
So the term was invented by and I forget the guy's name.

00:18:44.920 --> 00:18:45.920
Greek mythology.

00:18:46.640 --> 00:18:51.360
Yeah, that term was but in the context of AI and actually my daughter

00:18:51.360 --> 00:18:55.200
were here, she'd be able to confirm we denied it was actually the Greeks.

00:18:56.200 --> 00:18:56.920
That's what I said.

00:18:56.960 --> 00:18:58.760
I said the Greek, I said Greek mythology.

00:18:59.280 --> 00:18:59.880
Yeah.

00:19:00.400 --> 00:19:03.640
Anyway, they have taken, they have didn't invent the term.

00:19:03.840 --> 00:19:06.560
They've taken the term and applied it in a new way.

00:19:06.960 --> 00:19:07.880
In a new way.

00:19:08.080 --> 00:19:08.400
Right.

00:19:08.840 --> 00:19:12.720
We are all about accuracy on the AB testing podcast.

00:19:13.280 --> 00:19:18.080
I learned from my daughter the other day, the following, and I

00:19:18.080 --> 00:19:19.280
may get it backwards.

00:19:19.320 --> 00:19:20.080
I don't care.

00:19:20.800 --> 00:19:26.960
But I learned that I believe she said the Romans invented the unicorn

00:19:27.240 --> 00:19:30.160
and the Greeks invented the Pegasus.

00:19:30.400 --> 00:19:33.720
And then she's like, I have no idea who came up with the

00:19:33.720 --> 00:19:34.960
alicorn, right?

00:19:34.960 --> 00:19:40.280
But to your point, Steven Johnson, the Jason possible, no one was able to

00:19:40.280 --> 00:19:44.960
invent the alicorn until the unicorn and the Pegasus were invented.

00:19:45.600 --> 00:19:49.600
And for those on the call who have no idea WTF is an alicorn,

00:19:49.840 --> 00:19:51.800
it is a unicorn Pegasus.

00:19:52.600 --> 00:19:55.040
It is a unicorn with wings.

00:19:55.280 --> 00:19:58.360
No, it's a Pegasus with a horn in the middle of his head.

00:20:00.040 --> 00:20:05.320
The age old debate is a zebra black on white or white on black.

00:20:05.360 --> 00:20:05.560
Yeah.

00:20:05.880 --> 00:20:08.560
I clearly see where you stand on that to be.

00:20:08.920 --> 00:20:10.600
Whatever, whatever is the opposite of you, Brent.

00:20:10.800 --> 00:20:14.040
I yeah, like I said, I clearly see you wherever you stay.

00:20:14.240 --> 00:20:14.440
Okay.

00:20:14.440 --> 00:20:15.320
Where were you going?

00:20:15.360 --> 00:20:16.440
Where the hell was I going with the floor?

00:20:16.440 --> 00:20:18.360
Tell me about the centaur.

00:20:18.680 --> 00:20:19.240
Oh, centaur.

00:20:19.280 --> 00:20:19.640
Thank you.

00:20:20.200 --> 00:20:22.400
Welcome to the ADHD podcast.

00:20:22.560 --> 00:20:23.080
I'm Alan.

00:20:23.720 --> 00:20:24.360
Hi, Brett.

00:20:24.920 --> 00:20:26.000
We'll see you next time.

00:20:26.880 --> 00:20:28.560
The centaur was invented.

00:20:29.160 --> 00:20:35.820
The context of use it in this context was invented by the guy who first, the

00:20:35.820 --> 00:20:40.100
chess brand master who got first beat defeated by deep blue, but then came

00:20:40.100 --> 00:20:41.340
back and beat deep blue.

00:20:41.980 --> 00:20:42.980
Is that Kasparov?

00:20:43.420 --> 00:20:44.340
I think it is.

00:20:44.900 --> 00:20:52.580
And what he has discovered is that him with deep blue is basically

00:20:52.580 --> 00:20:54.140
undefeatable, right?

00:20:54.180 --> 00:20:59.740
He calls it a centaur because it's literally man and machine working

00:20:59.740 --> 00:21:01.780
cooperatively together.

00:21:02.340 --> 00:21:02.740
Yeah.

00:21:03.180 --> 00:21:04.540
I'm going to jump in.

00:21:04.540 --> 00:21:08.180
Is that if the man's leading it, it's a centaur.

00:21:08.420 --> 00:21:11.260
But if it's the other way around, it's just a mechanical Turk.

00:21:13.000 --> 00:21:15.960
I never quite understood what a Turk was in that.

00:21:16.000 --> 00:21:19.360
The idea was the mechanical Turk is that it's like this.

00:21:19.400 --> 00:21:23.880
It's like the concierge MVP where you think there's a computer on the

00:21:23.880 --> 00:21:29.200
back end doing stuff, just a human doing it for them is I'm asking is the

00:21:29.200 --> 00:21:33.000
opposite of a centaur, a mechanical Turk where it's machine on the front,

00:21:33.320 --> 00:21:35.440
but there's a human in the back making the decisions.

00:21:35.840 --> 00:21:36.480
It might be.

00:21:36.480 --> 00:21:38.800
Sorry, I like you on a tangent, but I was just thinking out loud.

00:21:38.800 --> 00:21:40.600
In this case, as we do.

00:21:41.120 --> 00:21:48.320
In this case, it's, it's, uh, yeah, the human making the final decision,

00:21:48.320 --> 00:21:50.280
but heavily augmented by the machine.

00:21:50.320 --> 00:21:50.520
Yeah.

00:21:50.600 --> 00:21:51.960
But anyway, I love the idea.

00:21:51.960 --> 00:21:53.240
This is the way I work.

00:21:53.400 --> 00:21:55.320
Generative AI helps me.

00:21:55.800 --> 00:21:59.680
It accelerates me in exactly the way you're describing with chess.

00:22:00.240 --> 00:22:00.560
Right.

00:22:00.960 --> 00:22:05.200
Now, one of the things that, and when I go on my full sort of

00:22:05.200 --> 00:22:10.600
philosophical talk around, oh, one of the things I bring out is.

00:22:11.620 --> 00:22:16.980
There are three personas that I've discovered that an LLM is, and I, and I

00:22:16.980 --> 00:22:22.060
have talked about this to some degree and I basically say a parent, number one,

00:22:22.870 --> 00:22:29.100
number two, a genie, and then the last one that's most important, which is an SME.

00:22:29.620 --> 00:22:29.980
Okay.

00:22:30.180 --> 00:22:33.980
And I find myself sharing this a lot, even with my own team who

00:22:33.980 --> 00:22:39.140
has now heard it multiple times, but it's with a data science team is, I find

00:22:39.140 --> 00:22:43.500
it's really important to share this so that they don't look at, at the LLM

00:22:43.500 --> 00:22:44.660
and go, Oh, it's magic.

00:22:44.980 --> 00:22:46.220
No, it's not magic.

00:22:46.260 --> 00:22:52.860
It's a bunch of cleverly strung together a set of probabilities.

00:22:53.340 --> 00:22:58.020
There's an example there that I might share later where one of my stronger

00:22:58.020 --> 00:23:02.420
data scientists, I walked them through a scenario and, and I said, and then

00:23:02.420 --> 00:23:08.660
I dropped the bomb on him, like to help him understand LLMs better, right?

00:23:09.300 --> 00:23:11.140
I'm like, this is stateless.

00:23:11.540 --> 00:23:13.380
It is a parent, right?

00:23:13.380 --> 00:23:19.300
And then what that essentially means, it is, it, it doesn't know anything.

00:23:19.380 --> 00:23:22.540
Anytime someone says, Oh, it learned this.

00:23:22.540 --> 00:23:25.420
No, it did not learn this.

00:23:25.700 --> 00:23:31.020
Are you familiar with, with the idea of a, of a one-shot prompt?

00:23:31.300 --> 00:23:35.320
No, it makes sense in context, but go ahead and talk through it.

00:23:35.560 --> 00:23:37.560
Hey, let me try it a different way.

00:23:38.280 --> 00:23:44.200
So if you were to go to LLM and give it a prompt, what is one plus one?

00:23:44.880 --> 00:23:45.080
Okay.

00:23:45.080 --> 00:23:47.680
Now today, the LLM will do just fine.

00:23:48.080 --> 00:23:51.200
Uh, when it first came out, it didn't do numbers very well.

00:23:51.360 --> 00:23:51.600
Right.

00:23:51.640 --> 00:23:51.960
Okay.

00:23:52.360 --> 00:23:55.960
And all the people said, well, look, I don't believe in this stuff.

00:23:55.960 --> 00:23:57.200
You can't even do math.

00:23:57.480 --> 00:23:57.800
Right.

00:23:58.240 --> 00:24:03.040
And even though now you go, what is one plus one?

00:24:03.040 --> 00:24:04.840
It'll tell you the answer is two.

00:24:05.360 --> 00:24:08.160
But I will tell you it's not doing math.

00:24:08.680 --> 00:24:11.280
It is 100%.

00:24:11.800 --> 00:24:19.400
Like the model has improved and it knows that the correct character to output,

00:24:20.380 --> 00:24:27.840
given that initial stream of characters is with like five nines probability,

00:24:28.780 --> 00:24:29.540
the number two.

00:24:30.380 --> 00:24:37.250
However, what you can do, let's say you did what is, you know, and you, you

00:24:37.290 --> 00:24:41.730
pound seven random characters on your, your number strip on your keyboard.

00:24:41.730 --> 00:24:45.330
Plus do it again, different random characters.

00:24:45.810 --> 00:24:46.130
Okay.

00:24:46.210 --> 00:24:49.570
And then, uh, and then you hit enter.

00:24:49.570 --> 00:24:50.730
It'll probably get that wrong.

00:24:51.250 --> 00:24:54.050
Every time I do this example, I often have to come up with a

00:24:54.050 --> 00:24:57.090
different set of random numbers, but I can get it to get it wrong.

00:24:57.490 --> 00:25:03.610
However, if you do that same thing and then follow it up with the prompt example,

00:25:04.170 --> 00:25:10.910
one plus one equals two, by doing that extra string, you kind of prune down

00:25:10.910 --> 00:25:17.140
the probabilistic paths in the neural net, the backs of the LLM into one that

00:25:17.140 --> 00:25:21.540
is far more likely to be correct because there aren't so many, um,

00:25:22.380 --> 00:25:24.420
possibilities for it to spread out.

00:25:25.100 --> 00:25:32.430
Uh, even at those very small probabilities, it will get things wrong

00:25:32.590 --> 00:25:37.110
and from our perspective or perception, but it's a parent, it doesn't know anything.

00:25:37.150 --> 00:25:41.950
It's just really good at simulating the correct response.

00:25:42.270 --> 00:25:48.830
Now, when I say a genie, a genie means a genie historically, like if, if you,

00:25:48.830 --> 00:25:53.670
uh, if you found an Aladdin's bottle and you asked it to be, um, you asked

00:25:53.670 --> 00:25:55.310
to be a world-class swimmer, right?

00:25:55.350 --> 00:26:01.230
The problem is genies are historically evil and it will fulfill your

00:26:01.230 --> 00:26:03.510
wish by turning you into a shark, right?

00:26:03.510 --> 00:26:05.230
You're a world-class swimmer.

00:26:05.510 --> 00:26:09.030
You know, I grew up watching reruns of I Dream of Genie and

00:26:09.030 --> 00:26:10.350
that genie was very nice.

00:26:11.100 --> 00:26:12.580
Uh, that was made for TV.

00:26:13.060 --> 00:26:14.220
Oh, yeah.

00:26:15.940 --> 00:26:16.900
What, that wasn't real?

00:26:17.220 --> 00:26:18.660
No, no, no.

00:26:18.740 --> 00:26:19.380
Oh, weird.

00:26:19.380 --> 00:26:21.060
Not, not historically accurate.

00:26:21.100 --> 00:26:21.380
Okay.

00:26:21.380 --> 00:26:21.740
All right.

00:26:21.740 --> 00:26:21.980
Go on.

00:26:21.980 --> 00:26:24.380
I just, I, I, I'm mind blown today.

00:26:24.380 --> 00:26:24.740
I learned.

00:26:25.020 --> 00:26:25.340
Yeah.

00:26:25.380 --> 00:26:30.740
So the way you, you battle the genie, this is like, if you, if you ever

00:26:30.740 --> 00:26:36.220
do encounter a, a, a, a genie in a lamp, the way to, the way to battle it,

00:26:36.220 --> 00:26:40.180
when you do your wishes, you need to make sure they are so specific.

00:26:40.740 --> 00:26:45.700
The genie has only the right way to grant your wish, like the way

00:26:45.700 --> 00:26:47.700
you want it to be granted.

00:26:48.100 --> 00:26:51.860
And that, that's kind of the issue with, with LLLMs.

00:26:52.140 --> 00:26:53.620
Like there is a risk.

00:26:53.620 --> 00:26:59.100
If you write a prompt and it is in any way, shape, or form, ambiguous,

00:26:59.220 --> 00:27:01.180
there is a risk that's going to go sideways.

00:27:01.980 --> 00:27:02.220
Right.

00:27:02.460 --> 00:27:06.460
I'll give you an example for the, for the community here.

00:27:06.980 --> 00:27:08.380
You give it a scenario.

00:27:08.460 --> 00:27:12.380
Let's say you write up a narrative, a bug report or whatever, and you ask it,

00:27:12.420 --> 00:27:14.180
is this a bug or is it by design?

00:27:14.740 --> 00:27:18.900
Both of those are kind of philosophical.

00:27:19.580 --> 00:27:19.820
Right.

00:27:19.900 --> 00:27:28.740
And the definition of bug or by design is subjective and it will very often go

00:27:28.740 --> 00:27:35.340
sideways. And one of my favorite examples is, Hey, if the product, let's say it's a

00:27:35.340 --> 00:27:44.060
service fails to integrate with another service that only began its existence

00:27:44.060 --> 00:27:47.420
after the first service was released.

00:27:48.100 --> 00:27:50.020
Is that a bug or is it by design?

00:27:50.580 --> 00:27:53.900
Well, it wasn't designed for it because the new service didn't exist.

00:27:54.930 --> 00:28:02.480
So it can't be by design, but then the product is still working as it was

00:28:02.480 --> 00:28:04.360
intended. So it's not a bug.

00:28:05.040 --> 00:28:09.800
And if you ask the LLM, you had to pick one of those, but it will roll the dice.

00:28:09.800 --> 00:28:12.320
You rerun it and it'll pick a different one each time.

00:28:13.320 --> 00:28:15.720
I know this painfully from, from example.

00:28:15.880 --> 00:28:19.680
Now, assuming you can battle the parent and the, and the genie.

00:28:20.000 --> 00:28:23.920
The last person on the LLM is, is an SME.

00:28:24.590 --> 00:28:29.710
It's a subject matter expert, or rather it can simulate the knowledge.

00:28:29.750 --> 00:28:31.950
I was going to say, yes, it can act like one.

00:28:31.950 --> 00:28:32.550
It isn't one.

00:28:32.550 --> 00:28:36.790
It knows nothing as we've discussed, but it can fake it super well.

00:28:37.190 --> 00:28:42.030
And it can, I would say, well, actually I don't need to say, cause when

00:28:42.030 --> 00:28:44.790
LLMs came out, there was all sorts of elements on it.

00:28:44.790 --> 00:28:47.350
Like LLMs can pass the freaking bar.

00:28:48.140 --> 00:28:48.380
Right.

00:28:48.420 --> 00:28:48.740
Right.

00:28:48.860 --> 00:28:56.340
LLHBS can at some point in time, it is so good at simulating as being an SME.

00:28:56.660 --> 00:28:58.780
You might as well just call it an SME.

00:28:59.420 --> 00:29:03.100
Now the challenge is figuring out when it's gone sideways.

00:29:03.880 --> 00:29:04.160
Right.

00:29:04.160 --> 00:29:12.680
But I argue that's, that's an equivalent challenge, um, with a regular SME.

00:29:13.000 --> 00:29:13.800
Sure.

00:29:14.200 --> 00:29:14.520
Right.

00:29:15.160 --> 00:29:20.240
Alan has talked about on the podcast, like I remember, like I enjoyed this story.

00:29:20.240 --> 00:29:23.440
Like you know, you knew shit about A-B testing.

00:29:23.560 --> 00:29:28.600
You were asked to write a presentation on it and I don't know, like three hours or

00:29:28.600 --> 00:29:32.960
something, I think it was like three weeks, but, and you're like, yeah, sure.

00:29:33.680 --> 00:29:41.880
You, you did your research enough to, to confidently fake that you were an expert

00:29:41.920 --> 00:29:42.560
on A-B testing.

00:29:42.560 --> 00:29:43.440
Absolutely.

00:29:43.640 --> 00:29:45.400
And that's been the history of my career.

00:29:45.400 --> 00:29:50.200
I can, I am confident now after, you know, going through a hard way a few times,

00:29:50.200 --> 00:29:55.680
but there are a few limits on what I can do given enough time.

00:29:56.360 --> 00:29:58.840
Cause I, I can suck in knowledge and remember things.

00:29:58.840 --> 00:29:59.800
That's my superpower.

00:30:00.040 --> 00:30:03.960
I somehow learn things quickly and find a way to conceptualize them.

00:30:04.590 --> 00:30:05.630
Now what Chad G.

00:30:05.630 --> 00:30:08.190
Pahita, it accelerates my ability to do that.

00:30:08.350 --> 00:30:12.470
Where before, like I have to make big batches of brain soup to learn things.

00:30:12.470 --> 00:30:16.830
I had to look at 30 articles on experimentation and statistical significance

00:30:17.110 --> 00:30:21.830
in order to understand how A-B experiments work and understand just the,

00:30:21.870 --> 00:30:23.030
the gist behind them.

00:30:23.030 --> 00:30:28.590
And I got out Google analytics and learn how to implement that in, in Google

00:30:28.590 --> 00:30:32.470
analytics and, but I had to do all that and just kind of let it sit there for a

00:30:32.470 --> 00:30:34.350
while and then the soup came out.

00:30:34.350 --> 00:30:34.710
Okay.

00:30:34.870 --> 00:30:36.470
This is how I think it works.

00:30:36.470 --> 00:30:38.910
Well, now I can get a lot faster.

00:30:39.310 --> 00:30:40.670
I can go to chat GPT right now.

00:30:40.670 --> 00:30:45.110
It says, give me, give me three simple examples to explain, uh,

00:30:45.110 --> 00:30:46.270
statistical significance.

00:30:46.590 --> 00:30:51.750
Now I don't have to go like in the past 10 years ago when I gave that talk, I

00:30:51.750 --> 00:30:54.030
had to go read a whole bunch of stuff.

00:30:54.030 --> 00:30:56.790
Like they say the best way to learn something is try and teach it.

00:30:57.150 --> 00:30:57.390
Yep.

00:30:57.670 --> 00:31:01.030
And like now I can learn it faster.

00:31:01.030 --> 00:31:04.830
Cause I focus on learning with the goal of teaching, uh, tell me stuff.

00:31:04.830 --> 00:31:07.950
Chat GPT, genie, parrot, S and E tell me stuff.

00:31:07.950 --> 00:31:09.990
So I can pretend like I know it.

00:31:10.470 --> 00:31:10.870
Right.

00:31:11.110 --> 00:31:11.430
Right.

00:31:11.470 --> 00:31:18.590
And of course the one thing in that particular example, even, even if GPT

00:31:19.190 --> 00:31:22.600
is making things up, right, which is the risk, right?

00:31:22.600 --> 00:31:25.240
Cause you don't know you're asking it to teach you something.

00:31:25.240 --> 00:31:27.600
So you don't know if it's making shit up or not.

00:31:27.640 --> 00:31:27.760
Yeah.

00:31:27.760 --> 00:31:29.000
But I have ways of checking that.

00:31:29.000 --> 00:31:30.440
I trust, but verify.

00:31:30.480 --> 00:31:31.760
No, but even then, right.

00:31:32.120 --> 00:31:33.480
We we've talked about it before.

00:31:33.480 --> 00:31:37.560
GPT is really good at bullshitting.

00:31:38.270 --> 00:31:38.510
Right.

00:31:38.510 --> 00:31:44.590
It would be really hard for, unless you are asking it, like if you're asking it,

00:31:44.590 --> 00:31:53.070
something's, uh, philosophical or subjective where you're, you're basically

00:31:53.070 --> 00:31:59.540
avoiding it, mentioning facts, things that can't be fact checked, right?

00:31:59.940 --> 00:32:01.020
It's going to be fine.

00:32:01.460 --> 00:32:03.060
No, it's wonderful.

00:32:03.060 --> 00:32:04.020
It's better than fine.

00:32:04.020 --> 00:32:04.500
It's great.

00:32:04.860 --> 00:32:05.180
Right.

00:32:05.180 --> 00:32:10.540
So what you have realized is if we go back to my little knowledge bubble,

00:32:10.540 --> 00:32:15.420
it's inside of you, but you're like, Oh, my knowledge bubble, as you just

00:32:15.420 --> 00:32:16.940
called out is pretty awesome.

00:32:16.980 --> 00:32:20.780
You are able to puff air into your knowledge bubble really fast.

00:32:21.020 --> 00:32:24.900
Chat GPT is a supercharged air compressor blowing into my bubble.

00:32:25.020 --> 00:32:28.580
That's one thing, but it is also its own bubble.

00:32:29.060 --> 00:32:32.660
That's that you have direct access to.

00:32:33.340 --> 00:32:35.340
Like so many bubbles.

00:32:35.580 --> 00:32:38.060
When we talk about the centaur, right?

00:32:38.060 --> 00:32:40.500
And, and I forgot we were talking about centaur.

00:32:40.580 --> 00:32:41.060
That's awesome.

00:32:42.380 --> 00:32:45.700
You actually forgot what we were really talking about, which is LLNOps.

00:32:45.740 --> 00:32:47.420
I'm tying it back.

00:32:47.500 --> 00:32:48.420
Oh my God.

00:32:48.420 --> 00:32:49.020
That's right.

00:32:49.020 --> 00:32:50.100
That was like a week ago.

00:32:50.180 --> 00:32:50.820
Keep going.

00:32:51.260 --> 00:32:51.660
All right.

00:32:52.020 --> 00:33:01.180
So if we, if we agree that AI plus humans outperform either of those

00:33:01.180 --> 00:33:04.340
components on their own, like AI plus you, okay.

00:33:04.460 --> 00:33:05.900
Nobody can disagree with that.

00:33:05.940 --> 00:33:06.340
Go on.

00:33:06.700 --> 00:33:16.100
And then we agree that in certain contexts, LLM is a, is a equivalent or perhaps

00:33:16.340 --> 00:33:18.620
better SME than the human.

00:33:19.340 --> 00:33:19.740
Mm-hmm.

00:33:19.980 --> 00:33:21.740
Then I go, okay.

00:33:21.740 --> 00:33:28.700
What can AI plus LLM be particularly in the application of ops?

00:33:29.180 --> 00:33:30.980
Because, you know, this is AB testing.

00:33:31.500 --> 00:33:32.620
Is that a rhetorical question?

00:33:32.620 --> 00:33:33.300
Do you have an answer?

00:33:33.580 --> 00:33:38.140
No, it's, it's, it, that's, that's rhetorical.

00:33:38.500 --> 00:33:40.420
I'm going to me.

00:33:41.090 --> 00:33:44.010
That feels like an adjacent possible.

00:33:44.330 --> 00:33:46.050
Free inventions right here.

00:33:47.730 --> 00:33:49.250
I got to, we're almost out of time here.

00:33:49.250 --> 00:33:53.130
I got to tell you one thing going back half a story is one of my

00:33:53.130 --> 00:33:55.010
favorite moments in life.

00:33:55.530 --> 00:33:59.650
So Brent has like a master's degree in data bullshit.

00:33:59.770 --> 00:34:00.050
Right.

00:34:00.090 --> 00:34:06.290
That the actual degree it's the time when in the middle of a podcast, I pulled

00:34:06.330 --> 00:34:10.010
out a statistical term and used it correctly.

00:34:10.250 --> 00:34:13.690
The look on Brent's face like, yeah, I can fake it till I'm in.

00:34:13.690 --> 00:34:21.020
I was just like, I don't, I don't remember my face.

00:34:21.020 --> 00:34:22.940
I remember, I remember the conversation.

00:34:23.540 --> 00:34:24.820
I don't remember my face.

00:34:24.820 --> 00:34:26.660
And I'm like, okay, which one did I do?

00:34:26.660 --> 00:34:30.020
Was it the holy shit or yeah.

00:34:30.840 --> 00:34:35.000
No, I think it was just to wonder, just to wonder like, where did he learn

00:34:35.000 --> 00:34:36.280
that word and how to use it?

00:34:36.280 --> 00:34:39.360
No, I've been aware of your superpower here for a long time.

00:34:39.360 --> 00:34:41.760
I was like, and I'm not going to confuse myself.

00:34:41.800 --> 00:34:44.200
It is absolutely one of Alan's superpowers.

00:34:44.640 --> 00:34:48.960
And the other one of yours that I'm jealous with is, is on writing.

00:34:49.440 --> 00:34:55.580
We're both INTPs, but apparently that doesn't come, come with like the

00:34:55.580 --> 00:35:00.260
ability to actually just sit down and write shit and then be done in 20 minutes.

00:35:00.500 --> 00:35:02.340
Yeah, but it's different now.

00:35:02.340 --> 00:35:06.580
I sit down, it takes me more than 20, but I write, I have written a blog post

00:35:06.700 --> 00:35:10.580
every week for the last, now I'm very last over a year now.

00:35:11.180 --> 00:35:16.700
And these days I write my post and then I paste it into a chat GPT and say,

00:35:16.740 --> 00:35:17.820
and ask it for feedback.

00:35:17.900 --> 00:35:23.220
And literally I say any feedback on, on this article and maybe I'll put

00:35:23.220 --> 00:35:24.860
some context, but not really paste it in.

00:35:25.340 --> 00:35:27.820
It gives me like 10 bullet points.

00:35:27.820 --> 00:35:31.580
I usually ignore about eight of them and two of them are like, Oh, actually

00:35:31.580 --> 00:35:33.180
yeah, I could do a better segue there.

00:35:33.260 --> 00:35:36.020
It is a cheap and quick editor.

00:35:36.860 --> 00:35:37.620
I paste it in there.

00:35:37.620 --> 00:35:40.340
It's never, it never like hacks up a red line stuff.

00:35:40.340 --> 00:35:44.660
But it gives some basic, like some of the tips it gives are the same every single

00:35:44.660 --> 00:35:49.220
week, but it can say like it lets me know if I have, I always look for a mix

00:35:49.220 --> 00:35:52.460
between anecdotes and like, that's the way I write.

00:35:52.780 --> 00:35:55.180
I have stories I want to tell from experiences.

00:35:55.180 --> 00:35:58.220
I have books I want to refer to because I want people to know that

00:35:58.420 --> 00:36:00.060
I have no ideas of my own.

00:36:00.500 --> 00:36:01.300
I am really good.

00:36:01.300 --> 00:36:06.980
Again, it's using these superpowers are related because I use the brain soup

00:36:06.980 --> 00:36:10.620
I get from reading like 50 gazillion books and I let them regurgitate.

00:36:10.620 --> 00:36:11.980
And I go, Oh, wait a minute.

00:36:11.980 --> 00:36:13.660
This came up in a book and I figure out what it was anyway.

00:36:14.460 --> 00:36:16.540
Try GPT is my, is my quick and dirty editor.

00:36:16.540 --> 00:36:17.140
It makes me better.

00:36:17.180 --> 00:36:20.420
Going all the way back to the beginning of the episode.

00:36:20.700 --> 00:36:21.620
Oh my God.

00:36:21.620 --> 00:36:22.140
We're good.

00:36:22.180 --> 00:36:22.500
Yeah.

00:36:22.540 --> 00:36:28.060
I am, I am absolutely impressed at my hit rate of being able to remember the

00:36:28.060 --> 00:36:33.990
tangent, uh, usually I get lost and can't find my way back, but on CICD.

00:36:34.590 --> 00:36:34.950
Right.

00:36:35.150 --> 00:36:35.470
Yeah.

00:36:35.470 --> 00:36:42.430
I, I will actually fully argue that that was the main point of the Phoenix project.

00:36:42.910 --> 00:36:43.110
Right.

00:36:43.110 --> 00:36:47.590
If you think about it, if you read that story, you think about how

00:36:48.070 --> 00:36:50.030
the world frigging changed.

00:36:50.730 --> 00:36:52.850
It was because they deployed CICD.

00:36:53.290 --> 00:36:53.530
Yeah.

00:36:53.810 --> 00:36:54.050
Right.

00:36:54.050 --> 00:36:57.970
It, it, it isn't what they built, but how they built it.

00:36:58.370 --> 00:36:59.490
They tried to move faster.

00:36:59.690 --> 00:37:04.330
Uh, there's a whole other blog poster, blog post, whole other podcast here.

00:37:04.330 --> 00:37:05.130
And we'll get to it next time.

00:37:05.130 --> 00:37:06.410
It's actually what I was going to get to.

00:37:06.410 --> 00:37:08.890
This was better, but it will be, other one will be good next week.

00:37:08.890 --> 00:37:10.290
Just, just don't worry about it in two weeks.

00:37:11.080 --> 00:37:14.360
Going fast highlights where your bottlenecks are.

00:37:14.360 --> 00:37:16.440
If you go slow, you never see the bottlenecks.

00:37:17.240 --> 00:37:17.920
You'll never see them.

00:37:18.560 --> 00:37:19.240
They don't exist.

00:37:19.400 --> 00:37:19.800
Yeah.

00:37:20.360 --> 00:37:22.280
You don't even know you're numb to them.

00:37:22.280 --> 00:37:23.200
They just don't happen.

00:37:24.030 --> 00:37:24.750
Move faster.

00:37:24.750 --> 00:37:26.030
Those little bumps get in the way.

00:37:26.070 --> 00:37:32.150
Well, in any end, if you're, well, so one of the important blessings that I did

00:37:32.230 --> 00:37:35.070
back in the days when I was actively doing agile coaching.

00:37:35.390 --> 00:37:39.990
Back in the day, everyone thought it was purely about moving fast and

00:37:40.270 --> 00:37:46.060
moving fast is very important, but the correct statement is adapt fast.

00:37:46.380 --> 00:37:46.940
Absolutely.

00:37:46.980 --> 00:37:47.780
We've talked about that.

00:37:47.780 --> 00:37:49.900
There's a difference between iteration and adapting.

00:37:49.900 --> 00:37:54.020
A lot of teams who fuck up agile do it because they focus so much on

00:37:54.060 --> 00:37:56.140
iterating and not on adapting.

00:37:56.140 --> 00:38:01.940
And CICD is so important because you're not waiting to integrate with main.

00:38:01.940 --> 00:38:03.300
You're not waiting.

00:38:03.340 --> 00:38:04.220
Feedback loops.

00:38:04.220 --> 00:38:11.500
You get the feedback instantly and you're continuously improving main, right?

00:38:11.540 --> 00:38:14.740
Um, which has a dramatic reduction in risk.

00:38:15.420 --> 00:38:15.700
Yeah.

00:38:15.900 --> 00:38:18.020
Um, one, one gazillion percent.

00:38:19.240 --> 00:38:19.640
Agreed.

00:38:19.920 --> 00:38:20.760
If that were possible.

00:38:21.660 --> 00:38:22.420
I think it is.

00:38:22.900 --> 00:38:23.340
Why not?

00:38:23.740 --> 00:38:25.740
Two Brazilian, two Brazilian percent.

00:38:27.020 --> 00:38:28.180
A Brazilian.

00:38:28.540 --> 00:38:28.820
Yeah.

00:38:29.680 --> 00:38:31.240
How much that that's like a law, right?

00:38:31.880 --> 00:38:35.380
Uh, uh, well, no, that's like the person who lives in, in a

00:38:35.460 --> 00:38:36.620
country in South America.

00:38:37.820 --> 00:38:38.980
Oh, weird.

00:38:39.260 --> 00:38:39.860
Yeah.

00:38:41.020 --> 00:38:41.380
Okay.

00:38:41.500 --> 00:38:42.300
Well, thanks.

00:38:42.540 --> 00:38:46.100
This has been the, um, the dad joke portion of the AB testing podcast.

00:38:46.100 --> 00:38:47.260
Really appreciate you coming by.

00:38:47.460 --> 00:38:48.140
We'll be here all week.

00:38:49.180 --> 00:38:50.420
Let's call it a day.

00:38:50.420 --> 00:38:53.300
I got, this is cool because now I have a topic queued up for next

00:38:53.300 --> 00:38:54.780
time when we talk in two weeks.

00:38:55.060 --> 00:38:56.940
Yippee-ki-yay mother.

00:38:56.940 --> 00:38:57.660
This is Alan.

00:38:57.820 --> 00:38:58.660
Yes, this is Brent.

00:38:58.820 --> 00:39:00.220
And we'll see you next time.

