To me, that's how you approach understanding and acquiring knowledge is one way is when something causes you cognitive dissonance, you don't just chuck it away and call it bullshit. You try and figure out where that idea is coming from and what could make it true for this other person in their context. Welcome to AB testing podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now on with the show. Good afternoon, everybody. I'm Alan. And it is time for the AB testing podcast and happy new year. Holy crap. Happy new year, Brent. Happy new year to you, Alan. Have you enjoyed 2023 thus far? Yeah, 2023 has been good. Yeah, we can get into my 2023. It's been actually really good so far. You know, nothing exciting, nothing new calendar. I wrote my five for Friday today. I remember to put the right year on there. So I'm getting you after, you know, 57 years on the planet. I've learned not all of those writing, but I've learned to I've learned that the year changes once every 365 days or so. That's my big accomplishment. I don't know how's your new year so far. You're 57. Man, you're old. Oh, I'm practically dead. Practically dead. No, I knew you were older than me, but I didn't know that much. No, I turned 57 in December. I think yes, I did. You did. Yeah. Yeah. It's easy to do the math now because, you know, my daughter is a teenager who is almost exactly 40 years younger than me. If I forget how old I am, which is when you get this old, you do sometimes have to think about what year is it and do the math. I just have to remember, I know how old my daughter is. So I just add 40 to that generally works up for nine days out of the year that works. That actually works for me and my daughter as well. Her birthday is New Year's Eve. She just, the Jensen family household successfully added another teenager to society on New Year's Eve 2022. So she's now 13. And in just a couple of months, I turned 53. There you go. The math that's 40. Yeah. There's your trick for the year. Yeah. So episode 172, I'm hoping, I'm going to work backward here. I'm hoping we have a few new listeners this time because I want to talk about something I did to close out 2022. Yeah, your clickbait post. No. So I must give context and tell a story. I'm sure our listeners have seen this and rolled their eyes and done their thing. But I'm hoping there's some people that I made mad. We can really go back to the beginning here because oh my God, I didn't. Let me tell you what happened. I was in Toronto for New Year's Eve with my son. We're up to Toronto for the weekend. Had a blast. He was like sleeping in the afternoon as 19 year olds do. As a lot of our readers know, I used to blog a lot. I stopped blogging. I have started blogging again. And I did a post a couple of weeks ago where I said I'm just kind of talking about like what I've been doing. I haven't really done testing in a long time. I know a lot about testing. I care far more about quality than I do about testing these days, which apparently is a minority opinion. But I made a comment in there about something like the usual thing we've been saying on this podcast for seven years. Most software teams do not. You'll say all here. I will say most software teams do not need dedicated testers. And then someone on LinkedIn said, can you elaborate on that one? I said, great, I'll do a post on that. So I did. I sat there on my son was sleeping. And let me explain my writing process. Do you understand the amount of editing and structure and content and storyboards I do for my blog posts? I sit down, I write them stream of consciousness and I press post. That is my, I will read through it once for edit. So there was, but it's really, if you listen to this podcast, there's nothing new. And if you remember back to the origins of the AB testing podcast, seven years ago, eight years ago, almost eight years ago in April, right? Yeah, Brent and I were noticing things changing and testing and, and how software was delivered. And we have not once we don't go out and preach like this is the way it should be. We're telling you what we're seeing. More than that, we're telling you what we're seeing. Number one, number two, in many of the things where we're seeing it, we've experienced it. And in both of our cases, we're like, oh my God is usually when you when you think that the grass is greener on the other side of the hill, you get there and you go, no, no, it's just as green or maybe, you know, less green. But this was one where certainly my opinion is, and I think yours as well, that the grass was definitely greener on the other side of the hill. Yeah. Maybe, maybe too metaphorical. But like, I, I remember my, my blog post at that time, right, this is the time where you were still here at Microsoft when I made the move, but when I moved into Bing and officially, you know, sold out and joined the enemy and became a dev. And with all my perspective on quality, like I remember it like yesterday, like, oh my God, I am now actually delivering quality, actually now. Whereas before I had told myself over and over again, that that I was delivering quality like I had Jesus, nearly 15 years of reviews where I was required to articulate how me and my team delivered quality. But then I realized just after a few months in that team, oh my God, I was so freaking clueless. It's the order of ignorance that got into play for me. You don't know what you don't know. And then suddenly I was in an environment where I learned what I didn't actually know because I was so spun up in this, in the Emperor's fancy new clothes that I had believed that I was wearing. Yeah. Also, you know, we've seen, and we watched this happen at Microsoft and we saw this happen in the industry. We watched lots of teams make this move to not have dedicated testers on the team and really, really screw it up. And we wanted to recognize what are the flavors of that? What are we seeing? Blah, blah, blah. So we can try and help people, just trying to help people navigate. So I wrote, I'll summarize the article. There was, I made the statement and I explained. And as a reminder, before you continue on, as a reminder, I will remind you, because I have read the, what you're talking about, I've read the comments. And I will remind you that we have a large number of people that have already thanked us for the transition. So we have, don't let the haters bring you down, man. No, no, no, I'm not. I'm not. But what's really funny, what I, as a couple of things I find funny about this experience. And of course I didn't write it as clickbait. I wrote it because someone asked me to and it makes sense. I was just kidding. Giving you, I know, I know, I know. I know. I know. I know. I know. Okay. What I find really funny is in the original, I included a paragraph that says something to the effect of when I make this statement, some people say, yep, that makes sense. We do that. And some other people say, it's the dumbest thing they've ever heard. And I'm harming the craft of testing and sure enough, predicting the future. I was right. So the article in summary and sorry. And the thing is, I don't know about you. I don't remember you ever seen this. When, when in particular, Agile first came out, right? I was right there with that crowd. 100%. I'm like, wait, you want to do what? I was still a test manager and they were saying, yeah, we're going to skip code and we're not going to document. And what I interpreted it as is, okay, my peer in dev wants me to still be accountable for, for quality, still be accountable for achieving the timelines. But what he's proposing to me with a very serious look on his face is, uh, he and his team is going to do whatever they want, whenever they want. And we should just adapt to it because that's Agile. And I'm like, Oh, this is, this is going to go really bad. But hindsight, 2020 was at that point in time because I had just begun that journey. I had not learned how to do it in that model. Right. We were so reliant on, on the structure and the separation of concerns between individuals. Anyway, it's too early for me. You're still in the, the setting the stage. So I'm not going to go on. Yeah. Yeah. Yeah. Thanks man. I'm gonna have to chop this up, put it in the right order. It's like that. Have you heard about the series kaleidoscope on Netflix? It's like eight episodes and you can watch them in any order you want. Okay. That's I'll do that with the podcast. Um, so anyway, I talked about, like to me, I think there's two things that have to happen and they're complex things. I didn't go super deep, but I explained them well enough. I have a strong feeling. A lot of people who left comments on LinkedIn did not actually read the whole post because they just ignore the, I mean, I don't, are they skim departure? I don't know. So to me, I think two things have to happen. There's probably a lot more things, two big things though, to not need dedicated QA on your team. There was one person who said, well, they talked about all the other things they do besides QA. If I wasn't there, they wouldn't get done. And I said like, you're not, you're not doing dedicated QA now. I don't get it. Uh, one is the developers have to know how to test and I have given talks on this so many talks. I have now taught hundreds of teams and thousands of developers how to test. And you know, my experience, I can only tell you my experience. My experience is that developers can become very good testers, very, very good testers. And honestly, when they own, I mean, you know, from Forsgrind that when developers own the automation, there's a correlation with quality that does not exist when your testers write the automation and people say, Oh, they, they, they can't just write automation. What developers will do once they own the automation and once they know a little bit more about testing is they design far more testable code. They build, they go, they thinking about how they're going to test it because they have to do it. It, it works. I've seen it work with over a thousand developers. I know, I know they can learn and I state my experience and people say, I disagree. Developers can't test said, how can you disagree? You could say my experience is different. So anyway, so that's the first step developers have to be able to test. You can coach, you can help them, but if you don't want to have dedicated testers, you need developers to own the testing. The second thing you need before you interrupt there and go back to the beginning. Actually, do you want to comment on that one before I go on? Yeah, it's, I can tell you look like either that or your, or you move to the bathroom. It looks like you have to take a big old dump. Brett changed his camera angle. I was like, like, yeah, it was briefly considering showing you what that face looked like. Um, the, no, you should continue the dump comment. Maybe one, maybe flush my whole comment. One flush your whole comment. One devs have to be able, devs can test. They need to own testing. That's it. That's the part of it. But a lot of people stopped there and thought, well, my devs don't want to test or they aren't a good, they're not as good at it. Like there's another half to the damn post you didn't read. The other thing is you need fast feedback loops. You need the fast feedback loops you get from understanding customer data on how they're using your software. The reason that it is more inefficient to have dedicated testers because they're getting in the way of a fast feedback loop. Brent's drawing on the whiteboard, but I'll see what it is in a minute. I don't have paper. I'm just making notes. I'm trying to let you talk. Okay. Okay. All right. I got, I got, I'm gonna stop looking at the video. Shouldn't do that. This is a, yeah. So fast feedback loops, which come from data and people didn't read that part or they read that and said, you're making the customers do all the testing. No. And what really pushes my buttons on this is the group digging their nails in. And again, welcome to the conversation. I am so glad some of you were so mad and think I'm stupid because that's what I expected. Uh, but in my opinion, the lack of critical thinking from some of these people, like I'm telling you, I have seen this work and I've seen this work for years and years and years and you jump in and say, no, it doesn't work. It, it makes no sense to me. Like when I, when I hear something I don't agree with, like when I think something stupid, I don't immediately dismiss it. I will take some time to try and figure out, okay, somebody believes this in what situations could this be true? To me, that's how you approach understanding and acquiring knowledge is, or one way is when something causes you cognitive dissonance, you don't just chuck it away and call it bullshit. You try and figure out where that idea is coming from and what could make it true for this other person in their context. And that was missing from a whole bunch of the haters. And I just, I don't get it. Were they so, maybe they're just so, so mad. They threw out, they just went into fight or flight brain thing and they just were mad and they wanted to lash out versus actually apply some thinking to what I was saying. So that's the part that kind of bummed me out a little bit. Now, and that for sure happened. The other thing that I think happened here, and I'm not certain how it, how it did, right? But there's something that I call, uh, Jensen's law of politics and that is he who is defensive loses always somehow in all of this, you are, are being put into place where you had to defend your point of view, right? In my view, I'm just like, I'm now laughing it off when someone says, okay, well, Dev can't test. I'm like, yeah, you know what, dude, prove to me they can't test because I now have 11 years of experience that is a hundred percent contradictory to what you're just saying. And if you say, well, Microsoft ship, well, great. But you know, me and my stock price does not align with that statement. Right. But my company is providing value to the customer. Go back to principle five, where a lot of the same folks mad at me for what I wrote. I did not mention the modern testing principles directly, but I do believe that only the customer could evaluate quality. Absolutely. And here's another one that I think we might want to take on head on. Okay. Ellen, you're just making the customer test. Okay. Here's my response. Yes. So fucking what? Right. Well, they don't know what it means. I mean, number, number one, number one, as you know, we're not, but here's the thing. That's that argument leads down to some defensive path. I'm like, look, I'm not going to play semantic battles with you. So what if we are? What's the harm? Right. Google, Google got to number one because guess what? They had their customers test and they found looking at the data streams that, oh, we have a bug here. And what they optimize for is the minimization of the time that customers in pain. Yes. They optimize for, okay. Once we, we have to have the engine in place to identify where we're, where we are having negative impact to the customer. Number one, we have to have that quick and fast. And then once we identify those, we have to create and a fast engine to get repairs out. And over and over again, you find a bug in Google's assets, right? Come back two later, try two weeks later, try that case again. Oh, not even two. Sometimes more likely two minutes or two hours. Not two minutes, two. All right. Depends on what you're doing. Because it could be an auto rollback, but you said it quick and fast. How quickly can we understand if customers are successful with this thing? How quickly can we understand if customers are hitting an error case? I don't think people really grasp the power of these fast feedback loops, which have been talked about. I don't know when the DevOps handbook came out. That was somewhere 10 years ago. I have a copy. I'll go look it up in a second. But I will tell you, I will tell you, just straight up from my own experience, it wasn't until when I went to Dev and it wasn't until I had been doing Kanban with sort of continuous releases and had put my whole service on the floor that I learned, oh, oh, oh, right? Because with a scrum or a waterfall model, you put everything on the floor, everything's dead. Like when I first did scrum, we did a big, complicated scrum, took two weeks, screwed the whole pooch. We completely failed on integration. It took us three months to undo the two weeks worth of work that we had done. But with Kanban, we put everything on the floor and guess what? Revert one change. Hey, everything's back online. When I realized the power of that and experienced it, I'm like, Oh my God. Okay, that is why they say this. Yeah, I'm with you. It is super powerful when you when you stop trying to do it theoretically. And even even more so stop trying to do it vicariously. It's give it an honest try. And you're gonna I truly believe you give it an honest try in your, I don't know, 80% chance to immediately align with us. One thing I did, I think I hinted at pretty well, you know, I don't go back and read my read it. I write it, I read it once again, make some edits, and then I post it on look at it again. So one thing I think I made, I did add a few edits afterwards for clarification. But one point I think I discovered, I want to get your opinion on, because I actually haven't said this before explicitly. In the post, I brought up like maybe medical equipment or NASA, maybe they needed a dedicated tester. But it wasn't really because I think a lot of things that you do with medical devices, and a lot of things you do with rocket ships, do not need dedicated testers, but depending on simulations, etc. But my hypothesis is the point I brought up is that it comes down to feedback loops. When you and I first started at Microsoft are, you know, we did betas, but things shipped very, very slowly. Yes. Years. Yes. The slower your feedback loop is, I think the more need you have for a dedicated tester, because you don't have a pro... Here's the proxy there trying to cover up for the fact you have a crummy feedback loop. No, since that... Oh my God, that's 100% correct. The test team, number one, I'm going to stand by the number one value proposition of a test team. Hopefully, this is not true worldwide, but I'm fairly certain it is. Number one value proposition for a dedicated test team is to be the scapegoat for the org. Okay, that's... I 100% stand by that. It's essentially so that my awesome developers have someone to blame when it goes to shit. Okay. The second thing in a non dysfunctional team, then the number one value proposition of a test team is to be a heuristic for the fast feedback loop that you would get if you were a service or in production or things along those lines. Okay. But in that context, a test team is an educated guesser. Okay. Yeah. They're a guesser. And this is the reason for principle number five. I'm the customer. I'm pretending to be the customer. They need me. They need me, Brent. They need me. And certain teams, they may be well trained and they may be really solid in their craft, but here's the deal. At the end of the day, it is still just a guess. The value comes from the customer. It does. And it's this angle, that little revelation around, because I started reading The Ways of DevOps again from DevOps Handbook. And I thought, oh my God, this is... And somebody a long time ago had mentioned the overlap between the modern testing principles and The Ways of DevOps. So go check that out if you haven't. Principle number one, our priority is improving the business. You improve the business via fast feedback loops. The faster you can get feedback on whether you're building the right thing or whether customers are getting value from it, if you can get that on an hourly basis, why wouldn't you want to? Why are people happy? I mean, when you ship on a monthly or a yearly basis, you're guessing customers are going to like that. And if they don't, and you have to wait another month or a year to give them something they like, you're going to be out of business. Well, if you have competitors, right? When people come to me and say, oh, well, you're allowing your customers to test for you. Okay, great. If you're a competitor of mine, I hope that you are not. Because, and these are speeches, I know I've given repeatedly, speed wins. Yeah. Yeah. A couple other things to bring up from that, that whole comment thread. And I did not even attempt to, I put a few replies in there, but I just not even replied to most things. There were two things. One, I called this out in something else we haven't talked about. I thought of in my stream of consciousness blog post is a lot of full-time feedback. And I'm not going to have to do that. But I think a lot of folks in the current testing community, or what do they call, what do they call themselves now? Not schools, communities, factions, one of the tester factions, they bring up this idea of critical distance a lot. I don't know which one of the, the, the bolt in their box, I brought it up, but they think you have to have critical distance. This is, this is the thing people bring up when they say that developers can't test. They don't have the critical distance to do this. And I found that to be a bullshit. They, they test very, very well. And then I don't think so. I don't believe a tester, this dedicated tester, the testing mindset, air quotes, gives you that critical distance. If you want critical distance, you're going to get that from the customer. In fact, I think it's my comeback to critical distances. If you think it's important and you're, and you're convinced it has to be there and, and shut up with your, you're making the customers test you. Yeah. Yeah. They maintain, maintaining Brent, imitating a whiny, a whiny person, a customer can provide that critical distance. Critical distance is, here's what I'm going to say. I am going to translate critical distance. Critical distance is bullshit cycle babble for, for an accurate fast feedback loop. Yeah. Okay. When I translate it in that way, and I have a similar thing for tests or mindset, I don't like their definition, obviously, the people that are publishing this thing. But to me, critical distance is you must have something in place that you have confidence in that will tell you your shit does in fact stick. Okay. That's all critical distance is. It's, it is some objective measure. Okay. Now what you and I are basically saying is in this current world, you don't always, you don't even mostly need another human being in your same team sitting next to you to do that. Now, there's a lot of studies around pair programming that shows that, that there's a value add on this one, right? But, but, uh, in my view, the relationship of between a dev and a tester should be very, very, very similar. It's not an exact match between two devs pair programming. Yeah. I agree. Because it's, it's, in fact, in a lot of the talks I've given, I talk about teaching developers to test. One of the things that I've seen work a lot is the best way to teach developers how to test is pair program with them. Bring your testing mindset to a pair programming session. To me, critical distance is nothing more than a trustable objective feedback loop. Okay. And that can come from all sorts of things, including my customers testing for me and to be snarky, like to me, critical distance could also be, uh, one centimeter, right? It's the thing that I find offensive about this called this concept, this concept of critical distance is the way I interpreted the framing. And I hope I'm, I'm, I got this wrong because the more and more I read, right? I, I, I am potentially committing the same crime where I have a preexisting point of view and, and I evaluate against that. And it just continues to blow up in my mind this wrong point of view. The concept we've talked about before, but when I look at critical distance, the way I walk away from it is that it's around knowledge siloism, that you get objectivity from siloism. And in this world, anything other than proactive knowledge sharing, I am at a point where that is offensive. Like it's insulting, but that's where I go. I go to a very emotional place. Knowledge sharing where knowledge workers knowledge sharing is absolutely a key to success. People working together, not separation. I can do a similar rant on test or mindset or a similar translation. Cause I, I spent some time thinking about that. This is well, do you want me to go into that now? Or do you have other things that you, Oh, no, no, you can do whatever you want. I have just one comment I want to share from the article. We can get to that whenever. Keep going, man. Test or mindset. I sat down and thought about it. Number one, test or mindset. The first question is, is there such a thing? Now, when we are evaluating, is there such a thing? I think we got to look at it from multiple different lights, right? Back in 1998, I would have absolutely argued there was such a thing, right? You, the number, and I think you would have as well, like the number of testers that we interviewed, some people fit the bar. Some people didn't, right? They didn't have a structured approach for this. Today, I actually see a similar problem, but it's implementation is fundamentally different, right? When we've talked, I don't know, back in November about AB testing the technique, not the podcast, right? And I will absolutely tell you, there is, there is number one, a mindset to do AB testing correctly. The experiment design, it's not hard, but it doesn't take much to get people to do it right, but they almost always do it wrong initially. And it does take a structured mindset. But here's the thing. It's absolutely trainable. I don't know if you recall two years ago, I wrote an article for a test project called the test, the testing mindset myth. I made the same art. It just doesn't exist. It's crap. Now it's age old arguments, right? Are testers born? And right. And I am, I am that some are yet. Absolutely. Some are, but in general, the answer is no. In my humble experience. Yeah. Again, not, that's probably my last testing post. My next post coming out tonight or tomorrow is really into the designing experiments part, maybe not that deep, but, but talking about that half a little deeper into that half of the article that nobody bothered reading or didn't understand. I'm surprised from the comments. People just do not understand. They're so far into their echo chamber of their echo chamber of all the traps of testing the old school, wherever they are, if they just, they're not exposed at all to how you can use data to understand and telemetry, whatever I call it, remote measurements, metric, et cetera, to understand the customer's experience. Somebody was in the comments thought I was taking like customer PII or, or if I was paying for the customer data, I said, no, it's not that kind of data. I don't know who the customers are. I just know their experience, but one, I did get one. I'm trying to figure out which of the last, last half, but the one thing, the way instead of being judgy or, or even getting close to being judgy, you, you, I'll, I'll just repair or paraphrase what you just said. You don't fully under stand number one, why they, why they seem unable to leap over the hurdle. Number two, what exactly is the hurdle? Right. It may just simply be experience experience. Right. Um, which is kind of what I suspect. Well, I don't know. I'll re I'll rephrase it. It was experience that, that flipped my bit and solidly. Yeah. I was going to read a comment. I'm not going to, because I don't want to call anyone out. Everybody's entitled to their opinions and I'm okay with people calling me wrong or, or it, in some cases, just going off on the fact that all my arguments are weak and not based in reality, et cetera, et cetera. Uh, I'm just sharing again, going back to the AB testing podcast. You know, that was based on sharing experiences and I talked to, I mean, here's, this is the problem of me posting something on LinkedIn. Some people start attacking like, Oh, Ellen, you're a VP at unity and unity software has bugs. And yeah, it does. And we also have dedicated testers because the feedback cycles are slow for the, for the editor. Yeah. So what it just, I'm telling you what I've seen. I'm telling you what Brent and I have been talking about. It works. I've given talks on this stuff. I give talks on this. And like you, people say, makes sense. Thanks for putting a name to this. Thanks for, for observing this. Really. We're observing this world going on around us. So I just get a little surprise when I share observations and experiences and people say, no, those are wrong and your data's weak and you do. And you're harming the craft of testing. I just, in regards, Alan, Alan, Alan, here's the thing. Like in some regards, we might be harming the craft to test it. Okay. And you know what? If that's the sacrificial lamb necessary to improve the craft of delivering quality, I am okay. Yep. Me too. I talked to someone else today, this week, another company, just a brief chat. And I didn't even know I wrote, let alone wrote this post, but in conversation, I brought a fact, like I said, I don't think that most teams need dedicated testers. Is it someone or another company that's not Microsoft or unity? They said, yeah, fully agree. No pushback. No. What do you mean by most? No. Define what you mean by dedicated tester or other crap. It said, yep, makes sense. No, but your article, your article, I wish we had a sense of like, I know our, we talk about the bubbles between the traditional and the modern. And I know when we started it, the bubble was relatively small and I'm fairly certain the bubble is not only large now, but even larger than the traditional bubble. Yeah. I think it may be. Can I prove it? Nope. Nope. Well, actually, I'm a dentist. I think a lot of the haters who may not read something, anything else on LinkedIn got there. It's this weird concept that I saw happen in my post as well as a couple times to other people's posts in LinkedIn, in Twitter and everything. It's this concept that Merit gave a name to. It's called snitch tagging. Oh, yeah. So this idea that when you talk about testing, you need to tag in the two leaders of the, I'll call the traditional test faction and tag them into your post. I will tell you right now that I have told Michael Bolton that his, and he didn't descriptive, I said, you're targeting your business at people who have done very little testing. And he did exactly there. There is so much turnover in that traditional test world. If you go to star, they ask how many of you have been testing for less than a year and three quarters of the hands go up. And at that level, when you are brought in to learn testing and you are at a company, you don't know what they're doing and they have slow feedback cycles. You absolutely need the things from Bach and Bolton to get started. They don't have bad views on testing. They're just views that are outdated compared to what you and I are seeing. So we have this sort of mutual respect between the three of us. I, Bach, 10 years ago, I was, you know, his big thing is if he loves to debate and he loves to like, let's get on a phone call and hash this out. Not a phone call. He's actually super nice. And he, I think he, he's a little pompous, but he knows his position. So we had our phone call 10, 12 years ago, whatever it was. And, and we're all on good terms. Just, we, we do well at staying out of each other's lanes. So I think Bolton got snitched tagged and he didn't even jump into the thread because we just, we stay in our lanes from each other. And I also don't go to their blog posts or their Twitter posts. And, and I, like I have, I have Bolton because I, I have nothing interesting I want to hear from. He's muted for me on Twitter. So I don't even see repost of his stuff for replies. And then we just avoid each other. So Bach got, got tagged in and he replied in there, but he did it in a very, I mean, he didn't say a lot. He was, he was nice in putting down my work. So I didn't worry about it. But this idea though, that, I mean, this is what really scares me and makes this, this, this faction feel like a cult because it's happened to somebody else. I know on LinkedIn over some sometime late December, this idea that if you say anything controversial about, not even controversial, say anything about testing that people will come tag the folks in to say, do you approve of this? Basically, what do you think about this? It's happened for years. And I think, I just think it's the dumbest thing ever, but I think that's what brought a lot of, cause then it's the changes, the algorithm shows up on more people that follow them's feed and they come in and tell me I'm dumb and don't know anything. But yeah, I don't care. Because I'm, because again, it's not a right or wrong. This is what the whole podcast is about. It's what we see. Yeah. Yeah. I see one of someone we used to work with even did a response blog. I'm not going to read it on air. I'm curious about it. But you're thinking that someone else posted on LinkedIn and it was, I don't know who it was. I can't remember, but they mocked me. They tagged me into a post and mocked me a little bit saying if some of the effect of, if you're one of the dedicated testers that Alan thinks we don't need come to this community thing and I'll show you what we really do. But that was pretty funny. No. And to me, I mean, that one's, that one's easy for me to sort of just laugh off, right? Because I'm like, okay, I understand where they're coming from. Right. And I used, I used to hold that opinion until I learned otherwise. So you don't know what you don't know. That's the thing that I don't know keeps me sane. I have too much experience on both sides of this coin to ever go back. And that's the sad reality. And I am probably locked into that. Like at this point in time, locked into that in a familiar rational standpoint, but no one is ever, ever, ever, ever, ever, ever going to convince me to go hire a tester for my team. And I ship services all of the time. Right. That days are done over. No mas. Because I know so many other ways to mitigate the risk. Yeah. And I just, there may be other reasons, right? There's certain criteria. If you have a government contract, they will make you provide the test results. If you are shipping, shipping product in the old school fashion. First off, I'm curious as to understand how or why you're doing that. Even now, Wi-Fi chips, shipping, forget everything. So even if it's an IoT powered screwdriver, you can still ship updates. I don't know why you need an IoT screwdriver, but that's irrelevant to my story. Yeah. I'm just simply not going to go back. And I'm kind of, I freely admit I'm closed minded on this in terms of that direction. Now, someone comes to me and says, Hey, Brent, I think I have a definition of postmodern testing. I'll be honest. I will just because I know me and my behavior, I'll probably react very similarly to how I did when Agile first rolled out. I might probably be right there with these guys and go, no, this is wrong because of theoretical bullshit that I haven't any experience on. Now, I would hope that I'm older and wiser and that I wouldn't do that. And it would sit down and say, okay, explain to me how would this happen. But this sort of bullshit theoretical proof that then I keep going back to, well, you're making the testers or the customers test. Well, if you hold as a standard that your job is to prevent the customer from seeing bugs, then yeah, that's like the number one principle thou shalt not violate. Right. But when you when you change it to a focus on quality, your standard changes. Yeah. And you really see, I mean, one thing that comes out in that discussion, and by the way, I got a whole bunch of Twitter and LinkedIn DMS with more scathing and more matter content that I won't read for privacy reasons, but off the charts that explains your comments on Slack at the time, because I went and looked at LinkedIn and I'm like, okay, I'm not really certain. I could very clearly see we've worked together. Why very clearly see you were upset, but I couldn't see why. Yeah, there's a lot of people reached out to me privately to tell me how wrong I was and how and again, I'm just sharing stuff I've seen. So what I would do is just reply to them, Alan and say, what's your proof? What is your actual proof? I choose to not and you'll see this on a lot of the people really trying to attack me versus the topic. I just did not engage. Fair enough. So what really comes out if you look at those and I wanted to paste all of the comments into chat GPT and have it summarize, but it was too long. Oh, okay. Well, I yeah, but then I had it was it was a pain in the butt. I didn't do it, but someday I'll do that. I'll figure it out. That's why it needs an API. And but one thing really comes out is what we talk about a lot. Maybe last thing worth mentioning for we cut off here is maybe not go too deep into it looking at the clock, but the people against me are really, really, really into testing. Talked about the test craft and how important testing is. And I just found that interesting when I care about quality so much. Testing is to me part of development. It's definitely part of quality, but it is not my main concern. A lot of these folks. It's just an interesting you and I have both made this jump in our last seven, 10 years of we're passionate about quality. We want quality for our customers. Not like we don't believe in this stuff because we don't give a crap about quality. We just think there are more efficient ways, massively more efficient ways to reach quality. Other than a whole bunch of testing by people dedicated to doing that, slowing down our feedback loops. Yes, it's essentially it's interesting that particular insight around. So on the podcast is the first time where I sort of ever in my mind equated these concepts. Which is essentially a dedicated tester is a heuristic for a feedback loop. Yeah, that's what I discovered in writing this article. So that's my new thing. And number one, it is a heuristic, which means there is a strong chance that it's incorrect. Or maybe not a strong chance, but there is. Heuristic by definition are a guidance that can be incorrect. Correct. It is still feedback, but it's not necessarily prioritized feedback. Right. Whereas the feedback loop that you get from the customer telemetry as an example, is neither. It is both prioritized and it is not a heuristic. Now, the third condition is depending on how you've implemented that feedback loop, it can be slow and it can be fast. So the speed, I say it has a potential of being the same on both sides, but generally with the techniques people use using today, the speed is much faster on the customer side. It's a lot easier to get a fast feedback loop in today's world than in the old 1998. Oh, yeah. Oh, yeah. It wasn't until, you know, well into the 2000s were downloading updates or downloading things with the thing you did. Right. All right, man. I got to go. You got to go. Everybody got to go. All right. All right. Everybody. Yeah. Yeah. I think we're good. Are you emotionally okay? No, I'm totally good. Should we fake hug it out? No. Oh, all right. No, not even through the camera. Okay. Fair enough. All right, everybody. I'm still Alan and he is still. The other one. The other one. Yeah. The B to my A. All right. We'll see you next time. Bye. 
