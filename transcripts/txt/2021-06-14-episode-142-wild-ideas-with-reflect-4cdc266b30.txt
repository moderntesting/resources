Welcome to AV testing podcast your modern testing podcast your hosts Alan and Brent will be here to guide you through topics on testing leadership agile and anything else that comes to mind now on with the show hey Brent we have guests yes it's so exciting it's pretty exciting it's pretty exciting isn't this our first no this is not our first we've had guests before we've had lots of guests no I'm talking about Brent Brent stop talking so we can say hi to Nick and Todd Nick and Todd say hi to the AB to all three of our A B testing listeners wait if there's two here there's only one left so say hi to the other one hey everybody nice to nice to meet you I bet it's Percy I bet it's Percy he's the one listening we're not live streaming today but I Percy Patrick although in this case Chris is probably gonna be keenly interested miss so when Nick reach out to me he said here's Brent running off into the weeds so it's the A B testing podcast so Chris kentz one of the three yes I believe is who referred Nick to us oh that's right that's so if this whole credit goes horribly wrong Christmas ball well I'll collectively just blame it on it may be good to blame it on Chris in advance right now perfect yeah I'm gonna certainly blame it on Chris this is our a hundred and forty second episode still season one because we're too unorganized to have seasons baby testing podcast and I want to thank Todd and Nick for being here and they're here for a reason and maybe I shouldn't try and explain it nor should Brent even though we kind of did we have a little there was a little bit of a spoiler there and that Chris sent us to you can one of you tell us why the hell you're here on my zoom call and being recorded right now this is me looking back to see yes we're recording good why are you here tell tell your tell our listener why you're here okay so this is Nick I can do that I had been referred to your podcast by by Chris who is the other listener and episode 135 was you had a discussion about record and playback tools does it make sense to be writing code now that recording playback is you know so effective and much less brittle than it used to be and beyond that should we be specifically targeting recording playback at developers and marketing to developers and so I'd reached out to you both and said hey it's kind of what we're doing and we we kind of agree and so here we are it's controversy out there but maybe not in here but I yeah let's that's this is this is gonna be good and I'm gonna use this as ammo because not next week but the week after no no no calendar wrong next week next week I am giving an AMA and ask me anything at test bash home and I will be talking about the future of test automation well I will be sharing my opinions about many of these things we'll be talking about today which is great gives me it gives me ammo and food for thought and and plants my brain with all the things that that sound crazy coming out of my mouth until I hear other people saying oh yeah that's that's what we do you're cool let's start with this question maybe this is one because I haven't done a ton of this is purely purely based on my limited experience some of the new tools that are out there and then writing you animation years and years ago and then watching people struggle and struggle and struggle with getting Selenium to run some tests that end up being flaky in the long run so talk to me about my belief that record and playback has gotten to a maturity where we perhaps don't need to handcraft UI tests in Selenium anymore am I am I in the ballpark with that or am I out to lunch is that today a year from now like what do you think of when I when someone crazy like me makes that statement yeah I can answer that so I think number one we're definitely biased because we've built a tool to to handle that so we definitely do believe that that's the case I think the the thing about it is that when you think about the the actual process of creating Selenium tests you're taking user actions and translating that into code and then that kind of process can be very painstaking I had done it myself my background is as a developer and kind of doing it I've seen kind of like how you know unit testing component testing integration testing and end testing and then testing the code base and then testing always felt like a poor abstraction layer to me because unit testing is you're testing little bits of function it's the perfect abstraction layer because the interface is code for a function for integration testing there's a lot of great tools for it and for things like H you know like API testing of a HTTP server or something code is a good abstraction layer because you can represent HTTP calls well in code for replicating what a user actually does it never felt like a great way of doing it so I think the approach of recording your user actions and as long as those that recording as high fidelity it actually records things that can be you know replicated I personally think that is a better approach to building those kinds of tests hey I think he's saying I'm not completely out of my mind and that's that that's I'll call that a win for a Friday so he mentioned a technical answer to that question right he didn't mention anything around psychological evaluation which I doubt he's qualified for so I'm not quite leaving that one off the table definitely not qualified it's interesting so you're selling a product on this line and I'm on your website like the the marketing that I am I am seeing right now seems to be targeting sort of the the manual tester audience which is kind of more of the traditional audience for recording playback tools what's your marketing shift like so Todd I heard you just say that yeah you know what we think the capabilities are there for for for dev to do this on their own what sort of distinction that I'm seeing are you are you still primarily sort of targeting the the manual test audience are you going to dev what's the strategy shift let me jump in first because I disagree with you Brent I think it like I went through their documentation and they go into details that the air quilt manual tester would not be interested in so I think they're trying to cover the whole audience but but now that I've I'm actually the actual expert who actually knows what they're talking about I'm gonna shut up with you guys go what I would say number one is I'm I consider myself a developer but not a marketer so the marketing language that you see on the language on the site is mainly me and I'll take the blame for it our tagline there is automation that anyone can use but it doesn't really speak to any one particular person I think the the why we think that we're a developer a tool that developers would use it because developers are using it like of our customer base right now roughly it's hard to say but about a third are Estets and QA testers a third are developers and then the other third is basically everybody else in our organization so like a smaller organization that just any old person might be testing manually and now they're you know automating automating that so so yeah just kind of it it I think the reason why that developers are using it is because the recording fidelity is good enough that it saves them time and also because we're developers and we we built it for our own need so I think some of that like you know scratching your own itch came into play why you see it's why you see the documentation is kind of like so extensive like you would see with like a any old API yeah and I like it being for everyone's and I think you in my opinion it is the website is written that way I was in a panel discussion maybe three weeks ago I can't remember the last 15 months for Evan a blur for me not sure about you and I mentioned that I knew of developers that were using these I hate to even call them recording playback you know Brett and I grew up in test in the 90s when recording playback tools were just so awful like you you were it was a it was a stigma against you if you even mentioned them but again they're not the same anymore but anyway the point is I mentioned that we're talking about now that record and playback tools are actually useful I see developers using them it caused a mental shift in more other people on the panel said I never thought of developers using these things this is making me rethink a lot of stuff and I've taken great because I there's something to it I'm so glad to hear that that developers are using your tool I think it's good one of the things I'm talking about in modern testing with Brent and I is again we're not inventing anything we're just talking about the things that are already happening and one of those is developers owning more and more and in some cases the vast majority are all of the testing being done for a product not saying testers aren't needed but in some products the contact dictates that the developer can do all of the testing including that bit of UI testing they need to do and they can do it in a quick and reliable and repeatable format hallelujah yeah it's pretty cool to see like when we started the company like we had I hadn't had any experience with it we had evaluated some record and playback tools but hadn't really had a great experience with it but it's cool to see people using it like you know people that I was in their shoes a couple years ago I think with when developers are using it the way I the two ways I see it is it's either the dev team themselves there isn't a tester in the organization so the dev team kind of owns their own tests like they would like a unit or integration test or there's a testing organization and they kind of set the framework and guidelines and then the developers are creating still creating for their own applicant part of the application that they own but you know the tester is saying okay this is when you would want to use reflect this is when you want to have a component level test or API test or whatever one of the things I talk about frequently is just pairing that developer with that tester and thinking about all the testing needs to get done then dividing it up one other thing to call out and again not trying to just plug everything about what you're doing because honestly I hadn't heard of your tool reflect until you email me I'm looking over your docs it's like oh thank God someone gets it like I just I saw the like the test you should create which is one of the things eventually I'm weaving a story together here but my brain isn't keeping up I am part of an organization I was until recently that very large organization with no dedicated testers and but we'd a handful of people are coaches consultants helping helping the team do better testing understand what testing they need to do and our VP would sometimes ask me for our listeners today you've heard the story before but he'd ask what how is our team doing a testing and I say they do all the testing they know how to do and which implies the good news they do testing but there's obvious they have some unconscious incompetence I mean to work on that but in your tests you should create section it's so common sense to like Brent and I but maybe not to a developer brand new to testing like here here's your core workflow here are the things are gonna stop you from making money you need to be able to log in and register and go through the main workflow and yeah those are good things that you should automate because you want to test those through the UI that makes sense and you want to make sure they work otherwise you're not making any money and of course you can use you'll have monitoring in place to make sure it's scale you know everybody's getting through the workflow but but it just it's just a common sense that sometimes missing from some of the marketing is out there so so kudos for that and that's maybe sort of a add-on to Brent's original question I think I think the the the direction how you're marketing the website is actually very good I'm gonna stop looking at it so I can ask the questions I wanted to ask versus go through your website right anything to add before I wrote keep on rolling through here I feel like you should send me to the bar for beers while you actually talk to these folks so I'll shut up go on Brett yeah except you're also the the tech for the podcast so that kind of puts the podcast in harm yeah and to be clear like I wasn't making a judgment call around how you're marketing the the and since Todd admitted credit slash blame for it like I was using the opportunity to say you were wrong Brent that's all yeah yeah but let's let's focus on our guests and then you can you can leave the bridge abuse for a future podcast you probably exceeded quota for this one already I have written down a written down a bunch of things in a bunch of different ways we could connect to these two other other sort of topics like first off I see like one on on your blurb where you request additional information like the the headline I'll say I love this one and Alan will absolutely know why like ready to accelerate your testing efforts is the key highlighting question right in one of our mottos is that the goal of testing our goal of our podcast is to help people accelerate the achievement of shippable quality and then third I don't know if you guys have had an opportunity to read this but what is now becoming just basically one of our favorite pieces of literature is just a book called accelerate are you both familiar with this book no I'm not I'm not either oh well I'll just say given your your role I highly recommend that you read it it's so one of the things that's highlighted so it was it's done by a PhD Nicole Forsstrom and I forget her partner the co-author of the book but then what they did is they went through and deeply analyzed they did they did a bunch of scientific studies research papers and then took that and turned it into a book one of the statements that they call out is actually dead teams who own their own automation are significantly more or rather the products where the dev teams own their own automation those products are significantly higher performing than those products that don't and the the rationale is on that one is essentially the separation of duties by forcing your dev team to own the test it holds them accountable and then they maintain that could be better more frequently etc go ahead I have a question for you on that Brent yeah I think do you think that's a capacity thing like do you think that teams that put automation on the devs just have more horsepower to create more more automation or do you think it's like you said the kind of separation of church and state like just testing what you own means that you're just somehow implicitly gonna test it better so what's your take on that so so my take is based off of personal experience so years ago I spent the majority of my career as a as a middle manager in QA but I've always been super technical I have patents on test automation there's an exam then I left enjoying death in a point in time when I took on a dev manager role in the Bing organization so I'm here at Microsoft this is my Microsoft office hopefully I don't have anything our listener can't see it right it looks different yeah so I I'm one of the last messages that actually has an office anyway so I made that move and it was during a time where Microsoft was making a shift towards combined engineering sort of beginning the process and so I started on the dev team no testers whatsoever coming from tests and I had a mix of people who came from the old-school dev and a mix of people who came from the old school test and I turned them into a high functioning dev team and the big secret sauce that I encountered was that test role was a sort of mitigate risk and in a world where you're doing services as an example there are other strategies so we we we actually learned that we can mitigate risk very rapidly by shifting to a like a Kanban model and so for example if I had one in good continuously shipping if I had one thing that shipped out the small chunk of code it wasn't like a scrum model where I would be integrating at 11 p.m. the night before and then breaking everything the next day it's constantly shipping and so it's suddenly everything was on the floor in the next morning we know exactly what it was it was this one thing which we could quickly revert and then go okay what test did we miss why did this go out etc like so in that world I never actually discovered I needed it I had a need for a test someone that that safety net that checks things beforehand and then as we grew grew further we realized things like TDD or or even things like using having the dev owning the test cases basically one inherent thing and maybe you will agree I'm looking specific so make do you have a dev background at all I know I have a a operations degree so I studied supply chain theory of constraints Ellie are who gold red we love theory of constraints yeah new podcast topic yeah we could talk for hours about that and so for me you know not to make essentially the answer the short answer is no but the long answer is I've always been in software I've always sold software I used to sell primarily to marketers and then when I moved back into when I moved into this world and I joined up with Todd and the rest of the team who we all worked together at a previous startup called cure late I suddenly realized like oh wow this is supply chain management this is theory of constraints this is so I don't have the any of the technical skills to actually make anything useful happen but I can understand it and talk about it a bit technical skills are a dime a dozen you got that so let me redirect because so then I'll just specifically ask Todd on this because I'm actually curious is this a universal truth so from my experience this idea that working smarter not harder really resonates with with the dev facility or the dev community or put it in a different way developers are inherently lazy yeah that's a much better way to put I don't like that with smarter that's all BS but acknowledging that developers are lazy 100% behind that yes and so first there's gonna be resistance when you try to get them to take over test but if you can't continue to push and enforce that then really magical things happen because of this principle that they're lazy do you want to do the least amount of work possible and if they realize that they can't escape this work then they start thinking through okay how can I do this how do I get the right abstractions in place how do I leverage tools that where I just do it once and then it magically resolves itself from on out I think and I'm gonna go ahead I'm I'm gonna let you finish I'm gonna wait no I hear Alan answer well because you like for Babylon forever and I lost interest I have said before that the industry has a unhealthy infatuation with UI automation and I think that if we would have let like one of the reasons we're in this conundrum and why we need products like yours and and all your competitors not gonna single anybody out here is that and that we need developers to own this stuff is because it's because developers hadn't had to own it is why we've created this huge bottleneck of huge number of these handcrafted on a UI automation that are generally flaky because of the nature of UI the UI behind it if devs would have owned it it never would have got to this stage because we let separate teams own that stuff completely we're in the inefficient cesspool we're in but go go a commentary over go on go on guests we have guests on the podcast today hey I sort of agree and sort of disagree with it I think I think the I agree with the sentiment I think like a lazy developer like laziness little bit of laziness or somewhat laziness is like a good trait in a developer I certainly have it like I'm a procrastinator so that's that's a sort of a way it's a subset of laziness but what my experience was the last startup that Nick and I both worked at was those actually did own end-to-end testing and we did it all manually and it was like it was felt like a huge it was valuable but it felt like a huge waste of time in the sense that like you have these people who are like some of the highest paid people in the organization doing going through a spreadsheet and clicking on things every time there's a deployment so you know that I think the question the question is like why does that happen and I've talked to you know we have had plenty of people that we talked to you that they were in the same boat like remember one one of our earlier customers we talked to them and said yeah we deploy twice a week we have this 80 page word document that will go through before the deployment I think the I think the reality is that the tools are the tools just weren't good enough like you you have this kind of push and pull between you need to ship code and you want to get enough testing in to find those bugs but you can't have a developer hundred percent focus on test automation so what are you going to do they can't get it done with five hours a week of writing selenium so they just stick with the spreadsheet and maybe there's fits and starts where you have the test automation project like starting up but it never gets done that was that was my experience so in that in that world why was the answer not well let's just hire a tester we we went with an out in the last company I worked at we went with the outsource crowdsource testing so that it sort of was it's we sort of went that route that didn't work well for us because again we had the developers writing the test cases and so the developers weren't great at writing for like a non-developer audience and then the testers who were you know it was different tester every time weren't ever familiar with our application it was like a big pool of testers and so we got a lot of false failures where it was it was either the developer didn't write the test case specific enough to like reduce remove ambiguity or the the the tester wasn't familiar enough with application and you know didn't know that this actually was the way it was supposed to work so we kind of tried that and it didn't work well either there would be people in the industry that would argue that what would work better or worse if you just had between the developers owning all those tests and just actually hiring a tester to you know co-locator at least co we don't co-locate locate during covid but hiring a full-time tester as part of the team to work with you same tester working with you every day what are the pros and cons of that versus having the developers own what you ended up with i i think that would have worked a lot better for us honestly like i think the the pros definitely the tester becomes a as long as it's their setup within the organization like the right way they become an expert on the application and so they are not dependent on developers or product managers or whatever to build the test cases beyond just like hey i need to know like what the new functionality is coming so i can build the test cases for it i think that the con in that case is just time it just seems like the test teams out of any part of the organization time is the biggest limiting factor for them here's for the theory of constraints comes into play go ahead yeah i was about to say nick how do you feel about that from a from a constraint point of view yeah i it's something that i observe all the time is that teams seem to have an abundance of capacity or at least plenty of capacity on the developer side and then they go okay well i guess we should test some of this code and then under capacity that they're under um you know staffed on the testing site and so i i talk with a lot of teams that can create you know build code uh 10x faster than they can test it and so in that situation when you talk about the startup that Todd and i were at before we certainly could have brought a dedicated test team in but would there have been um the enthusiasm to staff that team at the level that would be required to be able to um break it as a bottleneck to be able to make tests to to not have testing impact the velocity of the overall dev team and certainly my experience and i'm of the four of us here i'm the i'm the greenest grass i'm the least experienced but certainly my experience is that a lot of leaders a lot of cto's don't feel like they're getting enough value out of out of a code-based testing process or their current testing process because um you know they're they're not they're still not a they're they're chucking resources at it they're they're in whatever way they're trying to solve it whether that's a dedicated team of s-tets whether that's manual testers um and testing is still a bottleneck like okay well how far do we have to how how how deep does my pocket have to be to solve this problem uh and so i i think that's where the tar then and fits in the team here and and other other tools like like these two that are in our space that's our advantage is you know we can just do this faster and i think alan that was the impetus of the whole conversation last time like what why would we be writing all this code if we if we don't have to like it has it reached point where we don't have to and so yeah i think um i think i've gone down a rabbit hole of a tangent here and i'm not sure where to get out of it tangents are part of the podcast so that's totally fine but but i want to tell a story of systems thinking or theory of constraints gone bad from my past and this is actually overlaps with probably all of microsoft so it'll it'll relate to branch as well but there was a time in for several years microsoft had very much a one-to-one developer to tester ratio a little bit less in some places a little bit more in others and i remember i worked on link which was called communicator before then it eventually became skype for business which was part of office so i gotta i got a little insight into the office machine of test automation they had a one-to-one test ratio across a huge huge org so thousand testers maybe more all writing almost all are writing lots of automation the bottleneck was the flaky tests this is pre this isn't even web selenium stuff this is just just flaky tests because of flaky tools uh flaky ownership all kinds of problems don't even go into today but where the theory of constraints would maybe if we actually paused and stopped to think about it we may have gone with a different approach to try and figure out how to handle that bottleneck of testing when every day 90 of those testers spent the first three or four hours of their day debugging failed tests to see if they were a false positive or not to see if there was a product bug invent and then the the ones that weren't were investing their time in writing tools to do automatic analysis of failed tests to try and figure out uh we had this afa automated failure analysis to try and figure out if the failures were really failures if there are false positive great reports about it and you know in hindsight i can look back at the time was like oh cool this is cool technology afa but as i stop now and look back i go oh man that was bad that was really bad and i don't know how you feel about that brent but those experiences and then watching what had happened over the years with dedicated test teams writing a lot of selenium i just knew it was bad too so uh i want a time machine and go back to fix the things that i allowed to happen 10 and 15 years ago but in the meantime like i said a half hour ago in this podcast i'm glad that my wild ideas about how we should approach this ui animation today aren't actually so wild so that's cool but back back when i was a test manager i i actually had conversations with my boss very similar to what nick was saying where he had a firmly held belief that the number of testers he needed to ship his product under the constraints no matter what end was the number he needed was always n plus one right always and he felt like he was in a trap that he couldn't get out of and it and it was it and as we hired more and more and then tried to solve more and more problems like bigger problems i blew up alan was just talking about and then i was just thinking about one very i'm not mentioning names to protect the innocent but one very big example was there was this really important suite that was executed and nearly 75 percent of the tests pass without the product being even installed right so it was the industry's fullest stories like that i think i think it was brian merrick i could be wrong told the story could have been chem caner but someone told a story about how something similar happened when the power was off in the test lab or something similar to that fun stories hey um you were going somewhere never mind brent were you done we we all have sort of stories uh i think we have stories on this one so is it right to assume that at this point in time does reflect have special uh test specialists within itself you mean do we have uh like s deaths or on our in our organization right dedicated to testing so we're we're a team of five so we don't have any uh testers in our organization we have two engineers myself my co-founder and nick who's uh business development sales oh no yeah you're fine you're fine i'll see myself out no no no totally yeah i mean okay so you have no testers right one of our wait hold on a second no the way you're saying that is is is setting up the wrong thing like one of our principles one of the things we've discovered and not and again our modern testing principles aren't anything we've invented they're about what we're seeing happen in the industry and we see is depending on context and knowledge there are many examples where you don't need a dedicated testing dedicated testing specialist on your team you're cool in our eyes it was just a fact i wasn't judging i know you sounded judgy we we felt judgment we did yeah yeah all right so so let me set the space let me let me set the bar uh because now i'm going to say something super judgy all right so you guys don't have any testers it's like so clearly you understand and you probably get this from your customer feedback that you don't care about the quality of your product and are actually shipping crap to your customer branch shut up right no no no it's tugging cheek i'm not actually accusing you but this is the sort of crap that me and alan get all the time right so yeah but we're we're all we're all smarter than those people well no so they're customers and so here i want to you want to bring in bringing nick like what do your customers say who who are you targeting we started this off of hey hey we do have a marketing plan to attack devs or market to to these dev camps are they coming back are they feeling successful yeah yeah and obviously you wouldn't come on a podcast and say no to that so maybe you're comfortable sharing like a horror story as well to sort of balance it out yeah i guess i guess in terms of a horror story i'll certainly leave that to todd because he has uh he has a longer history of horror stories potentially to draw on but uh we we target um today we work with businesses of all sizes we work with like large financial institutions we work with small teams of five or ten people our most sort of common customer is a hyper growth startup with 30 to 100 employees in that sense they are they understand we're a startup too and so if there are ever sort of chinks in the armor we generally have a fairly forgiving crowd i would also say we have pretty high standards um the i think being a testing tool requires that you have a pretty decent testing or at least remedying process in place so yeah i mean todd i don't know if you want to speak to some of the horror stories but in terms of numbers you know we just passed we're a young company but we just passed our millionth test run um and and so we're uh i think we're certainly seeing people come back to the well and and feel like the quality of the product um you know ultimately we're a team of five we have we have thousands of users hundreds and hundreds of paying customers and so the quality that we're putting out there people what i mean to say is people have to teach themselves how to use the tool and people have to come and adopt it and try it and like it and so quality is huge for us because if you try it and it's broken you're not coming back um so yeah from the horror story perspective though i will pass it over to todd and uh and uh and let him take the take the bullet yeah uh definitely i definitely can share our story um i would also just add because so many testers are users that's very helpful because it keeps us honest you know they you know they're greater giving us bug reports and telling us you know have an eye for detail so that's that's helped us um you know improve the product for sure um in terms of horror stories like what what are you looking for brand is it it's like a a customer gone wrong or you know just an organization that kind of seemed upside down what what what can i provide for you um no i actually so i'll just say upfront i'm certain you're going to give me the the the horror story that that the audience would want because i don't actually think there's one here right and see the the uh one of the arguments is all known no we need we need testers within the team uh because number one first and foremost devs can't test because they they they don't have objectivity they they you know we're asking devs to test their own code and therefore like you're you're providing testing a testing product uh you don't have testers hey brent you're bringing you're bringing our own baggage into their baggage they don't have well they get it actually that's actually my point alan is you know what this is a startup uh that's that's succeeding as best as i can tell uh doing exactly sort of the the the modern testing yeah principal and what yes exactly and what i want to talk about for our last uh eight or nine minutes here is another thing this is a variation in what i led with at the top but i believe that the future of success and testing tools is to stop marketing those tools to testers and to begin marketing and target of course you're for everyone and that's that's where the market is that's totally good but i think the the future spoils comes to those who find a way to get the developers interested in those testing tools and this covers all that that i'm conscious and competent talked about and knowledge etc i think there's something huge there so uh you've had success there it's wide open what are you doing like i didn't know what my question is it's really just a statement how are you actually how are you getting developers to discover your product because often they don't look for testing tools yeah so we uh it's a couple different ways so we're a freemium product so people will just come on and start using it so by being a freemium product in some sense you don't control who how people find you or you know you kind of do your best you try to write relevant articles and kind of um get in front of people but people just seem to find us and use it and the and the subset of them are sticky and part of that subset is developers but we do go out and talk to cto's directly and so when we talk to a cto at an organization some of those organizations are going to have a team of developers some of them are only going to be you know or sorry team of developers and testers some of them are only going to be developers and it's it's stuck with kind of both camps but i think if you're marketing a developer tool like there's some things that you definitely need you need a freemium product number one because devs aren't going to pay for something unless they can use it number two you need really good documentation uh we think like ideally like with videos that's particularly helpful and then it needs to be an intuitive enough product that's powerful enough for the the developer i think that's probably where we're lacking right now is just the power we started out with building something that try to do everything that it can do for you uh and you kind of what you lose in expressability it does kind of automatically for you but as we had more of like the sdette developer persona come in what we found is that they need that power in order to define exactly what they're trying to do and so those are the kind of features that we're adding we're kind of approaching it from like the standpoint of like what would like a testing ide look like you know what would debugging a test look like very cool and the point i want to call out here just for our listeners and our principles is is you have a product out there people are using it and you're adapting it as you go based on their needs and that's that's where quality comes from right because only the customer could evaluate whether your product is high quality or not whether it's giving them value and the way you do that is you listen to them and give them what they need yeah we we had to rethink some of our assumptions about it for sure yeah and maybe it's worth pointing out that like we talk about these modern testing principles again we didn't invent them we just tried to document what we're seeing and as we talk about them people go oh yeah that's what we do you have a name what we're doing of course that makes sense what we hear i don't know if you ever even looked at the modern testing principles but as i hear you talk it's like yeah here's a company out of nowhere never heard of them they they didn't it just made sense i think it's a probably a combination of smart development and having someone on staff who knows theory of constraints will they want well it gives their credit to nick there this is a career high point for me yeah i mean it's thank you i appreciate that i mean i think i think it's still important to have a product that's opinionated like we still we still take some pretty opinionated opinions like for example in the products um you can actually add a step uh through the gooey like you have to record it like there's no way to button to like click add step so everything's done through our recorder uh and then another one is like we try to abstract away the concept of selectors so you can't actually manually add a selector we generate them all automatically for you with the idea that we feel like that's kind of like a subset of the testing problem where we think we could solve it for the 90, 95, 99 percent case exactly don't get me started so i don't i don't know if you were aware we did a podcast with hugs on on the podcast oh really yeah so we alan alan made a statement it reached out the hugs hugs came on and we had it we ended up having them this is this is worth this is worth calling out for our guests i made a statement i found a blog post someone talked about how clever they were for solving a problem in selenium and it was a 80 character selector statement that they used and i quoted it and said you know reason number 435 why i don't like selenium people are excited about statements like this and i showed that big big huge like and uh a whole bunch of people got really mad at me and again it's not selenium's fault there are all kinds of ways to fix that no but i but but anyway he he caught onto that thread then volunteered to come on the podcast and he was very gracious he wasn't mad at us he he tried to pick a fight with us we did not fight we did not tried to bait us i guess we did not bite and end up being a really good conversation yeah the and and i do think that he he uh i mean i'm old and it was a while ago but i do think we got him to eventually eventually agree that actually selenium was constructing a dysfunctional behavior particularly around these selectors so yeah i i even based on the this the conversation we had with hugs i would actually say he probably would would likely support the fact that you're blocking people from going that direction right because your goal is you want the the the maintenance cost of these tests to be zero yeah right you can go down this path you go down this path it's going to be non-zero almost guaranteed yeah and the the best way to do that is to do things on behalf of the user as long as you can get it right like the i don't want to knock on apple but like when siri gets it wrong that's an extremely frustrating experience but you know when you have something that gets it right it's a magical experience so it's like a higher bar to achieve it but that's that's what you can actually have you know a tool work for you instead of you kind of fighting the tool yeah yeah i was excited to have you on here to talk about those two big questions around you know statements i have made and and even if you didn't back me up uh i just really love the way like the engineering approach like you got you are all taking the five of you at your company it's it's the way i think successful software is made and i think you've are also on the right track and remembering that developers are part of who you need to target and to hear that you have developers not testers well in addition to testers you have developers who are using your tool it's like yes yes this thing that i'm imagining in my head it's actually true and i didn't have anything to do with it it just happened because it's the right thing to happen so that's awesome any closing words from you all before we call it a friday afternoon slash evening yeah i'll say hey uh thank you very much for joining and actually we haven't talked about it did you part of your closing words do you want to go ahead and plug your product sure yeah so so reflect is uh freemium tool you can go to reflect dot run and sign up and uh use it right away uh i would say the biggest difference that we we have is that the recorder uh runs in the browser and we try to make a frictionless experience where you can start from zero and end up with a test in a couple minutes give us a try love to get your feedback i guess i would only add to that the evidence that you can create a test in a couple of minutes without writing code is proven by the layman of the podcast me being able to create hundreds of tests at this point uh and so uh you know from uh for what it's worth i think the proof has been in the pudding from that end but but guys it's been really great to chat with you and thanks for having us on to you know talk about allen's uh stroke of genius and our our hurry to catch up and make it happen for there's a phrase we never hear yeah it hurt my head i'm confused as to what you're talking about all right then we'll uh again our our immense pleasure having you on uh incredible conversation thank you for being here and we'll see everybody next time 
