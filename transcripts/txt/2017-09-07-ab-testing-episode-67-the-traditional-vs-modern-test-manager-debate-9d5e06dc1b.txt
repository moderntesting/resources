Howdy, I'm Alan. I'm Brent. And we're here for another episode of EBTesting. Which number are we Brent? 67. 67? Awesome. I'm very happy about that. I don't know why. I'm just happy. The numbers get bigger and I'm still alive. Yeah. So, as I mentioned to you while I was plugging in the microphones, I went to PAX, the Penny Arcade Expo, this weekend. It was actually really awesome. Lots of great cosplay. Some fantastic, like six foot, seven inch tall orcs. Did you cosplay up? No, I did not. I played as... That's too bad. I went as the 51 year old gamer. You know what? I don't think that's abnormal anymore. I borrowed my kids' Rocket League shirt. The great thing is, you are almost of the age of me and they have... I am nowhere close to the age of you. And in the hall upstairs... I gotta live 10% more of my life. In the hall on the top floor, they had some retro arcade games. Nice. And I... This old guy still got it. I set the top... The fourth highest score. This is on Monday, mind you. Fourth highest score on Tempest. Nice. The reflexes came back, had people watching over my shoulder. I felt pretty good, just like at the arcade. Just like it was a long, long time ago. You had the cassette player with Rush playing. Yeah. I went to the Infinity Mirrors exhibition at Seattle Art Museum afterwards. Before we do the podcast, anything new and exciting with you? We went to the Evergreen State Fair yesterday. Alright. That's in Monroe, Washington. All the way up in Monroe. My daughter had a blast. My 15 year old boy, he just wanted to do the rides and the carnival games. Dude, 10 tickets, 10 bucks. One game of throwing six balls at the iron milk bottles, 10 bucks. I'm like, whoa. When you look at the thing and you go, 10 tickets. Oh, 10 tickets. That's what? $2? No. Evergreen State Fair funding Monroe for the next 12 months. Yeah. The one thing that we did discover though, we were shopping for a new kitchen table, our new dining room table. Oh, yeah. And in one of the exhibition rooms was a full-length pool table that had leaves that you can cover up the pool table and turn it into a kitchen table. And it was really nice. That's a smart move. And I'm like, I got to look at that. And I also, the one thing though is it had four leaves that you put on it. And I'm like, okay, if someone spills on this, then I, then as it go through the cracks and destroy my felt, don't know. Anyway, it was nice. Cool. Totally. We're here in the world to discover these things and share them with our three listeners. Further proving why we have three listeners. Exactly. Shall we start the podcast? Sure. All right. We had something special today. Do you want to describe it? No. Do you want me to try? Go ahead. So we have talked on this show very much about what we're calling modern testing, testing in the agile world. And as it's funny, as I write more about it and tweet more about it, I see it's not just me. It's not just Brett. This is a sort of testing and changes in testing that are happening everywhere, yet everywhere it's happening. There is a counter example of somewhere where the 1990s mentality test manager is apparent. So I was thinking, we were thinking that me as a manager of a modern testing team here in 2017 and Brent, who happened to actually manage a 1990s test team. Yep. We thought it would be interesting to have a time travel discussion between a, what should I call you besides a 90s test manager? I prefer to be called a traditional test manager. Okay. So we're going to have a conversation here through the magic of AB testing podcast and the technology we have put in place to have a conversation between a traditional test manager and a modern test manager. And I'm going to play the role of the modern test manager. And Brent, if you remember him, uh, we'll play the role of the traditional test manager. This should be interesting. It should be. It's something that would traditionally require rehearsal or preparation, but no, welcome. Yeah. AB testing. That's why we only have three listeners. So Brent, can you give me a summary, like maybe the elevator pitch of what you do as a test manager? As a test manager, it's my job to make sure that the product goes out bug free. So I manage a team of STEs and a manager, as well as a team of S, S debts whose job it is is once, once devs done with their code, we test the crap out of it and make sure that it's good enough to ship. So actually my job is to sign off. My role as a test manager is I manage a community of testers. So all of my test team, STEs and Estes, they're really just test specialists and test and quality specialists who are embedded in the dev teams from the very beginning. And they work together to ship a high quality product. My, my role is really much more of a test coach than of a day to day test manager. I'm in charge of their career growth and I'm in charge of vision and philosophy for the team. But most of the time I just give them a framework and get out of their way until I need. So how do you manage the growth of, of a test tester community? So I have regular one-on-ones with my team. I find out what's growth mean. So I want to make sure that they become more of a quality and testing expert. So I will give them, point them towards tools they may need, connect them with each other. Again, I said, I manage a community. So as I find areas where I think even though they work in completely separate teams where they can help each other out, I will help make that connection. That seems stupid. It seems like your job is to just make sure, your job to make sure you said that they're experts, right? Who's actually finding the bugs? The whole team is. In fact, one of the great things about my team is the whole team owns quality. The developers actually write most of the testing. The testers help them. They write some more complex tests. They'll write some diagnostic tools, but the dev team finds most of the bugs themselves. Then how do you grow your testers? I make sure they have. If you have a bunch of testers whose, testers whose job it is, is to find bugs and you have all of your dev team finding the bugs, then what do you grow your dev team? That's an interesting statement because I don't believe it's the at all, it's the responsibility of my test team to find bugs. The roles I put it on my team is they're there to accelerate the achievement of shippable quality. And they do that sometimes they're finding bugs and really complex, interesting bugs. But most of the bugs they find are actually through data analysis and looking at how customers using the product versus doing a writing typical tests or test automation. The dev as far as functional test automation goes, our development team writes all of that stuff. So you have testers, that's what you call them, right? That's, I could call them anything I want. Okay. So you have testers who don't find bugs and you just got through saying they don't even write automation? No, they don't because the dev team writes that. It's redundant. It's inefficient for the dev team to not test their own code. So how do you stack rank these guys at review? What I heard you just say is that you have a whole team whose job it is, is to occupy space. No, not at all. In fact, well, one, we don't stack rank. That's just a process we don't use trying to reward people, everyone based on their impact and value to the team. But what was the other part of your question? How do I, what do they do? Oh, what do they do? You say you have testers who don't file bugs and they don't do automation. This is all moved. You said it's all moved to the dev team and we should talk about how stupid that is. But what I want to understand is how do you grow a community, a community of people whose job it is, it sounds like to occupy space. Like you, you schedule, you know, fatty lunches so that they grow how much space they occupy. What are you talking about? Sounds like you got a bunch of people who do nothing. It's actually quite the opposite. And in fact, and almost every single one of my teams, the test person, the quality person and some teams is called productivity engineers. I know some teams call them quality assistants. They are viewed across the team in most cases as the most valuable person on that team because their job is to help see the big picture. If you look at testing top to bottom, including everything from unit test to Explorer, short testing and Illates and code analysis and code coverage, it's their job really to make sure all of that happens versus do all of that. So while the devs on our team may be focused on, again, they're writing, they're implementing code and they're writing tests for that. Often even the writing more tests, they may not see the big picture. And what I see across my team is everyone on the team helps the entire feature team see the big picture and see what's missing in order to ship a quality product. It seems terribly inefficient. We have that same thing in our space. We call it a release manager and a program manager. That's their job to figure it out. So it sounds like you got a whole army of people who do what we do with a... I wouldn't even say it's an army. We have one tester per feature team. So the ratio, if you're counting ratios, it's going to be somewhere between one to five and one to nine tester for every developer, for everyone focused on implementing code primarily. In fact, one thing maybe worth to mention is our whole team is made up of of what we call generalizing specialists or specializing generalists. Everybody can do a lot of different things. And what my team does is provides their expertise, their specialization is in the areas of quality and testing. But most of the testers on my team have also written shipping product code as well. I don't worry about one group of people having one responsibility. Everybody can do everything. Of course, they have specialties and they're better at some things and that's the way they work. Actually, I want to point out one more thing. You said talk about inefficiency. It sounds like from your summary, with doing all the testing at the end, when you find those bugs, what happens? We ship them back to dev. And don't you find that inefficient? If they find these bugs, they have to go back and relearn the code because they may have written them in it weeks or months or longer ago and they have to go relearn that code. Remember where that bug is and try and fix it. And then they fix it and you test it again and it's not fixed. And there's this horrible game of ping pong going back and forth where if you just fix the bugs as you found them, wouldn't that be more efficient? I'm here representing the test discipline and that's a dev discipline problem. I'm really happy with my partner. But you said your job was to sign off. And how can you sign off ever? It's like if you keep on moving half the distance to something over and over and over, you never actually get there. So how do you know when you can ship? I just do another test pass and then I won't sign off until it's clean. My job is to test. I've been doing this job for a very long time. My peers have been doing their job for a very long time. There's no reason for me to distrust that that dev hasn't figured out how to do their job optimal. Well, I've been doing this for a long time also. One thing I've noticed is the way we make software and the type of software we make has changed quite a bit. So we're shipping our software. Most of what we ship ships every day. How can I do a full test pass and ship every day? How can you do a full test pass, ship every day? We don't ship every day. That's too risky. I got a huge full test pass that I got to get done in order to make sure it's clean enough to go out in the customer's eyes. Like, look, here's the reality today. We got a bunch of new kids coming from college. They're trained in what you call agile, I call cowboy. You just yourself said there's no controls that your company has over your dev. They fix whatever they want. They ship whenever they want. Who's to say without these controls in place, you don't know what risks are going to ship out to the customer. At the end of the day, my job is on the line. I'm the one accountable to make sure that when I sign off, this thing's good. So many things you said I want to comment on. I understand, I guess, why you might consider a process that moves so much more quickly to be air quote cowboy. But what I've actually found on my team is we, in a very brief meeting every morning, we know exactly which bugs and which feature work we're doing every day, what's going to ship that day. And we can make confident risk, even all even yours, yours, we can make confident risk based decisions on getting those things out. One thing is we count on customer data a lot for determining which bugs to fix. So if we find a bug, but we know through the data we get from our customer analysis that customers aren't hitting it, we may not prioritize or we may not even fix that bug. When we roll out, when we, I said we ship every day, but when we ship, we ship to a subset of customers and we use the data to find out whether that's working for them and ensure that it's working for that subset before we go to larger subsets and eventually all of our customers. And that's one of the things that allows us to ship every day. You mentioned you have a full test pass you have to run. When I should mention, we also have many, many tests that run, but they run in production as tests against our production data and they fire alerts that we can react to when any, any anomaly, I wouldn't call it a failure. Any of those happen. That allows us to react quickly and fix. And when we make a mistake, we can, when a bug happens, it's not the end of the world. Cause we're able to roll back quickly to a previous version that doesn't have that bug fix and then roll forward again. It's a different world for you because I know you want to, you want to have control and you want to save your job by signing off on a high quality product where I live in a world where we want to get better every day. And one of the things to get better every day and to improve every day is we're going to make mistakes and those are okay. They're not job ending. And we're going to re we're going to react appropriately when those happen and learn from those and not make those same mistakes again. All it takes is one regression that is rarely hit, but has such impact that causes your customers to reject your product. All it takes is just one of those in your company's gone. So one thing that your one regression that doesn't get tested in your daily test pass because it's, it's super rare. All it takes is one customer to go through that. They lose all of their data and company's gone. So a couple of things. One, we still have monitoring for those things. Find out if they're failing. Two, one of the things, because we're not, because we're not focused on writing a test pass or running a test pass every day and writing a tons of functional automation. One of the things my testing quality experts do is write a lot of reliability tests to hopefully catch some of those things. But again, rely on monitoring to figure out if those things are going to happen. I would argue and say, I'll agree and say you will losing customer trust is an awful thing to do. But if you're only shipping every year or two, you're not able to get new features or new bug fixes your customers need. I would say that also loses a lot of trust and they'll ship to a competitor who can give them what they need quicker. And I would argue that psychologically, and again, you, you described a sort of doomsday scenario bug losing data, which again, I agree is horrible. But if we ship a product and has a little inconsistency, that's a bug, and you may never ship with that when we're able to fix that again, the next day, plus give them new functionality or fix something else, that product satisfaction goes up even higher. I'd like to see your, your numbers for that. From where I sit, I mean, it's, it again sounds like you got a bunch of Cowboys producing lead code for, for a disorganized product. Like we have armies of PMs that go through talk to customers and upfront understand exactly the scenario we need. So I don't believe that customers, you can, we talk to customers a lot, but what I've found is it's, they really don't know what they want. And it's very important to give them what they need versus what they want. We have some PMs and they do connect with customers extensively, but we experiment a lot. Meaning that if we think we have, when we, as soon as we think we have an idea of what a solution for it to solve the customer problem, a real problem that we've discovered, we will run an experiment. Meaning we'll deploy the solution to that, the feature that fixes that, the new application to a small subset of our users. Look at the data. We format, have specific business objectives in mind that we want to measure as part of that rollout. We give them that information. We, we, we roll that out to that subset of customers. We collect that information and we'll only roll forward if the metrics show that is a good business decision to move forward versus, and that works much better for us. And we can run multiple experiments a day, try many, many different solutions versus the, let's build, let's build something for a long time and give it to you and tell you you like it. That seems cowboy to me. We're very adaptive. Your model is a predictive model where you know that this is coding time. This is testing time. This is what first, this is PM talking to customer time. Then we code and then way down here we test. We are constantly adapting and learning. I have a customer. I have a very far from cowboy that you it's, it's ridiculous. You would use that term. I have a, an easy heuristic. Let's look at the Microsoft stock price. When Microsoft was full on waterfall, the stock price was unstoppable. Once we started shifting to agile, stayed flat and kind of has stayed there since I think there's an obvious conclusion. I think you throw in the baby out with the bathwater a lot there because, I'm not throwing no baby or, I don't think that if, and again, one thing that my team does is we look, we understand a lot more about data and how data and statistics work. And what you have described here is a classic example of correlation without causation. If the only thing that had changed at Microsoft was, the only thing that had changed in the industry, if you look at this as a big, as a system, if the only thing that had changed was Microsoft air quote moving to agile. And I can tell you that one, Microsoft didn't and has not made a wholehearted across the board move to agile across the teams. But even if that were the case, you also have seen a huge shift in moving from shrink wrap products you buy in a store to services and subscription models, which end up bringing in money over the long-term, much better than the short-term. But you have to look at the entire system versus one anecdote of change to be able to make any reasonable correlation that can be believed. So there's just a lot more going on there. Brent is doing a really good job making the face that of a traditional test manager to these comments. This should be a video cast because he's selling the roll well. So let me ask you a question. Yeah, go ahead. Talk about you want to sign off on the product and you run these test passes and they pass. And so do you fix all the bugs you find? How many bugs do you have in your products right now? Just throw a number out. We have a pretty big product. So we have a couple thousand that still need to be fixed. And when are you going to fix these? But that's okay. We have, I mean, CodeComplete should close in a month or so. And then we have already in our schedule another four months just for bug fixing. So I'm not worried about it. We have a big dev team. They'll knock those down. And how many new bugs will they create in fixing those couple thousand? Well, you talk about stats. I've done the stats here. And in general, the rule of thumb is for every 10 bug fixes, they'll regress or create another one. So it's predictable. I'm not worried about it. So what happens if you, do you have a ship date planned? Yeah, we have to hit November so that we can get our product out by Christmas. Okay, great. So what happens if bug fixing goes slower than expected? It doesn't seem very predictable. Well, so with that amount of time, we have about six months. Oh, I just changed the calendar in our stick. Yeah. We're in a time machine. It's all right. We're in a time machine. So I guess it's May. For our father, dads and grads. We have about six months. Our listeners get the point. It's okay. So we have about six months. Like the scorecard is automated. The exit criteria is published. Again, I have in the dev team, I have what I view as five star developers will will continue to track progress. My dev manager even has has created a new principle around how many bugs they got a fixed fix to get it down. And they're constantly re triaging the bugs. The dev team and the PMs are constantly re triaging the bugs. And towards the end, like whichever ones they haven't fixed, they'll be prepared to to cut. You mentioned, and I'm going to bring this up because you brought up the word first, but you mentioned inefficiency. And I talked about the inefficiency of bugs going back and forth from test to dev. But if you're re triaging constantly, you're looking at the same bugs over and over, determining which ones are going to fix. We talked about regression rates for bugs. Also, given that code complete isn't done yet, you will find a lot more bugs after that given your model. So let me talk about, and I don't know if I mentioned this already, but there's quite a bit of difference in between the way you and I approach bugs. When our team finds a bug, or if a customer reports a bug, we look at it immediately and choose to either fix it immediately or to not fix it at all. Our bugs, and again, Brent is doing the great traditional test manager phase, our bugs are fixed or punted daily. What this means is across our org, we have zero bugs on a daily basis. What you have zero bugs in your database, you just got through saying that you're punting things proactively. What's going to happen if a customer comes back later and that thing that you punted, we already know about the bug. Usually when we decide to punt a bug, and there's another point I want to make in a second here, usually when we decide to punt a bug, at the very least we'll add some extra data, some analysis, some instrumentation in the code around that. So if it happens again, if it or when it happens again, we have enough data about that issue and when it happens to make a better choice on whether to fix it immediately or not. But it's purely a business decision. We look at the work we have to do for new features, for new functionality, along with incoming bugs, which are risk, and we make a business decision on what is this important to fix right away or not. And it's funny you mentioned you have the bugs in the bug database. This is giving your reaction so far. This may really freak you out. It may cause you to use the Calaboy word yet again, but we don't really use a bug database. Since we're fixing bugs as we find them, bugs go into our work item tracking and they're fixed right along with feature implementation and there's no reason to track them in a bug database. Again, traditional TMs head just exploded. Again, if I don't have a bug database, then I have no way of measuring my testers at review time. So the only value your testers bring to the team is bugs? Yeah, that's their job. His job is to find bugs to make sure that we build a quality product. Again, I'm still a little unclear as to, you got through telling me your testers don't find bugs. You don't even have a bug database. They don't automate their features. To me, it seems like your dev's a bunch of cowboys and your test team is a bunch of scam artists sitting around getting paid to do nothing. And then you, you as a community leader, you've also broadened this scam in a way that such that when the product ships and it's crap, no one's going to come after you because your job is to build a community. Yet we ship dozens of high quality products that customers love every day, despite the scam. And when I look at the value my team or my community provides, it's one of the things I look for, one mantra is better quality, better product every day. And my better product is more value to the customers, which is it could be more functionality. It could be higher quality and measured in a bunch of different ways. Their role again in achieving, sorry, accelerating the achievement, higher quality. Higher quality you said. I did say higher quality. Right. So higher quality, by definition, the way you've set up the year, how you guys do things, if you have zero bugs every day, right, you already have perfect quality every day. Well, that's, and that's because you're only if you have the moronic assumption that lack of bugs equals quality. We have dozens, if not hundreds of health metrics we track across our products that everything from reliability to performance latency to every single error that can happen in in our code pass all monitored to make sure they're improving constantly over time. So bugs are one measure of quality, but all bugs are not created equal. In fact, I wouldn't, I would even argue that bugs maybe aren't a measure of quality of bugs. Could be a measure of engineering quality. You can't just as part of this debate, you can't just redefine the definition of quality. What is quality then, Mr. Test Manager? Quality is the bug count. It has been the bug count for forever in the test community. In my world, it is not. Quality is the value of our product to our customers. And you, I think are losing your left nut over this because you cannot fathom the idea of it being anything but bugs or anything you can't distinctly measure in one way while using bug metrics or code coverage metrics or test pass rates. Those things all have their place, but I don't even, we run tests all the time. I do not have a single dashboard in place that shows me the pass rate of those tests because that information isn't useful. If there are failures, we fix the bugs. Of course it's useful. Useful for what? What's your test pass rate good for? What do you use your test pass rate for? It's primarily to understand which dev teams need to consider a change in their practices to avoid regressions. So if you have a dev team that has any failures in their tests, what do you do to them? How do you go fix their practices? Well, we file the bugs and then again we have dashboards that will display which dev teams are excelling at preventing regressions and which teams are sucking. So sort of shame dashboards. Yeah, it's very effective. Have you ever heard of Hawthorne's principle? No, I have. I have the Hawthorne effect, which means that you get what you measure. Right. And I want fewer regressions. So do you think there's any chance at all of the dev teams that are, air quote, excelling are gaming that system? Well, certainly not in your system. Since you hide all bugs away from everybody. But we don't do that. We, you know this for a fact that there are bugs you find that you go, Oh my God, you got to fix this bug that no customer has ever or will ever hit. Yet you ship your product and customers find bugs. Then you go back and add a test for that because you forgot it. And then three months later, six months later, a year later, they get a fix for that bug. In my cowboy world, what we do is we constantly monitor and react. And if there's something that a customer finds that or customer is into something that triggers an error, we will look at that and fix that right away. The biggest flaw I see in the, you're just talking about your team, right? I'm in a big organization. Like if everyone, with several thousand developers contributing, if everyone were to do this, then we would end up with one big piece of crap product that didn't integrate. And you could see the seams in the product left and right. Like I don't see that. Number one, yeah, I still stand by you've got this beautiful cowboy scam going on. But number two, the practical reality is I don't see how this scales. I think scaling to large teams of a thousand is certainly a challenge. Scaling up to about 300 has proven to be very effective, especially when that team is focused on a, what we call a large vertical or a vertical slice. Maybe not a vertical slice, that's something different. A large vertical. It's a challenge. We do write many, many integration tests and we work, we find many ways to connect those teams together. But I'll admit that can be a challenge, but I think the fact that we can, because we're shipping so frequently and the fact that we can react when those integration problems come up, I think it works for us. Scaling is always going to be a challenge. And there are very few products in the world. And again, you work at a very large traditional company, but there are very few products in the world that have development teams of 500 or a thousand or more. Those are unique to some specific companies. Let me close the door on the time machine now and bring back real Brent. Oh my God, it was horrible. So the frightening thing here was if you couldn't see Brent's face, but he actually really was in that persona. And I could feel 90s Brent coming out and it was, it was, I spent, I think I've shared this before. I've spent the majority of my time at, in my career with the title test manager in, in the waterfall world, right? What, what I fell back on on my inspiration is like, I remember being a test manager when Agile was rolling out and how horrific it was. Everything you said I've heard, I've heard myself. Everything I said of a great deal part I used as winning arguments back in the day. I believe it. I believe it. And I had, I've mentioned on the podcast before, I had a dev director, I was, it was a dev director on Xbox who said he wanted to have this conversation with me about test and the role and, and didn't really in the end care what I had to say. But what he said was the role of test is to sign off. And I thought, you, well, you know, I mean, as we all know, the role of test is to inform. It's not it either. Right. And I don't think I did in my traditional test manager. One thing I want to work on my elevator pitch on is that answers like, and you could say very abstractly, we accelerate the achievement of simple quality, but the points I brought up were absolutely true. I think if you have someone who is looking at that big picture and finding bottlenecks and mitigating them and enabling your feature team to move quicker with higher quality, people see that it's, it may not be measurable in the, to the exactness of bug counts, but it is highly appreciated and valuable. If you rely a little bit more on user, on peer feedback, I think the model works. I don't know if in your world, in our world, if there's a better way to measure that, but I don't really like measurements metrics applied to people. Metrics applied to people. I will do it today only in the cases where I'm trying to hot thorns effect can be used for good and for evil. But one of the things that I do think for those listening, and I think, I think the number one thing. So first off, Alan did a very good job, in my view, defending the modern role and incorporating tasks. I think the one thing that's probably important to hit hard is the thing of, yeah, we are redefining quality, right? Quality is from a customer point of view. And I think, I think hitting that from a business sense. Yeah. One thing I've said before to interrupt is the traditional approach was a lot about measuring engineering quality. Yeah, I call it correctness. But like we wanted to test the quality of the engineering system we were making versus the quality of the product that customers use. Right. A lot of our scorecards back in the day, a lot of our scorecards, their whole purpose in life. And I will say this now, I spent a lot of my time, I used to have this speech about, again, as a test manager, how I rated subjectively different test leads, not really rated is the wrong way of saying it, but your skill on being a test lead heavily correlates to how well you can answer the following question. Why was this bug not found before? That just hurts me. And so a lot of the, a lot of my infrastructure was built around not only building the best product I could, based upon a greed upon exit criteria in a predictive fashion, but it was also sort of playing a chess game in assuring that I was super transparent all along that when the inevitable dev doesn't have enough time to fix bugs, PM has decided that we need to get these other additional DCRs in like we didn't talk about that in their discussion. DCR is design change request for those asking at home. How do I assure that when I come back and say, Hey, you've now compressed me, and we have to slip that, that I'm on the moral high ground. So Hawthorne's effect in that particular case was all about proactively or part of it was about proactively identifying the teams that are, that are actually causing the product to slip. And it's not just because I come last. Correct. So there's a lot of proactive finger pointing in that model. And to be fair, because we were shipping very infrequently at that time, the measuring engineering quality was the best proxy we had, but we just have better, we have better ways to measure customer quality now. And we need to take advantage of those. It is a shift. And I see why you're stuck where you are, Mr. BTM, Brent, traditional manager. But it's just a different world we live in both in the end. And it'd be funny, maybe 10 years from now, I'll be dead, but you'll be having a podcast with, and maybe it's shifted yet again. But probably not going backward. I think we've talked before, drink, that my role doesn't exist. In fact, I've started to float that idea more up than down that if I do my job correctly, I work myself out of a job because the community I've built is self-sustaining. Yeah. I completely. And I'm okay with that. Maybe I just take out, I don't know what I'd do at that point, take out a larger community or, I don't know, I haven't thought about it. I'm not going to fix it this year. I'm not going to build that in. But it is not just my community. I mean, my community needs to become all the engineers for the features I'm responsible for. What I realized so several years ago, and when I came to that sort of conclusion, and this isn't the first time you've done this, it just so happened. This isn't the new learning for you. But when I came to that same conclusion, I realized that I needed to begin to invest my time in a different direction. Right? And a lot of kind of, there is a lot of alignment between my philosophy as a test manager and my philosophy now as a data scientist. I'm still just as passionate about customer quality. My inner tester is still there. Yeah, for sure. It just doesn't care about testing. He cares about how do we win in providing the most customer quality against our competitors. I think one thing that has changed, if you look at it from a little bit of a meta point of view, is our approach to testing and quality in a traditional model is very formulaic. We knew we had bug metrics, and we used the same bug metrics from product to product and code coverage. We measured a bunch of things the same way because it's the formulas you had to use. And like software, our approach to quality has become much more adaptive and relied on our own and our team's constant learning. And I think that will continue. That's where the growth and the wins will happen. There'll be other things that will happen around applying AI and machine learning and much, much more of what you're doing in the data science world. The reason why I brought that up is, to me, I still think it's an obvious thing. As you are working yourself out of a job, begin to stepwise incorporate data analysis and data analytics into your team's duties. Hey, now Dev's doing all feature automation themselves. Great. That's fantastic because I need these resources to spend 25% of their time building statistical models that help us precisely. And actually, in real world, that's what the two big principles I have on my team is working. Some teams are closest to others to this actually working from zero bugs. Most teams are very close or there. The two principles are zero bugs and let's use data. And we'll start off with, we'll start off a little bit. It's heading in the right direction where I've been a little bit of a tipping point. I'm really excited about it. So, it's fun to be a modern test manager. Okay, everybody. That's it for this episode of AB testing. I'm Alan. I'm Brent. We'll see you next time. 
