The Modern Testing Podcast. Join your hosts, Alan and Brent. I am Mindless Agile Robot. I must iterate. As we talk about software engineering, software quality, leadership, and whatever else comes to mind. Now, on with the show. Hello everyone. Howdy. We're back for another episode of AB Testing. We are. I guess if you're listening, you probably figured that out already. I never thought about that. I think Lisa Crispin namedrop even mentioned in one of her tweets, like, the stuff at the beginning can be kind of interesting, but you can just skip past that. Oh, yeah. Yeah, it's generally a good idea. This is where we catch up. So, how you been, Brent? Great. You know, one of the first sort of informal reviews of the podcast was the podcast is different. It's like two guys babbling about crap over beers. Yeah. I still think that's part of the charm. It well, you can call it charm. You can call it charm. I have been... Mystique? Yeah, I don't know. Mystique? The X-Men Mystique? Yeah, it's not part of the X-Men. All right, cool. So I got back from PNSQC last week, went down for a couple of days, had a really good time, gave a talk on... Oh, I didn't tell you this. I didn't tell you this. I gave my talk on modern testing. You didn't tell me that. I'm getting to the part I haven't told you. Just shut the up. It was a 90-minute talk. I timed it very well, went deeper into the principles, and ended with about 10 or 15 minutes early, took a bunch of questions, had a pretty good-sized crowd. I was an invited speaker, not a keynote speaker, so I didn't have the captive audience of everyone there, which was actually kind of good because I wasn't able to alienate people who might have been in a different world. So anyway, I'm giving this talk, and I was really excited. After my talk, there was lunch, and during lunch, Ward Cunningham, who was local to Portland, was there just telling stories about stuff he did. It was really cool. So I finished my talk. I'm wrapping up. There's, of course, the people hanging out afterwards. I sit down in the auditorium. There's a little break there. I do about five minutes of work. I pack up my stuff, and I walk out. A little bit of rockstar, I walk out, and I see, oh, there's Ward Cunningham talking to some people. Cool. I can't wait to hear what he has to say. Then I walk him by, and he goes, hey, hey, good talk. You didn't tell me that. I didn't tell you that? You did. Wait. It was on Slack. Did I? Yeah. But still cool. That's awesome. Whatever. Whatever. My little fanboy moment. So anyway, I got back. I'm heading off to Copenhagen and Helsinki next week for work. I haven't been on a work trip in a while, so I'm going to do those when I travel now instead of going to conferences. That's my thing. It's fun. Going to do my job, and that's about it. I think. Anything big going on with you? We may be able to dive in and freak people out and dive into the actual podcast before too long. Oh, I think we should. No, the thing I think is interesting. Or not. The thing I think is interesting. So before the conference, Alan up on the Slack channel expressed something I hadn't seen from him before on any, like you've gone, since we started MT, I think you've done five, six different sort of public events. No. Well, probably. But they haven't all been about modern testing. No, but this is the only one where I saw before your talk you expressed huge nervousness. So it wasn't nervousness about the presentation. I was definitely skeptical about the audience. And that'll actually bleed over into our topics for today. There was very much a. See how I did the clever segue there. I don't know if it was clever, but sure. Yeah, sure. I'm calling clever. All right. You call it whatever you want. I was a little worried about the audience. One, there was a vibe of hardcore testing, test automation, test design and techniques. And we're modern testing isn't so much about testing as it is about quality and building quality in and doing the right thing for the customer. Yeah. So it's a different take. And I wasn't sure. One, I wasn't sure how receptive the audience would be. And then two, given the topic and what people have heard about it, I wasn't sure anyone should would show up. What people showed up. I was actually worried they wouldn't. And one of the things that I do when I talk is I play the room pretty well. I crack some jokes, find some people. It's as we mentioned last time, giving a presentation, it's entertainment. I need to be entertained. I need to be entertaining. It's hard to be entertaining for a small group. It's a different kind of thing. I can do it if it's very interactive. But when you're on the big stage in the auditorium and there's bright light shining on you, it's difficult to be interactive. So I was worried mostly that no one would show up. And if I did, I would get people just saying, what the hell? Or giving me what the hell faces all the way through. Fortunately, I didn't get that. Yeah, it's interesting. I have not been in that situation that you're referring to. Generally, when I'm presenting and the room is smaller than expected, I just convert it to an interaction. Damn, the rules. I don't care. Yeah. Well, I had a pretty good sized crowd, so it was okay. And everything went well. I had a good time. I got lots of good feedback. I haven't seen like any scores. I never really look at those anyway, because one, any feedback on the content, I'm not going to give that presentation probably ever again. I may do it for an online conference sometimes. I guess maybe I would look at the feedback. And I know what my flaws are as a presenter, so I'll continue to work on those. Yeah. No, the feedback for me is always interesting because I can often get a glimpse into a key unaddressed point. People just have a hard time seeing their own faults. It's a lot easier to sort of see the faults than others. And so that sort of feedback, so you can get myopic into your presentation. Think that you've told a complete story, but a lot of times the feedback will tell you, oh crap. Yeah, I totally don't talk about that. And one of the things just before we go on here, we thought we were going to go right to the topics, the content. I do think a lot about the story when I give presentations. Like what is the story I'm telling you? What's the journey I want to take you on? What are the subplots? It's very much storytelling. So anyway, that aside, I'm going to take a break from presentations for a while, although I am doing sort of a little mini workshop with my team. And this may be worth it in the modern testing context to go into because this will come out Monday for the masses over the weekend for the people on the Slack channel, but well before anyone on my team will listen to it. And they know this, but we're going to talk deeply about it. I'm meeting with my air quote leads next week, but actually only one of them still works for me because in my quest of moving the organization towards modern testing, I'm moving more and more people away from my org. I still work with these people. I advise them. I coach them. Some are in different roles, but still part of my quality community. And we're going to have some pretty in depth discussions about what, what is our community, our community of former people that used to work for me. How do we continue to interact? How do we continue that support and help with each and helping each of each other that we had when we were part of a team when we're now part of six different teams. Hey, as a, as a complete, completely irrelevant non sequitur. Wait, those never happen on AB testing. Completely entirely non sequitur. You need a theme music for that. Go ahead. When is your 18 month point at unity? Already gone. Thank you. We can carry on. Brent's still convinced I changed jobs every 18 months. All is well. There is no, no exit plan on the horizon. But my role, one thing is, you know, Microsoft, the reason I last lasted there so long was I could change roles all the time. You can already tell from my previous conversation, my role is changing. Yep. My role is moving from a director of quality to something else. Well, actually, we figured out it was, it was a community leader early on, but it'll probably be something else fairly soon. Are you going to change your title? I don't care. I don't even know what my title is. I guess it's director of something, but I don't know. That part doesn't matter. One of the things I need to do is my role needs to change and not because I need to do something else is because I actually enjoy constantly working myself out of a role. I like building self-sustaining teams and self-sustaining communities. You know, there's a theme that we've been talking about and it's, and it's, it's hit me a lot this week. Hitting a lot of, so one of the, one of the big problems a lot, particularly at Microsoft, you have a lot of people who've been here a long time and have, particularly with sort of the, the, the, the test change and the move towards unified engineering. You end up seeing a new sort of power culture and you'll, you'll often see things like you go to a particular org and you can tell right away it's PM driven, right? It's PM driven orgs are almost singularly focused on the customer. They're all about features. You can never get any sort of architectural, aren't necessarily the same. No, but you can never in this particular thing, you can never get an architectural improvement or a hardening sort of task on the backlog until it becomes a crisis. Yes. Right. Yes. Yes. Yes. And then, but the other one is more of a dev org. When stereotype I'll say is, is a lot of times you see the dev driven orgs focused on technical solutions to problems and then struggle with it. Then they go back and say, they try another technical and another one and another one and another one. And, um, um, lately I've been doing a lot of face palming because I'm like, this is a human problem. Oh, it's always a people problem. This is absolutely a human problem. Now I'm thinking I firmly believe that's true, but a lot of the times I go back and they think through, is that, is it because of my training now where I'm focused on things like behavioral economics, which is all about, um, how do you move? How do you create win-win situations and move people towards a situation that benefits them and your business? Anyway, it, it is fascinating to me. Uh, one day when I have free time, which is probably not going to be any time in the near future. I want to do some research on that one. One thing I'm realizing farther into my tenure at unity is how much, how easy it is to ignore the fact that most problems are people problems. I'm actually looking part of my, the, the evolvement of my role. I'm exploring. I've always been pretty good about figuring out what work I need to do, but part of that is, uh, some leadership coaching and just, and training and management coaching and I shouldn't say training management, coaching and mentoring and helping people get through, uh, some of these people problems to find better ways to make software. Sliding back just a little bit to my fear at PNSQC of the people focus on testing. And the weird thing is that modern testing principles are neither particularly modern nor are they about testing contextually. It's important to remember, and I have to describe this as they're getting more widely shared is that they grew out of the, I call them the antidote for traditional testing, tradition test, last test quality and testers at the last line of defense. Originally, uh, I think the original edition of Caner's books said the role of testing is to find bugs. He later redacted that and changed that to provide information, uh, which is sort of passive and finding bugs. Isn't right either. Whitaker wrote the how to break software. And then others after that, I'm not sure where they started said, we don't break software. We just remove the illusions and, and it's already broken when we get it. And we've heard all those lines before, but that is not at all where I want to focus. And it's not at all what modern testing is about. Modern testing is about how do we make software in a way different, maybe completely, not completely different, but much differently than the way you and I made it 15, 20 years ago in a way that gets quality software to our customers quicker. And that's what modern testing principles are about. And someone on Twitter suggested, maybe call them modern quality culture principles and no, keep the name as it is. And if it doesn't make sense to you, and there was a user group that I think we mentioned a podcast or two ago who recognized the exact same thing. Well, these are neither modern nor testing. They don't mention risk. Or principles. That was another one that we were, we were accused of. No, they're absolutely principles. But I will, I will stand by that because I, one of the reasons, one of the impetus for me creating these was one studying a lot of thinking, one just thinking we should have principles. But I also happen to be rereading Ray Dalio's principles at the same time and studying definition of principle, because sometimes people screw up principles with beliefs and needs and things, but they're absolutely principles. So screw that. So one of the things, but I will say that neither modern nor testing, I agree with that completely, but that's not the point. It actually, it's not the point. And multiple times people have suggested that we rename them. We will not. The primary reason in my mind why we should not rename them, as you call out, the antidote to traditional testing. But there's a reason why we call it modern testing. And then essentially, if you, if you follow the principles, you realize very little testing is involvement. Matter of fact, one of the things, if you follow the principles correctly, we're saying there is this long-term battle that test has been fighting around how to move quality upstream. They call it, it's really how to move testing upstream and modern testing is essentially saying, no, that is no longer sort of a nor star do it. Testing belongs to dev. Make that happen. But why do we call it modern testing? Why is it not modern development? Because number one, it's called modern because we want it to distinguish itself from the old methods. Maybe there'll be one day a post-modern testing principles. I don't know that I'll be part of developing that, but it could happen. Why is it called testing? Because that's the audience we're trying to reach. We're trying to, we're not trying to influence the dev teams and say, hey, you should be doing this. The people we're trying to reach are the ones who we think are going to be impacted by this cultural change going globally. We want them to realize that there are more options than just panic and show up at the unemployment line. And we want them to start thinking through how to do that shift. You can't do this shift overnight. Absolutely. That's, in my view, why the name is right and why the name will stick. Agreed. We've gone through the principles before. And of course, there's people question them, often out of context and often, I don't know if that's even fair, but recently, a Twitter discussion around principle number five. And if you haven't memorized the numbers, I'm getting used to this one though. This is the one, the first one where people go, uh-huh, what? I want to talk about what it means a little bit again, not the deep dive. We did a few episodes ago. Drill down. The drill down. Sorry. Whatever. God, give me your Microsoft terms. Anyway, number five is, we believe that the customer is the only one capable to judge and evaluate the quality of our product. And this is pretty much not quite word for word, but this is out of the lean startup. You don't get value from your engineering effort until it's in the hands of your customers. That is a quote from the lean startup. Yes. That's a direct quote, but I don't think it's just out of lean startup, right? I'm just pointing out the fact that we are not- Jerry Weinberg says the same thing. Actually, Whitaker said the same thing. All that testing is getting, one of his most popular, but all that testing is getting in the way of quality. So let me talk about the way this is misinterpreted and the way it's been misinterpreted for at least 10 years. I think it's been 10 years since Whitaker gave that talk, but at least 10 years. Some testers, and this is okay, you'll grow, still feel like the gatekeeper, the salvation for customer, the ones that are there to find all the bugs for the customer does, regardless of the fact whether those bugs are important or not to the customer. There's been a fear ever since someone 15 years ago, at least 10 years ago, because it's mentioned in the book, it's now 10 years old, this fear of testing in production. Oh, you don't give a shit about your customers. Why do you want the customers to find all of your bugs? And it's like, not quite the point. And one, bugs do not equal quality. Absence of bugs does not mean high quality. Prevalence of bugs doesn't necessarily mean low quality. Quality is value to the customer. Oh, Popendike, that was the other one. My favorite quote was from Tom Popendike. Software developers need to remember no customer ever wanted software. They want their problem solved. Right. So the way we... I don't want to get into the data principle, so let's leave the data principle out of this. If you build software and you test the crap out of it, in fact, I can't remember who did this quote, I'll just throw out random paraphrase quotes. Someone said, maybe I think it was Eric Ries actually, said, show me software that ship with no bugs and I'll show you software that ship too late. Yes. Because the point, anyway, the point is of this principle is not that you're expecting the customers to find all your bugs, but you can use techniques to let them find some of the bugs, but in most cases, they may not even know. It's not like you're not doing any testing. There's plenty of testing happening earlier, but the point of this principle is you have to get that software into customers' hands so they can get feedback on the actual value of the software. In fact, if you're following the rest of the principles, you're at high quality enough every day, every hour, you can ship to them as often as is feasible for their tolerance of taking new software, which is for a service 50 times a day, for a mobile app, maybe a couple times a week, for a desktop app, maybe a little less. But you need to get that feedback from them. It's not that you want to give them your buggy software to find the bugs. And I find that interpretation of this principle, that this principle is about giving customers buggy software they can find your bugs, irresponsible, ludicrous, it's completely without context or critical thinking. It doesn't make any sense to me how this principle is interpreted that way. Are you really thinking I'm going to do that? No, so I'm going to go a different way. There's two things I want to make on this. Number one, I got called out in that Twitter feed as well. And as you will have noticed, I did not contribute to the feed. As you will have noticed, I saved my rant for this podcast. But no, I'm not judging you. I want to explain why I didn't. Because I've realized, I realize actually in between our last two, between the last podcast and now, this is a recent realization. Those who bit flip MT because of an interpretation of one principle, are not our audience. No, and it because I'm fine with that. So to me, it's I looked at that feed and I'm like, okay, this is not a person who's ready to hear this content. But then I sat back and I realized one of the things that we we talk about is the necessity of systems thinking. And this to me was a sign of a person standing on a soapbox on a principle that quite honestly, 15 years ago, I defended as well. I went maybe even longer ago. But like, I remember when when agile first came out and started running through Microsoft, and I'm like, you want to do what now? What the hell? So I get that. But it took me time to figure out the wholeness of all the changes being proposed. You can't just take one thing on. Because when you look at actually one number one principle and lean, or not the number one, but one of the key principle and lean, is you optimize the whole, not sub optimized because sub optimization creates dysfunction. How many times have you been on a been on a team where where Dev celebrated? Hey, we brought in Dev or code complete by two weeks and everyone passed themselves on the back and it's all awesome. And then they wonder why the test schedule shipped slipped three months. It's because it was a sub optimization. So the next story, this actually goes back to, I think I may have told this on the podcast before, but one of the things that I first observed from Google and then implemented myself. Right? So number one, principle does number five doesn't actually say anything about testing. It says nothing about testing. What it says is testers, you are not the champion of customer quality. The customer is, and your path forward is going to be far better and far more accurate. The second you take that facade off and start trying to figure out what actually is the customer's definition of quality for whatever context you're working on. Mm-hmm. One of the things I observed with Google, like back in the day when it came out, it was shipping bugs all of the time. And at the same time, stealing market share from all of its competitors. Okay. If bugs were that important, someone explain that to me. I can't explain that to you. Okay. In a narrow view of quality where quality is bugs, then, but that's, I can't, I can no longer fathom that. Okay. I will tell you what actually is happening. You can sort of think of a benefit to pain ratio. Okay. So bugs have, some, every bug you ship has some percent of pain it's going to cause the customer, but some percent of benefit it calls the customer. Yes. Right? Because you're shipping some new value. Sure it has bugs, but if that value exceeds the price of the pain, ship it. Yes. Because our goal is to solve problems for the customers. Now, why do we put in, I don't know what principle it is. Why did we put in the, essentially the continuous, Improvement principle. Yeah. The continuous improvement principle. Because that ratio changes or how preventative you need to be changes based on your ability to react to indication of pain. Meaning if I can ship small things continuously and I have the right monitors in place, when I see that I ship something small and it's causing a lot of pain, typically I can revert it near real time. The old days, you couldn't do that. This is where this, the strong, we must validate everything beforehand. Why? Because it was extremely costly, not only in dollars, but in time to get it right. What, one of the important lessons that Google taught me is no, if you have the right monitors in place, you see an indication of pain, you can ship a new release. And actually that ends up being a better experience, a better quality experience. Because the customer, every time they come back, they get this overall impression. Every time I use it, it's getting better. Good point. I'm going to move on because we did a drill down slash recap anyway. So yeah. So all the principles are open for interpretation, but it's also with the lack of the whole picture, easy to forget what we're going after. So the short thing for me, like if I, I'm not going to contribute to, to these threads where it's somebody who's taking me to task on one single principle. My experience has shown is that they're far from ready to have the discussion of how the principles all fit together. And in your particular feed, actually the individual that you were talking to specifically called out that they refuse to, to dive in deeper to understand. And I'm like, okay. I quote craft all over the verification aspect of software quality by writing nonsense in number five. Yeah. So I want to move on. I didn't mean to deep dive there, but I think it's important because that one has come up before in it's a, it's a shift from the old school of testing and speaking of shifts on the old school of testing. I watched a webinar from James Bach. Actually, before we do that, one, I'm going to, we'll come back to that very shortly, but it's interesting talking about people who aren't ready or may not be the right audience. And also that modern testing principles going back to our first topic are neither particularly modern nor testing, but it is aptly named given the car context. Yes. Someone very new to Twitter. I looked up, they had like 10 tweets or something popped up on Twitter just to loop in Michael Bolton and James Bach and say, Hey, what do you think of the AB testing principles? Just trying to cite a debate that did not need to happen. Luckily, I immediately replied and said, they're neither modern nor testing. So see what you will. It stopped there, which is good. Cause I didn't really, they live in a different world. And I, as an example of that, can we talk about that briefly? Like, yeah, I'm not at all certain what a point of a debate would be. No, cause cause it is context sensitive. It is, it is. I think that cause they like to watch Bach and Bolton shit all over people. It is entertaining, but on the debate side, like a debate implies that there's a winner. No, it doesn't. No, I don't think so. To me, that's how I interpreted it. No, I don't, I don't think so. I don't think so. When we see a political debate, both sides may see their candidate as the winner. They gotta make their points. But anyway, it's not, it's not worth the effort. So anyway, moving on. Finally, I watched a webinar from Bach talking about his, he does not like the test automation pyramid as described by cone. Again, I have a lot of respect for Bach. I don't always agree with him, but he can usually make a pretty good argument. I didn't think his arguments were that good. So I'll go over the main points and I'm not doing this to mock him at all. First of all, it's not a test automation pyramid. It's a test automation triangle. Okay. I'm good with that. All models are wrong. Some are useful. It's not pyramids, three dimensional. So it's not 3d. Yeah. So anyway, the way I, we got that important point out of the way. So there's a, let me finish here. I said I wouldn't mock. I guess they didn't mean you wouldn't. Uh, the point of the, the, the, there's a couple of really good points that cone calls out. Well, one, he doesn't call out any number of tests. He said, she should have more unit tests, moderate number of integration tests. And, and as almost exactly quote cone for the UI test, for the end end test, write as few of these as possible. And I think that's really critical. That's, that's one of the biggest takeaways there. And also one of the biggest ignored things. He argued against the triangle slash pyramid. Uh, he got in his test test case counting thing again, says that more down here, how many more, how do you count them? Why are we counting test cases? And to me it's like, I, I feel like you're missing the point. And, the way is this not in cones article, but no assessment. He wrote this and nailed it. You want to write as many unit tests as you can so that every bug that can be caught by a unit test is caught there. You only write integration tests for bugs that can only be caught at the integration layer. You write end to end tests for only bugs that can be caught at the end to end. There's no counting of that. It's a reflection of your architecture. He showed the, uh, software testing ice cream cone anti-pattern where you have the small number unit test, it looks like a cone at the bottom upside down pyramid. And then at the top they have, uh, obviously a drawing of the ice cream that says, you know, manual exploratory testing, you know, to cover, to cover everything, not cover my automation. And then he dove into that one. His argument against the, the anti-pattern was the, the part that was the drawing of the ice cream cone looked like a cloud. So it was like testing that testing was ethereal, like a cloud and didn't matter, which was an odd interpretation. And also he didn't like it cause it was an anti-pattern. It shouldn't be an ice cream cone because ice cream cones are delightful and anti-pattern should be not. I get, you know, the thing I'm wondering Alan is, is maybe you miss box point because to me, if, if you look at it, if you look at it as feedback from Bach on the testing pyramid concept, this all seems like nitpicky semantic crap, but what you didn't see it. So I should probably just, no, I, I, actually saw it, but I didn't go deep. But what you, you started off with the premise is that he didn't like the testing pyramid. Maybe what he was saying is he didn't like how that concept is being expressed. Well, right. Cause I've given feedback similar to that. The point is that I was trying to get to is somewhere in the middle without much transition. So the cone wrote about the test automation pyramid test automation, how you'd write your automated tests. Didn't talk about manual slash exploratory slash critical thinking, intuitive testing. That's where Bach lives. And he's very, very good at it. So he turned it into a justification for that deep thinking exploratory testing that has to happen to some, not all products. So he ended up calling it the round earth theory where sure you want some, you know, you want some good tests at the core, but these are much the representation of size is no longer a count or number or ratio of tests. It was about the depth and, and, and, uh, skill and intelligence required to conduct those, you know, testing is very easy. The stuff at the top is very, very difficult and hard, but the whole thing was transitioned from cones test automation pyramid, saying don't write so damn many UI automation tests to a talk saying these in my context, in my world, these in depth exploratory testing activities are very important and critical and difficult. I found that transition. I found his conclusion absolutely correct. Was it based on the initial premise? Not as far as I could tell. And of course, like all models, people interpret them incorrectly. Just like we talked about principle number five being a model. People, of course, can interpret the test automation pyramid incorrectly. They can, they look, people call it the test pyramid and they go, well, where are the manual tests on the manual tester? And things evolve from there, but that wasn't the point of the original model. The original point of the original model was not to demean or discredit exploratory testing, which is watching box presentation the exact way he took it. So that's where I thought he went off the rails. There is a history that I've observed in the last couple of decades with, I don't know if it was Bock or Caner or Bolton or whoever, but in the beginning phases, they were anti, these people, the ones I can't fully remember, were very much anti automation in the first place. Right. It's just not their, it's not anti, it's not in their, they don't get it. They're not, I don't know if they're against it. They don't understand it at all. One thing I think that, so I think they actually came to a similar conclusion that you and I have on multiple occasions and they went a different direction. I don't think I'd necessarily support that direction, but the direction I think they went is they said, okay, doing testing right. It's knowledge work and it requires thinking. And as you and I have actually also called out on multiple occasions, test zombies are prevalent and test automation zombies are prevalent. Oh, absolutely. Where, oh, I just, my job is to take manual test case and automate it. That's it. Yeah. And you don't think about how, how, you know, six months down the road, you can't write any new automation because all your time is being spent maintaining it. Selenium Conf is going on right now. Maybe it ended yesterday. I can't remember, but it caters towards an audience of testers who in general who write and, and web test UI automation as the bulk of their work for teams writing massive amounts of end to end UI automation. That's all it's, I'm not arguing that world doesn't exist. I argue whether it should exist. Selenium, I have no fault with Selenium doing that, right? It, they're, they're playing to what their customer base wants. What if we want the customer base to want something else, we have to influence that. Because unfortunately that cussed, that's not what that customer needs in my humble opinion. Yeah. But in the end, I watched it and I, like I said, I have respect for Bach. I just could not follow his argument. And in the end, I realized this is all about testing. I really don't care that much about testing anymore. I really, really, really care about quality. The testing part, I mean, I know it needs to happen and I can help people and coach them, but it's not, I shouldn't say I'm not excited about it. It is not at all the focus of what I think about or what I do. I think, and you're going to agree with this. So I personally disagree. I personally am with you on that, but the testing topic is important. And in my humble opinion, it's important because it has not culturally moved to where it belongs. And that is back in the hands of the people writing the code. You cannot test. Someone smart recently tweeted, it's 2019 and you still can't test quality into the product. Someone very smart. Iffy, how does this go with smart? And that's absolutely true. Test cannot win this quality upstream battle by taking defense and still holding on to test and saying the idea that test, the idea that you're trying to ship bugs to a customer, in my view, just just exposes a defensive argument and it doesn't allow the system to move forward. If you enable that one, then things like cone stuff, in my view, begins to fall down. If you have a test community, if you have these champions of quality, then that broadcasts, right? The champions of quality, one of the big problems is the codependency loop that we've talked about. If you think you're the champion of quality, then hey, Dev will too. And then Dev can blame you when they ship a bug. One last point to bring up related to something you said a minute ago is people will naturally be defensive against these principles if it threatens their core of who they are and what they think they are. Remember, there are people whose role in this world is to write test automation and do testing. And if we say, as you did, and as we believe, that testing can be done well and easily and more efficiently by the author of that code, what do those people now do? But we're not threatening them. No, I know we're not. The thing I'm trying to do, they feel threatened. The thing I'm trying to do to reach them is to state that there is another way, right? We're not saying your thing's going to disappear. We're saying you need to think about where your skills are going to be valuable now. Absolutely. Okay, we got to go. Bye. See ya. 
