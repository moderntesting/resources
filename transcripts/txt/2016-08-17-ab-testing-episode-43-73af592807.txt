Hello everybody. Hi. I'm Alan. I'm Brent. And we're back for episode 43 of AED testing. We are back after a break. I apologize again for the audio yuckiness in the last episode. Hopefully it is better this time. I have upgraded some equipment to try and make it Brent proof. That implies that the audio yuckiness was due to me. Well, some of it was. And I believe it was your channel that went offline. We had multiple issues, multiple issues. When you try and take my little weak signal and amplify it, I end up amplifying some of Brent's microphone stand fondling. I have fondle proof at the microphones. And the Darth Vaderism, that's what bothered me the most. I'm like, wow, I really should go check out my breathing. Yeah, hopefully this one is better. So what's new with you, Brent? I have just come back from Kauai. Nice. It was. Have you been? I've been to Maui. OK. I've been to Kauai. Unless you're an outdoorsy guy, there's not much to do in Kauai. Sounds fun to me. We're not outdoorsy people. Oh. But there are beaches in Kauai. And we are definitely beach people. I spent at least 50% of my time in the ocean. My favorite story was I was out. You're as pale as ever. So you must have had some good sunscreen. I am not as pale as ever. Slightly less pale. I am golden brown and delicious. My best story is I was out snorkeling with my younger son. 14. This is only his second time ever he's been snorkeling. And the first time, he was like eight. And the waves were pushing him around. This time, I was following him. I was a good father paying attention of his son. And suddenly, he just starts freaking out. So father syndrome kicks in going, ah, crap. I'm going to have to punch a shark in the nose or some bullshit. And he was actually freaking. He starts pointing. And he was swimming right next to a giant sea turtle. Oh, that's cool. That's way cooler than punching a shark in the nose. It would. Surviving punching a shark in the nose would probably be a much better story in a UB podcast. But as we kept on swimming, we discovered we were actually stuck in, I don't know what you call a group of them, but a swarm of sea turtles. There was like 10 of them. And that rocks. It did rock. Just to amp up the drama just a little bit. You're not allowed to touch the turtles. They're an endangered species and touching them equals bad. But so it was a real challenge when you're stuck in a swarm of these things, battling the waves and making sure, because the waves kept on pushing me towards the sea turtles. Anyway, there you go. Very cool. It was fun. Nothing new with me, but there's something new with A.B. testing for episode 43, something brand new. So we have a sponsor for episode number 43. My good friends at Star West asked if I'd give a plug for their upcoming conference, and I'm going to do it. Because if you look at all the conference talks I've done over the years, one conference I've done way more than any are the Star Conferences. And in fact, I've probably been to more Star West conferences than any others. Huge fan of Lee Copeland helped put together that conference. I just enjoy the crap out of it. And I make, I'm not, obviously not doing this Star West, but I'm trying to get back down there again. I kind of like going to the conference near Disneyland. It's kind of a lot of fun. Is it in LA? It's in Anaheim. It's in the, it's, actually I didn't look up the hotel. Usually every time I've gone it's been in the Disneyland Hotel. This is, as far as speaking goes, I've enjoyed it and I encourage A.B. testing people to go. If you're looking for a conference to go to, looking for a way to connect with testers. The talks, there's a couple of great keynotes. Jason Arbin from Applause. Do you ever meet Jason? No. Jason helped James Whitaker write the book that was ripped off of my book, the How Google Test Software book. Oh. So he's at Applause, but he is a really smart guy. Does a great job. Jeff Morgan, also awesome. Tons of track speakers. And what I encourage people that go to these conferences to do is, is you get some value and learning and ideas from the speakers. It's not like you're gonna get every single word said or things you're gonna take home and apply directly, but you're gonna find nuggets, dozens if not hundreds of nuggets, but some of the best learning happens at these conferences. For me, and for pretty much everyone I've talked to is in just talking with other people at the conference and getting a chance to talk to some of the speakers one on one after their talk in the hallway at lunch, at the pub, whatever. It's a great way to learn. So that's where I get the value out of conferences is from just talking to people and learning from each other. So I have not ever been to any of the stars. Back when Whitaker was a prominent keynote speaker, he's invited me a couple of times to Star East, but it never was convenient for me to go all the way to Florida. Getting knowledge around current techniques, I have seen particularly Lee is, of the leaders of these conferences that I have visibility to, Lee is paying attention to the data movement, the agile movement, and he's not fighting against it. Absolutely not. And there are, again, you need to take everything with a critical eye. Everything you learn whether it's from a conference, from a book, from an article, from a blog, from Twitter, you need to apply critical thinking is how you decide what's worth learning versus what's not learning. So you need to apply that always. Blind faith or blind, like, oh, what you said must be true, is a recipe for disaster. So that's with that caveat in mind. So Star West is, I'll give a little plug now, now that Brent is totally taking us off the rails, but Star West is, I think, one of the longest running test conferences out there. And this year it's in October, October 2nd through 7th. It's in Anaheim. I'm going to assume Disneyland Hotel, although I did not look that up. There are over a hundred sessions. One thing about Star is they run massive concurrent tracks. So you can always find something. And one thing I encourage, and Lee will encourage you to do as well, is if you go to a talk and it ends up not being what you want, it's okay to leave. Don't throw up your arms, go, ugh, and leave, but it's okay to go check one out. And sometimes if there are one or two or three talks I want to see all at the same time, I will plan to rotate through them, try and get the best knowledge I can. So there is great information and everything from test automation. There's a bunch of stuff on agile testing, perf testing, leadership, et cetera, tons of junk. And then I mentioned networking as the way to learn. And they have, in their expo, they have a bunch of exhibitors, I think like 50. You can speak one-on-one with the speakers, get some free consulting. Even more, there's a full day session on, a bonus session, Women Who Test, which I think is really cool. And a great way to, another great networking opportunity for women who test. Hey, Brandt. It gets even better. So AB testers, if you want to go to the conference, do you like to save money? Who doesn't like to save money? I love saving money. So if you, you can, believe it or not, and not maybe applicable, but if you go to the Star West site and you sign up for the conference and you want $200 off, just enter the promo code AB testing. I've never had my, I don't think I've ever had my own promo code before. Me, but either. It fills me with glee. It really does. So I'll post a link. It's well.tcwaxstarwestab. I'll post a link to that. And I encourage AB testers who, looking for a conference to go to, to attend. And I have confirmed that it is at the Disneyland Hotel. Yeah, I didn't want to make an assumption, but thank you for confirming that. It is at Disneyland Hotel. I'm really not going to try and edit that in. Great conference. The AB testers can find some value there and use that discount code to save you or your employer a few bucks. What's the discount code again? AB testing. All one word. You know what that's worth? $200, $200 off. Sweet. Thank you Star West. All right, man. Thank you for our first and potentially our last ever sponsor. Okay, moving on. So our good friend, I wouldn't call him a good friend because I don't know if we have any good friends, but our one of the three, Danny Thott. Software Alchemist posted just this morning last night. I just saw the post today. So did I. And I wanted to talk about a few things in the post because he gave us a shout out. And the post is about the demise, the fadingness. What does he call it? A black box testing. Brent's learning how to operate his mechanical electronic device here. The name of the post, the black box tester role may be fading away. And we could have a long discussion about the truth and non-truth of that. I think it's definitely true in some ways and not in others. For example, on, you know, I am not a black box tester. I do a bunch of different things, but I employ black box testers. I was trying to think through the last time I had a black box tester as an employee. Yeah, and these are vendor positions. And it might be measured in decades. I don't know that all teams would need a black box tester. I think, I think it'd be, again, a long discussion. Happy to have it. Maybe not today. Cause what we really want to get to is Danny gives us a little bit of a shout out. Yes. So can you read what Danny says? Yes, and thanks Danny for the shout out. Of course. What Danny says is I have been following Microsoft employees, Alan Page and Brent Jensen on their AB podcast. That's us. The AB testing podcast. They have had fairly traditional testing roles and are now in roles that seem much more future proof. Brent is now a data scientist, specifically principal data scientist manager. Alan, principal software engineer describes himself as a helper, doing odd but challenging tasks that don't easily fit the developer roles on his team, which is something that appeals to me, Danny. Both are still involved in the testing process. The project structures they describe seem to be on the leading edge of the future of the software testing role. In my limited view of the software industry, I don't see many companies that are anywhere near Microsoft in their evolution. If the traditional black box tester role is fading, I think it's going to happen very slowly. I think it will require a very broad view of the industry to track a slow evolution like this. And I'm curious if you've heard from anyone who is in a good position to see it. Danny does have a caveat there. He says in his limited view of the industry. I think Danny actually has a fairly non-limited view of the industry, yet I'm going to say it right here. I'm glad you think that Brent and I are on the leading edge of the future. I'm going to get the words wrong here, but I don't think Microsoft is anywhere near the forefront on how unified engineering or how to function in an org without black box testers. No, I- My org is because I'm awesome, but Microsoft as a whole. No, even I have a ways to go, I think. I would say like us being on the, Microsoft being on the leading edge here. When I read that in this post, it kind of took me aback a bit. Alan and I have been talking on this topic in various different communities for it seems like five years now. Okay, for the record, I don't think you have to drink for that one, that was well put. Thank you. All right, go on. And when we first started talking about this, we, much like I think Danny is doing referencing us, we were talking about other companies that we were aware of that were pushing the boundaries and leading this edge. Like when, like we've talked about, I'll go ahead and drink. He's drinking coffee for the record. Yeah, make believe Irish. We've talked about it on the podcast that not all of Microsoft has really moved towards this transition. Although I, except for vendors, like I can't think of a single team that employs a full-time black box test area at Microsoft. Oh no, absolutely. I'm 99.99% positive that they don't exist at Microsoft. I would tend to agree. And if they do, it's an extremely rare context sensitive situation. So when Microsoft gets accused of the leading edge, what happens to me is I go, but we're so far behind. Yeah, that's the way I feel too. I see a lot of companies really better at this, better at doing, like I'm trying to work my team, for example, towards continuous deployment. Yep. Which requires such a high level of daily quality that we, and I know plenty of other companies do this, plenty of other companies bigger than and with more baggage in some cases than what my team has, but they're able to pull that off. They've made it farther. I still feel like I'm chasing where I think we need to be to be a good 21st century software engineering team. I do, I spent some time thinking about why do I feel this way? And we've heard from other of the three around things like, oh, we are isolated, Microsoft environment isn't like everybody else, right? And that may in fact be true. The thing that as I was thinking through about what could be the difference between my perception and say Danny's or other listeners perception. And one of them I thought about is all the companies that I know of that don't employ testers, the vast majority of them never had testers to begin with. Absolutely true. So it's very possible we're leading on the executing this transition. Yes, and that's actually a good way to put that. Nice insight there, Brent, because I think that the transition is something that we've struggled with and sometimes done well, sometimes done poorly, but it's that transition that's really hard. And to be clear for any first time listeners who haven't heard this before, nothing to drink. These companies that don't have testers or teams that don't have testers, they still guaranteed, still have people focusing on quality or focusing on testing activities. Those things still happen. So I don't want anyone, every time we say no testers, I feel like I have to remind people that means it doesn't mean no testing. That is correct. Like, and we're proponents of it on the podcast. There is a new balance that you need to strike in terms of preventative care of your software. And in my humble opinion, it's just simply more productive and more efficient to have the testing tasks accomplished by a specializing generalist versus a single individual whose sole duty is to be paid to do that testing. Yeah, I agree. I think, and Danny talks in that article about, and even he references Kim Caner's article talking about the need to pick up some programming skills or some, doesn't be programming, could be technical skills. I'm looking for someone from my team, not necessarily to do programming, but someone who is a wizard with configuring websites and deployments and Azure site management, et cetera, et cetera. And it's technical, it's not coding necessarily, but it's critical to have. And those people that can do that and keep an eye on quality, I think they're as good as gold. You can stay employed if you have a breadth of skills. The one thing that Danny has talked about that I do think is, so I got a LinkedIn message from one of our listeners in India. You wanna take a stab at the name? Looks like a Polish name. Ewa, it could be Eva with a W, Barzakoska. We'll go with that. Working in India. Yep. Barzakoska. Barzakoska. Barzakoska. Barzakoska. And she sent me a LinkedIn connection invite thingy. Okay. A linker? Yep. And wanted to be able to ask questions, but in her last message sent the following. Hey, Brent, you know, one thing I really liked that I had not had enough characters in the invite to express is that you guys are still learning. One of you mentioned taking a MOOC in analytics. Do you know what that is? No, master of occupational crime. Yep. And I'm learning to code right now and from time to time, I also take MOOCs and find it really encouraging to hear that people who are already established in their industry spend their free time learning new things to stay relevant. I wonder if OC is online course. It might be. Something cause I do, I'm not always doing one, but I frequently do online courses in things I just need to know more about to feel relevant. But to hijack the thread here. Well, so I got to the, then she goes on to discuss how awesome the podcast is. I'm blushing. So thank you for the feedback. But the point that Danny and Eva, Eva, sorry about is. Who by the way, no longer likes the podcast, given our butchering of the name. Yeah, I hugely apologize for that. Is it's really important to have a sense of what you need to grow into? Yeah, and so first off, the easy part is if you're in your job and you think, I'm pretty good at this, I'll just keep doing this, no need to learn anything else, either at best career stagnation, but at worst career limiting or more likely career limiting. But there's that challenge of what should I learn and how much of it should I learn? And that's a contextual and an experienced thing. I read this book maybe as long as 20 years ago. I think it was on management or leadership. And one of the things that suggested is that you should always look for ways to learn. Like read, give examples of magazines. And don't worry if they're not in your field because you'll pull ideas from them. That breadth will allow you to connect things in the way that Steven Johnson, in where good ideas come from talks about that network effect. And so for about five years, I subscribed to this little 12 page magazine called Science News, which came every week or every two weeks. It was the most fascinating thing in the world. Nothing to do with testing, nothing to do with software engineering. Occasionally about technology, but it was, imagine like scientists doing active research could publish not in the boring academic way, meant for people like me to read, articles on their research every couple weeks. It was a great outlet for them, I imagine. But just, it was some of the best advice I'd ever been given through a book in my life. But I learned just enough. It tickled my brain enough to make me want, to give me some expansion. But as far as learning, look for online courses, look for things you think you need to know more about. Sometimes, I can't remember why, I remember why. When Microsoft was making this move towards data science and data analysis and business intelligence, I wanted to be ahead of the curve. So the reason I'm able to fire off, Brent always freaks out, Brent is a real data scientist, freaks out when I fire off mathematical terms or data science terms in the right context and make them make sense. I've gotten used to it, but I would say- It does freak him out, and it's because- When he tells me he's gonna go present how to do A-B testing to a workshop of hundreds of people, that freaks me out. And then I tell him what I'm doing, and he goes, oh, okay. Because I took the time to go through courses and reading and videos to learn how this stuff works. And then if you wanna make sure you know something, the best way to make sure you know it is try and teach it. So if I can, it's one thing just to repeat things I've read, but that works great as long as there are never any questions and as long as I can explain it perfectly. But you have to teach something, you have to know it well enough to understand the nuance and how to- And the other thing I would add- Yeah. You should target teaching it, and you should target applying it. Yes, that's absolutely true. But the reason, and I'll finish up here, but the reason I learned those things, I wanted to know them and I wanted to know them well enough to be able to teach them to other people. I think there's some valuable things in there. So that's how I figured out how much I had to learn of it. So I could teach it to other people. There's an old phrase, and that is the only constant in life is change. And back in the day when I had my first management job here at Microsoft, it occurred to me pretty quickly that in the software industry, we change much more frequently than most industries I'm aware of. And I discovered very quickly that the people who did best were not those who learned to survive change, but thrived on it. And learning new things, getting ideas from other industries. I'll give you a recent example of something that I'm working on. I run a service organization, and I'm busily spending a lot of time reading white papers on healthcare. And the primary reason why is most services have some form or another of signals, often referred to as pings or heartbeats around how well the service is operating. Well, the healthcare industry also spends a lot of time worried about how well heartbeats are operating. And they've done a lot of advances around near real-time analytics. So I'm reading a lot of these white papers and mapping several of their efficiency concepts into something that's service-based. It reminds me, one of my bachelor's degree was in math. And one of the few things that have stuck with my math degree was this concept of a mapping function, where if you knew how to prove something in one environment, but you're in a different environment, you could map from environment B to environment A, do that work, then map it back. It's kind of the same thing with these concepts. Healthcare, yeah, they're worried about real heartbeats, but you can map it to a more technical concept, use the same solution. In terms of learning, I would recommend people to not only look for online courses, I would spend 30% of your time just looking for things that are, you have no idea if it's going to apply, and then spend a good portion of those things that are something you've never done, but you have a 50% confidence you can actually use it. One of the things I got from, I mentioned the science news story earlier, the little thin magazine, one of the things I got from that was exactly what you're talking about. It's nothing to do with software, but every once in a while, I would find something I could map. I wasn't looking for it, I just read it because it was interesting as hell, but I was able to find, it tickled some ideas and it may not even be direct. It may not be like, oh, I can use this plant genome mapping to figure out software, no, but the ideas would tickle something. And I think if you want to stay on the edge and you want to really advance your career, and for me, if I'm not learning, I am just bored as hell. I have to be learning, I have to be learning something new. So I think that's why I'm still employed, even though my degrees are in music, perhaps. I know it's why I am. The last thing I'll say on this topic is, I know what really spawned this for me. You mentioned your, what was it, science news. For me, what inspired this was really reading two books. Number one, Green Eggs and Ham. Nope. Christopher Alexander's book, The Timeless Way of Building, as well as, you know the head first series of books? The head first design patterns book. That's a very good, that became my favorite book on design patterns. Even if you don't know coding very well, I would recommend the design patterns book because what we're talking about, like in my healthcare example, and the example you just gave, what we're really talking about is universal process design patterns that apply in a context free fashion. You know, that's a really good example or recommendation for, you know, testers talking about being technical. I better learn to program. I better take a course in whatever. But as far as breadth goes, and specializing in generalists, that head first design patterns book is maybe a great recommendation as one of the first few books you learn is you wanna just start broadening. Yeah. It's really good. I've read the Alexander book as well and I've even read, have you read the Gang of Four patterns book? I have the Gang of Four. I do not recommend reading it. No, I do. Like what if you can't sleep at night and you wanna. It works for that. Yeah, it does. Exactly. It's great for insomnia. For design patterns, like the way I've, Al Shalloway and the, okay, he's a buddy of mine. And yeah, he and I go to lunch quite often. How come you never invite me to lunch with Al? I'll do it next time. You better. The way I tell. Purely so I can name drop Al Shalloway too. The way I tell people if they wanna dive deep into design patterns and you don't need to, if you're one of the three and you spend most of your time as a black box tester, just read the head first one. But if you wanna go deeper, then pick up Al Shalloway's book, Explaining Design Patterns. Head first design patterns will teach you why design patterns are freaking cool. Okay, and. In great ways. In great ways. With examples using pizza and other things. Yeah, and what was it? Star buzz. And the duck thing. Yeah, it is just. It's a great. It's really well done. It's really, it really is. And just to interrupt there, I've read, that was the first head first book I read and I've read a few others. They're good, but not as good as the design patterns one. It is the best head first book. But the head first, the head first style, if you have a choice between getting a blah, blah, blah for dummies or the head first blah, blah, blah. Or even the textbook. Get the head first blah, blah, blah. Yeah. Then go look at the explaining design patterns. Because what Al does really well in that book, is it shows you how to design with design patterns. How there's this fantastic process called CVA, which I'm no longer remembering what the acronym stands for. Where it allows you to pick and choose. So let's say you got to connect two components. Do I choose the adapter pattern? Do I choose the bridge pattern? There is a, there is a great process that he goes through and describes around how to pick which design patterns to use. And then the head or the gang of four book, Dear God, never start with that book. That book is a reference. This book is, it's like going to the index card section of a library. It tells you what other things exist. So if you going through and you're designing, you're going, you know, I don't think this pattern fits. That's a good book to use as a reference. And I have, I probably have 10 different books on design patterns in my library in my office. Those three are the ones I use the most often. Very cool. Hey, we have just a few minutes left, but I want to make sure that we get to the mailbag. I love mailbag. Who doesn't love mailbag? And we've done this before with other people where we've highlighted one person for pretty much the whole show. And today is apparently Danny Fot day. Because Danny Fot had a question. He asked in our one of the three.slack.com channel team. He asked, what do you think of property based testing tools? And there's an example, I haven't tried one in anger, but from what I've read, these appear to be useful tools for exploratory testing, but are not a substitute for unit tests given their non determinism, et cetera, et cetera. What do you think of these? So the question is really, there's some discussion afterwards in the slack messages, but really it's, Brent's looking at me very confused. So go back to that lesson, they're not suitable for unit tests because of their non determinism. Why are they non deterministic? Not the point. There's a link in which Dave Thomas makes somewhat trolly assertion that unit tests are a waste of time. Everyone should be using quick check. I said trolly because other comments make it clear he's actually railing against bad unit testing. So his position is probably a bit more nuanced in the statement implies, which is absolutely true. I also am against bad unit testing. I'm also- I will take a strong stance. So- Wait, I'm gonna lay down some controversy. I am 100% against bad testing in all forms. For once I'm going to agree with you, Brent. So since the dawn of time or the dawn of software engineering, there have been companies who strive to not only make our lives easier, but to advertise the crap out of how much easier they've made our lives. Whether it's in most testers have seen the dozens, if not hundreds of tools that will allow you to automate all your testing without writing any code. Here we have an example, not quite as bad as that. We have an example of tools and Microsoft has had some similar tools. They're part of Visual Studio now, which do some automatic unit testing by looking at the parameters and the contracts and generating a whole bunch of unit tests, different parameters and finding the bugs for you. Great tools. Great tools. They help you find some bugs. Some of which may be very obscure and wouldn't be hit, we'll call them edge cases. But as testers, we know the value in fixing the edge cases because when you have a million users, you have a significant portion hitting those edge cases. So all good. But are they a substitute for unit testing? And my answer is absolutely no. They are a supplement for unit testing because the value in unit testing, even if you're not doing test driven development, is it makes you write better, cleaner code in the first place because you have to write code that is testable. I have trained. You said even if you're not doing TDD and that's not true, if you're not doing TDD, then you're writing your code and then using the stupid tool to generate stupid tests. So, no backup. So now I wanna say, if you're writing unit tests without doing TDD, I have trained dozens, hundreds of developers in both writing tests with using TDD and writing unit tests without using TDD. Now TDD, we can talk about it. Okay, I will talk about it. Because you have to write your tests first, you consciously think about how that code has to function. Using TDD makes you write more testable, better designed code. But what I found in experience, even if people don't jump onto TDD and write the unit tests afterwards, once they, in my experience, not everyone, but the majority for sure, even if they know they're going to be writing unit tests, just the fact that they know they're going to be writing unit tests, they write clearer, more concise code than they would have if they knew they weren't going to test it. Or when they go to write that unit test, the first thing they do is go, oh crap, I can't test this. Let me refactor my code and make it more testable. Those things do happen. TDD, in my experience, is the most efficient and best way to get that code that way in the first place. Yep, but for those that won't jump in, I do think that just writing the unit tests and having some discipline around that has not equal, but in some cases, similar benefit. One of the questions, I don't know if he had it in this particular response, but one of the questions that popped out, Alan alluded to it, or Luke, Alan eluded to it. Hey, is unit, is, I'll call it PBT, a replacement for unit testing. And I'm going to say, absolutely not. Well duh. And the biggest reason, because, so I love tools. Tools simplify our lives, they make everything better. But there's one thing that they do that's really bad. And that is they encourage intellectual laziness. So any developer who picks up these tools that go, hey look, I just generated 3,000 test cases, my unit test suite is done. Get none of the design benefits that you just referred to. Yeah, I think for a lot of things in testing, if there's some sort of dichotomy, it's probably wrong. So does automation replace testers? No, does it enhance them? Yes. Replace testers? Testers or testing? Testing. Testers, yeah. If you have a bunch of automation. It's not the point you're making. So the point is, we look for things, and we don't look, we're told that a tool or something will replace something else. But in most cases in testing or software engineering, the tool merely enhances or supplements. In fact, not merely, that's the main benefit. And I think if I could jump into marketing of testing tools for a while, I wanna talk about how they supplement, not replace. These tools we're talking about as far as test generation based on inputs, totally supplements unit tests. If you wanna run those, I think it's great. I think you'll find a lot more bugs. You still have to write good unit tests. If someone pick random Juju Fruit, it's probably too close to something, there's so many names of automation tools, but Juju Fruit automation framework. Great test automation, don't have to write any code. Is that gonna replace my unit test? No, my integration test? No. Is there no use for it all at all? No, because I may decide like, oh yeah, this is automation tool, works at a very high level. May have some false positives. May have some flakiness, cause you're automation, but there may be some, I would guarantee there's some places I could find to use that, that would give me some coverage I couldn't do by myself. But it doesn't replace anything I do. But the point is I can use any of these tools to enhance what I'm doing. Not to replace things I'm doing. You can, the thing I still wanna sort of rail against is it is not, none of these tools. So let's talk about it a different way. The benefits of these tools. Okay, so if the goal that you're trying to achieve is a checkbox claiming I have a unit test suite. You know what? Test generation tools kick ass at that. If that's the goal you're trying to do, knock yourself out, you're going to have a bunch of tests really rapidly. That's worth a tangent because, very quick, I'll just make a point that, I love that you bring up the checkbox, cause so much of what I hear and experience in engineering is about, there's a lot of checkbox engineering. We don't need to get the benefit of this, we just need to say we've done it. Right. Go on. And I do know, particularly in the last five years, as Microsoft has gone through this transition, sort of a bookend of the podcast today, there are several devs that just like, God damn it, I don't wanna think about testing. I just want a tool that just gives me that checkbox. Right. Now, if you wanna move up the stack and have your code be better maintenance, easier to maintain, then, even then, these test generation tools will help you and you'll still have to provide a modest amount of thoughtfulness in order to maximize that benefit around the maintenance cost. If you just go, I'm gonna generate a bunch of test cases, yay, checkbox, well, when that test suite runs, you're going to be stuck into a bunch of maintenance, understanding why this test case is suddenly failing, blah, blah, blah. So even then, so winding up, this can help reduce test maintenance. Now, if you wanna go even further and say, hey, I want the code I write, have the maximum ROI, which means reusability, the ability to integrate new features on demand, reacting to the marketplace, then you're going to need 70% thoughtfulness and 30% this type of tooling. This is where, again, where TDD comes into play. TDD forces you into that thoughtful design process so that you can construct your code in a way that yields that top tier of benefits that I just mentioned. Yeah, I think there's a whole podcast on sort of, I've been thinking about it while you're talking, checkbox engineering versus craftsmanship. I love these tools. The biggest problem, there's a new catchphrase. Automation is great. Automation allows you to take a lot of these painful things and do them really easy. But the problem with automation is it also allows you to really reduce the friction on delivering on really crappy outcomes. Like, now I have a test suite of a million test cases all auto-generated where on any given day, 25% are failing. Oh God. Yeah. Do the math, that's awful. Yep. All right, we are done, and it looks like my microphone's still recording, so I'm pretty happy. I'll get this edited and out to the people soon. Thanks everybody. I'm Alan. I'm Brent. We'll see you next time. Yep. 
