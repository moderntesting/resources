This is the AB testing 343 podcast, a podcast where we ask one of the three listeners of the AB testing podcast, three questions about almost anything. ABT 343 is a fun slice of what's going on in the world of modern testing. Let's get started. We are back again for another shot at the ABT 343 podcast. And this time we have Dave Westerveld all the way from Kitchener, Ontario, Canada. Say hi to everyone, Dave. Hi everyone. Glad to have you here. Thanks for taking part. Once again, the ABT 343 podcast is a chance for really for the listeners of the AB testing podcast to get a chance to learn about each other a little bit more and for people who maybe aren't in our Slack community to get a chance to see kind of what's going on in the world of modern testing. The format is, as I've described before, is I will take one of our three listeners because there can only be three listeners to the podcast and I'll ask them three questions in about 20, 30 minutes at the most. We're out of here. Are you ready for these questions? I'm ready. You were a superhero doing modern. No, just kidding. That would be fun, but awkward. I am a superhero. DC or Marvel? Oh, actually, I'm Marvel, I guess. Okay, good. I have to admit I'm not a huge, like I enjoy the movies, but I'm not like a big fan. All right, then you can stay. You can stay. It's all right. But that wasn't my real question. Don't count that one. I'm going to merge the first question into two. So can you tell the listeners a little bit about what you do, including what kind of software you work on? Catch yourself on that background. Tell me a story. Yeah, so I guess what I work on right now is something known as LMS or learning management systems, which is kind of similar to, well, CMS is maybe another big word that you hear a content management, but with an LMS, what we're doing is kind of helping educational institutions manage their learning of the whatever system it is. So an example might be if you want to do online submissions at a university, we kind of provide the platform or back end for that. So it's a it's, you know, fairly complex corporate ish software in that way. And then before that, I worked actually at a company that made desktop simulation software. So like engineering software, heavyweight computing, so kind of some pretty different realms of software testing in that way. So I've kind of had a lot of, I guess, a few different things, never really been in the, you know, highly startup tech world of that sort, but getting exposure to some of these more corporate approaches to testing and ways of thinking about software. Yeah, and my background is pretty much I actually went to school as an engineer for chemical engineering, and then ended up in software testing because they wanted this engineering software, they wanted someone in engineering to test it. And over time kind of got more and more into the automation and that kind of stuff. And it was like, you know, I don't think I'm ever going to have a career as a as an engineer doing chemistry stuff, I'm going to be in software. So that's kind of how I move to more software roles over time. That's cool. And a couple things I wanted to dig in on one is I'll go backwards. I don't forget too much. So because you're an actual engineer, I know Canada has laws around, like you can't call someone a software engineer if they're not actually an engineer. Can you still call yourself an engineer? I actually can't because I never finished the requirements. So I did my education. But then to officially become an engineer, you have to have five years of experience and you have to write another exam. And I've never did that second exam. So I am technically not an engineer, although I do have a ring that I wear on my pinky finger, which is something a lot of engineering Canada do, which is called an iron ring. It's like a reminder of being ethical in engineering. That's that's worth the price of admittance right there. And then I wanted to talk a little bit about so are these universities and schools using your LMS system? Yeah, so we have kind of three main segments like this K 12 schools and then universities. And then we have another segment that we're trying to get into more, which is corporate training. Yeah, that's my exposure to LMS systems. We have one at my previous company, a place called Microsoft. We had a couple different LMS systems that actually worked in a learning and training group for a while. So I know I have a love hate relationship with those. Yeah, we're just looking on Reddit today at some comments about our system. And seems that many people have a love hate relationship. It's quite entertaining, actually. Reddit's just generally good for observing love hate relationships. Yeah, that's really interesting. I love hearing about the different kinds of software people are working on because I've worked in the you almost made it sound like a bad word, but I've worked in the corporate environment for my entire career. Even before Microsoft, I worked at a company making audio software, mostly with sound cards. So and then of course, I'm at Unity now and I'm still making it's corporate sounds like a bad word, but it's still corporate ish. Yeah, and it's and that's probably a good point, right? It's not necessarily a bad thing. It's just a different kind of environment than being in a startup or something where there's more rules, more processes, more of those kind of challenges to sort through. Yeah, speaking of challenges, can you talk about some of the big challenges you face in your day to day? Yeah, I find that where I'm working, we're trying to be more data driven, which is good. I fully agree with that. I think that's a great way to approach it. But there's definitely a lot of challenges that come related to that. I mean, there's a typical challenges of time pressure, like got to get these features out, got to get this thing that we need. And, and then it's like, well, what about putting telemetry in and yeah, we'll get to that next sprint. So those kind of challenges where it's hard sometimes to collect the data that you need. And then we also have a lot of challenges around how do we synthesize the data, so we get data from a whole bunch of different sources, right? You can get data from even in a sense, kind of some public data, I mean, talked about Reddit, Twitter, those kind of things, where you can get some data about things that your users are thinking and feeling. But then also taking in those sources of data, your telemetry, your system logs, error rates, all these different kinds of data sources. And then we have business intelligence data as well. So we can kind of, you know, look into what's happening in some of the databases. And then finding creating a, I guess a narrative out of that is a big challenge. Like we'll have someone come they say, well, here's a problem. This thing is going wrong. And we look at it like, well, I can't reproduce that. So I'm gonna go so I go try and look, okay, where in the session, did this happen, try and find the actual error that they saw in the logs, and then try and cross correlate that to figure out what was actually going on. And so there's a lot of challenges around using data effectively. You know, we talk a lot about data, but to get to the point where we can actually use it to drive business decisions is a big challenge. It's a part of my job that I really like and enjoy, trying to figure that stuff out. But definitely a big challenge that we face right now where I'm working. The cool thing about that is, is you are, despite those challenges, you're a step ahead of a lot of folks on their data journey. You know, Brent and I talk about data maturity and your data oblivious, and then your data aware, you know, it's there. And then you're on that, you're actually into data driven and working on becoming data centric. So it's kind of cool to see that, that, that growth. You know, for me, I worked on debuggers a lot during my career and I love debugging. And when I got to a stage where I had enough data around me that I could begin to debug stuff actually happening, happening, which kind of word is that debug stuff happening in the real world from the data we collected, that was magical. I love it's having, it's detective work, but all the clues are there. Yeah. And there's nothing like the thrill of, you know, having looked through, gotten some alert from, oh, there's some error, looking at it, digging in, figuring out what it was, getting a fix together, and then having a customer complain and say, oh yeah, we've got a fix ready. Like there's just nothing that beats that thrill of we found the error before you even told us about it. I've had times in the past where a customer reports and error said, yep, we knew about that, we fixed it, we'll be out in the next batch because we saw it happening often before they even noticed something was weird. And I love like the being able to figure out and break stuff down and go, oh, we were seeing errors, but only on users running Firefox on Windows XP with this video card. Interesting. Okay, we can go ahead and spin up a VM for that and find the repro and see if we can figure it out. But those things are always fun. And I like the differentiation here. There's data for making sure we're building the right thing, making sure that we have, we're able to measure customer success. And then there's sort of the BI, which sometimes fall into the vanity metrics. But talk about some of the BI metrics you look at that influence your testing. Yeah, so an example, actually, not that long ago that we did, we're looking at kind of free working part of the UI. And as part of that, we're like, okay, what do we what functionality do we keep? So partly, we wanted to simplify it partly, we have a couple different experiences for the same thing, we want to unify it across those experiences. But what we're looking at is like, well, this is an opportunity to kind of revisit what we actually do functionality wise. So we pulled data on, okay, what are our customers actually doing in aggregate on these things. So in our case, it was rubrics. So you have, you know, basically a table that you can use to help assess how your students are doing. And like, okay, we offer HTML in this like HTML editing, you can kind of do your HTML editing right in there, do people actually use this? So we looked at the data and so okay, some people do what are they using it for? You know, is it basically just bold and italics? Like, can we just limit the functionality or are people doing full kind of HTML editing stuff in there. So we got data on that kind of stuff, we got data on how big people make the tables. So you know, how many rows, how many columns they would have, you know, how long the names are, and just kind of look at that statistically and say, okay, if we support this, like that's gonna affect x percent of users. If we support this, it's gonna affect y percent. So using some of that information to help us decide what kind of functionality to implement in our UI was it was pretty interesting. And and it was, it was interesting too, because you know, we decided we're not going to do full HTML, it's just it's a lot of work on our end, and very few people use it. But then there's also the factor that shows you where the limitations of data are in that what if those very few people are very big, powerful, vocal clients, you know, does that change the way that we're thinking about it, and we ended up going ahead with not doing it. But what is interesting to see too, you can't just look at the raw data, or you have to make sure you're looking at the right data, I guess, to get the answers that you want. I love that story. It reminds me. Yeah, it reminds me, there was a product of Microsoft, I won't mention which one, but they added a new feature. And of course, they had metrics to track usage of that feature. And it was all it was, it was a menu option, a toggle to do something to well, I'm not there anymore. It was Visual Studio. And they had a new feature to a camera was to affect formatting in some way. And they saw a huge amount of usage when they added it. And the team was very excited. But the more diligent investigators, you want to look at context around that. And they found that 90% of the time after that never could be made up, it was a high percentage. But for now, we'll say 90% of the time, the command immediately following that format command was undo. Meaning people are going, what does this do? Oh, crap, I don't want that. Which is way, way more insightful. And then going way back, you know, Microsoft has used data collection telemetry for a long time. And I remember, if you remember the old versions of Word, and I haven't used Word in a while, so can't remember, but you would paste something into Word. And you go, Oh, crap, I don't want that formatting. And you immediately undo. And they found that the most the most common command by far in Word was paste. And by even more overwhelming margin, the command almost always following paste was undo. And that's that's where the when the ribbon came and that's they made it easier to paste without formatting. And now I've heard exactly how they did it. But it was from looking at data like that. So it's, there's a lot of value in there. But you have to look, you can't just look at the surface level on one metric, you have to look at some correlations and some groupings of actions to see if there's really an action to take based on what you learn there. Yeah, yeah. And that's exactly like data is really powerful, really cool. But you do have to actually understand your system understand your users and understand the data. Well, something Brent talks about a lot, which I try and remember to do is form that negative hypothesis. Like, let me prove that this actually isn't working like I expected and see if you can use the data for that. And if the data can't prove it, it's it actually is working. Yeah, there's some it's fun. I really love it. That's why I'm excited and delving into it. I do know that I think I know that you have also as part of your job do accessibility testing. Yep. Can you talk more about that and sort of your approach there and educate the the three for three listeners a little bit more on that area of testing? Yeah, I mean, I'm definitely not an accessibility testing expert. But it's one of the things coming to where I work now that was, I guess, eye opening to me. So in the especially in universities, in the states, it's, there's a lot of emphasis on this kind of stuff, some legal, you know, some of these universities, we've had clients where like, we need help with this accessibility thing, because we may get sued if we don't get it fixed. So definitely, there's a big kind of business case for accessibility, and it's also at our company, it's it's marketing thing as well. We talked a lot about that in our marketing material that, you know, we're accessible, we build accessibility right in. So it has a big business need for testing it. But before I came here, my exposure to accessibility testing was basically, we had a co-op student one time who got a concussion and tried to use our accessibility tools and couldn't, and ended up needing to take a month off work because she couldn't use the screen and our accessibility tools didn't work. So it's so common here was definitely eye opening, like, okay, here's what it looks like to actually care about accessibility. And, you know, as a company, we have, we have a blind tester who works full time here, we also have, you know, access to other disabled users that can help us with this stuff, through kind of a contract company that we work with. And then also, there's just the tools that, you know, use NVDA JAWS, some of these voiceover, I think is on on Mac, but some of these screen reading tools, and learning how to use those things, how to think about testing those things, it was, it was really interesting approach in empathy. And in actually thinking about users and the fact that there are many different kinds of users who use your software in many different kinds of ways, which I think is something, you know, any experienced tester is aware of and knows. But getting to that point where it kind of opened my eyes to see this is something that is helpful for your testing skills, let alone, you know, just the fact that you're hopefully helping people by making your app or website or whatever more accessible. So it was a really neat thing for me to see an experience, be in a company where that is a big emphasis, and a whole new set of tools and challenges as part of initiative where we were trying to, to get some automation around accessibility testing in. And so kind of seeing how we could integrate some stuff into our unit tests. And, you know, some things in accessibility are fairly easy to automate, some things are not. And so is another interesting case of just like test automation in general, if you want to test if you want to automate UI stuff, it's really hard. If you want to look at some other stuff, there's stuff that you can do that you can really help accelerate the delivery of that kind of stuff. But you have to use your context and again, understand what's going on to be effective at what you're doing. You mentioned, I have so many side questions, which are totally allowed in Three for Three broadcast. But you mentioned, you mentioned how it helped your testing. So can you talk a little bit more about that? Yeah, I think in just thinking about the different types of users that there are, so it's very easy to get stuck in one track of thinking, at least in my experience, where you just do things a certain way, you click through the app in a certain way, you're, you're used to, you get used to the way that it works, and you just fly through it, right? So you click here, click there, go, go, go, you're where you want to be. And everything looks the way that it always looks, because that's the way you always do it. But then when you turn on a screen reader, or use zoom magnification or something, all of a sudden, you see your app in a whole new way in a whole different light than you did before. And that's helpful from the accessibility standpoint of, okay, this is actually what it sounds like for this page to be read out, or this is what context I can't see when I'm zoomed in. But then also, I found, at least for myself, it helped me think about, okay, what are some other things that other types of users might have, or other ways of approaching and thinking about how you work with the application. So getting that user empathy there of the fact that not everyone does things the same way that I do things, some people, you know, might use right click instead of left click, prefer that mode of interaction. Some people might not have the experience and context that I have in being able to quickly know exactly where to go based on some message that they saw. So getting out of your own shoes and into someone else's I think is just a really helpful thing in software testing. Yeah, it's a good context switch. And also worth mentioning, and again, I think tangents are allowed. This is a baby of the AB testing podcast. Baby tangents. We'll do baby tangents. So depending on the platform, you're working on, there are some varying levels of quality static analysis tools that can help you find access potential accessibility issues. Is there any use there? Yeah, so I mean, we use some of the kind of built in stuff, the axe stuff, and we're doing the stuff that I'm working on is kind of a lot of on the UI area of the of the organization. So we're working with some stuff, web components, which is kind of API driven. And then you have the web components that we're using, Google Library called polymer, which is now called lit element that we're moving towards. But in that context with accessibility testing, there's some built in tools that that polymer and then lit element that they provide. So we use some of those things for kind of static analysis. Dynamic analysis is when it gets a lot harder. And yeah, we have an accessibility lead here. And and trying to explain that difference is actually tough. Like there's people that say, well, I was expecting, you know, because we put this new tool that was kind of built on top of axe. And like, this is what it can do. And they're saying, Well, I was expecting that it would, you know, tell me when I click on this, and then things change on the page, and it would tell me what happens like, well, you need to re run the tool after you click, because it's static analysis, it's not dynamic, it's not kind of monitoring what's happening on the page and dynamically following along while you do that. So definitely a very interesting difference there that a lot of people maybe struggle to understand or who are in as technical roles, that there is difference between the way that what you can actually check with test automation easily, and what you might think you can check with it easily. Yeah, yeah. I think one thing that's definitely reminded me in this conversation, but maybe a good takeaway for the listeners is that accessibility testing is a much bigger, it's like an iceberg. It's a big, it's a much bigger area than it may seem on the outside. On my last project at Microsoft, we, of course, I brought up accessibility a lot early on, it was our VP was very much in the app, we'll get to that, we'll get to that, it's that's the problem, it's easy, it's some tab order stuff, it'll make sure the strings are right. And we tried to shove bolt accessibility on late in the project, and it was really, really difficult, and it caused a lot of problems. So definitely something to think about early, is that been your experience as well? Oh, absolutely. We just had this conversation. So a section that we're working on, and how, you know, and we have tended to like we involve designers upfront and think about accessibility fairly holistically, and it's definitely a part of our checklist is not like, oh, that can wait till later, it has to be done before we consider a story done. But yeah, it still is one of those things where it can slide. And so we kind of had this conversation, like we need to make this be a part of the core of the development that we do, it's not, I'll get all the page working, and then I'll go back as a developer and review it for accessibility and see what needs to be done there. But it's kind of like, well, you're working on this, think about accessibility is the whole shift left term, which I don't know what I think about that. But but that idea, I think this is one of those things that needs to be in that realm of upfront development, like it can't be an afterthought or else you end up really struggling with making it work. Yeah, a lot of times I think about, you know, my role is sort of the modern tester role, at least when there is one, but it's, I do a lot of just making sure we think about things before it's too late, try and avoid surprises. And there's a lot to that. Yeah, definitely. And accessibility is definitely an area where that plays in. Like I, you know, I do some testing of it. But again, it's an area where a lot of what I'm doing is helping developers make sure that they're thinking about it and working on it early on. That's a good role to be in. And that's really, I think, I think it's probably where a lot of our listeners are is there's a lot to the modern testing role on coaching, or consulting, or however you want to talk about it is helping people do the right thing. One thing we can bring from our testing background is that breadth of knowledge or breadth of ideas on the different things that make up quality. And it's tough. It's it's it's tough. It's fun. Do you enjoy your job? Oh, yeah, I definitely do. And to me, that's the as you know, if you listen to the podcast, Brent and I are both into the, the idea that if you just, you should enjoy what you're doing though, this, this work we're doing is, it's not boring. And to me, the fact that it's changing all the time, it requires me to learn new things in order to be successful. That that makes working fun. Yep. Yeah, absolutely. Yeah. And I find that like, I love my job and and working towards that. Again, it's interesting in terms of the, you know, modern testing principles, you know, you guys talk about it for a bit. But the idea that testing isn't the exclusive job of the tester on the team. And, you know, I'm trying to continue to work towards that on our team to that it's, and anyway, I work on a really great team where we have a lot of good testing done by developers as well. But but that idea that it's not my job to test the quality into your product, right? It needs to be there. I can do some stuff, I can find some stuff that you might not have thought of. And honestly, our customers are going to find a lot of stuff that none of us thought of. And that's why we need to be able to respond to things that come up and be have some kind of telemetry monitoring, whatever in place that allows us to know about things that come up. You know, our job is to set up a good scaffolding in place so that we can be responsive to real customer needs, and not impose our ideas of what quality is onto the product too much. You hit on I'm gonna touch on these things, and we'll close here, but you hit on something that that I bring up a lot, both internally at my company, and just talking with testers is this, you can probably remember you've been doing this for a while when you mentioned that some bugs are just going to be found by customers. And there was a time that we can remember when a bug found by customers was the test team's fault. And it in for a lot of us are well past that. And I really like if you ever saw Noah Sussman drew a picture a year or two ago now where he took the traditional test automation pyramid and flipped it upside down and described it as a band pass filter, which I like a lot. It's it's rather than try and make the thing be a ratio. It's like we want to catch all the tests at the unit level that we can. Yep. Yeah, I love that. Like find the integration level we'll find there. And the only bugs will find at the end to end or whatever you want to call that bottom level is fine there. And then some bugs can only be found in production. And then rather than you know, the old school way was we'd find a bug we go, okay, let's figure out why we missed that. We'll go write a test for that. But it's not about blame. It's about creating that environment. Like what this is. I do a lot of retros in my current team and the discussion usually starts off on I say something like this isn't about blame is about trying to assign, you know, figure out what we didn't do. I want to figure out how we create an environment where these things are less likely to happen. Yeah. And then from that framing, we figure out like where what broke down. Wow. How come this? How come this issue was that we, is it true that it should have been caught at a lower level of the band pass filter? And why did that happen? And what can we do better in the future? And I like, anyway, I like that model of Noah and I use it a lot. So, but it's important to realize as part of that, there are some bugs that just cannot feasibly or efficiently or inexpensively be caught before the customer. It doesn't mean the customer is doing the testing for us. Some people freak out at that phrasing. It just means that there's these combinations of things like we talked about with data earlier, that you're never going to be able to find without the masses of people using your product. Yeah. Like there's always that person who does that thing in that really weird way that you never would and that's awesome. And that's awesome. We can probably both still remember the times like when we hear, well, no user would ever do that. And we can immediately go and say, well, it looks like 10% of our users do or you're right. No user would actually do that. Yeah. Yeah. So, and actually, I, it's I have two great examples of that. So I had one bug that I filed that was like, okay, this is, you know, if you do this, whatever, it's really weird workflow. This problem happens. And then I said, and if you think that no user will do that, here's the data in the logs. Like here's a link to the data that shows that we have users that are doing this. And then I had another one where, you know, I was looking at some data and log and while I was testing some stuff, I found an issue. And so then I went back and looked to see if that issue had happened in production and it hadn't. So I put an alert in so that if it ever does happen, I'll get alerted about it. But, you know, in the meantime, it's like, why would I spend any time trying to figure out or fix this if no customer has ever done it? Yeah. Yeah. And I've done that before too. Like you end up in a rat hole discussion with a group of people around. Well, what if this edge case happens and sometimes those things turn into like, oh, we better do some work to make sure that never happens. But I love the idea of instead of like trying to mitigate it now, we don't think it's ever going to happen. Let's see if we can be, we can be proven wrong and we add the alert. That's like the, you know, that assert you bury in your code. That's just a case that ever happens. I want to know. And well, customers don't run the version of your code that has a search. Maybe you may never know, but had added that alerts and see, oh my God, somebody actually made this happen. Let me dig in and see how it's fun. Yep. All right. This has been, I, this has been a fantastic conversation from my end. I don't know if you've enjoyed it, but I have, you have woken me up. It's a 8 30 a.m. here on the West coast. I, of course, have had meetings since six. So this is better than my, this has been better than my coffee and getting me going today. So thank you. Well, I'll take that as high praise. That was definitely high praise. I'll put it on my resume, better than coffee. Well, I have to have data to prove that up. I said, can be better than coffee. Whatever. Anyway, Ben, a pleasure having you Dave. Anything else you want to add any social media information or upcoming things you want people to know about before we sign off? I don't have any, I just did a talk actually at the KWS QA, it's a local beat up here at the conference. But I don't have any other talks coming up right now. If you want to find me, you can find me on my blog offbeattesting.com and the same on Twitter as well. If all goes well, I'll type that up in the show notes when this goes out. But Dave, like I said, it's been a pleasure having you here. So thank you very much. And I hope you continue to be a listener of the show. And thanks for sharing a bunch of your great thoughts on quality and testing today. Yeah, thanks for having me. It was a lot of fun. All right. We'll see you later. 
