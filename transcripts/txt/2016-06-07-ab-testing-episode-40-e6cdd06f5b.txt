Howdy. Hey, guess what? It's episode number 40. 40. 40 of 80. The big four-o. So it's a week of anniversaries, because Monday is my 21-year anniversary at Microsoft. Congratulations. And I am not, to celebrate, I'm going to Vegas. And we talked about this last time. You're not a gambler yet. French, oh, here goes the tequila. We don't have tequila, so we'll do coffee shots when Brent says we've talked about this before. All right, so you're not a gambler, so what's happening in Vegas? I'm going to Vegas to talk about A-B testing, not the podcast, the actual experimentation. That's right. That's the conference this weekend. Well, it's Monday. My workshop's Monday. I'm flying down Sunday. Give them a workshop Monday, flying back Tuesday. That was it that I could announce last week, but those that follow the conference world on Twitter. Holy cow, it's going to be a fun day of Alan not being able to talk. Every day for A-B testing, recording, I bring in two cups of coffee for me to drink throughout the episode, and neither one have been started yet. And I think it's a problem right now. You rode your bike today, I guess? I did. Yeah, so you weren't pre-caffeinating on the way in then. It is really hard to drink a cup of coffee on a motorcycle. First of all, balancing on the gas tank, pain in the butt, and then getting the cup in your helmet without pouring coffee all over your chin, really difficult. What about one of those football beer helmets? That is an idea that I feel just dumb for not trying. I will be in the city of brotherly love in November, Philadelphia. Philly! Before test bash. Oh, are you going to be bashing test? I'm going to be talking. Sweet. I'm the one giving a talk about, I think I titled my talk, you know, I'm kind of, I mentioned this perhaps before. But I have. Oh, no, no, drink. I am evolving this talk around my experiences with being the guy, the test and quality guy on a team with no testers and talking to other people with the same experiences and learning what works and doesn't work and watching more failures with the move than I see success. So I'm talking about testing without testers and other stupid things or something like that. I think we learn well from mistakes. We can watch what others have done wrong. You only learn from mistakes. Oh, so and so teams tried that and they failed to have a dumb idea. I've seen that for, well, 21 years now. At Microsoft. At Microsoft. So I think it's important to understand how and why these fail and when and where they work. So you can figure out if they work in your context. So I'm giving that talk and it's, I don't like giving the same talk. So I've been evolving this talk. I can say this, I think, by the time I go to test bash, this product that I'm working on will be publicly available or should be or damn well should be. So it'll be interesting to talk about it in that context. Talking about a real thing in November. November, okay. So I'm pretty stoked about that. Appreciate the invitation. So conferences, we're looking at our burn down list here on the board. So we'll dive right into the, I forget what the section's called again. Mailbag! So Danny Fott, software optimist, asked if we knew, had any more information on the, what was going on at Yahoo. And I don't. Did you say software optimist? Software alchemist. Oh, okay. Software optimist is nobody in testing. It really should be nobody in testing. So I haven't heard anything. The one person, I used to have some good connections in Yahoo and they've all left the company. And wow, they're all testers, that's weird. My connections to Yahoo are, god, about eight years old. Yeah, so I don't know. I use a news clip of the Yahoo, getting rid of the safety net makes us better developers article that came out when they made this change. But again, that's press and it's hard to tell. There could be a lot more going on under the scenes. Or there may not be. It's hard to tell. But Yahoo is getting a lot of PR on this. But they're far from the first big company to go down this path. Oh, I know. But there was, I think he brought it up because this is the one that got media attention about it. That Facebook has been doing this same sort of thing for years. And Facebook-ish, the thing to remember, and this is something important when you think about unified engineering, combined engineering, or no testers, whatever you want to call it, is that. Unified engineering. The story I bring up is when I went to the Google Test Automation Conference in Kirkland. It's been two years, maybe three years ago now? I can't remember. I'm old. I think it's three. Three-ish. The guy from Facebook, giving a talk comes up, swaggers up there. I work at Facebook, and we don't have testers at Facebook, like trying to get a rise, like, these are test-automators. I'm the engineering lead for a software reliability team. I roll like a dude. Whatever. We don't have testers. We have software explorers and evaluators. Right. We don't have testers. We have performance analysts. It's like, and the point is. Quality analysts. The three know that the work gets done, or the work should get done. The way you fail at this is you stop doing testing. But the activities still need to occur. And what happens in a good unified engineering team is that they occur in a more optimized manner than they typically do on a waterfall, or a stage, or a separated team. Yeah. Anyway, I have no insight. It's not really talking about there. But question acknowledged. Thank you. Who's Peepee? Patrick. Oh, prill. Testpappy. I only know people by their Twitter and Slack handles. Yes. But I think it probably is not too cool for us to continue to call in Peepee on the podcast. Patrick asks, how does Microsoft grow and recruit into quality-oriented roles? Damn good question. Yes. I'm going to let you start, because I have to think about my answer. The prep was low for this episode. I think this one's all about you, honestly. Because when Patrick is asking around quality-oriented roles, like what Alan does, I'm thinking it's there's two versions of this in my mind. Number one, what Alan does. Number two, what Windows does. Oh. And between the two of us, there's only one of us that has an opinion on this. OK. All right, while you were talking, I had a chance to form some thoughts on this. So it's interesting. We should probably give some background. Because it's very recent. Microsoft has had a lot of Estets, testers, software divide engineers, and test. When we made this move, I can't say we kept them all. But people moved into roles, whether development roles, some of them. Some of them did very well. Some not. People that were good test developers, they're pretty good developers. The thing I have discovered is that people who were good test developers, for the most part, are making themselves very well known as better developers. My experience as well. In Windows, they just decided to not have a test team, but to have a quality team. And they focus on telemetry, data science, really business intelligence, and some of the illities. Yeah, the fact that you said business intelligence data science, I find nearly offensive. And that's what they're focusing on. I may poke a little bit. So this is for background, where everybody went. So when I came to this role on my team, they wanted someone to think about this quality role. Think about quality and being a role, to think about quality and make sure all the activities were being done. I mentioned a minute ago. That doesn't require a drink. I mentioned a minute ago that on a unified engineering team, all the activities, the quality and testing activities, still get done. And they wanted someone who actually knew what those were, so they could get done. And that's my role on the team. So how do we grow and recruit those people? I don't know that we do. Not every team at Microsoft has someone like me. Some do. Most don't. One of the goals for me on this team was to show after this product ships, and assuming it's massively successful, I'd like to show this is how a unified engine. This is the example, the template for how a unified engineering team works. You have some specialty around quality. And this is where this is the role that people who really get that systems thinking of testing and quality, as you can see across the product, can have a place to fit in a unified engineering team. How do you recruit those roles, NFI? One thing that's actually a little bit different these days when you're recruiting from universities even external, it was difficult, not hard, but difficult, more difficult to recruit for test roles than it was, SDET roles, than it was for SDE roles. And the difference being mainly the industry stigma around the test role. I think at Microsoft, most people got here, they go, oh, wow, this is not banging on buttons. But it was hard to get over that hump. Now, since we recruit just for SDEs, but we make sure that they have some aptitude, good aptitude for testing and quality, recruiting is actually a little bit easier. But how do we find those people who are going to grow into quality roles? I don't think you know when you recruit them. So a couple of thoughts as you were talking. Number one, so we have probably a more relevant expert on the Slack channel than us on that particular question with respect to Steve Rowe, who is in a quality team within Windows. So I'll definitely publicly invite Steve to address that question within the Slack channel. Everybody who isn't on the Slack channel, of course, you can still. I'll say two things. Let's do this. Let's do episode 40. One, I think Steve's quality team is better than most or all the other quality teams as far as what they actually have embraced it and figured it out from the beginning. But also, I'll go out on a limb here without consulting with Brant and say we should maybe have used that little input right there I'm pointing out on our new mixer. And we could have Steve come in and talk about this himself. In fact, that would be a good topic to have. We could talk to Steve about what is the quality role in Windows and how does that differ from what I do here? Because I can't really speak for him. One of the things that we just talked about, I like your loophole here, is the Google Test Bash. And essentially, all Google did is there's still testers. They renamed is the accusation. And I will state that at a still far too big. I think you're confusing listeners. We're talking about Facebook and their software reliability. No, you talked about a, oh, I write Facebook claim. We have no testers at the Google Bash. Yes. At the Google Test Automation Conference. Right. The best bash is the thing I'm going to. Please, I've had coffee now. Obviously, you have not. Because you're getting your words all confused. OK. Thank you for adding clarification. So one accusation could be, isn't Microsoft just doing the same thing? And I think the correct answer is, in a large degree, yes. I think any time you, as briefly tangentatized, any time you say Microsoft does blah, or Microsoft, the answer is wrong, or right, or both. Because the company's too damn big. Ooh, new fallacy, listen. My point is, even though Alan and I speak to a particular direction and push for what quality means and should mean, even large chunks of Microsoft isn't fully aboard on this. And I'm aware of one team that had moved to a unified engineering model. And now they're blatantly moving back. And I think they're going to pay the price for that from a productivity standpoint, not a political standpoint. But you brought up Steve and quality roles. Like the one thing that I do very much like about how Steve's team's been rolling is they're very deeply focused on the topic of quality and recognize that it is not a synonym for testing, whereas a lot of places it is. So I like your suggestion. In terms of recruit and grow, like if I were thinking through, and there are many places where I could see at the company and even in my organization, where we could use someone like Alan. The real challenge is, unlike when we were recruiting for S-TET roles, there is no way in bloody hell I would ever recruit someone for this role, say from campus. This is someone that you would want a crapload of experience underneath their belt. So this is. Yeah, I think the way you do it is like right now I have a very small team and there's one of the guys on the team who I could see doing what I do or growing into that role. And I think that's the way you grow them. And then there are, I mentioned on my team, there are a lot of developers and some of them, many of them are really good testers too. And some of them, as they grow in their career, they'll figure out how the pieces fit together and look at them with experience to get all these quality issues. I think the person you put in this role, not necessarily a direct testing background, but I think the thing that helps me in my role is the fact that I've been doing this for frickin' ever. Yeah, there's a high degree of experience required. And I think the recruiting model for this role, even though, so Alan a couple of years ago, no, it was just this year, sent me a message saying, hey, I'm looking for another person like me, who do you know? And that was a really hard question. That was a really hard question. Because it is very similar to today saying, hey, I need a deep performance expert. I need a deep security expert. And I think that's where your role is today. Someone needs a deep quality expert, but quality as a system function, not as necessarily a super tester. I think we'll see as time marches on that we'll see a few more people in roles similar to Alan's. But we will then, in fact, view it as a specialist role. I think so. Something I was going to say, oh, which reminds me, I have, if you know anybody, I'll ask right here, so the three can know over here in case they know anybody. I'm happy to have someone external. But my team owns everything from the moment the code is checked in until it gets deployed to our production servers. We deployed to 12 servers now. We're in data centers around the world. You are the code factory head. The crank of things. And in between there, of course, between the code checked in and going to production, there's a lot of quality stuff that happens. So the four of us own all that. And it's not enough because with scaling, we need help with mainly on the DevOps side, Azure management, scripting, getting builds together. So if you know anybody. I know a lot of people on DevOps side. OK, if you know anybody who's like an independent worker, can just look at ambiguous instructions and figure out what to do and get crap done, send them my way. The challenge is who's looking. Yeah, yeah. Same for the three. If you know anybody, send them my way. You know, I get ahold of me. We're underwater. I tell my team, we're like a car crash away from being like setting our schedule back like two months. We're a little bottleneck. I don't like on a team when one person has all the knowledge. It's a bad situation. Yeah, that's a scary situation. It's one of the things that is interesting. So as you know, I've been running Agile teams for a long time. All of my reports have been I've been sort of training and guiding them for a better part of the last three years. And now when it comes to knowledge sharing, they are like in my office want to talk about it at a 101 irritated when knowledge sharing isn't occurring. I don't know if we really answered that, but we talked about it. Yeah, so far two questions down and both answers are, we don't know. You know, you think for a nice even-numbered episode like 40 would be awesome and not like wishy washy lame. Right, and the positive thing is, look, we're already halfway done. Great. Well, actually, it won't look that way after I edit. Oh, I see. I see. So even the word, so let me just say this right now. As you're listening to the AB podcast, I want to consider what you've paid for it and you get what you pay for. Actually, you don't even get that much because you've paid with your time already. Yeah, that. And hopefully there is a reason we have three listeners. Hopefully editing has made your lives a little bit better than it would have been if you're actually in the room with us. Oh. Completely random, like more random than random. What is more random than random? Now as a mathematician, we realize how stupid that is. This is more random. So we do tangents. This then would be a random tangent, which is not redundant. This is like inception. So I. Tangent to inception. I was quickly reminded that we have three listeners which connected the dots of the fact that today's June 3rd. Which. Which means what? Today is National Donut Day. Holy. Rollin' O's. That's right. B man. I have to remember this. Before I leave, because you have my favorite donut shop right in this building. One thing about donuts is we have a frost donut shop right on our first floor. Yes, I'll be spending 50, 50 friggin dollars for two dozen. Yeah, they're. But they're worth it in my mind. I should buy some for our team. I'll go down with you. All right. OK. Next question. Shall we continue? And who's this one from? Really? Danny. Brent writes like this cryptic shorthand on the whiteboard. Yeah, it's it's Danny's initials. Oh. Tough algorithm there. I only know people buy their internet handles. So software alchemist, Danny Faught, asks about what's the best way to communicate about? Brent, do you write English? It's English your first language. What's the best way to communicate about testing the emergent properties of a system? Oh, you just left out a few letters. Yes. Got it. Oh, so this is about non-functional testing. Yeah, what he calls para-functional. Chem Kater calls para-functional testing, which I think it he asked why it didn't catch on. I think it didn't catch on because it's a weird frickin' name. Absolutely. I think non-functional is also kind of a weird frickin' name. I just refer to him as the Illities. Yeah. And that's I've used non-functional, if I'm in a group of people, that's sort of language they expect to hear. But usually I just call them Illities. And then I list a bunch that don't end in Illity just to add more confusion. Stress. Yeah. The Illities, you know, like. Stress-ility. That's how we fix it. We just make up new words. The Illities, like performance. Illity. Security. Illi. Security. Yeah. That's a tough one to say. If chem can make up para-functional, why not secure-ility? Are you going to talk about this question? Well, I actually read what Danny wrote, so I can figure out how to answer it. That's somebody. OK. The best way to communicate. So I have a problem when I see the word best, because I don't ever believe there's such a thing. There's always a better way to do something. I'll talk about a story that I had just two days ago, where we have this one team that's essentially our DevOps team. And they're operating off of what I view is the dysfunctional equation that all of these teams do. The first time, if you have a team that is with a mission of something that's a broad scope, and in our case, it's division scope, and their job is to sort of cat herd, amongst other things, their job is to cat herd the illates across all of the vision. And what is the way that everyone does this? V teams. Yes and no. But if I were to torture you longer, I know you'd get to this. The way everyone solves this is yes, with a V team. But a V team that does what? Of course, Alan, you're correct. Produce a scorecard. Boo. Right. So the discussion at hand was integration scenarios. So I'm in Azure. So an example would be creating a VM, deleting a VM, launching an app service. And performance comes up, which is a Iliity. Perform-i-lity. Yep. You've pronounced that correctly. Thank you. And the typical way people do this is they put the scenario, and then they put P95, or in this case, it was P95 or P99. Those who don't understand what that means, it's the 95th or 99th percentile of their performance. It essentially means 99% of all transactions were equal to or below this value. And you do that because, in general, average is stupid. Average is stupid. But I pushed back and said 99 is also stupid. I'm curious to hear why. So the way that I have found the best way to communicate in terms of performance is by showing the distribution. So generally, the distribution will be a head-tail curve on almost every performance situation. Agree? Now, so if you go with 99th, then you are including a large portion of the outliers. By looking at the distribution, you can now decide, OK, what's more important? Pulling down the outliers or improving the experience for the head. So if you find that the head of the distribution, again, a head-tail curve, if you find that 90% of that head-tail curve is in the head, then you're going to be better suited ignoring the outliers and focusing on how do we make that 90% faster. OK, thanks. So this is one of those things where P99 forces you to collapse it all into one experience. And I'll introduce what I call the 80-80 rule. It's not my rule, but I do share it. 80-80 rule is 80% of the value costs 80% of the effort. But the other 20% of the value costs an additional 80% of effort. Yep, makes sense. And so if you are looking, in my view, you look at these things and you look at it as a system. You look at it in terms of business value or customer value, and then you break it down in Pareto order. You want to not just have an overly simple, stupid system that potentially wastes money on outliers. Have you had enough time to think about? Yeah, because that's very interesting. Not sure to answer this question. So I said a boo when you mentioned scorecards. But often, depending on how you put it, how you build it, scorecards can be the right solution. Because people like to watch things that are red, turn green. And the thing to be careful of is driving the wrong behavior. So this got brought up in the thread. So the best way to communicate about these things, I think, is the same as the best way to communicate anything. So a scorecard. I do like scorecards. I don't like red, yellow, green scorecards. It said good, bad, or on track. Because the problem is, those type of scorecards, you need to be able to look at something if you're trying to communicate. So let's view this. You have this knowledge, and you're communicating outwards. So the readers of this are your customers, of your output. Your customers must be able to clearly understand what actions need to be taken without questioning you. And these red, yellow, green scorecards, don't do that. I agree. I had an angry moment a month or so ago. I run our ship room. And we had one of our, so the way I took care of making sure our ILLITIES were handled across the team with no testers is I put one of my peer engineering managers in charge of every ILLITY I cared about. So the engineering manager in charge of one of the ILLITIES for the second week in a row showed this huge mass of red, yellow, green. And all I said was, everybody's working hard. We're making some progress. It's hard to tell from here, but everybody's working hard. And there's useless. You may as well show me a picture of a unicorn taking a crap in the forest. It would have been as useful. Did you see the unicorn rainbow snippet at the end of Deadpool? That cracks me up. So anyway, the feedback to him was, what does the team need to know about where this area is? What are the action items? What needs to be done? What are the risks? What are the goals? Whatever. Something around those areas versus a mass of red, yellow, green. So it was going back to your point, there was nothing on there actionable. There was nothing on there I could read and go, oh, this is good. This is bad. It was just too hard to figure out. If you have a scorecard, it should be painfully obvious what it's telling you and what it means, whether there's action to take, whether there's risk to worry about that needs to be mitigated, or whether it's truly on track and nothing to worry about at all. But if those things are missing, the scorecard becomes useless. There's, to piggyback on that, the other problem with these scorecards is what's the goal? And oftentimes, like, say, with performance, oh, the goal is to bring P99 down to 200 milliseconds. No, that's not a goal. That is a stake in the ground, a target. Yes, something along those lines. The goal, in my mind, needs to be something that says, this is the business benefit. When I try to communicate these type of things, generally what I try to do is I look at it, I analyze it, and then I break chunks down into sort of Pareto order of benefit to customers. A lot of these things, you can just do ad infinitum. Great, we have everything all green. This is what we agreed to. However, a customer still hates the product. So great, we all agreed to. We can all pat ourselves on the back that we did the best we could, I guess. But in my view, it's because you're targeting the wrong goal. You're targeting the goal of, I actually had this conversation with an employee this week. It's like, look, you learning to drive is an uninteresting goal. What's an interesting goal is we're doing a road trip to Vegas. And you're going to learn to drive in order for us to execute on that road trip to Vegas. So you hit on something, a nerve that really, where I agree with you, really hard. Because I think teams can often focus on doing really good engineering. We're going to put some targets and we're going to hit those. And we're going to make sure that we can do this. Our test is going to run. Our meantime to failure test is going to run. And we're going to make sure we get all this. And they focus so much on just doing the engineering work, they forget to make a product that customers like. Execution is really important. But executing in a valuable direction is also really important. And I find that these engineering teams that are so deeply focused on progressing on execution are often it's the age old trees versus forest discussion. Right there they're inside the forest. So like, oh, neat tree. Let's do the one to the left now. Yeah. And going back to my role, that's my part of my role on the team is to make sure that the guys working on performance aren't just, it's not hard work. I go, so what's the user benefit of getting to this goal? And make sure there's answers for those questions. So the answer I think so far that we've brainstormed through is number one, communicate in a way that makes actionability readily apparent. Communicate in a way that discusses the system in business terms. And I would, there's with any change, and a change can be an organizational change, or a change can be an improvement in engineering. One thing like I have said a million times before, when you move to agile, if you make a big organizational change, explaining why, like we want to be able to ship daily. So in order to do that, we're going to combine our engineering teams, we're going to agile methods, blah, blah, whatever you want to explain that. Versus people go, oh, we want to ship every day. This is the goal. And to do that, we have a bunch of other stuff we're going to do. We're going to learn to drive. Versus what happens to a lot of teams, is like, hey, tomorrow you're all engineers. We'll figure out what that means later. It's like, the what? So let's trench that over here. We need to get performance better. All right, let's work on performance. Versus our customers are complaining about performance. By June 30, we want them to be bragging about the performance of our app over our competitors. Maybe not the best goal. I would love to see someone brag about that. So we want to improve the user experience around performance. We want to make sure people feel that the product is snappy, whatever. Because we think this will sell a million more units. So why you're working on performance, why that's a target. Yet, Amazon did this very, at least to me now, very famous experiment, where they basically, their landing page, they put a configurable sleep in the loading of the page. And they ran a series of experiments, A-B testing, to be discussed by Alan Page in Vegas at a place near you, as long as it's in Vegas, where they artificially slowed down the render of their page. And they tracked the abandonment rate of the customers. So long story short, they were able to determine two things that are critical. Number one, how much a millisecond on their landing page is worth, and where, what's done in economics terms, marginal utility, where that utility dies off. Where it's no longer useful to eke out that extra millisecond. Because if they had a 10-second rendering page, going from 10 seconds to 5 seconds is going to keep way more customers. But if you're already at 400 milliseconds, cutting that in half to 200 milliseconds saves you not as many customers. It is not a linear curve by any means. OK. You went off on a tangent there, but did we finish the topic? No, so I'll wind back to the performance thing. And then this will be my coup de gras on this topic. On the P95 and the P99, if you understand how stats work, and again, if you guys remember what I just said around the distribution and it being a head tail curve, basically I communicated to them, because I have found that a lot of business leaders don't understand or appreciate data science lingo. And I basically have shared with them, look, if you want to have, let's say, P95 as your one metric that matters for performance, then if you want one metric that matters, is what I said, is you need to be clear then on what you're focusing on. Are you trying to reduce the variance of customer experience, meaning what's the likelihood of outliers happening? Or are you trying to improve customer satisfaction rates? Because these are opposite things. And right now, your metric is going to focus you on removing outliers from the system, which I'm sure it's just code, it's not magic. We'll get there. But at the end of the day, if you're expecting higher customer satisfaction rates, I don't believe you'll get there. Now, if you're expecting, say, a reduction what's known as customer calls on CSS, yeah, you'll probably get a reduction in customer calls, because the customers who we really, the two or three that we really tanked, they're going to be upset. But that's not going to have a big impact on customer satisfaction rates overall. So you guys have to decide. And in that case, I told them, there's two metrics that I think are better, one's known as the coefficient of variation. That's much better at focusing on reducing the variance of the experience. How do we reduce some people having super fast, and some people take 10 days for a thing to render? And then median, I find, is a much better stat than P95. All right, cool. Hopefully that rambled around, and some sort of answer to the question. Yes. We're generating more mail-back items. One more. Hey, Alan. Here's a topic for your A-B testing show. Our product has been rolling out more and more users over the last few months. Our CEO has decided that he does not want to roll out to more users, because we're hearing the exact same feedback, or very similar feedback, from everyone we've rolled out to so far. My thoughts are that this is good, because we're not finding more issues. But I'm fighting against management to try and push out our product to more users, even before we fix these issues that are known. What would you do? So the feedback. So it sounds like, to paraphrase, the product is, there's a core set of issues. We know that A, B, and C need to be fixed. And we go to more people, and they tell us the same thing. A, B, C need to be fixed. Should we go to even more people to hear those same things? Or should we go to more people? Or should we fix those things first, before we go to more people, so we can get more data? Do you think you could get more data by going to wider customers? Do you think you could find out about DE and F? Or are you blocked on finding out about DE and F until you get more people on there? There's a balance here. So one is, hey, we know about this feedback. I presume work is in flight. So the errors, how shall we hypothesize on the president's feedback with respect to the customer? Do we think the customer feedback is critical to the business? Or do we think it's, hey, it's a couple of one-off bugs, like, hey, this button's in the wrong place? I think more fun. It sounds like it's more fundamental things, not necessarily button in the wrong place. But a feature that would make the product more usable is missing. But it doesn't sound like it's a showstopper. OK, I just thought of it another way. So the question really is how I feel. The feedback we have to date. So this, I think, is probably the best current answer in my head. The feedback from the customers that exist today, to what degree do we believe it is causing abandonment? If we think it is not causing abandonment and it's really more around taking something that's good and making it delightful, then, yeah, I would turn the crank another order of magnitude, but only another order of magnitude. So that's actually a good way to put that. What are some good ways to measure, I guess you could just look at usage. I'm trying to think how to measure abandonment. But it's how you can look at active users and figure that out. On a web app, one of the better ways of measuring active users in this case for abandonment is essentially the frequency and the, they call it frequency and recency. So how often do they engage with the product? And how often do they re-engage with the product? So you want to look at, you would want to start tracking new users to the product and then look at what are the typical rates, how frequently did they come back to the product, and then how many re-engagement sessions generally do we see before that starts to slow down. How do you correlate those A, B, and C features that are missing with abandonment? You can measure abandonment and you can look at customer feedback to see what they think is missing. But how do you correlate the two? The easiest way is to put a feedback app on the product. So here at Microsoft, the most famous one is we call Send a Smile. Send a Smile. And it sounds like they have something like that because they're getting customer feedback. Right, but now if you track what's known as active, engaged, and then disengaging customers, that's really what I describe there. So looking at new users, then, and you light up the Send a Smile, then you can, most people when they instrument, you're able to see the sessions of the persons who have contributed feedback. And then you can say, OK, is there anything interesting in the session? In particular, you can look at the specific paths under question and say, hey, you know what? The specific paths under question, these guys aren't abandoning with respect to, or in comparison, to say this other category of folks or not even abandoning at all. What you're trying to draw a connection to is does this group that requires this, what's the size of that group, and how does it differ from the ambient abandonment rate of your product that you're already an expert on? Because that is just pure A-B testing right there. Very cool. All right, we are out of time for episode 40. Yes, mailbag only. Our second all-mailbag episode, we did one before a while back. Percy will know the number because he memorizes them, I think. Yeah, actually, have you seen them? I really like what Percy did on episode 39. Oh my god, yeah, he did the live play-by-play. I had to go listen to get it. At 4302, I agree with you. At 4323, I don't. Yeah, I was like, what are you talking about? Oh, yeah, he's right. All right. Later. Hasta la vista. I'm Alan. I'm Brent. Bye. Bye. 
