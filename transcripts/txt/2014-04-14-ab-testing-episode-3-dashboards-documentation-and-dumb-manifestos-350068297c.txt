Hey everybody, I'm Alan. I'm Brent. And we're here for yet another episode of AAB testing. I just got off the, not just got off, but last night I returned from a quick trip to Florida. I think my total time away from home was about 40 hours. Talking to some college students there at University of Florida in Gainesville. Great group of kids, two great groups of kids. And just started talking about sort of what testing was like at Microsoft. And a lot, it was, I didn't really know, I had some exercises. It was very data driven, which was interesting because that's kind of where, you know, what Brent and I have been talking about, where a test is going. He's talked a lot about data, analyzing data, how to look at data and some things like that. So Alan normally does these events and it's been primarily around helping recruiting get people into tests. So what did you find different this time around? As the role evolves, I'm finding, you know, when we go to a recruit at universities, we show them the three Microsoft roles, program management, software development, and software testing. And if you were to ask them to look at those roles by title and, you know, rank what they wanted to do, they go, well, I've gone to school to program. So obviously I want to be a developer, but if I can't do that, I want to be a program manager because they have manage in their title. And here's the real truth is that program managers don't program and they don't manage. And then they look at the last one and go, well, then I'll, I guess if I have to fall back on something, I'll be a tester. And we want to give recruiting a fair shake, a fair chance to land candidates for the right jobs. People are going to be successful in their career if they find a job that aligns with their sort of mental model and how they approach things. So what we find is in these classrooms, you do have people who want to take a design and implement it as, you know, as has been spec'd for them and just, you know, write features. And those people make great developers. And I asked them in the class, you know, if you think testing is just writing unit tests and functional tests, that's great. You're probably going to make a great developer because we need people to think more system thinking. I talk about program management a bit too and how it's project management plus a lot of, you know, coordinating schedules, managing dependencies, looking at plans, making sure all the pieces are kind of put together, require systems thinking and people skills at kind of a high level. Then I talk about the role of test and what we do. You all know there's a million things testers do, but I approach it a lot from sort of the how we look at data and how it's a very creative job and how we solve problems with code all the time. And I give them plenty of examples from my career. And then we do some exercises that they can pick and choose from. They form groups and they get really engaged. I was amazed how excited they were about working through some of these exercises. So it was a lot of fun. It gives them some insight into what testing is at Microsoft. A couple of the kids there are going to be doing internships. They're already on the books for that this summer. So I'm excited about that. Excited to see them here in action. And they're going to be great. So all in all, a good trip. How many budding new data scientists did you discover? A few, a few. And I did kind of delve into that. You used to ask people if they had software testing courses, but and they do, but they're all about formal verification, which is interesting, theoretically. But this time, instead of probing on that, I probed on who heard about big data. And actually, one of the questions I give them is analyzing a relatively small in the world of big data, but big enough to cause problems at a two gig data file and talk about the challenges in that. From there, we really, it's more of a discussion where they talk about strategies for parsing that, knowing that that disk access is really expensive, eventually lead them to a discussion around how MapReduce works. And one person in maybe two in the two sessions I had had heard of that. So I said, yeah, so I said, well, something to go read up on. I don't think universities changing and we like we have a lot of influence over what universities cover. Years ago, we say we need universities to teach more testing classes, but we still need that maybe, but we need now is we need data, data and huge data and big data is becoming so much a part of how so many software teams do business. That it's really important. I think that, you know, budding software engineers of any discipline have these basic knowledge of statistics and how big data works, you know, or if they're going to get a picture of what they're going to be doing in the industry. So I agree. We need we need more. We need more testing classes, but in a different context, it's more around how to test your own code. We need more TDD type classes. We need to train developers to do this upfront. But on the on the testing side, do we really need all these statistics? Don't we just need people to produce great dashboards? Yeah. Nice. Nice. Say, great. One thing I did get to do while I was off on my trip was I got a few tweets out and and one of them was inspired by a discussion I had the day before I left. And we'll just say it was a good thing. I left my tweet was, look, sometimes I just want to measure something because I think it will help me answer a question. And then in all caps, stop asking me to make dashboards. I'm all about communicating. I love communicating information. There's a value of that. But just because I'm measuring something doesn't mean I want to set a release criteria based on that or decide what the threshold is. Sometimes it's important for me just to know how many people downloaded this. I can do that without going, well, if I'm going to measure it, I should have a goal for what I want it to be. And I've been working with a lot of people who are really dead set on if you're going to measure it, you know, you want to know exactly what question you're going to answer and what your threshold metrics are and what your what does pass and fail mean. Oh, oh, God. Yeah. Oh, God. So let me just tee that up one little bit. Then I'll let the rage of Brent emerge upon the room. Another thing came out of the same conversation was because I was talking about measuring. Actually, I put this in my last blog post. But when you're looking at a scenario, how do you figure out what do you want to measure to know the health or how people are using that scenario that will help you kind of answer some questions? And the dichotomous deterministic group wants to take those measures and determine pass and fail for the scenario, which I have a hard time with. But Brent, unleash your comments. Guys, first and foremost, stop building dashboards that say pass or fail. The world that we're moving into, it is impossible to produce such a result that has any value. There is a place for this in the short term we talked about last time about what and why we're doing these changes. One of the things that's also important is to address how we're going to get from here to there. I do think in the short term that there is some value in these dashboards. The primary reason is because we have a whole community of people who don't know what they're doing. They don't know how to get there from here. And so they're trying to map this new set of goals to the world that they're used to. I'm sure I shared this quote, but the thing I've used a lot lately is surging into the future with one foot anchored deeply in the past. Yes, I went to a presentation by Alan Shalloway. For those in the Northwest, he runs a company called NetObjectives. And he shared a process where he talks about pickup sticks where changing culture is like playing a game of pickup sticks. For those who don't know the game, you have these multiple colored sticks. Each color represents a different number of point values. You kind of hold the whole pile of sticks in your hand and you drop them about six inches above the ground and they create a pile. And then you take turns with an opponent pulling sticks off of the pile. And if you get a stick off the pile without moving any of the others, you get to go again. And the whole point is to go back and forth. And once all the sticks are gone, whoever you add up points and whoever has the most wins. I just realized this is a real game and not something you play on the Xbox. I'm with you now. This is old school game. Games that Alan and I might have played when we were young kidlings with our marbles and jacks. In pickup sticks, there is a black stick. It's worth 25 points. And generally, the person who gets the black stick wins. Now, what he talked about is that black stick is like the goal. If you go straight forward, there's a non-trivial chance that you're going to disrupt the whole pile and eventually lose the game. What he said and said is that what he noticed is when you play pickup sticks, there is always an easy stick on top to take. So if you just take that stick and you get to go again and then you take the next easy stick until the game is over, obviously, this dashboard world or this culture shifting world is kind of like that. It's how do you leverage the model that they have today? How do you leverage the capabilities of the people, the processes and the technology to move them forward in the correct direction? If you go straight for the black stick, you're going to cause all kinds of dysfunction, particularly in some of the larger groups because you can't disseminate what and why fast enough so that people yet have the ability to collaborate together. We don't have the technical wherewithal just yet to really reduce the friction around collaboration. So a lot of the top-down command control processes are going to be in place for a while. Dashboards you can fix. Fine, let's create a dashboard. But we're going to stop using pass-fail. What we're going to do instead is we're going to identify key metrics or KPIs that are important to the business. And we have to call them KPIs. That sounds so old school and dorky. What would you like to call them, Alan? S swims. Swims. Yeah. Shit we want to measure. Shit we want to measure. Does that work for you? No. How does it? Oh, should we measure? KPIs remind me of a time and I get it. And there's a big push here to classify the measurements into KPIs and CTQs, critical to quality. And these just remind me of a time of measurements that were purely top-down and almost purely useless. And so I get, yeah, they are the things we want to measure that are important, the key performance indicators. That name to me just it bristles my skin a little bit just because it has so much history with it. The biggest problem with the dashboards that we use today, there's two of them. Number one, they're focused on vanity metrics, not actionable metrics. Number two, they are focused on deriving action with the top of the hierarchy. Exam. So pass-fail. We have the scenario. It's failing 25% of the time. So? I don't even know what that means. I can't fathom what a failing. I mean, if it's unable to run, if it's functionally not there, it would fail. If you can complete it, does it pass or there's other criteria that they run? I don't know what group of measurements come through to make sure that it passes versus fail. And let me just equate this back to the age-old test pass. A stupid metric where we look at how many tests are passing, we go, oh, 98%. That's good. Is it? Because if those 2% of failures are data loss and crashes, maybe not. Or if it's 90% or 80% or 50% and the tests that are failing are these arbitrary test failures or arbitrary junk, it doesn't matter. What that number tells you is there's stuff to go look at and figure it out. I don't care about test pass rates. I care about the issues that we found and what we need to find. And let me tie that into what we're measuring now. And if you look at data-driven engineering of a scenario, we want to measure how long it takes to draw the screen. In my viewpoint, I want to know how long does it take the screen to appear or our web page to download, for example, our web page to appear over end. Page load time is a common metric. But I don't know if I want to put the actual pass-fail number there. Page must load in 0.5 seconds or less. Well, what if it's 0.51 seconds? Is that bad? Well, no. I don't know where to draw the line, so I don't want to draw it. I want to know how fast it loads, but I want to look at it in aggregate and then look at whether it's causing downstream effects, like if it's too long, are people leaving the site, et cetera. So I think that's a great example. So page load time by itself is absolutely a vanity metric. And just so people understand, difference between a vanity metric and an actionable metric. A vanity metric is, in general, a metric that makes you feel good. Hey, page load time is down to 2 milliseconds. Yay, we go celebrate. That's great for arguing for a beer budget, but for creating a solid business, it's not so good. What would be better is a ratio. Page load time over customer impact or over dollars earned. Because by turning it into a ratio, you now can graph it or you can look at it in a way that creates action. If you know where you're going with the action, it's really easy to create a ratio for ratios. I can create page load time divided by the days it rained last year. That's not as exciting for me. And you could drive action that way. The next thing would be to tie it to something that's important to the business. So when it comes to dashboards, the high level summary that I would say is, number one, they need to produce actionable metrics. Number two, they need to be consumable by the people who will take action. A lot of the times these actionable metrics, people produce scorecards because they want to publish it to the executives so the executives have confidence that that team is on track. You actually bring up a good point because a lot of people would tell you both inside and outside Microsoft say, well, management wants to see these numbers and what they want to see are vanity metrics. If I see another red, yellow, green, smiley face, sad face, frown face chart, I'm going to rip my head off and chuck it into this microphone. You know, the one that bothers me is the yellow arrows pointing to the right. What the hell does that mean? You got up arrow and it's green and down arrow, it's red and then yellow arrow pointing to the right. It's trending, Brad. It must be important. Okay. The last thing is the actionable metrics. So again, first one, you want actionable metrics. You want it to be presented in a way that's actionable towards the people who will take action. And the last one, it's tied to a KPI or a SWIM, whatever the hell you want to call it today. We'll come up with a better name. In fact, we will take suggestions from our four listeners. And it has to be tied to something that matters to the business. For example, a buddy of mine, I got a quote from him. One of the things he does, he likes to shock folks. And one of the things he does when he mentions, when he talks about the culture changes, he has this quote, features don't matter. Quality doesn't matter. Moving the metric is what matters. Defining the metric that matters is key. So explain what those last two mean. What is it? What go those last two sentences? Tell me what they mean. So moving the metric is what matters. What that means is learning how to take action on the metric, learning how to deploy code, experiment, measure the right things that will actually cause the metric to go up or down. All right. And then the last sentence, how do you find the metric that matters? At a high level, you measure the behavior you want. So when you talked about peach. But by measuring, I mean, we'll need a concrete example because most things you measure, you'll get whether for the right reasons or not. So you'll be really careful about defining that metric that matters. I forget the Hawthorne effect. Yeah. Hawthorne's principle. People's behavior change in accordance to how they are being measured. In this case, what we want to measure are things where our customer impact. And a lot of times they don't know that we're measuring it. And actually, it's really important there. I was leading you a little bit, but when you measure your metric or about the product or business and not about the people and what they're doing. Yes. So we talked about page load time. Amazon knows to the millisecond how much a millisecond is worth to its business in dollars. That's fantastic. Yes. How do they know this? They measured it and they did the correlation between page load time and churn rates of their customers. We talked about last time that right now in the service space, switching costs is rather easy. People don't have to go to Amazon. They can go to eBay. They can go to other e-retailers. So they measured what is the minimum page load time where they stop seeing people switching away. They associated latency to customer impact and then reassociated customer impact to the business. They want their customers to stay. That's the behavior they want. How do we keep them sane? Cool. Makes sense. All right. So dashboards are great when used properly. Crappy dashboards are crappy. That is words of wisdom from Brent. Yeah. Anything else on metrics before we move down the Kanban list, the lean coffee list? So we have up on the board, we have a discussion around dashboards. And next up is estimation. No. No. Screw it up, Brent. Oh. Can you read? There's a one and then there's a two. A one and a two. This process works so much better when we're doing it. There's not even a number by estimation. Matter of fact, there's an X through it. So folks, I warned Alan beforehand. I have been up since 3.30 this morning. It is now eight o'clock as we're doing this and the coffee is just not working. All right. What do you want to talk about then, Alan? I would like to pause for a moment and give all four of our listeners a chance to weep a little for you. And then I want to talk about... All right. Move on. You wanted to talk about shifting to a data-driven engineering culture and some pieces about that. Let's talk about that a little bit. Okay. So we've been talking a lot about why and specific things that we see as problems as well as quite honestly things that drive us crazy. If you look at the root cause of all this problem is really, we have strong command and control structure within the organization. Very hierarchical. And in there was a historical context to why this was needed. But today there is so much data that that as a decision-making framework slows us down. What we really need to do is entirely change the culture from one set of paradigms to a different set. To do that, you really need to use a process known as systems thinking. It isn't about how do we just fix dashboards. Dashboards is one bit. But in addition to dashboards, you got to fix the people. Yeah. There's a huge people problem. Great Weinberg quote, probably paraphrased, but he says it's always a people problem. It is always. You're talking about systems thinking and that's the challenge in making organizational change is you have so many things moving at once. And if you don't try and get a full big picture view and only look at one part, you're going to screw something up, which isn't necessarily bad, but you're going to do better if you know what the effect is of making this change here. What will that do downstream? And working within a larger, largest organization I've ever worked in at Microsoft, my team is very small, works within a very large organization, Windows. And he has a team of one books. I work in a application group, I don't know what to call us, an organization of about 120 people that sits inside this behemoth known as Windows of, I can't count that high. It's like a zillion. I don't know. Big number. It's many thousands. Yes. Um, anyway, systems thinking in this particular context, there's really three things you need to change simultaneously. You need to change the people, you need to change the processes, and you need to change the technology. When we talked about the dashboards, right? Okay. We need to change what information is being communicated in that dashboard. That's a technology question. But we also need to change the processes. Right now, most of the dashboards, as Alan pointed out, most people are used to dashboards as a preventative measure. We need to see this dashboard all light up green before we can ship. Right? That's primary reason we see a lot of dashboards, at least in the test context. The dashboard that I'm talking about, that would change that model. Right? Because you're always going to have what's the current state. Well, you want to monitor more than for release criteria. You want to know what's going on, which is great. Yep. But we need to release more frequently. You can't, it needs to be green all the time. And when it turns to yellow or red, you'll get it back to green right away. Or green could be relative. Like the whole idea of green is that we have the ability to have a predictive model around what is the threshold. What is the acceptable value? The one of the paradigm shifts that we will see people moving towards right now, we're all absolutist. Everything's got to be true, false, pass, fail, black, white. Hate it. I hate it. We're going to shift towards one that's much more probability based. There is a 77% chance that this will increase the revenue. All right. Let's go back to the dashboard example. So if we have this dashboard that communicates this monitoring and it says, Hey, this new deployment dropped the page load money KPI that I don't even know what it's called, but we talked about just a second ago with Amazon. We also need to change processes such that people know how to act on this data. They're used to publishing the data. And now what we want to do is have the data pre published. They're looking at it actively and they're reacting to it. That will also cause people to be retrained because the model that they're used to is again, very absolutist. We create a bunch of automation. We make sure they all sum up the pass and then we publish it. One of the key things that will have to change is reaction time. So we need, we need PMs to be able to say, Hey, I had this new data point. It had this customer impact. I don't understand why they need to be able to either reach out to the customer directly or use the data to, to surgically understand what is the customer's intent? What problem are they trying to solve? We'll need quality team, quality engineer team, whatever. Whatever. Yeah. Titles need to go away. That's a different podcast. Go on. We'll need them to be able to do two things important. Number one, in this world, data credibility is key. When I say the, the page load time dollar thingy is up 2%, it's up 2%. One of the challenges that we see right now is reminds me so much of the, the automation problems we had, uh, 10 years ago, um, where I used to joke, if you got five test architects in a room, you'd end up with six, uh, automation harnesses. We're going to have the same problem with respect to metrics and data. We need to, we will also need to change processes and people around collaboration. Yeah. I think it's, we're moving there and a lot of people in the industry move in there and data is becoming really important. We get that and it's going to be hard and we're going to screw it up and we're going to get some things right. And we're going to move forward. It's pointing at the clock. Like, like I'm out of time. Like is my, no, I got time. I got time. So I think this is now third time when I tried to do invisible hand signals and he calls me out, but yet I refuse to learn this lesson. Yes. Well, it's Brent and we all, we all know Brent, but Brent is right. That conversation is wrapped. We're going to move on to the mailbag. Yes, that's right. We got questions from our mailbag. Uh, I feel like we need like a little mailbag jingle, like in the mailbag. Yeah. Something like that. I thought about it, but copyrights and things like that get in the way when you go back and edit it, the, what you just did, just get a hot button for that. Yeah, I should, but again, I don't want to like, I'd have Nicola who made blues clues. I don't, whatever company made blues clues. Like Nick, Nick Jr. with like, like Sue me, it's like, I want all the money you've made off that podcast. And I would do like vanilla ice and just change the beat just a little bit. No, no, I never even heard that under pressure song. Dork. Um, so anyway, we had a couple of questions you're welcome to send them in. And I had a thought earlier one time we should do like, uh, like, uh, have people tweet us questions live during our recording. Just thought for the future. Do you think our four listeners are up at this time? No, I have way more. I have way more Twitter followers and we have listeners so we can, we can probably get some answers. All right. The first question is, does that mean we have to do live streaming? No, no, just, just, just don't throw it too hard. I'll go with the flow. Uh, Mark asks two questions. First one is how do you document your test efforts in a, such a fast environment, a fast based environment? I could tell you I'm going through a little transition right now as, uh, again, working with, uh, some of my larger organization with one foot anchored in the past and surging into the future with one foot anchored in the past. So you're, you're producing 30 page test plan documents for what you're going to release. But I got to tell you, there is the, the, my stance on documentation and planning is this, and this is where I drew the line and actually got them to make a change, um, which was getting a change in this group is hard. So I'm going to pat myself on the back without hurting my arm. My stance around documentation, especially test documentation is there is all the value is obtained in thinking through the problem and use the document to describe your intent to purposes, describe your intent and communicate with those around you what that intent is. I've used the verse that's wanting to do mind maps for that, but whoever you want to document that it's about making sure you think through the problem. And we'll give you a template, a very short one to kind of help you think through what are all the ways we can do a B and C. And you know, you want to describe your intent just to, and then by communicating that the second step you've one shared your ideas. People can learn your approaches and you're going to get new ideas and bring in there and you're going to enter the act, enter the project. This is all very quick. Um, you know, Google had their 10 minute test plan. And I think you want to spend enough time on these to be thorough, but no more where I draw the line. However, beyond those, is do not try to predict the future. There originally was a, you've all seen the test case design documents, whatever they're called with a list of here are the test cases I'm going to do, which is one of the stupidest practices I've ever imagined in my life. Because you don't, you can, you're imagining some test cases you might, but you don't know if those are going to be the final list, which ones are going to be irrelevant, which ones you're going to add to that. We talk about maintaining documentation. I don't believe planning documentation should be maintained at all. Cause it should be written in a way that areas that need maintenance do not exist. Talk about intent, what you're planning to do, you know, some ideas and how you're going to do it, and then go do it. Don't list your test case IDs. Don't list, don't link to things in your work item tracking database. Those things will change. I think we've, on our soapbox, that we have spent so much time, you know, worrying about maintaining documents and we don't, and we make jokes like, well, sustained engineering will use them and they don't, they would use that intent, but listing a bunch of stuff that you don't know predicting the future, unless you have a crystal ball or a time machine is a complete waste of time. It's okay. First and foremost. Did I get too high on the soapbox? Are you a little dizzy? No, the, I didn't follow you up there. First off, the answer to test plan, in my view, is, it's certainly something snarky. It is, hey, however dev wants to ride it is okay by me. The second one, planning. So Eisenhower, I believe, had a quote, which is, plans are useless. Planning is invaluable. I think it's indispensable. Whatever. Sure. Go on. In, indivisible. And liberty and, hey, go on. The plans, there are really only two goals to a plan. Number one, get you and the people who will be working with you on the same page. Number two, get your own self on the same page. Isn't that exactly my two intents? Describe your, yeah, describe your intents. That's exactly it. Share them. That's exactly it. In my view. So, so the question was, how do we document your test efforts in such a fast environment? I'm going to take the word test out, because that word bothers me. I'm going to say, how do we document our efforts? Anything we want to plan. Right. I am a huge believer in Agile. And the way, as an example, the way we do it on our team is the goal is collaboration, or it's not really the goal. The goal is same pagedness between you and the people who are going to work with you. You can call it collaboration, but the reason why we produce these docs is to make it easier for everyone to get on the same page and contribute to the outcome. What I think is the three most important things listed is the intent of the, of the plan, what we're trying to build, specifically the outcomes. And what I mean by outcomes, I mean a customer impact outcome. We, we want this to boost throughput for our customers by 10%. You want to have an implementation agnostic plan starting off with, and then you want to have a measurement of success. How do, how do you know if you're in the weeds? How do you know if you succeeded in this? That last one is the most important one forgotten. And it's the way you get out of this predictability BS, because the measurement of success is what tells you, are you failing or succeeding? And the goal is to fail fast. I get you. I get you. I want to, I have an estimation story I want to tell. Okay. And, and really the answer there is that the long answer for the short question was, plan just enough. And actually one thing to add is that one thing that helps us on our team is we're sort of scrummer, fallish, fragile, working our way towards agile. But one thing we do, your waterfall with short sprints, we're a little better than that. We're a little better than that. So don't judge if you're not living in my world. All right. And we do list every work item has in the work item itself. We make sure people define what does it mean when it's done. And for me, you know, what is the done done list? What is the done list? And for me that's, and I described that as I'm working on this team. We're trying to get better. I found that describing done before you start eliminates a ton of the problems and helps focus the work you do. And I believe it is also the gateway drug to do using behavior-driven development, which is where I like to get the team. Yes. The, the biggest issue I've seen with, with that lately though, is people are still defining done in terms of work items. Oh, I will have a database online. I will have a reporting online. And that's not allowed. Done means it will do this, this, this, and these things exactly. Yes. And then there's some other implied things about the crossing the I's, dotting the T's or whatever, however that thing goes. Yes. Anyway, let me tell my estimation story. Then we'll get on to the last question maybe. So I was sitting in a room, another tweet. I had to tweet. I had to sit next to a developer who wanted to cost estimate, I guess they call it costing now. These words, he wanted to cost every work item for the next year. And my, I got a little twinge on the back of my head and I asked him, so what granularity do you cost? He goes, man months. I go, I got a little good, good, good. I got the little face and I didn't quite wrap the telephone cord around my head, but I said, so, um, you know, that estimating when you say something takes two weeks or more, it means you have no idea how long it's going to take. You just don't know. I know there are studies that say anything two weeks or more as, you know, as a variance of 200%. And the, his justification was, I have done this in the past over three releases. I won't say which product I'll be nice to him. And they've been remarkably correct. And I'm thinking, well, it's all, it's a false. It's a false because hot thorns principle in play. I don't think it's hot for an effect. And I don't think that's it. I think when you take, sure, if I put these huge wide swaths around how long things I think things are going to take, not every, not every task takes more time than it should. Some take less. So I think just the law of averages, when you say everything's going to take somewhere between one and, you know, one in, you know, or 12, 15 man months to complete at a very high level. Sure. They're going to, you're going to get that because well, one, you're right. You're measuring it, but two, it's such a huge buffer built in there. It doesn't help you. It doesn't help you get better. It's just, to me, it's a fool's errand, a useless cause to try and estimate like that. So here's what happens. Here's, here's what I've noticed what happens with people in that situation. So they have this pride point. My estimates are always accurate. Well, as measured by what? By my ability to deliver on the time that I set. Okay, great. Now, this person is often in a management role and he has a pride point that my team will deliver on this. So the first question I asked these guys is like, I want to see your time tracking. I want to see how many hours your developers are working because I can guarantee you that team is working 30 hour days the last week of that month. 30 hour days. That's funny. Yeah. Anyway, I had to get that off of my chest because it's just, look, planning is good, predicting the future. Don't try to waste a time. So actually, before you go on the next one, there's a couple of things I wanted to say. Number one. So in a fast environment, this is the number one dysfunctional thing that I have learned from agile. Okay. And I will tell you, I despise the agile manifesto. We just lost half of our half of our four listeners. Yes. You better. And the reason why is because it lays out principles. And if you read it, it says, we favor collaboration above documentation. What it says, and then you have a bunch of clear, it does not say that it says this is good, but given a choice between this side and this side, we prefer this. If you read the language of the four motto thingies, it says favor this above this. Okay. Now, when it gets deployed with a bunch of old school people, I'll call them. I almost got us kicked off for what I was about to say, who are trained in the absolutist mindset. This, this favor model gets switched to no documentation. Yeah. That's just stupid. I hate that. Okay. Oh, we're agile. We don't have doctor. Agile does not freaking mean no planning. It means lightweight planning enough. And that's, yeah, I think that's clear. I think what you despise is not the manifesto. What you despise is the massive misinterpretation of the manifesto. I do that. And I think the manifesto itself could correct it if they reworded that in a way so that, so that the people who understand the spirit of it, agile as a term is so bastardized that we kind of need a new word to replace it to actually describe what good engineering is. People say they're agile when they're not all the time. It's so they can ignore the parts of the old world they don't like and not actually work any better. We've talked about this before. You know what? I think the world needs an agile manifesto for dummies. I do. Where it's just laid out. This is how you interpret this. Anyway, the next thing, my team, one of the things that I'm very proud of my team, we have this phase that we call the analyst phase and to get, this is on our version of the scrum board. Okay. There's a column analyze to get out of analyze. You have to have sign off from two design reviewers. Okay. Now, why is that key? Because if this project, whatever that ticket is being worked on, goes into the weeds, I don't, at our retrospective, I torture the design reviewers. I go, why did you let this thing go forward? What was wrong? And oftentimes it's like, well, we didn't understand the design. Again, why did you let this thing go forward? It's by doing that, by putting, making designs and doing it in a way that applies social pressure, you get, you converge very quickly to what is enough design. And that's going to change on a context by context basis. Yeah. I don't always believe in social pressure as the way to derive on the right answer, but it's not always. No, we got one more question. It's going to be a quick answer. Mark also asks, what challenges surprised you at Xbox things you didn't see coming? And I can tell you, Xbox was the, everything I've worked out in my career was the fastest moving and so many moving parts that everything was a surprise. It was, it was a crazy, crazy time. It was a load of fun, but I'm honest when I say that, I don't think anything threw me off, except for maybe this five year old hack in the password thing, which I probably, that's all I should say about it. But it every day, something was moving, something changing, and just keeping all the pieces glued together and the, you know, the pieces bolted down to get that, to get not the product as much as our test infrastructure and which was solid, but with the product changing underneath, it's a huge long story. But the short story is nothing surprised me because everything surprised me. Wow. That was remarkably Zenny and, or Zenny. I don't think. All right, everybody. That's all we have time for. All right. Thank you and see you next time. All right. Bye. 
