Hey everybody, I'm Alan. Brent is not here yet. Took a little bathroom break, so we're going to get started with episode 81 of the AV testing podcast. Oh look, it's Brent. Brent, did you wash your hands? Yes. All right. All right. Excited to be here. How you doing, Brent? Fine. I'm sorry, I was a little premature in getting started. I couldn't wait. I was excited. I understand. Do you know why I was excited? How? Because we're going to talk about modern testing principles. Deep dive into the first modern testing principle. But we'll get to that in a little bit. Let's catch up on stuff and do our normal banter. So if you don't like this part, that was your chance to press that fast forward 30 seconds button about 75 times. Yes. So what you been up to? How you been? What's new? It's crazy. It's the period of the year where everyone realizes there's two months left before the review and then all sorts of people are doing stupid stuff. I don't miss that. I don't miss that at all. I went through it last year. I realized I got to September. I go, wait a minute. I didn't have my blood pressure go through the roof. People weren't freaking out. People weren't all behaving differently. That's so weird. I really do think. I really do think. I haven't done the data analysis on this one, but I strongly suspect that I would find that we would, if I measured, that we would find that from an hour basis, the Microsoft year is 15 months long because the last three months people are putting in double time. I had a boss who had a phrase peak in May and it's absolutely true given it's so weird how the review system of course isn't and paper isn't tied to results. Well, value. I'm not going to, I was going to just, I have a lot of rants about the system. Had them while I was here. It seems unfair for me to rant too much about them while I'm not there. So I will just say I don't miss them. It has improved. There's still a long ways to go. If I'm going to be fair and realize that I'm still a Microsoft employee. Yeah. I wish to wish to remain. So for the least the short term. Huh. So I don't, for those of you not in the Slack channel, the one of the, one of the three Slack channel, one of the three dot slack.com, or just go to anger.com slash AB testing. Click on the link to join. I posted there that I have a new member of the family, Tara, the modern testing puppy. The three have decided that your dog's name is Monty. Can Monty be a girl's name? Sure. I think, give me time. I haven't quite figured out what it's short for, but I'm going to go with yes. So, uh, rescue pup lab mix. I'd even do the doggy DNA thing. So we'll find out, uh, what actually Mick. She is, but she's pretty hilarious, but it's like having a kid again because, uh, they, yeah, it's like having a kid again. Except the time they're a kid will last much less longer. You know what else I did. You know what else? You know what else? Yes. What you took a trip. I did. I took a trip on a ship. Where in the world is the weasel? Uh, I was, I, Alaska airlines shot me down on Monday to Orlando, Florida, uh, where I gave a workshop Tuesday morning on web testing tools and then flew back Wednesday evening, got back late. Wait, wait, wait, wait, wait, wait, wait. I was going to go back and fill in the middle. It's like the memento. I'm pretty certain that I only briefly care about this, but I am curious workshop on web testing tools from a guy who is publicly allergic to UI testing. I did not talk. I talked a little about UA animation, but it was about, It was about actual web testing tools, not UI automation, not automation. So what's a con... So we spent a chunk of time at the beginning. So you didn't spend a lot of time on Selenium or things like that. We spent no time. In fact, let me come back to that because I am... My rant level with UI automation is about to go through the freaking roof. But rewinding, flashback, da-da-da-da-da. I started off with some of my favorite Chrome plugins, Chrome extensions for web testing. Simple things like there's a web developer where you can turn off CSS elements, makes it easier to edit CSS and just going into source. I did go through showing people how to use developer tools. I think it's very important when debugging to know what things look like when they're working correctly. So if something's acting weird, if you go in and look at it and go, this doesn't look like it looked what it usually does, now I know it's wrong. It works on kernel debugging, application debugging, it works on web debugging as well. So we dove into that a little bit, showed them how to do performance, how to show the waterfall graph of page load times, understand where the time is being spent. Extensions like check my links, very handy for making sure that you're quick crawl to make sure the links are correct and they actually go somewhere. Found some interesting issues. A few people found bugs in their websites, but some people found that if your site has a bunch of Twitter links, Twitter will throttle you for doing so many requests at once. Oh, yeah. You can start to fail some. I thought that was pretty cool. One of my school helps people understand how a little bit more how the web works. But go on. I was just going to share that one of my school projects a couple of years ago was on sentiment analysis, and I first started to do it via Twitter feeds and I hit that. Yeah. Yeah. Yeah. So I also spent a reasonable chunk of time on Postman, and which was a lot of fun. You use Postman? You know what Postman is? So Postman is a... You crazy kids and your new OSS tools for testing. So Postman makes it very easy for anyone to do or rest API testing. Oh, OK. So does it cover security concerns? Not directly, but I can't. We could do a whole episode on Postman. I won't. But the great thing was I had actually here's the weird thing. I had a full big room and I honestly asked them, what are you doing here? What were they doing there? And some of it has some legitimate reasons. And so we hung out. We talked. We did a bunch of things. But I asked them, how many of you do API testing? Four hands go up. I say what tool do you use? We use Postman, Postman, Postman, Postman. I mean, this is like 50 people in the room. Great. Let's all become API testers. And the great thing about Postman is you can just set it up from scratch and put in a URL and a web request to start mucking with stuff. Look at an API we used, an app that someone wrote called Mark Winteringham to be is his name of someone called Restful Booker, which is a little hotel booking app. Maybe it's hotel, some booking app. But take some rest APIs for your basic CRUD operations. And you can just start with nothing and explore. But the great thing you can do that can help accelerate the team or accelerate a tester who may not know about web testing is you can just configure a JSON file with some basics already set up, including some variables. So you'd like you want to type in the URL changes. You could just change it in one spot. So I gave him some JSON files from Danny Dayton. Again, a huge shout out to Danny Dayton, who's a huge Postman advocate. He and if you go to Stack Overflow and find questions on Postman, they're probably answered by Danny. But he's also posted on his GitHub, a bunch of utilities, tools, tutorials, and he has a couple of JSON files, which I just borrowed. I could have made my own, but his or I know his would work. And mine may not. Anyway, I give the class these JSON files and all of a sudden they can actually start running API tests and then using their exploratory brains, because most of them are just manual testers most of the time. I shouldn't say just our manual testers most of the time. Now they can use that brain to go, OK, what happens if I do a start date? That's before an end date. What happens? What happens if I for the more advanced kids? There's a little programming interface you can use to it and just a quick loop to create 4000 random bookings, which made the get call very slow for everyone in the class because everyone's working at the same date. Right. So it was fun. So at the opportunity to show how the personal work was great. I could use those four people that had the API testing experience to help other people out around them. And within a half hour, 40 minutes of this workshop, I went from having four people that I've done API testing and kind of had the gist of it to 50 people who had done API testing and knew the gist of it. We went over a few other tools. Did a little bit of security testing showing them Google Greer to know how to do that, which is if you don't know that, it's just an application they built, which has it's a simple app with a multitude of security issues, but not like super easy to find like a lot of these testing apps out there, which are just totally broken. But you have to do a little bit of work. But people thought, oh, OK, I get how I get how easy it is to break software security. So anyway, we crammed through it all in three and a half hours with a 15 minute coffee break. And I haven't seen if they liked it or not, nor do I really care because I had fun and I got lots of head nods where I wanted head nods. So I felt good about it. There it is in a nutshell. OK, so star east star east. I didn't attend very much. Wait, modern testing? Question mark. Was there modern testing at star east? Yeah. I'm thinking of a word that rhymes with no. Oh, it's no. Yeah. I was thinking KNOW. Oh, that wasn't it wasn't a good rhyming word. Yeah. So not really. And I, to be clear, I think star is a great conference for newer testers. It's a great conference for people because they have so many tracks. It's great conference to break into if you're a speaker. It's a great conference for getting a lot of different knowledge. If you have enough basis to start using that to form your own opinion. But to the but the audience they cater to is mainly new testers. The speakers are mostly consultants trying to get business from new testers. And that's fine. That's their business. I'm nothing against that. But I didn't get a lot out of it other than meeting some people that I hadn't met before and saying hi to some people. I was practically frustrated. And if you have a question to ask and then I want to share a few of my stories before we go into the modern testing principles. No. My question will be a tangent. So go into your stories. All right. The opening keynote, which great content and very well presented. I forget the name of the woman who presented, but from a company called I Sims and I followed her on Twitter. So I forgot the name. So I apologize if for some chance you're listening. But a great story on how she took her team of a combination of Estes and manual testers, test developers and testers. I'll call them a titles piss me off. And how she had transferred her manual testers to learn how to write automation for their team. Yeah. Yes. Again, not a I'm not a fan of the concept because it goes about 100. We have a whole new slew of you. I there's about 150,000 things that as a tester, I would love for you to do with code before you thought about writing automation. But unfortunately, even before the talk, I heard a person behind me say, and this is this part's literal. I might I might forget a word, but it's pretty exact. So I'm just a manual tester now, but I really want to get into automation because that's kind of the way way to go. Right. I want to get into automation. And I hear that and I just might get sad inside. And Brent's going to puke a little bit. I go, oh, and as an industry, we it just hurts me that we preach to this so often. And again, I thought the presentation was really well done. But two big takeaways for me. One was it came out in a tweet from me. I said, wow, we all reflect. We can think about how much we've seen talks and articles on getting testers to write automation. It's it's on every testers mind like, oh, I need to write automation or I don't want to write automation and I don't want you to write automation. I want you to learn some code so you can write some tools that will help you do much better testing. I don't want you to write automation. So the tweet was something to the to the effect of we seem to pull a lot of there seem to be a lot of talks and articles on teaching testers to write automation, but very little on teaching developers how to test. And the reactions to that were although while Brent's nodding his head, yes, absolutely. And some people contributed to that. Modern testers would say, yes, absolutely. The pushback on I'm not surprised, but I'm saddened by how much it was. Well, developers shouldn't have to learn how to test because we have people already who have special snowflake skills that allow them to test and developers don't want to learn that, so they shouldn't have to yet. So I hear. Yeah, I hear comments like that all over the place. And, you know, I go back and forth when I hear comments like that, it depends on my mood. One one version of me will sit down with a person and try to help them not screw themselves. And the other part of me is a firm believer in social Darwinism. I honestly believe and I get all the feedback and I understand people are in different situations in different contexts than we are. But I honestly believe that the direction software is going and where there will be a tipping point soon. The majority of software companies, developers will be doing the majority of the testing. And people will agree with me. People will disagree with me. People will say, no, no, they can't do the testing. I still get the shit about the up there with our explicit tag. I still get the shit about the mindset thing. Oh, developers can't learn how to test. They don't have the mindset for it. And for the last three years of my career, my life, I have been teaching developers how to test. And maybe I'm just in my own special little snowflake world. But what I've learned is guess what? Developers can learn how to test just fine. And I'm just blown away by this whole mindset thing. Well, only testers have these skills they've honed their craft over a year. No, it's yeah, it is a bunch of. I'll just tell you, it's a bunch of wrapping. That that individual is put around themselves to justify their existence. And and if you tear off that wrapping, you're you're you're it's it is a threat. And I and so the really the way to convince these guys is like, no, no, I'm going to tear off the wrapping, but you'll be safe. You will. But you won't be safe. If you keep hiding behind this facade that you've convinced yourself of. Yeah, it really when I see the the the negative reactions or the argumentative reactions, a lot of it comes from fear of you can't move my cheese. You can't take my wrapping off. So the worst. This is actually something this is a a brain Jensen management philosophy that I teach. I need a theme song for that. But go on. Yeah. Where is it? Where when mentees or employees come to me and you're like, I don't understand why that person is doing that. I would never see them do that before. Like, this is not normally how they do things. So I am an amateur psychobabble list. Yeah, me too. I have spent a lot of reading on psychology stuff. Ditto. And done. Honestly, my unfair share of counseling myself. I for a long time at Microsoft, I called my job test therapist. No, I meant on the receiving it in this case. And there is a pattern that I've observed in that is when you see an individual doing this particular pattern where they are, you observe them and you go, they are totally self-destructing. It is 99 out of 100 times due to a fear and their fight or flight thingy, amygdala is triggered. And the only way you can help them, they can't help themselves. If you see someone like this and even if you talk to them and they speak rationally, they cannot help themselves. They are afraid and they're going to continue to do this. Now, one of the things that's worse about this is that when you fear. So we've talked about Myers-Briggs before. I'm not going to... One of the patterns that Myers-Briggs has noticed is that when you're stressed out, you switch all four of your letters to the opposite side. Really? I don't know if I buy that. I'll think about that for a while. So if you're a thinker, you start getting more emotional and go, why does everybody hate me? And if you're an introvert, you become more extroverted because you're trying to figure out a clue. Now, what happens though... I'm going to reserve judgment on that, but I'll let you continue. What happens though is because you switch to the other side, you start doing a catch-22. The stress causes you to switch, but now you're using the part of your personality that you hardly ever use, so you don't really have the skills to use that part. And that then creates this death spiral. Some of the studies actually think it's... You switching is what causes the stress, but nevertheless. The useful advice is if you see someone like this, pull them aside, be a friend, help them think because they are not. They're running out of fear. Yeah. Anyway. So anyway, I saw a lot of... The tweet responses were... I started replying to them. Some of them were... The initial reaction was visceral. It's like, aah! But then I just let them go because people... Like you said, I can't change, especially over Twitter. I can't convince them that the world is not changing around them. But I really push buttons with the UI stuff. The funny thing I see with the UI... Every time I go off on why we have way too much UI automation and way too much emphasis on UI automation, the constant answer I get is... For people who write UI automation, they say, yep, you're totally right, except in my case, we need to do it because. Except in my case, we need to do it because. Well, it's different for me. There's a bunch of snowflake world that means we have these ice cream... Agile doesn't work on enterprise shrink-wrapped products. I know what you're talking about. So anyway, that was my experience. It's a little weird. I ended up just doing a lot of work by myself. I met with a few people that I wanted to catch up with, which was fantastic. But yeah, it's... So before you lose hope. Here's the difference. Let me just throw this out there. All right. And again, nothing against Star. It's the right conference for a lot of people. That's probably the biggest test conference out there, and it will remain to be. I think that's great. I love the people at Star. But as far as modern testing principles go, I was kind of blown away when I went to Test Bash. I talked about modern testing principles, thinking I'd freak some people out. And I had so many head nods, and so many people say, yep, that's what I'm doing. I would have been lynched at Star for giving the modern testing talk. Oh, that is exactly the reason why to do it. Oh, one last thing I want to mention, speaking of being lynched, some of those so-called high-profile consultants, it was weird. After the keynote, I was walking the hallway, kind of just looking for a place to work, and I heard more than one of them muttering about, complaining about that opening keynote, saying, teaching testers to write code, that's just awful. They should just focus on testing. And, oh, they can't learn to code that quickly. They can't, this is just ridiculous, ludicrous. I won't give any names, but the usual suspects. And all kinds of absurdity, because again, it's for the same reason, if testers grow their skills and get technical skills, whether they're automation or actually valuable technical skills, it takes away from their business. Because their business is a threat. Yes. Right, it's a threat. But I don't give a crap, right? Because there is a lot of people in this stuff that are focused on doing the right thing, want to learn, like the millennials. You know what, I find it goddamn offensive that Star East would be teaching the millennials coming into the industry, these freaking concepts from 30 years ago that no longer apply. Now, so like, yeah, I could see going to Star East, that there's a chance that you would be lynched, but only verbally, not physically. Right. And you know what, you can't convince someone to do this, but you can begin to open their eyes. Right, I had a Twitter with, so a buddy of mine, Al Shalloway, he is frustrated because he's not seeing a large, so he actually pushes the edge of Agile, and he's seen all this effort we've done, and there's still some of these key behaviors, the Agile community has not figured out how to root out, like scrum and iterating only. Yep. Right, things like that. We just haven't figured out how to solve that. And I said, you know what, dude, I'm with you. Like you can lead a horse to water, you can show them how to drink water, you can show them other horses that have benefited from drinking water, you can talk about the, whatever, but you still can't make the goddamn horse drink. Right. And we can't, and all we can do, and that's one of the things I think about this podcast, this is why I come in. If every time one of the three refers us to another new one of the three, right, then that person begins, look, you know what, we're full of shit all of the time. We're just here to plant some seeds and hopefully some sprouts of good ideas grow out of them. And then help you control your own destiny. I can tell you, I agree with Alan, this tipping point is coming. The last survey, we still have to follow up with Joel, but the last. I did check the website, since it's coming out very, very soon. Okay. The last series of surveys that we're seeing is, yeah, the business leaders are accelerating towards understanding what they should be doing in this space. And it agrees with us. I think so. Hey, speaking of the three, we should probably do them a favor and get started with the podcast. I agree. So over the next seven episodes, we are going to dive deeply into each of the seven modern testing principles. Maybe more than seven, given the time collected in this one. You can read those on moderntesting.org. I think this one is good, and I'll talk about why it is in a minute after I read it to you. Again, moderntesting.org, if you want to read them all. But the first modern testing principle, which we'll talk about here for the next 25 minutes or so is, or maybe 20 after editing, is our priority is improving the business. I like that one. So I like it too, but I didn't realize how much until I started thinking about it and taking a bunch of notes and writing about it. And I realized that, I don't know if we did it on purpose or how it worked out, but I think the reason why I thought that one was first, but in many ways, the way you improve the business. We had a whole episode on that. You had it as number three. Do you have to drink and talk about this before? So let me. No, but any chance I get to remind you of how old you are? I'm pretty dang old. So the way you do, the way you improve the business, the way you make that a priority, is you execute on the remaining principles. So rather than talk about the other principles, I wanna talk about what it means to improve the business versus the task. So Brent, I'll ask you, actually why? Why is it important to improve the business? Why is that an important principle for a modern tester? My perspective on that is, the reason why a modern tester and why focus on the business needs to be important is because the modern tester needs to be viewed as a positive contributor. Exactly. Not. And in addition to that, the other reason why is people who are looking at our work here and trying to decide what to do. I want them to have a sense that that's their responsibility, that that is the goal to achieve. How do I take my strengths, my assets, and improve the business? Not theoretically. None of this. So I think there's an important shift from the traditional tester, or maybe even the agile tester, that modern testing wants to promote with this principle. And being a modern testing team would promote. We've traditionally seen test as a cost center. Test is a cost that comes at the end. We look at the ROI of testing. How much should we invest in testing? That world is over in modern testing. Let me tell you a story. Testing, in one second. So I think the testing activity adds to the value of the business and we have ways to demonstrate what that value add is. By accelerating the achievement of shippable quality and by add it, we do things to add value to our team through testing efforts, through quality efforts, through acceleration efforts, where the value of doing testing on a modern testing team is a value add for the product instead of the traditional cost. So that's a huge shift. It is the pursuit, I love how you said that, it is the pursuit of adding value to the business. I'm actually bouncing up on my channel. I'm so excited, yes, yes, yes, yes, yes, yes. So in not only that, but non-theoretically, my very first blog post so many years ago came after I had lunch with Alan, or not Alan, your Alan. Wait, who's old here? Yeah, we both are, you're just slightly more. Whitaker, James Whitaker. The George Carlin of software testing. I completely agree, software engineering, I think it is. Now, he was, when we had this conversation, he was in a test role at Microsoft, as was I, and I was having a conversation with, I was a middle manager then, I was having a conversation with my, this was back in the day when Pumps were still here. Those are product unit managers? Yep, were. Were. And he noticed the behavior. He's like, you know what, I think we should just get rid of automation, go back to manual testing, because one of the things that he observed was essentially every time I add a new rec to work on automation, they come, the team comes back and says, oh yeah, we weren't able to achieve the goal because we need one more rec. So he concluded that automation, and now this team was doing UI automation, he concluded that automation was a black hole of payroll, which by the way, I completely agree. Yes. V3. Yeah, now, but long, long, long story short, I took this and I said, hey, I posted to James and I said, hey, you know, have we figured out what the ROI of test is? Like if we figured out, if I have 12 people on the team, how do I articulate what the return on the business's investment will be for the 13th person? And, you know, I wrote this blog post, and James is really smart in the test business, but neither of us had a really satisfying answer. His answer is, hey, you manager wants to know what the ROI of test is, get rid of test, and then he'll see immediately. Now in those days, yeah, get where he's coming from, right? But that was mostly, it wouldn't have articulated an ROI of test. It's an ROI of a scapegoat or ROI of something else, right? And if we got rid of the test team, you can't change a software engineering process like that with a band-aid rip, right? So yeah, it's gonna cause a lot of chaos, but it's not because test is valuable, it's because you changed the process. So let's see, let me, I wanna explore. But that theory around the ROI of test is one of the reasons why the traditional view of testing is broken. So let me take that model and go a little farther. Let's say we got rid of the test teams who recognize the ROI. Yep. Everything you said. But Microsoft did that later when they said, okay, there are no more testers, everyone's an engineer. And in many, many cases, they failed because they didn't find a way to make the testing activity a value add. They had a value deficit because a lot of the quality things were just dropped, and the value add to the customer failed to happen. Yeah, but remember, so famous quote from Edison, right? He learned 2,000 different ways to fail to make a light bulb before he succeeded. There are places where you and I could argue that, you know, like for example, there's a particular org whose name rhymes with windows. Where what I observed is that they didn't learn from the year plus of being executing this. Right, they decided it, which is something that these large organizations do is like, oh, you don't know what you. Absolutely, and then just to dive in, the reason why I think the modern testing transition, I'm happy with the way it's going at Unity so far is you're being very gradual and very purposeful about shifting these roles and making sure people understand things. A lot of teams, you know, Yahoo famously did it, but a lot of organizations at Microsoft did it, took no learning into account, just said, okay, it looks like that's what they're doing. We're gonna do this. So it'll just magically work. Now I wanna be super clear, mostly because I don't want Steve Rowe emailing me. It's gonna happen now. Yeah. I wasn't privy to those discussions. This is what I observed as well as what I heard from bits and pieces of others. You were in Windows at that time when the treasury. Unfortunately, yes. As was Steve. Although I think Steve was far more likely to be connected to the business decisions. Yep, I was on the periphery in something that was part of Windows because it used Windows, but we were on the edge. But there are multiple places where I did not observe them picking up with what I went through, the Bing experience, and what I thought was really useful learnings from that. In addition to that, I attempted to roll it out on two other occasions in different orgs. And yeah, there's certain complexities there. Like you have to be able to answer the question. Hey, if Tess is no longer doing the unit test, if I'm doing the unit test, what's Tess doing? Right, things like that. So let me get back to the value thing. And I want to, I just realized I have a suggestion for a future episode if you can find the right person. So I was briefly in the Office org, which was my first Sanofsky-ish org. They'll think Sanofsky had already moved on to Windows at the time. So they had a very, very, very carefully planned waterfall model, which included very short coding milestones, like in four to six weeks where you wrote the code. And then literally close to a year of testing and stabilization. So in that case, the testing was a cost that happened at the end. We tested quality into the product. Now again, on the periphery and the link, and actually that length of time, because they did the very short coding milestone, I strongly suspect that what happened is, hey, after this period, there's no coding that everyone's testing. And so you see a phenomenon of, nope, I just have to get the skeleton in. I can actually implement it. There is famously a story in one of the Office apps, I believe word early on when they had to, someone was given a task to write a function to calculate the font, this is all paraphrased, I've forgotten now, but calculate font height. So a coding milestone was almost done. So he wrote float, get font height, curly bracket, curly bracket, looked at the clock, looked at the calendar. Check in. Return 12, check it in. It was done, but it was buggy. So then he could fix it during the stabilization time. So that said, a team following modern testing principles, the Delta from idea to deployment is rapid. Yes. And that, so we wanna get just enough features, get that MVP out with just enough quality in order to get feedback from the customer, which is so different from that other world, but if we can accelerate that process, that's adding value. So we enabled the team to accelerate the achievement of simple quality, and then we can get the value of that engineering effort, which Reese in Lean Startup says, you don't get value from your engineering effort until it's in the hands of customers. So we can accelerate that idea into customer's hand, get the learning and loop and loop and loop and loop. That is how we add value to the business. I completely agree. One of the things that you bring up, we've talked about that's your job, accelerating the achievement of shippable quality. But one of the things that I have found, and I haven't come up with yet a cogent response, because a lot of times I see people like in Twitter, hey, Brent, this is absolutely fantastic, I love this. And then I see them using it for what in my humble opinion is evil. Because it's just generic enough that you could perhaps encapsulate it into what you're doing and use it to justify what you're already doing, even though what you're doing is wrong. And one of the things I wanna call out that you just mentioned, you can ship before you've achieved shippable quality. Of course. Right, so if you get an MVP out, see if anyone actually even cares, ship it with bugs, knock yourself out. But you have to be able to react to it. Yes, you have the kind of being ready for that loop. Reminds me of the story when getting ready for the team's launch. And everybody's freaking out like, oh, November 12th is the, or whatever the date was, November sometime. You know, oh, we gotta be ready, we gotta do the right things. And everybody's freaking out about that day. And I'm sitting there going, okay, yep, that day's coming, that's not an issue. How do I make sure we set ourselves up to be ready for the release a week later? And the week after that. And the week after that. Modern testers, if I may call myself one, or a team attempting to follow a modern testing principles, are thinking about that loop way ahead of time. Like, it's not just getting it out there, but how do I react from that and then take the learnings and make that do a better experience for the customers? And that's a value add. I do, I think actually one of the big things someone in this space can do that would add great value, I just brought up the Scrum and Iterations fallacy. I actually heard this in one talk at Star before I left, something to the effect of, I was working, I looked up and made a face, a little vomit in the throat thing. So the effect of, so we decided we needed to be agile. So we planned out all of our Scrum Iterations and then we documented them and executed on that process. I just made Brent. I cannot tell you how many times I have heard something like that. And how many times, I mean, Alan and I, we like to be on the leading edge of ideas. And so a lot of the times I forget that what is like a frickin' decade old concept to me is still new to other people. And I just get so, I'm like, God damn it, I have been correcting people on this for years. Why has this not stuck? Please, please, if you're listening and you're one of the three, please tell your friends, tell your neighbors, tell everyone you work with that agile and Scrum are about being able to adapt. Iteration is a method, it is not the goal. People are so focused, people get so focused on iteration as, look, we're iterating, we have six weeks Brents, we have whatever. It's iteration is not the goal. We iterate in order to be able to adapt. If we're not adapting, you're doing it wrong. I am mindless agile robot, I must iterate. Gah! But now therein lies the opportunity for modern testing. There is value in using customer data, customer telemetry. I am a huge proponent of if you can do this, do the minimum amount of testing before you ship. Cover the catastrophic scenarios, other security issues. As long as you want to ensure that you can get the learning, like you have the right analytics in place, you have the right monitoring in place. If you do that, yeah, do the minimum amount of testing, get the value. If you have a telemetry stack, guess what? There is no one who tests your product better than your customer, get it out. I fully endorse this comment from Brent Jensen. And now, where modern testing comes into play and where the pursuit of quality, that's what I'm gonna call it right now. It's the pursuit of quality. Quality is a problem solved for the customer. So when we ship a small release, I think it's the modern testers job to go and look and say, which of these things are working? Which of these things aren't working? Where are we seeing usage? What are the bugs? How do we fix that? Right, so in this case, I'd love to see the modern testing community looking at this telemetry with the intent of maximizing the reduction of actual customer paint, not theoretical. Again, you still have to validate the catastrophic. Still a bad idea to ship a software with a bug that could cause data loss. Absolutely. Still a bad idea to ship a product that has security issues, that will expose credit card numbers. These are still very bad ideas. But the number of cases that fall into this versus the number of cases that traditionally fall in a traditional test teams suite is tiny. Now, one thing I also wanted to bring up because we were talking about what's broken. And we kind of hinted on it, but the other thing that's broken about the traditional approach is not only is it a cost, but it's a delay. So let's talk about that. I was thinking about this from, not just that point in general. I was thinking about what would it be like if the principle is to, is that our priority is improving the business. What would it be like if our job was to unimprove the business or make the business worse? And oddly, the answers I came up with line up with some traditional premises. Let's say for example, if- Bug bashes. We hire a hundred people to do nothing but file a bunch of bugs that then the dev team has to spend their entire team to do the triage. There's a good example. Maybe. And especially if those bugs aren't bugs. There are bugs that customers care about. And the customers will find the latter case. How many test teams have you been on? And maybe fewer of these than Microsoft. I'm guessing the answer is going to be all of them. Where the test team is the gatekeeper and they have to sign off on the product, hence have the ability and power to delay the product from shipping. Saying no, it's not ready yet. So rather than get things to them quicker to get learning, we say, no, it can't ship because we want to fix these bugs. Actually, I was wrong. In the end- Very few in my actual experience, right? Because the biggest problem in my experience that I had was no, we were accountable for signing off but we were not given the power to extend the date. The ship date was given to us in all cases. Right. We can also, another thing we can do rather than accelerate the team, we can make the team resist change. Which, going back to our earlier discussion from before the modern testing principle- Based on what you just discussed at SIRIES, it seems like that's already covered. If you dig in your heels and say, nope, this is the way we're doing it, no need to change, you are actually costing the company money. If you are unwilling to change and unwilling to advance. Because- Let me give you a concrete example. You can't, what the hell? You can't teach dev how to test. We should resist that. All right? Now explain how it's costing the business. The back and forth between, from bugs, from basic bugs going from developers to testers. When the developer gives us this code and it just doesn't work, I have to give it back, we go back and forth. It's a delay, it takes time and time and time. If we work together, and I can help them learn to test, I'll pair with them, I'll help them out. But if the developers are writing some short tests, they run on every check-in or every change, they can get a lot higher quality code to me in the first place, so I can be better at finding the bugs that matter the most to customers. That will accelerate our ability to get higher quality code to our customers. I have positive news on this one. Do you have kids that have gone through a programming class yet? Both my kids have learned how to write games in Unity. Okay, but had they gone through formal training? No. Okay. So one of the things that I have observed is actually, so my son's in AP Computer Science right now in high school. This is a concrete example. He has to write unit tests. He's graded on it as part of it. That is wonderful. A lot of the millennials coming out of school are very similar, right? So I actually, I think- The new world. I think unit testing and TDD. So on that particular front, that particular comment that you heard is series, I think it's gonna rapidly decelerate. And when these blind people who aren't paying attention to how the world has changed, when they finally die off, this won't be a problem for society. So let me, we're just about out of time here. I wanna see if I can summarize what it means to improve the business. I think in the short term, we accelerate the achievable quality. The goal is to get value to customers frequently. But in the longer term, as we'll explore in the coming weeks, it's making sure the team follows the remaining modern testing principles. That's a good summary. I would also wanna call out, we believe, and hopefully we articulated it. We'll see what the feedback is. But we actually believe that the traditional testing approach at this point in time harms the business. It harms the business by creating delays. It harms the business by creating costs. And that's, maybe it ends up being a quick litmus test, at least maybe unmeasurable, but ask yourself, is the work I'm doing, am I adding cost to the business or value to the business? Is that, that could be a litmus test for being a modern tester. Or is my team following modern testing principles? And it is far too theoretical. I said unmeasurable, yeah. Yeah, and they've, the traditional testing has refused to measure not only risk, but ROI on their efforts. Absolutely. And then dysfunctionally, we complain that no one understands the ROI of test. Because you haven't explained it. And I think actually that's part of the problem with the traditional approach. Yeah, and that's why a lot of companies see it as just a cost you pay at the end. It's the cost of shipping. I have to pay X amount of money at the end of my product cycle to get, I won't even call it achievable quality, A level of quality. Yes, so first litmus test from principle number one, if you wanna call yourself a modern tester, you need to be able to concretely articulate either the current value or the future value of your efforts to the business. Yeah. If you cannot do that, you have not yet passed the test for principle number one. Okay, all right. This was fun. This was a lot of fun. I can't wait until the next time we'll do the second principle. Again, you can read those at moderntesting.org. But I had a good time today, Brandt, how about you? I did too. I got you to get pretty frustrated and yell, that was awesome. Yeah. All right, Brandt's sad now, but hopefully have a good day. Thank you very much for listening. I'm Alan. I'm Brandt. We'll see you next time. Bye. 
