Hey everybody, I'm Alan and Brent won't get his butt in his seat. Oh, oh, wait, Brent's actually going to join us for this podcast. Hey, Brent, how's it going? And I'm Brent. Yeah, what the hell? It's episode 23. Right. It is. Hey, don't forget to talk in the microphone. Twenty three. Michael Jordan's number, right? Yeah, it's fine. I can. God dang, you're a doofus. How about you talk in the microphone? And hello, here's the microphone. And then you'll be bigger and I will level it later. Because I always have to because you seem to want to wander to and fro from the microphone, even though you're just sitting in a chair. All right. Episode 23. Michael Jordan's number. More importantly, David Beckham's number at Real Madrid. I don't remember what number he played with at Galaxy, but I always remember. Beckham is 23 also. I'm sure there are some other ones. Twenty three. Can you believe 23? And not once have I punched you in the face. So one of the consequences that we now have, as as Alan has mentioned a couple of episodes ago, that we changed the time of the day. And so now it's now I think the second or third time. But you walk in and you're not prepared. Yeah. Hey, you know what? We're not going to hash this. We have this shut up. You know, I'll be on time when we start to win in my building. You know what? We're going to have a discussion about that in just a minute. Live. No, well, not live. You know what it means. We probably will. We're going to get there. So you didn't I say to the board to do one thing, which was to write the order up there. And yet I see no numbers next to our list. So I'm going to make it up. I am becoming the product owner and I'm going to take over. And if it's wrong, doesn't matter. I'm in control top to bottom. No, because number one on the list has to go third because it's not as exciting. I don't think. OK. All right. I'm product owner. You shut up. Hey, guess what tomorrow or today? If if I actually get this done when I want to and get this podcast out on Friday, June 5th, it will be exactly 20 years since my very first day at Microsoft. Congratulations. Twenty. How did this happen? Oh, my God. Twenty years. What do you mean? How did this happen? Twenty awesome. Twenty is down roller coaster years. Some people could have those easy lives. You have been a part of making history and helping to change the company. You know what? And you've experienced the roller coaster. Shut up. Great. Shut up. Yeah, it's been a lot of fun. I never thought I would be here this long. And guess what else? You don't have to get going to be here this long. This is no. This is where. Brit, this is where we blow it because we already have a list of what we're going to talk about. So asking questions like guess what else? Probably not the best way. Wait, wait. Let me try. Oh, Alan. I would like you to start wearing pants to the podcast recordings. That's not part of the. All right. We don't really have a contract. So I guess it's anything goes. Hey, 20 years and guess what? After thinking about it for a while and keeping my eyes open, I have a new job. I'll share a little bit at Microsoft. I I found this job, which is that I've told Brent about it a little bit so he can't act too interested. But it's I think uniquely suited to me, which is and some things we talked about, which is really cool. So I want to dive into that a little bit. There's a Brent and I have talked about the efficiencies that come with you to find engineering, how you need generalizing specialists, specializing generalists, especially people who really get quality. I shouldn't butter myself up so much. I found a team making a V1 product around collaboration. Tools. I'll just I'll stop there. They have about 100 developers, unified engineering team, and they want someone to come in and be the quality expert. And no, this does not mean I'm writing all other tests for them. Yeah. Because then I would just shoot myself. I mean, I actually enjoyed seeing people attempt that. Not going to happen. Not going to happen. Because the problem is, is that the people who want you to do it outnumber you. No, it's everything from, you know, Atlassian has this I read a couple of blog posts from Atlassian a few years ago where they called QA quality assistance and talked about the sort of role we've talked about where, you know, coaching and mentoring developers are running good unit tests, good code hygiene, et cetera. I'm really big and anal into code craftsmanship. And I believe in broken window theory and keeping everything kind of clean. And I don't know broken window theory. Yeah, yeah, sure. You do. Broken window theory. If there's like an apartment building, this is I first read about it in the pragmatic program, but I'm sure it's been around for a long time. In fact, actually, I first read about it in the context of software development and the pragmatic programmer. Great freaking book. But the idea is that if there's a building like in a rundown part of the neighborhood and one of the windows gets broken out, if it gets fixed right away, it's going to stay nicer longer. If you don't fix it, people are going to start breaking other windows. And the metaphor for code is if it's like, why should I make this part? Why should I clean this up? It's already kind of grody over here. So the idea is in code and I'm really big into make everything right. One of the things I noticed is like, hey, you know what? We're not very consistent. We're not the new team, which I haven't quite joined yet. But I spent some time over there isn't really consistent on comments on code chickens. I thought, let's fix it up right away because I don't want to see, you know, I want to know it's I want to be able to know what's going on by browsing history. And I think what happened there is a classic example that you mean specifically, you mean not people commenting the code, but actually people commenting the check. Yes. Yeah. I actually get value from browsing source history and I get I get context for what's going on there. And that's not accurate. I don't want to dive into the whole why I like it and what it's good for. But the point is, I think it's gotten to a stage. This is a quick observation after two days on the team that because nobody else does it, no need for me to do it. And these things apply everywhere. Anyway, you've already started kind of ish. So my official start date because of Microsoft and the weird things with our compensation model is in order for me to get paid by my old team for the last year of work I did. I can't I can't officially move until June 15th with one of the model locks. So don't give me that word. Look, I got I got it all checked down. They charge for check this. OK, yes. I'm I'm spending probably half to three quarters of my time between now and then doing what we should do all the time at Microsoft when we change jobs, ramping down on one while ramping up on the other. Yep. So anyway, I'm going to back out of made a point. Part of the job is just kind of being big on code quality and code correctness. I do believe that quality and care are two sides of the same coin. So I'm going to help people put more care into code and check ins and unit tests, et cetera, and do all the right things at the same time. Also, I'm going to look big picture at when there are people doing perf work, but I'm going to make sure are we doing the right perf work? Are we doing the right reliability work? I'll have some vendors doing some exploratory testing. I've got a chance to teach some E.T. for a while. How are you? I've been there a day and a half. Don't ask anything too specific. I'm making part of this up. You're you're you're talking about a straight strategy that you're going in with. When you say doing the right work, what's your primary pivot for that? It's the much of the discussion you talk about is that you you're going in as a test centric or quality. Now, quality centric. Definitely more than the correctness sort of thing. I think code correctness is because I have a passion around that and I can for a team that hasn't done unit tests before. Very well, very much unit testing. And again, somebody I in my intro mail to the team, I mentioned this podcast in hopes of picking up two more listeners. The point is, is there's things that can do better and I can help them do better in that area. I don't think my new manager that's even on his radar of what he wants me to do, but I'll do it because I think it's important and I think it will help. I believe I'll draw I'll draw this with my hands for now, which is useless for people out in the audience. But I've been thinking of this sort of pyramid of software quality where this long base at the bottom, I'm going to do it without with my hands to see if Brent can understand it without the visuals. So the measure at the base of this triangle or pyramid is sort of the functional correctness, which is where a lot of testing when I started 20 years ago, for example, where I spent a lot of my time at the base of this triangle up a level from there, you have maybe the illities belong there, the non-functional, you know, whatever you want to call it, NFI, performance, reliability, security, making sure you do the right stuff. But you can actually run a lot of perf tests and run a lot of reliability tests, et cetera, and do a lot of work there without actually making software that's useful and has value for the customer. Right. So that that top of that pyramid or the top of that triangle is the things I'll probably spend the most of my time, which is in that work, I'll look at is this useful? Is it solving the right problems? Even if the perf tests are passing, is perceived performance for the customer good exploratory testing will fit in here. I'll definitely ask people doing that. And this is where also data analysis as well, both from a qualitative and a quantitative aspect will come into play. Are you officially stating that you are entering into the data analyst scheme? I am officially stating that looking at because this is a web app that will eventually be used by large numbers of people, that knowing how to do effective data analysis and helping the team adapt to the insights from that data will be part of my job. And freaking testing. So anyway, I'm pretty excited that the breadth of the role was much better for me than some of the more in-depth things I've been doing recently. I don't think I'll say this in my blog, but I'll put it in the podcast. But I have said before that working on my two years in Xbox was the best five years of my career. Fantastic time. Oh, my God, that I work hard. My two years in part of the operating systems group at Microsoft were Project Astoria, cool project buried under windows somehow. But my two years in the operating systems group have probably been the longest five years of my career. The direct manager I work for the team I work with. Absolutely fantastic. As we build concentric circles out from there, it's like just not. It doesn't align with my passions, what I'm interested in, the way I like to work. But I didn't want to just go to a job to get out. I wanted to find something that really like that was the right job for me. So I took a long time to kind of what I call passively look and last two things and. And if you don't mind, I'll go ahead and tell a quick story how I got this job. Yeah, I was passively. I just won a Saturday. It was a Saturday. But I'm just going to look at the internal job site and take a look and see what's around. And and on a whim, I had emailed Brent like a week before I said, hey, I'm thinking of maybe being a manager. He was like, I'd be a fine manager. I'm thinking, oh, my God, I want to be. So anyway, I thought I'd look at manager jobs and I was flipping through. It's like, no, no, hell no. And then I see this one which sounds like the hell no. I don't. OK, it was probably one of my current group. I don't know. Yeah, current division. But I saw one. It was interesting. It was it was a lot about quality. And it's hard. One thing about Microsoft now, if I shift to unified engineering, a lot of teams, it's difficult to figure out who has which teams, which job descriptions I could kind of leverage my passion for quality and experience into. But anyway, this one, I read it. And it's not like they want someone with a lot of quality experience. And I didn't see much about actually managing a team in there. I thought, well, this is interesting. I'm not sure what they really want. It was manager. Yes, under manager jobs. And it was a Saturday. I sent some mail. I got a reply that day. Not that people should be working on Saturdays all the time. It just happened to. We set up an appointment for the following Thursday. So several days from then in. It turns out they wanted he put it as a test manager job because he had an experience with the really good test managers of the past or that kind of big picture thinking about quality, et cetera, et cetera. But he didn't mention much about the team in there because he didn't want a testing. He wanted a quality guy who didn't want didn't care about having a big team. I thought, well, hell, that sounds like me. So I talked to one of their dev managers on Friday. That was in the three day weekend came up. And then the following Tuesday, I talked to three more people and I had an offer. Wednesday or Thursday. I recently we talked about this couple of several podcasts ago about my job change. And we spent a lot of time talking about my experience of the new interview process. Do you have a similar? Yeah, this is really interesting because I've given a lot of feedback to our team on. Let's get out of old school interviews. And and it was great. The way the interview loop went, it wasn't a loop set up by HR. The guidance I got from my upcoming new manager was set up one on ones with these people. And so I did. I took care of setting up some time on their schedules. And and I met with everybody and they asked. This is weird. They asked good questions that I felt like both allowed them to understand what I brought to the table that would help their team and help me learn about their team. And there wasn't a lot of like, why are you asking me how many golf balls could fit in a bus or to reverse a linked list or whatever? And whole coverage. We actually we actually had conversations about crap that was relevant. It was refreshing as hell. You've gone through several other type of experiences. Were those more old school? So one of the things that you guys don't know is that this is something Alan and I have been talking about for, I don't know, months. It feels like right. Alan has been as he was was doing he. Alan's a capitalist. Yeah, he is that good or bad? It's good. All right. Great. I'm yeah, I'll let it be was passively. He's always interested in exploring a better offer. And one pattern that he's talked about here on the podcast before is he kind of has a preference to sort of change things up for himself over a couple of years. It's certainly come to that. I've changed even when I've changed jobs in the same group, it's been about every two years and he used to tease me about it. I said, no, it's not like that. And it's not like I it's weird. It's not like I change the jobs on purpose. Like two years I'm getting a new job, but I'm always looking for the new challenge. And it seems like I find the next cool thing about every two years. I actually think it's just how the world runs now. It could be. You know, I spent just over four years in engineering excellence, but I changed jobs partway through. I did two years as an IC, then two years as the director over there. And then I spent 18 months in in link, a little shorter on my on my spin. Two years, two years in Xbox. And then I just followed my manager over to Project Astoria. Two years there. And I don't know, maybe two years on this team. I don't know. I when I went to my last team, I was fairly certain I was going to be there five years plus. But I only did just over two years. Stuff happens. So anyway, lots more will come up from that. There's I'm excited about all the sort of the stuff that's in my core. I get to start doing some new things. I get to start trying. I think it'll be a good time. Yeah, I'm looking forward to. Oh, hopefully I'll be part of your communication chain. You start getting into the. Yeah, you'll have to be. You'll have to be. And let me plant a seed. We're not going to hash this out now and bore that and and further bore our listeners. But one thing about this job, it's in downtown Bellevue. So I'm moving about five miles away from branch. So we got to figure out where and when to get a decent recording in and maybe, you know, and how to actually meet. I am not going to Bellevue. Hell, you're not at 5 p.m. That's true. I'm just making that super good. The all right. So I spent time working in Bellevue and and no, I understand completely. So maybe I was going to say, I was going to say working in in in the Bellevue branch of the Bellevue area for the Microsoft stuff. Best work location I have ever done. And if nothing else, I am indeed incredibly envious of you for just that. I loved working in Bellevue that much. But going from Redmond to Bellevue, not cool, not cool. We'll go back to mornings. Maybe we'll switch and do like one in. We'll figure something out. But you and I can figure that out. It's closer to your home now, isn't it? Yeah, a little bit with traffic. It's about the same commute. We'll figure it out over time. Anyway, yeah. All right. Be fun. 20th floor. Oh, 20th. Yeah, that's cool. Yeah, good view up there. Should be great. All right. Oh, I wrote A.B. testing up there, but for that to tell you what it was about. We're changing subjects, by the way. OK, we're moving on. Is this A.B. testing the blog or is that a slash? That's I put a slash there. Wow. Because guess what? Now we're getting contextual. Do you remember the time? Double entendre. Do you remember that one time where I said I went in bank camp? No, sorry. It's a movie quote where I said I wasn't going to do as many conferences for a while. Yes. Well, I got a call. OK. I got an email. Nobody nobody gets calls anymore. You get an email or a text. But inviting like a speaker dropped out of the Better Software Conference next week. And I'm going to go down and give a talk on A.B. testing. Are you? OK, I think. And are you yet an expert on this topic? Oh, actually, I would say I definitely know more than the layperson. And I thought I haven't finished, you know, it seems like it'll be some good research to help prep you. No, no, no. I'm actually I feel pretty good about what I have. I'm giving a talk about inside the mind of the 21st century customer at Star Canada. And I stole some of that. But it kind of turns out, I think I kind of want to give half the talk on houses for an A.B. testing talk. I should give the talk twice. No, this is not A.B. This is like a then I don't know how you describe this. But I think I'm going to give half the talk on how awesome A.B. testing is and the insights you can get, how much you can learn from it. And the second half of the talk on how A.B. testing is impossible to get right. How you should never do it and trust the results. And then the A.B. test, maybe we'll see what their takeaway is. How long do you have an hour? It's an hour. Yeah, that's it'll be interesting to see if people get it. What you're doing. I'm not sure I'm going to do it that way yet, but there is some emphasis. And I can't tell I have the abstract from the person who was originally scheduled. So I can't really tell what they were going to do. But I think it's really important when you talk about A.B. testing to go into statistical significance and figure out what your sample size has to be to make sure you can really understand and believe and talk about confidence levels and and Z scores, et cetera, to be able to figure that stuff out. Wow. I know shit, man. I'm impressed. Hold up the Z score. Boom. I don't have a memorize. I have to use the little chart thingy. Do you know P stat? I don't P stat. OK, because that needs to be along. Yeah, that belongs in the equation. You're correct. The one thing that's really critical when it comes to A.B. testing, crafting your experiment is hard. Yeah. And that's part of the second part of that second half. Yeah. You know, your designers are going to come to you and say, all right, what we want to do is change the background to a gradient fill, reduce the banner and add one more icon to the top row. And that'll be our experiment. And your your job is the as the scientists go. That is not an experiment. Oh, that's an experiment. Well, yeah. A.B. No, that's ABCD and Q. I know it's a slash BCD. Right. And yeah, it's really hard. You really want to. And we'll talk about the null hypothesis and making sure that you like really what you want to do, and this is this is true in a lot of analysis. If you want to prove that I mean, if you go in and look at the data with the idea that it's a lot of opportunity for confirmation bias. If you go and look at the data and go, I want this to show that this treatment is better and and maybe if you look and you'll find it a lot of times. But if you go and I need to prove that there is no difference between these. No, when you can't prove that, that's when you actually maybe have some good interest. So all those points I'm going to bring up, I'm not an expert. Biases is is a brilliant topic for that. Yeah. Last bit. So it was a yeah, go on. And there's another story that I'm reminded of. There's another bias I'm trying to selection bias. Mm hmm. Have you ever heard the story about the World War Two airplanes? I heard that airplanes. They did go on. And because even if I told you I heard it, it probably important at this point for you to tell it anyway. Fair enough. Then I shall attempt to proceed to do that. Great. Yeah, I heard that story. OK, moving on. We're going to tell me about the airplanes. Thus airplane story. Now, so what happened at the NATO Air Forces? What they what they were finding is that the Germany was shooting their planes out of the sky because far too many planes kept on coming back are far too few planes kept on coming back. I've heard this. I do know this story. It's good, though. And at this point, you kind of have to continue. So continue. Yeah, I like it. Well, so do you want me to give away the ending ending? Spoiler alert. I'll shut up. So what they did is they analyzed the bullet patterns of the planes that did came back. Did came back. They did manage to return. Did have maybe came back ish. Go on. I'm sorry. Planes that returned. They analyzed the bullet patterns of those planes. OK. And they reinforced the armor in those sections and they they noticed that it was having no impact on the rate of decline. I'm not going to say it. Yeah. Go on. I don't remember the name. Do you remember the name of the guy? No. So there was one guy. You might have told this story on the podcast before too, but go on. There was one guy that if I had prepared for it today, I would have his name. We don't prepare that basically analyze what they are. Are read through what they're doing and basically did a head slap. You guys are being idiots. I love saying that. Because the planes you need to analyze are not the ones that returned, but the ones that didn't. Yeah. That was my head slap. Yeah. And there was a there was a recent adventure here locally where something very similar happened. A team, a partner team, very much like these guys, and they're getting into the data science base and real data science stuff. And they they've actually several of the seminars. They've talked about the work that they've done. I haven't seen the latest results. I got I got to them before they started doing the initial ones, but they were trying to do churn prediction. OK, and I said, well, that's fantastic. What are you defining as churn? Well, when they stop logging in. OK. Now, one of the discussions when you talk about churn prediction, you talk about these statistics known as precision and recall. OK, and I'm not going to go into it now, but the important one is precision. And that's basically how well the algorithm predicts the truth. Correct. OK, so you apply it to the old test data and then it tells you how good you did. OK. And they had a 70 percent precision on their algorithm. I'm like, that's fantastic. What features do you use? Now, for those who aren't data scientists, features in this context don't mean product features, but it's essentially a fancy way of saying what are the data points you're looking at? OK, and he's like, oh, that's easy. We just looked at logins. And I'm like, OK, so you're using logins to predict logins. So let me guess what your churn prediction has determined when people stop logging in, they're going to churn. He's like, yes. I'm like, that's because that's exactly what you're measuring. That needs a little bit more than a head slap. Yeah. And I'm like, you need to go up a stack and determine what are the actions other than login that are likely to predict that the next session is they're not going to log in. And so they have enhanced it. They're still heavily using login, but they're now pivoting their their turn prediction more around frequency and recency of logins. So they've pivoted more towards a time series analysis, which makes it a bit more valuable. Anyway, I'm just like, well, of course. But then I asked the important question, wait a minute. How is your precision only 70 percent? And they don't know the answer. They didn't know the answer to that one. Right. And which implies that people make a decision actually to to not come back. But they're still following a normal login pattern. Very interesting. That both confirmation bias and selection bias are very seductive. That they're there. You want to be right. And the I tell people if you're going to be in a data science space, especially being right is what's important. Credibility, the credibility of your data is what's important. And even one instance of being wrong hurts you. Yeah, a lot. This is why communication communicating things in terms of confidence intervals and and going through a structured way of designing your experiment and getting it reviewed with the intent of telling people, I need to make sure I am explicitly filtering out confirmation and selection bias. This is why I think people rather than what I've seen done on some teams where let's take all these people that used to do this thing and make them data scientists is let's just let the people who know what the hell they're doing or are committed to learning what the hell to do are doing this. I think there's this gap between data science and data guessing. That I want to that just it scares the crap out of me. Yeah, I don't see a whole lot of data guessing, but if I did, I bet you it would be wrapped under the shroud of what I do see, which is data puke reporting. Yeah, as you see data puke. But but for data guessing, I'm talking about the decisions made with selection bias, confirmation bias. Like I've I've I've I've I have some data and I made a completely wrong choice about it because I didn't look at it objectively or apply some principles around confidence, confidence levels, etc. There's a couple of instance instances where I did. So it's what's known as EDA. Have I used that acronym before? No, but you've certainly used a lot of weird acronyms. EDA is exploratory data analysis. Oh, yeah. If we were in dev, it would be XDA. And that's probably what caused confusion. Long story, not going to repeat it. Yeah. But it's a good one. You should go back to that podcast. Go search. Hey, be testing for. Hey, Brent. STFU. Yeah, I can guess what that means. I'm moving on. It's important to do exploratory data analysis. And I did I did something very recently. Like I can't talk about it here, but I did something very recently. And I have found what appears to be a significant customer, not only a significant, but if the data holds true, it would be the single most significant customer segment for the team that I'm on right now that no one knew about or it would end up being the single most important customer segment. But no one talks about it as if no one knows about it. And it's primarily because how they're looking at the data. They're not able to observe this. Now, so when I walk people through the story and I show them the data, I make it very clear. Now I'm very excited about this and I'm very happy to share it with you, but you cannot go off and explain it as truth because I have not gone through yet and validated that this still holds true against the entire population. I have a, I have a small sample set. It holds true here. I now need to test it against the whole sample set to make sure it still holds true and if it does, then I'm going to start going through and understanding a little bit more about this customer base because again, no one knows about it. Would the, um, in this case, would the null hypothesis be to prove that this customer segment has, is not important as compared to the others? The whole, the null hypothesis would be, let's say my, the distribution that I've drawn together, the distribution of this customer segment. Uh, in essence is not important, is not significant. Right. Right. I think I kind of said that. You did. All right. Great. I was briefly trying to think through the more data science-y way of doing it, but I failed to do it. It wouldn't time a lot of it. Hey, guess what? Yeah. Guess what? Red. Oh, so close. The answer is mailbag. Oh, it feels good. What has that been two months? It's been a while since we've done a mailbag. Yeah. All right. We don't really have one. I just wanted to say that. I'm kidding. I'm kidding. We do. We do. We do. Um, a couple of things. Always appreciate the comments, always appreciate the corrections, always appreciate the snarkiness. Um, one quick, or the help, like, uh, do you remember episode 22? Yeah. Um, where we, Oh, yes, there it is. Yeah. I did Brent. I had no idea how to say 22 in German. Brent gave it a shot. And fortunately our former colleague Ron Pilgrim corrected. And I'm not even going to try and say it because my German is horrible. Of course. So now we know that. Yeah. Um, Hey, and then Matt helped us out. We were talking about last time are, you know, people finding episodes. We're doing a little bit of exploratory analysis on our downloads, et cetera. And then sometimes, uh, there's a face slap comment, like, uh, your RSS feed only shows the last 10 episodes and nobody knows the other ones exist. So I changed the RSS feed to show all the episodes and we'll see if that makes a difference, but anyway, great advice. Was he just saying, Hey, this would add value to his life or was he saying, he said, or was this a hypothesis for all the bad things we did? It was a hypothesis. Look, I started listening recently and I can only see back 10 episodes. I didn't know the other ones. You even have posts anywhere. And I said, I'm using up all kinds of storage on my server. Might as well show them all. So we'll see if it makes a difference. He said, he started listening the most recent and then worked backwards and it didn't look like one was available. Yeah. But if there's only 10, the last 10, but we see one, two, three, four in the middle, some five is the bottom. I'm not saying everybody found them, but look, it doesn't hurt. No, Matt, you're fantastic. Thank you. Love it. Yeah. And you can tell Brent to STFU anytime you want. And then, uh, there was a, a thread on Twitter that, uh, brand wanted to talk about if I could find it. Yeah. I had made, uh, and I'm going to use this in one of my talks. I forget which one, um, but we've all, many of us in testing. I don't like to generalize have, uh, adopted the Jerry Weinberg quote around quality where quality is value to some person. Actually it's. So you quoted him. That is, well, no, no, you did it. I get it. Would you? I am. Go ahead. All right. Brent, I'm muting you. So, and I believe that quality is value to some person that makes sense. And there's all kinds of context around that, et cetera. So I was thinking about the who owns quality question. And, and then I thought I would twist that with that and make it my little recipe and then Brent liked it. And then discussion happened. So if we were to say a quality is value to some person and some person is the customer or the user of the software. And, uh, can we ask who owns creating value for the customer quality? So quality is value to some person who creates value for the customer. The answer is the same. It's still the whole team, but it sparked some interesting, uh, discussion. Well, no, the answer, if you grow up in tests, it's the whole team. Right. If you grew up in dev, it's a fundamentally different answer. I guess that's why I loved your, your value add to Weinberg's quote. It's fantastic. It's essentially a, but I'm not going to go into math, but I can't tell you the number of times I have had the conversation around who owns quality, or it wasn't the, a question mark. It was an exclamation point and there was a finger shoved in my nose. Right. The, the, because quality from a certain standpoint, particularly this is why we were talking about your job, right? Cause I have now gotten to quality is only the, from the customer point of view. Absolutely. And, and when, so when we talk about, we used to talk about years ago, then, um, we would talk about engineering quality or code quality. Okay. I don't use those terms in that context anymore. If it's code quality, the term I use now is code correctness. Quality is only the customer subject to point of view. Now I'm taking back the word. Ah, interesting. Yeah, I like that. That makes sense. Um, the, Oh, by the way, I don't own quality on my new team. So just in case you were wondering that are good. Well, so one of the, I'm the quality guy, but I'm just to make sure that people understand what it takes to, to create value for the customer. So Alan, it was actually interesting cause I'm, I am much slower on Twitter than Alan is Alan had actually put a period after, after slower. Fair enough. Or Alan, I am much slower than Alan. Period. Also fair enough. Um, so he had posted that like a week before I retweeted it, but me, when I retweeted it, it created a tweet storm. And one of the things that it ended up at one point in time, it was starting to become a battle of cliche. Right. Because the obvious answer is who, well, whoever owns producing value to the customer, therefore is the one who owns quality and the answer is the whole team. And then there's one of my least fricking favorite cliches on the planet. Well, if everybody owns it, nobody does. And I'm like, no, everyone owns it and they need to be living and breathing as if that is the only thing that matters. And yes. Anyway, I thought that was extremely interesting that the thread did go off and am morphed into a data science topic where they talk about the D I K W. Which is actually an old concept. I don't, I don't like the D I K W. Uh, would you please decipher? Uh, it's, it's how to shit or data flows, how data flows. Yes, exactly. So D I K W is data information, knowledge and wisdom. See that silent K that's that, that just, it makes, it makes it not work. The silent K in knowledge. Sure. If it's going to be silent and knowledge, we should just call it D I W. Actually. So it's, it was interesting because literally yesterday, uh, totally different contexts, I was having this discussion with another person who was familiar with the D I K W pyramid. The, the current practice is not to have the W it's D I K A. And what's a action. Oh, that the, the point of any sort of data process. So I don't know how they define it here. Well, they define wisdom is collective application of knowledge in action. Yeah. Um, let me, let me read this out loud. Data is decisive objective facts about the event information, a message meant to change the receiver's perception. Yeah. So that's different knowledge, experience, values, context, supply to a message. And wisdom, collective application of knowledge in action. Yeah. So I like the word action better because the word wisdom is hiding actually what you're trying to do. You're trying to, to change a direction. You're trying to change someone's behavior. Um, and then the way they frame information is, is weird to me as well. The data is just raw facts, no inherent meaning. Whereas information is that data. Contextualize semantic meaning. You have data puke data, a bunch of data. You have some insight into that. Yep. Or, or some ways of organizing that data around, uh, uh, business knowledge, et cetera, that allows you to get some insight and from that you can take some action to paraphrase the pyramid. The, the way I describe, uh, it's not knowledge acquisition and action. The way I describe information versus knowledge is essentially you've heard the term street smart versus book smart. Mm-hmm. Okay. Knowledge is street smart. Information is book smart. Yeah. So you know a lot of things, but, um, you don't need to know a lot of things. You, but you, the things that you do know, you need to know how they can be used. Knowledge is about the few things you do know. You know how to achieve a goal just because you know how doesn't mean you do, and that's where the action is critical. Now the nice thing, the nice thing that they put in there in wisdom is they're actually trying to say, they're trying to combine sort of collaboration and knowledge sharing along with actionability and that's valuable. But I think when people read this nowadays, they go, oh, this sounds like BS, the soapbox philosophy crap. How is it valuable to my business? Whereas if you frame it as the whole goal of a data science team is to, is to understand relationships between variables for the express purpose of accelerating decision-making or actions, whether it be by a human or by an automaton. Gotcha. Yeah. Anyway, we, we, we, we wandered a bit there, but I, one thing, just to bring that back is one thing I do believe that shocking on AB testing. I know. We wandered. So to bring that back to a point I just remembered I'm making in one of my, I think my keynote at star is I believe that this sort of this pyramid or, or whatever metaphor you want to use for using customer data to gain knowledge and take action is sort of reconnecting. I'm not saying sort of, I'm saying this done, right. This is how we reconnect the tester or reconnect the engineer with the customer. So then we lost a little bit through, you know, what we used to do with Explorer charter testing and, and, and the STE. I don't think we did. Uh, maybe we in general, didn't I think, I think in some places it was lost. Cause it was all about, it was, it was all about functional correctness or code correctness, as you call it, and less about usefulness and value for the customer. And I believe we can get that through data. I, I wrote a blog several years ago. Back when you used to blog. Yes. I was about to accuse you of that, but suddenly you started posting blogs again. You went for like four months without anything, but then I realized I had to get a blog out before I can tell you now that I draw just about everything I blog about is, uh, drawn from my work or from things I see going on in the world. I haven't seen a lot, nothing in honestly, no offense to the listeners. There isn't anything that I haven't said that I look at that, Oh, hum, ho, hum, ho, hum. So I draw from my job. My job wasn't that the things that I thought the things I could write about, do you want to know how to, um, really cool crap about pulling a build definition from Jason and then pushing it to a TFS through PowerShell. Not probably for my typical right list readers, not the coolest thing. So now that I'm kind of moving towards something else, I'm getting a little bit more inspired, et cetera, I'll find some cool things to talk about. Um, I expect, anyway, does you have anything else important to say? I did, but, but, but I was actively listening to you and it, it, it disappeared. Hey, guess what? Yeah. Uh, I think it's at a time. Yep. We're out of time. Someone's at a time. Oh, I do remember. Hold on. Hold. Oh, we're not out of time. No, we're not out of time. So in that blog, that's how, that's how we got into weeds. So there's a blog on my site. Worst podcast ever. Yeah. No. Not going into weeds again. Uh, titled test doesn't know the customer. Okay. And, and it talks about an experience I went through with one of my mentors a while ago, where for those who care, go to my blog test doesn't know the customer, read it, you'll see. But one of the conclusions that I came to at the end of it, spoiler, um, is test didn't know the customer and actually never did. I agree completely. One of the things I bring up a lot is, yeah, I, it kind of pushes my buttons. When I see the, uh, um, I'm the customer. I said, no, you're not, you know, who's the customer, you know, the customer is, you know, who the customer is the actual customer, not you. So I never heard that. What I heard is I'm the customer advocate. I'm the customer's champion and it, you know, bringing up visions of, of a gladiator and I'm going to defend the customer's honor. And meanwhile, the customer was like, what are you doing? I don't get a crap about that. Exactly. None of that matters to me. I'm reacting to your comment of reconnecting tests, which I think in your audience is probably the right way to phrase it, but here we like to speak the truth on AB testing and it's really actually connect test with the customer. Not re not really. All right. Well done. Okay. Guess who I'm not. Brent, which I am. I'm Alan and, um, thank you for listening to episode 23 of AB testing. And we'll be back to talk to you soon. See you guys. All right. 
