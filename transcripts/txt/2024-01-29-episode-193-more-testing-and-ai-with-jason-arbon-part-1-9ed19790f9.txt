I'll do a bit. I'll bet that both of you are in a QA titled role in 24 months. Welcome to AB testing podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now on with the show. Guess what? It's me again. It's Alan and we're here for episode 193 of the AB testing podcast. We have Brent and we have the return of Jason Arvin. Hey, the return of Jason Arvin. Because last time, last time, and I'll quote myself, Zen Caster shit the bed. And that was the last time then Castro was ever used on this podcast. So far been using Riverside FM so far. Very, very good. Very, very happy. Please don't let me down. That's where we are. Zen Caster saved me though, because I said some things I'm glad they weren't. You know what, Jason, you are clever enough. You could go in, like immediately go hack the Zen Caster server and just remove all that stuff. Maybe maybe I act, maybe I accent castor. Well, it's, it's good to have you here. We're going to do, I think I put it in our prediction show. We're going to do more guests here because one, the guests are more interesting than us. And two, more importantly, I am so tired of Brent. Totally. It's, it's, I mean, it's good to have guests because they bring in a fresh perspective until we figure out the next thing that's important to us to talk about. No, I always have important things to talk about. It's in fact, do you? I am practically incapable of talking about things that aren't important. Okay. I'm happy to be filler. I can't think of the last time I said something that wasn't important. Sure. Yeah. So, so Jason, your team's listening to this one. Is that what's going on? You know what? It's interesting. When I was at previous company, a lot of people listened to my podcast or read my blog. I think, I don't know if anybody listens to my podcast at the new place. They do read because I post that on LinkedIn all the time connected there. And they don't know how much I'm drawing from my daily life to inspire some of the topics. But we'll get more into that later. This is not about me. This is about Jason. And Jason is very interested in fine art and the variance of meatballs in the Italian market. So we're going to go into all of those subjects deeply. Jason, just as a reminder, kind of tell people what you're up to. And then I'll ask on my list of questions that will get us kind of diving into this thing. What I'm up to now, I'm actually taken a year off and focused on, focused on applying AI to software testing and like exclusively and voraciously. And it's been wonderful because the AI robots do what you tell them to do mostly. Yeah. And trying to figure out how to get experimenting and trying to see what works and what doesn't and as quickly as possible. And I think I've narrowed down a few kind of themes, but I've been experimenting for AI for human manual testers, exploratory testers to figure out how I could help them. Also kind of a full automation, like everyone's dream, which is how do you get an AI robot to just, my testing bot to just test the application and APIs without a human involved. So focusing on kind of researching them. Super cool stuff. And I have lots of questions to go in there but I had a conversation. Was it yesterday? I was talking to one of my employees. I was in Los Angeles and we're talking about AI. It's always a weird question to bring up with somebody like, what do you think of AI? Because you never know what people are going to say, right? He didn't know me well enough to know. And I said the usual stuff you've heard me say, but something I've maybe said on the podcast is referring to something Stephen Johnson calls the adjacent possible. Meaning all of a sudden we've made an innovation that enables us to build a whole new world of innovation. And I think generative AI is that. And I don't think many, if any folks, despite all the cool things people have done, have really made that leap that I know is inevitably going to happen. So I'm glad to hear, you know, I think you have a chance to be like, like there's people like this trying to figure out what, what is this adjacent possible innovation we can make. So I want to talk more about that, but I also want to talk more about testers and how AI replaces, enhances, helps, fixes testers. You had an interesting LinkedIn post. Was that today or yesterday? I just saw it today on Jason's looking at me like, I didn't post something like testing. I had about the 737 MAX. I don't think that's what you're talking about. No, no, no. Which one was, hold on, hold on, hold on. Oh, was it the quote that my prediction that people have left the testing field will come back to it? Yes. Yes. I want you to talk more about that. That's interesting. That was just click bait. I don't believe you yet, but convince me. It worked. Next question, Alan. Next. No, I just figured like testers were getting tired of me and they're scared of AI. So I just appealed to them. Yeah. Yeah. That's the comments. This is what I get for skimming articles. I actually think they're not sarcastic. Actually very genuine. I think it's for probably kind of not the reasons that people would expect. I didn't even really outline those. One is, first of all, we were saying a second ago, I think is very right. I think people are like the adjacent stuff, but I think people are not thinking AI first still. They're thinking about how to modify existing processes and software or to create, like use AI to generate Java code instead of actually just having AI run. So I think there's people still haven't kind of gone through that bias yet, which is normal when there's technology transitions. But so when it comes to what I said was I think a predict that there'll be that a lot of testers have left the field as they get further and longer careers. There's a lot of you guys have talked about this before, too. There's also like a salary cap and testing to be frank. And so people left it just for more money, let alone or like we were talking before that we started this, Alan, sometimes people just get sick of testing because it can become mundane or boring or exhausting in its own way. I think one is that there'll be one is there's just the general pressures are coming on. AI is genuinely helping in the software engineering world. People can and Brent's been talking about this too, like where you can just write better code, not just better code, but more code. So it'll be more stuff to be tested. But I also think that it's a little story where I'm a story. Brent's going out great. But when I walked into when I went from Microsoft to Google, I skipped my first week of nuclear training because I thought I was too cool for that. And so I went, but I went to the search building and I just walked around with my new badge because I wanted to see the Holy Land and everyone's title there and this one building that worked on search, everyone's title was a search quality engineer. And so I think, and the reason is, is that they weren't building traditional software. They're building AI and very high scale algorithmic type software. And so really what it is, it's very easy to write some code and then you let 10 or a hundred thousand machines or a million machines process it. The hard problem is testing it. How do you know that it's of high quality? Especially when you think about a search engine, and we'll come back around is that the hard problem with search box is people can type anything they want into it. And it sounds familiar. It sounds a lot like chat GPT. Google and the search story though is still constrained because it only returns things that are on the internet that exist already. The problem with chat GPT is it can return anything, things that either don't exist or things that are made up. And so quality is the hardest problem in software engineering, particularly in AI. So I think that it's not just that the people will come back to testing to do traditional testing. I think it's that a lot of those engineers that left testing to PM or dev will be far more valued because they have that testing context with that engineering or PM background. And they can be monsters and incredibly important when software goes through this transition where it's AI first, because you need a search quality like testing approach for that software. I see where you're coming from. I spent time in Bing. Right. And I need to enter up with the editor. Bing is like Google, you can use it to Google things. It's from Microsoft. Anyway, go on, Brent. I'm just currently pausing and wondering what episode is anyone going to view that horse that is already dead, no longer worth, no longer worth kicking. When Bing is known as a search engine, that's when I'll do it. Okay, hopefully you're paying attention to news because anyway, moving on, not doing tangent. So one of the things that that I learned when going to Bing, it was interesting because this was the same thing. Buds did not matter in Bing, because they were not helpful. Because we have a machine learning algorithm that's constantly learning the best way given the context, how to present answers. And not only that, but we have thousands of streams underneath it that are changing sort of the input to that same model. So that's one of the reasons why you might go to Bing and type in a search query and then hit refresh and get an entirely different experience. And hit refresh again and get an entirely different experience. What ended up being valuable there? Testers that went over and thought, oh, I've just my job is to find bugs. They got frustrated because no one, they all got one fixed. What was valuable is help us diagnose and find the pattern. So to Jason's point of view, I don't know that I agree with specifically testing, but that one important skill that I think underlines testing, which is the ability to structurally diagnose, that will be critical. Pausing for Jason to say I'm full of shit. Alan will always do that. I worked at Bing too, like on the search engine. I don't know if you knew that before I went to, I was there during the MSN search and the transition to Bing, which nobody wanted the logo logo where for some reason. That was before me. Yeah. So I totally get that. I hit that personally went through that transition from a functional kind of testing perspective to dealing with, yeah, bugs were uninteresting. There was a page that we shared internally, I think it was still around, where people could file bugs and you could actually do a search inside of Microsoft to do a search and it would do the search on Bing and Google. And then you could say A or B, which was better. And then you could type in why I don't know if I remember that. I remember that. You know, that was whole. That was still there. That was still there when I left in at least a couple of years afterwards. I haven't, I haven't tried the URL in forever. Yeah. So guess why it was built. It was because all these functional engineers at Microsoft wanted to file a bug. Here's the search. Here's the link. Fix the code. Right. It was just a honey pot so that people would type those things in and we never looked at the data because it was not useful. Like, like to what Brent saying, it was actually just a, a DDoS against people filing bugs. But what Brent said made me think of something. I'm going to add more fuel to the fire here before I forget because I'm old and I forget things is when you talk about how AI is going to help testers and help testing. And then Brent's comment around, I forget what he said, but there's a wide definition of what testing is. And there's a weird thing that Brent and I have talked about here where Brent and I care about quality and the testing we do is in the search of quality versus there's a school of folks who are focused on the craft of testing as a, as an end result almost. And do you think AI helps which school is AI can be more advantageous for the people who just want to just do better testing and for the sake of testing or the people who want to help use testing to find higher product quality? That's a leading question. You know, the answer is the people that search for quality in the day, but not to the world. And why is that? Because the, Michael Brent was saying, like you can't actually just go in and fix, rewire the search engine, right? And then it'll be good tomorrow. It's dynamic, it's data driven. And we can't possibly understand the nets, right? They're doing all that ranking. So the end purpose has to be quality and particularly how do you quantify quality? Like you might remember Brent, like there's NDCG, like normalize, just kind of communicate. But it's a way to measure, you actually have to quantify quality so that you can feedback the AI training systems. This is a better or worse building than yesterday's, right? And so it's, you actually instead of saying I have got, like the old world is I've written a hundred test cases, right? 94% of them happen to have passed. And these are the ones that I've happened to have written today. But that quantification doesn't work in AI. You have to actually measure the actual statistically significant quantification of quality. You have to measure it, actually get down to like a floating point, right? So yeah, that shift in software, how we build software from traditional like Java, C-sharp kind of functional coding to AI means that you focus on the end game, which is quality and to figure out how to quantify it. Then you have to figure out how to measure it. And you have to figure out how to feed that back into the system. So it's very much ends versus the means where traditional software is focused on the means to an end. I totally agree with what you're saying. So I want to share a little story here is there's nothing to do with AI, maybe AI helps here. So I did the thing that you do from time to time, Jason, you don't do because you have better things to do in life, where I responded to a threat on where I responded to a threat on LinkedIn just because I was bored, not because I felt like they were going to listen to me. And it was a thread around someone made some comment about asking customers to test. I see, actually, I brought it up because it seemed like three or four different posts where they think that continuous delivery or, or no dedicated testers means you're asking customers to test. And by that you're meaning you're asking customers to enter bug reports. I just added a point of clarity around that because generally, we're not we're getting feedback and data from customers to that allows us to understand how they're using the product if they're seeing failures, then adjusting either backing out making changes, we can automate a whole bunch of that. But a pause there because I think that's an area where I think AI can actually help us out a lot. It can help you can we got to define quality, I think we use AI to help define that quality criteria. Here's my application, here's what it does help me understand what are some metrics I can monitor in production to let us know if this if customers are finding value in this. It can help us zero in on the right set of engagement metrics, the right set of error metrics to look at to make sure we're building the right product from customer customer value point of view. That said, I jumped in and said that stuff and if somebody had the most interesting answer, he said, when we get into using engagement metrics to evaluate a feature, we're talking about software from a product standpoint. True. What I what I'm missing there is, and I'm way off in the weeds here before I get the actual question here. Because the product standpoint, customers don't want well tested software. They want I gave a talk of this like 20 years ago, I had this like fake box of software saying 90% code coverage over 97% of tests passed. Those aren't the bullet points you put on the box. The bullet points on the box are the value you give people. So yeah, from core for quality perspective, we need to think about things from a product perspective. And then Brent, you'll love this from principle number one, they said testing is about more than evaluating the profitability of a feature. It's about finding all the ways the software could be used that result in the company losing money due to user attrition or reputational damage. It's the last line of defense before the company does something stupid they could have prevented. So going back to something Brett and I had said, I don't think this is a test responsibility functional correctness is a responsibility of the developer. So what I understand is there's a shift here. And I don't care if the shift happens goes back to your also your post about the late adopters. There's gonna be people testing last. Hey, Brent, you get a cat? I've had a cat. She has decided that it is time for patents. What I want to get an idea because I don't know how I have a good idea how we'd use AI to help define those those metrics at the very beginning. But how do you think we can use AI to make sure that we're testing the right things? That's the point of question when I get to I took 20 minutes to ask this question. We in test I've been doing this long enough time to know that a lot of the time, and I read between the lines in this reply, it's there to we over test, we test too much, we test things that customers will actually never do. We spend a lot of time trying to hold products up because we find some edge case that's actually never going to be seen. And these are smart testers doing this stuff. They just don't know when to stop. How can we use AI to optimize like you talked about making manual testers better? How can we use AI to optimize that test selection? So we're testing just enough to make sure that we're delivering value to customers? I think it goes back to incentives, like management incentives, personal goals and incentives like so testers are rewarded for creating test cases, and having dashboards. So they got to help more. They're not as closely tied in with the business or product or even user satisfaction or happiness, even though they claim to be emulating it. There's no real tie to that in money or really rewards or recognition. So I think fundamentally, if the changes incentives for testers to agree with your premise, and I think the only way to change it is to change their incentives. But I don't think that's possible. Adding all that, I think so it goes, what do you say it's back to developers and product managers that really need to own that ultimately. Because I don't think testers can or will adapt. And specifically, like you're saying, I think it's all going toward, I forget the Microsoft E, Microsoft speak term for it, but it's analytics, right? What is it, Brent? What's the catchphrase right now? What do you use at Microsoft? It's not instrumentation. It's telemetry. So I think it's all about telemetry. And I think literally, I don't know if you guys have followed this, but there's open telemetry. So specifically how I can help that there's new open telemetry protocol. And that means that all these different, just so people are listening, though, like it's used to be like you purchase Splunk or you purchase some specific vendors' software and you log to it, and then they render it back to you and do analytics and alerting and warning and like New Relic and these things. But they've standardized that layer. So I think what's finally now that there's a standardization in that layer, I think the vendors didn't really want to do it. They're kind of being dragged into it. But now there's a standardization that layer, there's a standard scheme or schema for all that telemetry data. And now that means that you can do not just can you write a single kind of AI analytics thing against everyone's data, but that you can now compare data. So you can compare one coffee shops websites with another coffee shops websites data. Because the problem is baselining. So you can have alerts, but like you're saying, is there a bug? Is it important to the business? Does it cause drama? Is it ever going to be encountered again? Is it a one-off? And that's where I think AI can do the analytics, but particularly if it has access to the data from other similar sites and historical data. So I think open telemetry will help open that up a bit because it will have common tools to be more incentive to build common tools for analytics. And then you'll be able to share kind of that summarize data across these different verticals to get a baseline of quality. Open telemetry does a fantastic job in terms of if nothing else, data democratization. There is a couple of projects that I am working on within Microsoft today that leverage open telemetry. And it's just, I can't wait for that to finish landing, because a big part of my effort is often just data cleaning or conforming to schema. And I'm like, Oh, it will not at all make me upset if that part of my team's job is just eliminated. Right. And even the smart stuff, right and more value. Yep. Right. I want to double down on my comment from a few minutes ago listening to your talk Jason and YouTube rant is that I think at Microsoft we had written so much telemetry and there was so much internal knowledge that just writing good telemetry or reasonably good telemetry was common. I'm sure it was the same at Google. What I've discovered since then it's difficult for companies to learn like there's no guide. I guess there are guidebooks. They don't read them. They're not very good at figuring out what to measure and how to look at it. I think that's a great place for it. I think just that definition if it's hard, but it's teachable, I think AI can help bridge that gap. I totally agree. And this one, but I think it feels in the category still of like, we hand wave and AI goes here and then something magic comes out. But I think that's the feeling on a lot of AI tools. It's just magic. It's hard. Let's use AI. It'll get done. It just be magically exactly. And Gartner will write some charts about it. Actually, add double down real quick. There's real data. I did some biz dev work with one of those analytics companies. I guess you should be anonymous. But I went there and I talked to them about their dashboards for their telemetry analytics. And they said, and they make their worth billions of dollars, but they said, no one looks at their dashboards. And so they work because it's not actionable and they don't know if it's good or bad. So you have a chart wiggling around over time. But you don't know what the right thresholds are, like you're saying, Alan. No one knows how to set that. No one knows what the high water marks, low water marks should be. And if it's good enough. But I think that that's kind of what back to Brent saying, if everyone has this common scheme for representing that data, then you can start benchmarking, not just against historical timeline series, time series of your own data, but against other vendors and other verticals that can be anonymized based on just general purpose metrics to see, are you the worst coffee coffee shop website on the planet in terms of latency, for example, right? Then you know you should make it better. But I think it's a long way to say, I think it also will enable a relative evaluation of whether it's good or bad. So you just throw that data in. And it looks at all and says, ah, you're really sucking in this area, but you're really doing good in this. Congratulations. Like you don't have a lot of crashes in production in the JavaScript, but you have a lot of latency compared to everyone's slow loading pitch. So then people can know what to do with those. Those charts are finally actionable. I think that might come with open telemetry with a combination of some some analytics and the IML. It will. And one of the things like years ago, Alan and I had a long series of discussions around dashboards and KPIs. And it was right when we were kind of in our honeymoon phase of our love over Eric Reese. And I'll just say like, if you're listening to this podcast and you don't know the difference between actionable and vanity metrics, pick up Eric Reese's book. Understand that because that's stage one before you even begin to kind of go down the path that Jason saw. Also, if you read the book, you'll see where Brett and I stole most of our ideas from. A good portion. I mean, I stole from the Pop and Dykes. I stole from Leffingwell. And I have shared on the screen, right, to the point of I'm using Jason's listeners will need to imagine there's a screen in front of them. Yeah, so I went to Jason's expert tester spin off of chat GPT. I don't actually know what that should actually be called, but we've talked about this before. And it's super cool. Yeah. And I'm like, okay, if so not only did I ask it to sort of generate a KPI to evaluate the output of LOM, I didn't even tell it the context. And I said, do it as if you are Alan Page. And I just scan through it. I'm not going to board our audience on this, but I scan through it. And I'm like, all right, some of them, I think you're reasonable. Like it gave a long list that I do think Alan, like the categories, I think are definitely Alan. All right. Even includes diversity and inclusivity. Now, whether or not the actual measures, right, it, I think it did a reasonable job of not only saying what to measure, but the implementation. Now, I only scan through it. And this is a problem I face on a daily basis. So I'll go back and read there and go, okay, well, well, Alan LLM actually helped me unlock a solution to a problem I'm chasing. But yeah, I think that direction is, is, is kind of heading that way. The thing is, I'm trying to pair this to what we talked about at the very beginning, Jason's hypothesis that these experts will come back. And Jason, does that not potentially contradict? Or how do you see this happening at the same time of sort of the AI first? Because the way I see those people coming back, in my mind, it would be what they call the centaurs. It's basically those people working alongside LLM are going to nail this problem for us. Whereas you're saying that AI first. So what is the mechanics of what you're doing right now with that GPT is, imagine you had the standard open telemetry was more, you know, tactics, but this part is so imagine you have your JSON blob of your high level open telemetry metrics, right? And you just, you pass it to this bot or similar bot and the bot can come back and say, you know, oh, for a coffee shop, you tell it, this is a coffee shop. These are my core metrics. And it might come back with a reasonable assessment and tell you like what you should probably be focusing on. That's kind of an early step of in this process. But guess, guess who would be the best person or AI or, or person to analyze that those, like we did with this GPT with an emulated Allen, Allen, like, like, this is a very loose approximation of Allen. Allen, like if Allen want to spend a few hours, or I'm happy to work with him on it, we could build an Allen bot that deeply encodes a lot of his thoughts. And I'll make this bet you're ready for this wager. I asked you guys, asked you what kind of mood you wanted me to be in. But you had your chance. I will, I will make a bet that not not all my cash, but I'll do $1,000, whatever, if you want. But anyway, I'll do a bit. I'll bet that both of you are in a QA titled role in 24 months. Because of this broad because, because guess who's best at testing that system. It's not the engineer working on Brent's all worked up now, because it's a, it's a, it's a ladder thing, but it, but you'll be paid. And I also have fault with you'd be paid more than you are today. You'd be caught tomorrow because, because these systems are complex, the developers don't know how to test it. But the best thing is the best thing to analyze that is, is Allen and Allen bot or a Brent and a Brent bot, not the engineers, not the PMs that are out there today and not definitely not the tester. So just, just before you, I know you're like your antibodies are already out. Yeah. Brent's head's about to shoot off of his body. Yeah. Yeah. I've seen this progression over years, by the way, because, because what is the progression of every Microsoft tester, like to go into management and then somehow escape, get an escape pod to PM or dev. But I'm telling you that just building at Google with the smartest PhDs in search and the best engineers in the planet working on the most profitable, insanely profitable software on the planet, we're all called software quality engineers. When Microsoft acquired Softimage and then ignored it for like 10 years and sold it. Anyway, when they acquired it, one of the things I learned about them, this is 25 years ago, that was their leveling system was dev, senior dev, principal dev, fellow QA, because they were the systems thinkers at the top who knew so much about the system. They were in charge of quality. Now it's a different world because I almost, I don't know if QA would be in the title, but I could see a path where it's in the role because the for whatever QA test, S debt, quality, whatever, has been so bastardized and beat down across the industry. It's not even the right title for that. I, I fully believe there will be people doing exactly as you describe and Brenton and or I could very well be part of that. Sort of is like the AI is my pet and I'm teaching it how to do this thing for everybody. But I don't think we'll be able to call the role a QA role because of how badly it's been just smeared across the industry. But I think that's the problem is that that role is emergent. And so if you, if you look at the long, this long tail, you look, you integrate over like four, five, six, seven years and you assume things don't aren't static. What's going on is that the AI starting to write the AI, right? That's starting happening now, whether you want to admit it or not. And so guess what, guess what isn't automated? The evaluation, the testing of it, right? So what will happen is there'll be Brenton go like, no, I'm still going to be using SQL Server. And I don't know, but no, I'm not. I'm going to, I'm going to, I'm going to push back and say, no, the AI is also evaluating the AI. Like, I don't think that's a whole. How does, how does GPT for evaluate or test it today? Oh, by a small army of metrics by, by human beings. But not the, but so you know, the language around an AGI, you know, a language around an AGI. And I believe AGI's already exist, that this, this stuff is already happening. What happened to you since we talked last, Brent? What happened to me? You were telling me that stuff will never be thinking or sent the intent. It's a stochastic word. Oh, I'm still on the page that yeah, we need to do whatever the hell we can do to prevent this from happening. That part's not changed. But just because I'm fighting a, what is the old Greek myth, the poor dude that had to push the boulder up the hill for it, just because I'm that guy, yeah, it doesn't mean my opinion on that's changed. Right. It's, but so back to the, specifically the title thing, which I think is what the revulsion comes from in the, the cathartic. Go ahead and Brent, stop playing with your desk. I walk, I thought you told him that already. We know testing was a dirty word in quality. It was a dirty word. And this debate's been going back like in 2010, right? But I'm telling you, I walked into the Google search building and guess what the titles of the engineers were search quality engineers. And we're going to stop there for now. We'll pick it up again next week with episode one 94. This has been episode one 93 of the AB testing podcast. 
