Good morning, Brent. Good morning, Alan. It is time for episode 61. 61. I count on Brent to remember those things for me. I don't know about you. I'm happy to be here and talk about testing stuff, engineering stuff, quality stuff. Sure. Let's do the podcast. You're such a dork. I love doing a podcast with you because it makes me seem cool. It really does. Yeah. I go back and I listen through them. That's because I'm in charge of editing. I take out all the dumb things I say and add more Brent dumb things. Nope, no need to add more. There's enough. It's the privilege you get from doing the extra work on the editing, yes. And that's OK. You actually come prepared, and you actually have insights and acronyms and things that are valuable that I usually end up stealing if they're good, and then pretend I never heard of them before. Yeah, it's sort of the inventor's dilemma, right? So it reminds me of Book Plug. This is a very popular book. I'm sure a lot of the three have read it, but Austin Kleon's Steal Like an Artist. I've not heard of that. It's a good book to read. That's what I do. I steal like an artist. I have about 10 books I'm simultaneously reading. Oh, have you read? I started reading. Which is a horrible strategy. I started reading. Now, Philippe Canoriam on one of the three Slack channel just gave Brenton Isem nice tile devices. Yes, thank you. Thank you. Thank you. I have mine right here. He recommended a book, and I'm going to mess up the title called Everybody Lies. Pretty sure that's right. A lot about data science and analysis. Dang cool. Dang cool. Not even all the way through it yet, but I stopped reading everything else I'm reading now because I can get through this. It is, yeah, you should check it out. So far, it's a bunch of research based purely on one of the most abundant and unfiltered sources of data on how people think and what they're thinking about around the world. It's Google search queries. Oh, interesting. Brent who worked in Bing goes, oh. So now I really got to look at this. In Bing, I found the data that I had just fascinating. And that's only on a tiny bit of the traffic that Google is getting. Right. Zing. There were conversations when I was in Bing. We still hadn't done the combined engineering all throughout Microsoft. And so I still had a lot of my lunch buddies were like directors from Windows and things like that. And I remember going, OMG, do we have some interesting data? And I remember talking to, I'm not going to mention his name, but it is someone you know, former director within Windows. And I said, do you think it would be valuable if you had access to what users and at what rate people are coming in and doing search queries for your features? Because, and I showed them right there and then, because I had permissions, I showed them right there and then I'm like, look at this. Type, type, type, type, type. He's like, oh my god. Or, oh shit. Yeah, so it's taken a while, but one of the things that I did then is I went back and I said, hey guys, we got to figure out a way to expose this data to the rest of Microsoft. Because there is. What, if you think about it, what a massively valuable data set. It is. And looking at some of the different ways, I mean, I don't want to give it away, but some of it was like a sucker punch. Like, oh, I see. It's because the story is woven very well. They start correlating some data and go, oh crap. Wow. That I did not see that coming, but yeah, that makes sense. There is on the topic of, when you said everybody lies and then said it was a sort of a data science-y discussion. That triggered one of the concerns that I have around what people like me are doing today that's going to just completely change the world going forward. Now, a lot of us have this optimistic point of view, oh, there's going to be all these little AI bots. Everyone's going to have their personal Cortana. Or what, Cortana is in Halo, not Cortana is in the, the voice user interface that Microsoft provides. But I thought I had a small little holographic woman inside of my computer. Oh, that's when I use Windows at least. No, actually my older son, just as an aside to the aside, my older son had actually gone up and looked up on the internet. And there's a bunch of Easter eggs, Easter egg questions you can ask Cortana. Ah, cool. For example. Again, using a search query, I think we just went tangent inception, loop back. So for example, you can ask Cortana, what does she think of Google? And she says a very, oh, they did a lot of innovative stuff, but I use Bing. Oh, that's just stupid. I would just say they're a very successful advertising company. The funniest one I remember is if you ask Cortana, will you marry me? But I'll leave that for the audience to test it out themselves. Anyway, winding back up the stack, one of the things that I'm concerned with is ethics is not standard curriculum in data science training. It's not, this is actually what's interesting. And I've ranted about this before. I think the lack of critical thinking ethics courses in STEM are hurting STEM. STEM. Yeah, this is my big rant. I try and shove a bunch of sciencey technology, math facts into students brains, but then they get to the real world and they don't know how to use them. They don't know how to use them ethically. They don't know how to make decisions. There was a study I came across recently. We could do an all tangent episode. We're well on our track. There was a study recently that came out that basically said valedictorians and salutatorians. Very rarely impact the world the way they are expected when they're granted. Sure. And it's, in my view, it is the difference between street smart and book smart. Knowledge and wisdom. Yeah. Anyway, one of the key things that I'm worried about with data science is I think we're well on our track. If you think about where how the lens is going, you think about how far away we are from just completely automating 80% of decision making. I think we're on our track to number one. Everyone will have a built-in lie detector. So if I have Google Glass, facial recognition is just going leaps and bounds. And computers can pick up nervous ticks in ways that are imperceptible to us as humans. There's a lot of training that there's, what are they called, mentalists. These are people who kind of have trained themselves on understanding these nervous ticks. And they know if you answer and you look a certain way that you're accessing a particular part of your brain, and that part of your brain is where creativity is stored, and that is a huge indicator that you're not looking up information, but you're inventing information. There's a really good book on this. I read a year or two ago, but I cannot remember the name. There's like a 2% chance I'll look it up and put it in the show notes. So if no one has the ability to lie anymore, and then you have all of these AI bots making decisions for you, that kind of, whether you're religious or not, that kind of takes a big chunk out of this free will thing that we're so fond of. So somewhere there is a techno-thriller built around all these premises you just talked about. Yet this is something that is reasonably likely we'll see in our lifetime. Yeah, because it's convenient, there's gonna be a lot of people that buy into it. It'll be interesting. Like people joke around about the sheeple nowadays. Just wait until you see two decades from now. That's going to be crazy. So I spoke at the online test conference last week. You did, I managed to listen to about half of your talk and then I had to take my kid to school. I had a good time, it was a fun talk. It is always strange for me to give a talk from when there's no audience to look at. We talked about that a little bit on the last episode. Yep. Take a shot, because we talked about that before. But it was such a soapbox for me that I had no problem ranting for the invisible crowd. Yeah, how did the, the thing I was most interested was, that I missed was the profess Q and A session afterwards. How did that go? So there, the Q and A was, there were one, maybe just one question afterwards. There weren't a lot of questions during the talk itself. There was a special Slack channel set up for, Slack team set up for the event. And I answered three or four questions afterwards. And what was great also, we had more questions were asked, but many were answered by people already in the channel, who may have paid more attention than others. What sort of questions? Well, from the absurd things like, is Alan saying we should write more automation? And someone replied, no, that's the opposite of what he said. He very explicitly, yeah. I don't think they actually listened. Then I looked at the slides. One quick tangent there is, people ask for my slides. I posted them. There was, I did a little interview for info Q, little email back and forth, but he said, hey, can I see your slides? So I can form some questions. And I said, my slides aren't going to be very helpful. Yeah, I'm not a list of bullet points sort of presenter. But what else were the questions? I'm just like, how do you, more clarification on other things you could do to become more air quote technical. Yeah, I think, I think the thing that you did that was interesting there. Was not wear pants. Oh, that's a sec. Go on. Is you kind of brought the term technical back to its roots, right? Technical in context often just means, oh, you're a coder. Yeah, and I beat the, I wanted to beat the crap out of that concept. So I don't want to recap the whole talk. It's online. You can find it if you use Google. Bing may find it, I don't know. Or we can ask our crack editing staff to include a link to it in this podcast. There is a three or 4% chance that'll happen. But I'll pass that down the road and see what happens. No, the thing I realize, and you and I hadn't talked about this before you did this, but I realized upon listening to your talk, you're saying essentially, no, technical in your context is the use of rigorous techniques to, in this case, it would be testing techniques that would advance the business goals. Yes, yes. Right. So in this context, right, a technical tester, by definition, a new hire who just came out of college and never heard of a test position before is probably not a technical tester. Perhaps not, but I'm finding a lot of students come out of college knowing how to use Git, knowing how to, they definitely know how to use grep to parse for things. I think they know, they haven't learned quite how to apply some of the tools. Basic software engineering techniques, sure. But the testing techniques, what is the difference between ET and ad hoc testing? Ad hoc is, I don't quite know what I'm doing, but I'm testing, I'm... It's not even testing, I'm randomly going through Crap and hope I find something. It's an Easter egg. Where exploratory testing is, I'm learning while I'm testing, I'm trying new things based on what I've learned, I'm poking around, when I find a sore spot, I poke a little deeper. But generally, exploratory testing, in my view, exploratory testing, is a bit more structured. You kind of know what you're looking for and you're trying to see if it's there or not, I guess. Whereas ad hoc, in my view, is you're just clicking around, relying on your observation skills to notice something odd. Correct, I'm not sure why you're going there. No, because I'm basically saying ad hoc testing isn't really testing. It certainly wouldn't be in the way you're framing it, in my mind, it wouldn't be technical testing. I'm looking something up here because I saw a Twitter exchange that was begging to troll me, but I was busy and didn't play along. I'm seeing if I could find it. We're just gonna live here in Tangentville and we may do less mailbags than we planned. But I think of all the terms in testing, and they're all used a lot and some are buzz wordy and some aren't, but Brent, what's black box testing? You don't have to answer if you don't want to. Black box testing is where you test a system without knowing how it's implemented. Sure, sure. And then white box testing is a lot of times code analysis, code coverage, you're doing things at a lower level. And of course you can use tools in either state to help you out. But I saw someone, they were talking about whether full stack testing was black box plus automation, and he said, black box testing to me equals testing done by a human that is unassisted by any kind of tooling. So, yeah, I know, Brent made a face. But then it gets worse, and I wanna just, cause it ties in technical testing. He says, so as soon as you start using browser dev tools, you've crossed into using tooling to assist your testing, automation equals tooling. So, then he goes on. So it's like, whoa. Your world is shifted in a weird angle in the wrong, wrong direction. So, I absolutely, I just, so many things wrong, but I wanted to bring it up here because it does touch on what it means to be technical. I think one of my testers on my team listened to the presentation from Copenhagen. STE, I don't know if I've ever said this on the show, but I have a combination of software test developers and software test engineers, varying technical skills. And some of my STEs are very, very technical. They're former game producers at major AAA companies. They're indie game devs, building and making their own games on the side. They're very technical, but they're not focused on tooling or automation. Where was I going with that? Oh, one of my STEs listened to the show and liked what she heard because being technical for testing may mean you have a clue what to do when you press F12 in a browser window, Debugger Tools. You know how to use a proxy like Fiddler. By the way, I didn't mention this in the conference, but Fiddler now has a beta for Mac. Really? Yeah, you can now use Fiddler on Mac, which is awesome. There's a tool people use on Mac called Charles proxy, which works pretty well, but Fiddler is, Fiddler's very, very awesome. But just knowing how to use tools to do better testing, that's technical. Technical, as I said, and you pointed out, is just using the techniques or tools of the craft. In a nutshell, using techniques, that's technical. Yeah, but that can be an overloaded term. Yeah, it sure can. Ad hoc testing, in my view, is not a technique. And fortunately, I don't hear that term very often. Yeah, that's good. BlackBox, in this case, when I think about BlackBox versus WhiteBox. I don't even think about it, but. No, I do. So let me just go briefly on this. BlackBox, you don't know how it's implemented. So what you're trying to do is without knowledge of how it's implemented, you're trying to determine is it, is when the system gets inputs and it generates outputs, are these outputs expected or reasonably expected based off of the inputs? Now, the reason why you do BlackBox testing, or the benefit of BlackBox testing, in my humble opinion, is it allows you to make sure you're not primed by the assumptions of the implementation. It forces objectivity. Now, I think the more experience of testers have the ability, they recognize this is a problem, and they have the ability to accelerate their forward progress by even with knowledge of the implementation, blocking out that the threat of invalid subjectivity. Right, this is one of the reasons for BlackBox testing is the thing that you and I hear all of the time as pushback to some of the things we were saying, which is, well, yeah, but devs aren't very good at correcting their own assumptions, and you really wanna have an objective observer focus on what invalid assumptions the dev may be making. We've stated over and over again that's a crock of crap. Yeah, and I actually went over some myths like that in presentations like myth, devs can't test. Myth, knowing how to program skews your testing. Yes, actually, I agree. I mean, perhaps, but yeah, I mean. It does skew your testing. You will move a lot faster. A lot of these things are rationalizations, so sharpshooter bias, it's a bias. It's like, hey, I am going to proselytize with a rational argument the thing I want anyway. I think what we see a lot, I think it's very clear, and if you listen to the show, you know that we believe that testing is changing a lot. Actually, software engineering is changing, and the bit of that that is testing is changing very much as well. The way we make software, changing massively. I think there are a lot of these arguments come from people who are resistant to that change. It's for job preservation, or they're afraid of change, or whatever, so they're digging their heels in, and they're saying, no, this is not happening. They're putting their hands over their ears and screaming, la, la, la, la, la, la, la, la, but it doesn't matter, the world's changing anyway. If they don't, they can disagree, but that's going to lead one place, unemployment. So some of these people are smart, right? I mean, they may be, here's how I'll put it. These sorts of discussions around these changes definitely triggered the fight or flight response to a lot of people. Absolutely. If you didn't see the change coming, you didn't make steps to prepare yourself, and you realize, holy crap, I can't retrain in the time in which this thing's gonna land, so the only thing I can do is try to defend it from landing. Sure. It's human nature, and actually, it's one of the things that drives me to come and do this podcast, because there's still a lot of people in the world, as we know, and even as some of the responses we've seen from 60 or even from the online test conference, the statement that we said that the world of just providing information and being satisfied with that is over. That's a shocking statement to a lot of these people, and we're like, sure. At this point in time, I'm actually quite content shocking people, because if they want to not be in the unemployment line, start doing something now. I think it's really important that, not just you and I, but other people who are living on this, trying to live on the edge of pushing forward, continue to have a voice. I saw someone post an article a few weeks ago, kind of hedging around and saying, maybe we shouldn't write so many GUI tests anymore, and we've all seen the testing pyramid with the bit at the top for GUI tests, and it's been there forever, yet now people are going, hey, maybe these GUI tests aren't worth the investment. I looked up when I wrote my GUI, Shmooey blog article, and it's just about exactly nine years ago, and I might have been late posting it then, so probably something I thought about well before then, if I was able to write a blog post about it, but these things are happening. One last thing before we actually begin the show today, is I do a lot of talks. I like giving presentations, I think it's fun. I like sharing my opinions and my rants with those that will listen. I very rarely, with maybe one or two exceptions ever, will give the same talk twice, but this technical testing thing, it's a talk that I think I would give again in the right context, I gotta figure out where that is. I would, of course, it won't be exactly the same. I'll change some things around, but it's just, as the presentation started falling in place for me, I realized how passionate I am about getting rid of this myth that technical equals coding, and coding equals automation, and automation equals GUI automation, because I think that's just a horrible path to lead people down, and it's not the way of the future. It's not going to help you out down the road. And I see so many posts on testing forums and Twitter and here and there, I'm a manual functional tester looking to get into automation, how shall I proceed? It's like, oh God, the world is doomed. Coursera, C sharp Java courses, right there. I was really proud of 60. He's talking about episode 60 of AB testing, our previous episode. And it reminded me of a lot of the conversations that both you and I had when Microsoft was in the middle of this. One of the challenges, so you and I both grew up in tests, and we're both strong proponents of unified engineering. That's a threat to the test discipline. And one of the things that you and I are saying, or at least I'm saying, I think you agree, is that test discipline is not inherently valuable. It's the output of the skills of the people that make up what we currently isolate as its own discipline. Now, I grew up in tests, and I do feel a sense of membership to that community and a sense of leadership or responsibility to that community. Like when Microsoft was in the middle of this, my job, when I saw a lot of these people in panic, my job was to show a path forward and to bolster the element of hope around that path. I'm like, look, this move doesn't eliminate everything that's valuable about you. One of the things that, and I've said this here, when I get done babbling, I'll take my shot, but I have lived in a dev world where I did these combined engineering, and I have lived knowing that those people who have the ability to code that grew up in tests and understand those skills are actually better developers. Like, it's been my experience hands down. Those people who started with tests understand this model, added the coding skills, they are stronger as developers in this new world. One of my test developers, S. Detz, borrowed term from Microsoft, I kind of resent it, is doing mostly development work. In fact, we're looking for eventually, probably the next six months, we'll find a dev job for her at Unity. Former tester, the most conscientious test writing developer that I know at Unity, because she gave the pack, she writes great tests for everything, it's ingrained, like, of course, of course you write tests. Of course you write really good tests when you're writing code. Yeah. It would be dumb not to, they get that. I have a weaving story here that I haven't talked to anyone about this before, but I'm gonna bounce it off. You and the three listeners, Brent, you may remember when I worked on Xbox One. I do. I do. I had nobody. You joined, right, when I left. I joined, right, as it was starting up, yes. I had no one reporting to me for the entire Xbox One cycle, but I was responsible for growth and technology sharing in community for, at first, just the Xbox One console team, and then I expanded across live, and then across games. So I had, I managed a community of people concerned with Xbox One quality to make sure we all knew about tools each other were using to find connections, to make sure the smaller team of very senior technical testers, I was sort of the product manager for Xbox testing tools, that we knew what the needs were, very much like a test manager, test director role, except I was doing it all through, they weren't reporting to me. So, which I'm fine, totally comfortable with. So now at Unity, and I hope my boss isn't listening because he'll shit his pants. And this would be a long, long-term thing. Nothing would happen in the next even year or two. I've been thinking more and more about my organization. It's a matrix organization where I have QA people embedded in feature teams, yet all reporting to me. And the reason they report to me, I've talked about it before, is I run a, this is gonna scare the people that listen to the podcast that work for me, but I'm gonna go on with the thought experiment. Most of them are embedded in feature teams, they work for me, and I run a community. It's all good, Alan Skyes. Just talk to him afterwards and he'll set it. So I run a community of testers, and so I can find ways for them to connect and to share tools and technologies. We need to get a lot of that. This, by the way, is why you don't tell your team that you do this podcast. I did not tell them they found it. Of course, I believe in total transparency, so my calendar's open for anyone to see exactly what I'm doing all day, and then 8 a.m. every other Friday, there's podcasts. Oh, it's just podcasts. Nothing, go away. And they search for Alan Page Podcast, and they go, oh, hey, pop the stack. Again, at UNITY, I run a community of testers, they're embedded. There may come a point, and I don't, again, I don't know if this will happen, but logistically, I could just merge many parts of my team into the dev org, set them free, but still run that community of testers. They don't need to work for me to be the quality and testing expert on that team. There are some advantages to that for the short and medium term, but I could see a world, and again, one thing, if I was an old school test manager, I'd be afraid to death of that, because I need my org to prove my existence. I think in order to do that, I would one, have to get them to a level of independence and have that community strong enough where they would come back to it, even if it wasn't a community based on reporting structure. Before you can abandon the matrix, you do have to make sure, a huge portion of these have been sufficiently taught leadership. A lot, this would be a long term thing, it's kind of, it's a little sparkle in my head, one of those background threads that may pop up for me to think about once in a while, but I could envision an org, and I have to also build trust among the dev managers and my managers, other people in the org that I can justify my job, and maybe I don't, maybe I work myself out of a job. It doesn't matter. What matters is that we have the most efficient way to accelerate the achievement of shippable quality. I have found that literally every time that I automate myself out of a job, I attract the interest of hundreds around me that want me to go and solve a harder job. Yeah, and I really do see, I want my role to be one where I can just make everything better, and that's a horrible way to put it, but I wanna make, improve us, I wanna advance things, I wanna challenge people. If I do that to an extent, that what I do the role I was hired for is no longer relevant, that's okay. That can happen, I'm not scared of that. And then tying that back to what we were talking about earlier. Unless you're a geezer, so it'll probably be time to retire by then. I can do this one or two more times and I'll be dead. Yeah, actually as you were talking, one of the things I was thinking about, so I'm not the only person to ever manage a dev team of combined engineers. I know, unified engineers. Whatever. Actually, and that's actually an important distinction. Because I have seen tester folks being incorporated under a dev manager, and that person ran it like a combined engineering team. You know, it was a burden, oh no, I gotta manage these test guys. The difference that I took is I'm like, no, what I need is a unified engineer. I need people that can do code and do testing. And I know I've talked about this story, but by realizing that what I need to do is, all I needed to do with these test guys is remove that lack of self-confidence that is brow-beaten in the testers. And once I did that, these guys were rock stars. Now, what I have found though is leadership matters. So if they get re-orged into somebody who says something along the lines that starts with, pick your favorite sentence, but starts with, I've been doing this for the last 20 years. If you have moved these guys into that they had not been taught leadership in a way that defeats this lack of self-confidence thing I mentioned. Just gonna put them in a worse situation. So yeah, now Unity sounds like it's a lot more closer to an ideal world. And to be clear, there's a massive room for advancement. To catch up to some of these things, but their ability to move quickly is very, very strong. You have to have a way to make sure that when the unified engineering model happens, or something along these lines, to make sure that they're viewed as an asset, not a detriment. Sure, and again, it's not like I don't want, I don't want so many people reporting to me, I'm gonna push them off to the dev team. Not my goal at all. I think they'll become a point in it'll be different paces on different teams where it just makes more sense for them to vote there. They have the autonomy they need, and I can still swoop in. If you back that up to, compare it to when I was on the Microsoft Teams team, there were no, it was unified engineering. I was hired as the quality guy. There were no QA people embedded on the teams. So I had a different problem to solve. I had to do it the other way around. I had to coach and train people how to think like a tester, and find the people who are good at it, and make them, Microsoft called it the dotted line report. Basically just to have, exhort some leadership over them. Empower them to be the quality and testing expert for that team, whether or not that was their official role. Every feature team had, at first at least one, but sometimes one, two, three, four, five people who were actually got how to write good tests and cared about it, and had a level of craftsmanship to their code, and then would evangelize that across their team, and ask the right questions during code review about tests, and make sure they had the right balance of tests. So I did it the other way where I built them. That way works. I actually, But it's slower. That was fun to do, because it was a fun little challenge for me to do that. It's slower. I like having the QA team. I've enjoyed managing this team. It's been a ton of fun, but also I'm just throwing this out that we'll revisit this in episode 97 or something. But I could foresee downsizing my team in favor of the efficiency of our ability to get quality features to our customers efficiently. Yeah, and again, to Alan's employees, he means downsizing the number of people reporting to him, but the rest of you will be working in the dev org. There's no job loss being discussed here. And by the way, episode 97 is at least two, three years away maybe? In a good, yeah, I kind of think, yeah. We'll see. So again, don't be scared. Just, the thing is my team already knows I'm full of totally random thoughts. I have a very agile approach to management, means I'm gonna ship early, ship often. So, and those are my ideas, shipping early and often. So I'll throw stuff out. If it sticks, great. If it doesn't, I don't care. My feelings aren't hurt. I'll try something else. And I've had many, many thoughts in my life of things, oh, maybe I'll think about this, that I've just never done, never happened, and that's okay, and I'm okay with that. You're an INTP, right? I am an INTP. As am I. And for those who aren't familiar with the Myers-Briggs psychotyping testing, this is essentially a model that generalizes. It's not a exercise of putting people in a box, but it generalizes how people make decisions. And when I walk through the difference between a P and a J, I'm not gonna go through the whole speech. Perceiving or judging. Right. But I'll just. So the whole process, INTP, and I am an INTP as well, the whole process is about the decision-making process. The first one is I versus E, actually talks about what decisions you're interested in thinking about. N versus S says, how do you collect data? T versus F is around the decision-making process. What bar do you use to make a decision? T's are thinkers. And F's are? Fred and I are thinkers. We think about crap all the time. All the time. But P's versus J, the thing that's interesting about that is, once you've made a decision, how do you stick to it? And the way I describe the difference between the two is you can think about a decision tree. J's, once they make a decision, they prune the rest of the tree. If you know about game design and tree pruning, once they made a decision, they abandon the tree that they no longer need. Okay? Whereas P's always keep the tree, but for us, it's a balanced binary tree. So when we make a decision, we actually reweight the tree and rebalance it. And so when new information comes in, we go into the tree, rebalance it, and now a new decision is made. So P's are viewed as being spontaneous, or the negative side is we can't stick to a decision, but really what we're doing is we're adapting. Yes. Now, there is no positive negative to being a J or a P. P's, in a world where you have to constantly adapt, P's are quite good at it. But the challenge with P's is, if our tree gets too big, it mentally can be a challenge to rebalance the tree. I mean, it's not actually a balanced binary search tree, it is, that's just a metaphor. J's though, if they're forced to re-decide something, what do they have to do? They have to rebuild the tree in order to prune it again. So that's why you'll see them getting really frustrated when a decision gets unmade. So a couple things, a bunch of things. Actually, one is you can wave your arms around Myers-Briggs types indicator being a horoscope, be whatever, and if you take like the online test, it's gonna be probably inaccurate to some extent. I think Brent and I have both taken like the long form written test. It's fairly accurate many years ago. I wanna read a little bit about the INTP for our listeners in a second, but it reminds me, long time listeners will know that one of my favorite authors in the world, a man I have a huge man crush on, Stephen Johnson, in his book, Wonderland, talked about the fox and the hedgehog. I do not have, I have listened to the podcast, I do not have the book. So it's actually in the podcast as well, but the fox and the hedgehog is basically how people approach learning. And the difference is the hedgehog has one world, he just sees it. And the fox is always learning and adapting. And it tells stories about how important, and the Wonderland's all about the importance of play and learning. Talks about how the fox, the mother fox, will actually bring back live prey to the den, let the young fox play with it before he kills it and eats it. Little gross for Friday morning. The difference is the fox sees everything, and they adapt as things come on, and they're always learning. They have a more plastic brain. And Brent and I are always, maybe this is reflected in our ability to have ADHD and bounce all over the place, but we're always thinking, and sometimes the thoughts are so strong, we can't resist them. And pretty soon, there's a squirrel with a balloon on top of a truck or a train. With a ringmaster hat on. So I'm gonna read a little bit about us. I think his name is Petey. So you can get some insight. This is a definition I have, I guess this is the first hit on Google. The INTP personality type is fairly rare, making up only 3% of the population, which is definitely a good thing for them, as there's nothing they'd be more unhappy about than being common. INTPs pride themselves on their inventiveness and creativity, the unique perspective and vigorous intellect. I know I'm pretty dumb. Usually known as the philosopher, the architect or the dreamy professor, that part's pretty good. The INTPs have been responsible for many scientific discoveries throughout history. I don't actually like that definition very well. One thing I read about INTPs early on that kind of hooked me on, oh wow, yeah. Says INTPs abhor inefficiency. And that's me to a T. It depends on how you define inefficiency, right? Because I do not. This is Brent thinking. I do not abhor laziness, because I live that way. What's the Heinlein quote? Something about progress being driven by lazy men trying to find an easier way. Right. So nothing wrong with laziness. And often, I mean, automation is laziness. Like I'm too, I'm too lazy to write, to do this again, I'm just gonna write a script or something to help me out with it. Oh no, so that's absolutely true. Like there are times where I have gone against, this is back when I was still in IC, I've gone against, my manager explicitly told me, I don't want you to automate this. But the next three weeks, you're gonna be running the same 100 test cases every day. And I've told this exact same story. Kill me first. I had the same manager. Because I was, I told the story, I think I told it during my talk, that that was my first, maybe I did, my first day at Microsoft. Yeah, I mean, I definitely don't like repetition. The one thing in that thing though, that I think does resonate, the label of architect. Right now, to some people, this means a technical design. And yeah, I'll do that. But lately, I'm doing a lot of process architecture. I'm going, no, actually the tech side of this is actually pretty efficient right now. The bottleneck is all these damn people using processes invented in 1995 for a world that no longer exists. I like thinking it that way. And the architect for me is the systems thinker looking at all the pieces at once. That's what foxes do. Look at all the pieces at once and then they'll make decisions and adapt. I have noticed in, now that I know my team pretty well top to bottom, managing them is a little bit of an architectural task in some ways. I compared it this week a couple times to playing a big game of slide 15. I don't know what slide 15 is. Slide 15 is the box of 16 squares. You have 15 tiles numbered one through 15. You have to slide them around to get them in order. Oh, sure. That game is called slide 15. Okay. Because I'm thinking, okay, I want to be able to do this work here, but in order to do that, I need to make this person do this and that. So it's a lot of, and they have a backup plan. Because if they do that, oh then they can go there. And it's kind of fun. I know what you're talking about. It reminds me of solving technical problems, but I can apply the same thinking generally to making sure the best thing happens for the team overall. I kind of view that, like my metaphor for that problem is pick any game you've ever played that has a tech tree. Yeah. Right? It's essentially, okay, what can I do now? What do I need to be able to do? Now, what resources do I need to apply in order to unlock that next technical advancement on my self-visualized tech tree for my team? I'm in knee deep in planning right now. So I am doing a lot of thinking through what are all these possibilities, but at the same time, sort of writing down things in a firm fashion. And as a P, I hate that. So also in Wonderland, Stephen Johnson, this is I think gonna, not didn't quite have the tech tree, but a good example of systems thinking is SimCity. Yeah. In order to do one thing, you have to do a bunch of other things. They can relate to other things. You get all those, it's managing that is a good systems thinking problem that is fun for some and infuriating or even stupider boring for others. For me, I would love it if they released another version, an up-to-date version of SimCity. I lost a whole day of my life back in college. Long story short, I got so involved in this game during college, I finally got done, got up, went to go talk to my buddies because we were all supposed to go to the bar tonight. And they're like, dude, that was yesterday, where were you? And I'm like, what? Yeah, so shall we start the show? The good news today is that looking at the board of the topics we're going to get to, episode 62 is completely planned. Yes. Two weeks in advance, at least. That's pretty fantastic. So one of the things that I'm starting to think through because I talked without- But to be clear, I enjoyed the conversation. That was great. I actually think, so now, one of the things that I wanna do is go and retroactively title our episodes. Because we get enough of them and we get questions. I'm like, oh, go listen to episode number, I don't know which one it is. And I'm tentatively gonna title this one, Something Around Retrospective. Like this is sort of the 60 retrospective episode. Yeah, but we, it was, I like the term tangent inception. Tangent inception? The tangent inside of a tangent inside of a tangent? Yes, the inescapable tangent tree. But fun stuff. Okay, so we'll do an all mailbag show next time. Yeah. Cool with that, unless anything awesome happens in the world of software for that we preempt ourselves. Or in case we just start talking and never get to the mailbag like we did today. But for now, we'll plan on an all mailbag show next time we have at least three questions for the mailbag. If any more come in, we'll try and see how many we can shove in and catch up. So if you asked the mailbag question and we didn't get to it and you think you guys are lame, we apologize and we'll do it next time. Or we'll plan to do it next time. We'll do our best. Yeah, we go through all of them so far. Yeah. Okay, thanks Brent. Thank you, Alan. We'll see you next time. Bye. 
