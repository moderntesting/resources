Good morning. Hello. For whatever time you're listening, I guess good morning is presumption something. You should listen in the morning. Okay, I'm Alan. I'm Brent. And we're here for episode 70...72! 72. Of A-B testing and probably now because we have no strict agenda, I want to pre-plug 73, which will be our annual Reflections and Predictions episode. Yep. I have to go back and look, find last year's episode and write down what our predictions were. Well, I think the predictions got cut off because that recording went to shit. Oh. If I remember right. So then I have to go back and make up what our predictions were. Yeah, exactly. I think I predicted there would be exactly 73 shows. It's one of the shows that actually requires prep from both of us. It requires some pre-thinking. So it's kind of fun. So that episode will be way better than this one. I'm pretty sure. If you downloaded both, I would just skip this one probably. Yeah. I mean, in general, it's a good idea. One of my employees came up to me just two days ago and said, Hey, I listened to your podcast. I said, Oh, which episode? One. I said, Oh, don't start with one. That one's awful. Well, I'm sure it gets better. Exactly. There's only a couple I think is that are worse than one. And I forget we have always one for sure. There was one. I can't remember one time in the Slack group, one of the three dot Slack dot com. You can DM me on Twitter and I'll add you. One time in that channel, we had a discussion of like, what's the best episode to start on? I think it was somewhere in the thirties, forties. You don't have to look at it. It's okay. You can go all the way back. It's cool. I want to. I want to. I want to. I thought it was like in the sixties, but yeah, it was there was a there was a dispute. It could have been 60. Yeah. Now, I mean, it depends on the context. Like we've been we've been doing this, what, four years now. No coming up on our four year anniversary. Yeah. In April. I'll bring flowers. The industry has changed. And we've changed. Why the bit? It would be in a perfect world where I retired tomorrow. Maybe I'd go back from the day job. I still keep podcasting till I till I die on the air. But I would like to sort of plot our progression. You'd have to die after you finish editing. Yeah, of course. Yeah. Plotting our progression. Like how have our views changed? Like what is what has been the the made a story of A B testing over the last three point seven two years. That's a good topic for next episode. That requires way more. The episodes have horrible titles, which Brandon's promised to fix someday. But there are short descriptions there. Maybe I'll maybe I'll do that. Maybe I'll try to see if I come up in the mid story of the story of A B testing. I think we'll find that they haven't changed much. We firmed it up things. We've created language like modern testers. I think I think we used to talk much more about testing and we talked much more about quality now. That's my my hunch. My prediction for the prediction show. What I'll come up with. If you replay when we started this, the majority of our listeners and I think it's no longer true. The majority of our listeners were internal to Microsoft. Yeah, that is no longer true. And we were talking about to at least half of the members of the A B podcasting testing crew no longer work at Microsoft. It's true. When we started this, we were talking about this as I recall, we're talking about the similar things, but the context was different because then Microsoft was going to undergoing the transformation and that we are now seeing the rest of the world undergoing. That's how I kind of view it. I don't view it that way because there was a there were a lot of companies going under that transformation and then Microsoft then did it mostly wrong. And we talked about that a lot and they've sort of bounced back. A lot of the big changes around it's not HR, but around organization or yeah, I'll call it organization. How to organize a team have been very Microsoft. My experience was very much a pendulum shifting like it was all in on one of the other. It was OK, every testers in Estet and then 10 years later, OK, every tester is a developer without exploring the path of the pendulum. How you get from one extreme to the other. I would say when we first started, I think a lot more of our discussions were around agile. We should we still talk about that a great deal. The other aspect is back then we were expressing an element of fearlessness around the change, but now it's a lot more around validation because back then we were like, no, we don't know where we're going, but we do know everything will be fine because we see the system. We see the patterns and now we can look back and go. Yeah, we were right. So let's let's talk even more about that next time. You have questions about that ping us on Twitter or if you're in the Slack channel, ping us there and we can dive deeper in there. But let's spend a little bit of time on that next time. Yeah, so it is December getting towards the end of the year. I am off to Moscow on Tuesday speaking at the Heisenberg conference. Looking forward to it. I'm confident it will be a good conference because they are very what's the word I should use particular. They follow up a lot. They asked for I gave him my slides, which I'm giving. I mentioned last time giving two talks, a track talk and the closing keynote. And I will definitely adapt the closing keynote based on what I see during the week. But for giving a talk on the abuse and misuse of test automation, which will be largely interactive because when I if I do a track talk and I actually rarely do track talks and I will get up in my pedestal and I'm too I'm just too freaking old to do track talks most of the time. But since I'm already there for the keynote and they're paying my way there and they're taking me on a tour of Moscow, I figure I owe them a little payback. I'm very much I would feel bad if I said no, although they did ask me to do a tutorial as well. I said no to that because I do limit out on things. Anyway, when I give a track talk, I make sure it's largely interactive, one for two main reasons. One is I think there is more learning that happens if you can get people involved and get them to interact. So we will design some tests. We will write some test automation, kind of predicting what they will write. And now I've written it already. But we'll use the numbers app. I was going to ask, are you going to give everyone their own copy of numbers? Numbers is on my slide deck and I'll put it up in the show notes or on my blog. But it is up on GitHub. You can just go to github.com, angryweasel. It's all there. You can look at it. But the main reason, maybe the second reason, maybe not the main reason, main reason is learning. The second reason is because I just don't want to talk for an hour if I don't have to. The last track talk I gave was maybe five years ago at a star. And James Whitaker came to my talk just sitting in the back, you know, watch. I don't know why he was there. And he comes up to me afterwards and he goes, I can't do the James Whitaker draw. He goes, dude, you had the audience give the whole talk. And I said, yeah, that's my jam. So I think conferences are a great place to learn from each other. So if I can facilitate that versus me reading off some bullet points, then that's much better. So what they freaked out about is, hey, you only have like 14 slides for this hour long talk. I said, yeah, yeah, that's good. Yeah, it might be too many. So the difference is, and it's just a difference in approach, is for a track talk, I'm planning on interacting. We're going to have a few slides. We're going to talk a lot. And in fact, I will do a tutorial, a workshop on web testing tools at star east in May. May. And I will probably have 20 slides for a four hour workshop because it's all it's interactive and demonstrations. And we're going to do stuff together with hands on when I do a keynote. My keynote is like 50 slides. Really? I use a lot of slides. Some you only see for about five seconds. Some I'll park on and talk about for two or three minutes. But I use a lot of slides for a keynote. I don't think I've ever seen a slide deck from you that was more than 20 slides. Yeah. I have my keynotes are usually pretty big. In fact, I'm sure you have. You didn't realize they were that big because you just flipped by some because they were just some picture. Yeah. No, I'm thinking about the one where you had the the Avengers and the the janitors with the elephants. Maybe that was that that was probably at least 30. OK. Anyway, so that's coming up. That's the story there. I calmed them down. I said, look, interactive, blah, blah, blah. They said, OK. And they will be there in the audience watching me go. You make sure you fill this time. We will have a good time. I did a lot more work on the test for numbers, the automated test. And it is I think it's a good example of how to use automation or one way to use automation. There are 100 ways to use it. I am, as I've mentioned many times before, not a fan of UI automation and becoming less and less of a fan. As I feel like there's this this is balance. The world is falling deeper in love with UI automation. That statement is based entirely on tweets and blog posts and the massive number of UI test automation tools. I still I still see being created. So as the world falls more in love with it, I hate it even more. Why do you think that is? Because the test tools much better. No, the problems that we face back in the day. I think the problem is, is this we're going back to our transition into modern testing and agile testing. There's a world of crummy testers out there who think their way out is to. And this is the way they put it. I'm a manual tester and I want to get into automation. And they go, here's a tool for you. There are hundreds of thousands of those people who, as you know, the industry will take a long time to get where we're going and to reflect the testing pyramid. And I read Mike Cohen's original paper from 10 years ago, eight years ago on the testing pyramid. And when he points to the top, he says you should write as few of these as possible. Yet people invest all their time. It drives me nuts. It I don't know why it is. But it appears that it's just what we're proposing is just blatantly counterintuitive. No, I think it is. But then I see people I respect agreeing. Maybe not wholeheartedly, but agreeing. When Richard Bradshaw, who gives workshops on test automation, I went on a tweet storm about this and he fully agrees. It's like, oh, but it turns out that it is that test automation term, which can often, which means to a lot of people writing code to automatically manipulate the UI of the application. And if you in his world, he calls it automation and test, I think, or we need a little better terms because I think there is a lot of code we can write to make our testing easier. And if you call that automation, it often gets lumped in with that UI automation and gets lost. I almost think we need a new term for that. Canar calls it tool assisted testing. I just call it testing. It's a slippery road. Anyway, he agrees. There are a lot of people who I think the people I see who are experienced in testing, they get it. It's the up and comers. And this is a gross generalization, but the up and comers who are, quote, getting into automation are the ones who want to dive into manipulating the UI and living with the pain themselves and driving their teams to live in that pain. I just, I just, there's so much better use of your time as a tester. I could see some influencing or influencers on this topic, right? The we've talked about this before. There's reason to believe that the rest of the world, at least in my view, there's reason to believe that the rest of the world is following down the path of a unified engineering model or the reduction of a separate test team. We still see a lot of folks on in the Twitterverse talking about manual testing still being a dominant thing. One hypothesis is that these guys are, they're constantly under threat by, by, by the phrasing of not technical enough. And so in a world where you're accused of not being technical enough and you're moving towards a unified engineering model, then you, I think you start looking around trying to find how do I make myself valuable? And you go, oh, automation. And I've been doing UI testing. So the next obvious step for me is to automate my job, which I mean, you and I have talked about this. We agree with that approach. And it is in fact, this path you're going down is in fact, what my keynote is about at, Oh, is it? Heisenberg. Gotcha. Yeah. And so I actually, and it's because, and that's probably it is because there's a high demand for this space. People are willing to sell it, even though those of us who are, have experienced in this space, which is probably not the universe of manual testers, don't realize that these problems are significantly harder than the majority of our, of our testing problems, or at least automation ones. I'm being distracted by Alan monkeying with. So I had to pause because windows made by, I got paid by me. Screw you. And I'm going to keep this in the podcast. But of all the teams I worked in at Microsoft over 22 years, by far the most dysfunctional and Steve Rosen exception. He knows what he's doing. But what a piece of junk. I'm just so frustrated. Anyway, God, it just pop up. It's galore. I'm trying to record a podcast. That's awesome. Really happy with that. Really, really happy. So where were we? I just saw an article written this week talking about testers staying irrelevant in the new world. But over, you know, I was generalizing earlier, but over gross generalizations things, the article saying things like some teams think they can improve testing by firing all the testers and hiring just developers. I guess if you squint and back way up, you can look at it that way. But and I'm sure some pointy haired boss somewhere will say, oh, that makes sense to me. That's what I'm doing. But that's just kind of a stupid way to look at it. I know I could. And I'm not I'm not getting. But it is not. I mean, it's just you have to have the talent to be able to do that. Sure. And I'm in and I have seen that same approach implemented by idiots. And I have said this before and I'm and I'm sure I'll say it again, but I have told my boss and his boss that if I do my job right, I work myself out of a job. And I'm working. I am slowly embedding my team in development teams, not just as a member of my team, working for them as a member of their team working for them. So I am shrinking the size of my org slowly but surely while growing the size of my community. And that's and eventually I will just run a community. And then eventually that community will become self-sustaining. It will. That that's my goal. That's how I want to run. That's how I want my modern testing org to work. So eventually, eventually, I would expect all of my team to work for and be partnered with the feature teams they work with while I still run that community, help them discover ways to learn new things, facilitate learning across that community. But as they get better at that, as I get better at working with not only them, but with the developers and product managers they work with, that community becomes self-sustaining. And at that point, someone has to give me a new job. What I see you doing is you're slowly but surely within your org. This is my view. Slowly but surely killing the concept of modern tester because what I see you doing is boiling the frog towards an actual unified engineering organization. Yeah, but a good one. Yes. A very good one. So you're maintaining, unlike what I've seen several dysfunctional teams in Microsoft, you're maintaining control of the test org while you push towards this unified engineering model. And a big difference to what I see among a lot of people in the web verse, I'll even go beyond Twitter, is that I'm not afraid of my job going away. No. In fact, I embrace it and it's a goal. One of the things that I have learned, like this has been my secret sauce in my entire career, is the more I automate my job away, the more valuable I become to the business. And the point is, one way to look at this is I'm growing and learning too. So at the time that my job becomes irrelevant and I'm no longer needed, I want it to go away because I know there'll be something different or better. You'll be grateful because you're like, crap, all that old world crap is slowing me down. Now I can go all in on this new thing. Yeah, and it's not even just a shift like that's done. Here's a new thing. As the community becomes self-sustaining, I likely already have my eyes on the new thing I want to do. And so that's what having that sort of transition excites me. So I just don't understand and it freaks me out and it doesn't freak me out. I'm annoyed a little how much people dig in their heels and say, no, I'm not going to change. I could just do what I've been doing forever and it'll be fine. Sure. I guess it's not the life for me. And then one other thing before we actually begin the show is ... There is a saying, right? Knowledge is power, but ignorance is bliss. So pick which one you want. Absolutely. Absolutely. 40 years ago-ish, I used to go down to Tower Records. I remember Tower. Tower Records. There was one in Limwood. I sometimes go to the one in the U-District. Silver Platters still exists. It does. But Tower was awesome because thumbing through albums with album art and the days before the internet when you couldn't look up and see when a band had a new album out, I would go for the bands I liked. Every week I'd go and I'd check out and see if they had a new album coming out. They had a magazine there. Maybe it's called Tower Records. Probably had a name. I forget what it was called. But in the back they had Desert Island Discs. As far as I know, this is the first publication to talk about Desert Island Discs. You could only take 10 records. I think it was 10 for them. To a Desert Island, what would they be? And they were always albums. Almost every single one had Dark Side of the Moon on it. So the episode of Tester's Island Discs, the 2017 and going on version, Episode 5 came out last night. Did it? Which includes me and my eclectic but interesting selection of music and some stories. And I had a great time. I'm grateful for Neil having me on the show. But you can check that out too. I'm excited to have that out just in time for the end of the year flurry of podcasts and conference speaking. So there's a plug there. Plug away. Okay. Are we done with the intro? Yes. Okay. We're going to kick it right off with Marcus had a question for the... Mailbag! Yes, Marcus. We might have to get a new mailbag sound in 2018. That one was funny a long time ago, but I don't know. Listeners, tell us what you think. So I'll read one from Marcus. Okay. He had read an article from Bob Martin, Robert Martin, on cleancoder.com on code is not the answer. And he asks, any chance we can have your opinions again crafting great about crafting great code on a future podcast? What can the modern tester do to help if at all, and how does this tie into how much separate teams of testers test the product? I don't know, Brent. Do you and I have opinions? We do. I do. Do you want to start? I mean, start. I can start. Okay. And if I remember, I'll put the link in the notes to the article. To the article that inspired it. So just looking at the... I read the article. I'm trying to recall the salient points. If I had prepped it, I'd have written it down. We don't prep here on A-B testing? No. But what I recall, the general theme, the name of the blog is tools are not the answer. Although the link to it is code is not the answer, which is interesting. It seems like he renamed it afterwards. Correct. He did. Right. And he said, hey, people doing all of this stuff, and it's resulting in the square root of nothing improvement. But it's people claiming it. And I will first say, yeah, that's true. Tools are not solely the answer. There is always, I'll call it the process trinity. If you, every time you want to influence a change, you need to simultaneously attack three things. The people, the process, and the technology. If you just create a new tool, and the people don't use it, because you haven't changed the process to incentivize this, you're going to get nothing. So you can't focus on one of the pivots of that trinity. What Marcus asks is around, okay, recrafting great code. And then on separate teams of testers and the modern testers. So the first thing I'll say, in my humble opinion, modern tester does not focus on code correctness. I think the modern tester actually plays a very large role in crafting great code. And one in addressing that trinity, and two in identifying other ways in which the code may improve. And let me give you a crap ton of examples. Okay. One, I'm going to refer to your trinity, which is absolutely true. One of the things I've challenged every person on my team to do, if you're listening and haven't done it yet, it will come. One of the things, I don't like individual metrics, but I realized I don't like individual metrics that are shared. So in other words, I want to give, I ask you to measure something, but don't tell me how you measure. Don't tell anybody else. Just so you can measure your own progress. If you game that, then whatever. And the example I use for that is Brett makes faces here. Oh, I need the eye roll sound effect. Where's the eye roll sound effect? That wasn't an eye roll. That was a... What the hell? And I will explain this better. So I'll pop out of the tangent. That's a brief, this is going to be a circular tangent. This is going to be hard to get out of. I will challenge you. I'll just talk to you, Brent, as if you're on my team. Okay. I expect you to make the developers on your team better testers. Okay. And I'm going to leave it up to you to how to measure that. And some examples may be code coverage, number of tests to say right. It can be a bunch of stupid things like that, some combination. You figure it out. Don't share it with them. Don't share it with me unless you think it's really interesting. But have some measurement. If your goal is to, you have a goal to make the developers better testers, you should have an idea of what makes sense for you in your context in your team. And I don't care about the measurement. That's what I mean about that individual measurement. I don't care what the number is. I care about the output at the end. So I want you to measure it so you can measure your own progress and that's something to drive towards. But I don't care about the number going on with that. That's what I mean by the individual metric that I don't want to know. I want you to make the developers on your team better testers. Okay. Don't just write, going back to the Trinity, I almost got the stack popped here. Don't just write a testing tool. What I've found is, or an automation framework, if it's not brain dead easy to use and part of the system and or part of the system, people probably won't use it. For example, if I, as Brent Stairs said, it's fun. The funny face was for his phone, not for me. So if I write a framework and I tell you it's great but it's hard to use and I can't figure it out and as false positives, you're not going to use it. It doesn't matter. If I make it so brain dead simple for you to use that you can't use it, they can't not use it, you're going to use it. So there's an example there. I have spent a big chunk of my career, talking about code craftsmanship, implementing static analysis tools in code. Way back before we even had some of the more fancy static analysis tools at Microsoft, I would find a bug like, here's a for loop with the semicolon that's going, this is C code. Those of you that are not old like me, a loop. Java has semicolons. Basically someone created a loop where the actual, what they intended to be the inside of the loop would never execute. So I just wrote some Perl regx stuff to grip through a whole bunch of code and see if that existed elsewhere. This was back in 1996. So you scanned for a while one loops? No, actually excluded those. Oh, okay. Because those can be valid. I found. Anyway, baby static analysis. Found a bunch of bugs, file bugs, stuff got fixed. Then static analysis tools came to automate sure those were implemented, not just as something that was run outside of the like, oh, run this when you have a chance and fix the errors. Run it as part of the build process, part of the check in process to find things. I learned coding in a backward way. I, before I was really very good at coding, I read books like writing solid code and debugging the development process. So I, not being as good of a coder as my peers, as far as knowing all the language idioms and things like that, I proved my value by knowing a lot about how to craft good code and how, what well constructed code looked like. So I came at it from that angle and then pushed and spread that through static analysis, through code review, through making sure people could write tests, making sure people thought through the system of what they're trying to implement based on that learning. And that helped the teams I work with craft better code. At the end of the article, he talks about, the paraphrase here, he's in a room of developers and he asks them how many had written unit tests and nobody puts their hand up. He said like two out of 20 or something. Okay, alright. Artistic license. Yep. Which is frightening to you and I, but when you look at the industry overall, it depends who he's talking to. It's not unexpected. Not unexpected. One of the things I think, if my role as a modern tester, maybe this is, as Brent and I further define, what are the principles and actions of the modern tester, to me, one of the things they do is they make the people around them better testers. I'm not even going to say developers. They make the people around them better testers. And that includes from unit tests all the way up to exploratory testing. So I think if I'm the modern tester on that team, where nobody's writing unit tests, some of the low hanging fruit I go after right away is not just telling them to write unit tests or setting a mandate that they must write unit tests, but helping them discover why unit tests are important so they want to write unit tests. So there's a lot of discussion in the test community around testers being specialists. And what you just talked about reminded me of one of the things that I do actually believe in. And I don't think I've talked about it before. And that is, if you do have a specialist on the team, their role is to share the knowledge, particularly if it's something that's general, such as testing. The one of the things... Can you be a specialist in a generality? No. Okay, I'll hold on. Like, you're talking about, hey, make better testers. And I realize that I have executed similar themes, but I do it a different way. I operate under the assumption that a tester, let's say a test specialist in this team, okay? First thing I will say, the test specialists on this team will not do your testing for you, okay? I'm not going to spin up the dysfunction loop. True, right? So that's one of the problems I have with how Marcus has phrased his question, is how do we do this modern testing with several teams? And when I saw that unit testing statement at the very end, I'm like, yeah, I'm willing to bet you anything that those developers, that a strong or a large number of those developers are trained that tests owns testing. Sure. There's that... Yeah. And this is one thing we have talked about before. It's that codependent feedback loop of... I imagine the tester going, oh, you haven't tested your code at all? Oh, I'll go test it for you. Yeah, thank you. Here are some bugs. Thank you. I feel validated. Like, it's like, no. You're definitely hit it on the... You hit the button when you said that the role there is to help them test, not to do the testing for them. Correct. And that is the... Okay, good. So we're on the same page. Of course we are. So far. Important to bring that up. But wait, I got a couple more things I want to say there. No problem. The next thing is... You say I want to make it better... I want you to make your devs better testers, right? I would actually... If I did report to you, then I would say, okay, great. This is how I'm going to roll it out in my team. And what... The way I would approach it is I would look at what are the goals for test. And then I'm going to go and analyze how dev does their job. And either by changing the technology, people, or the process, make it hard... I'm going to add friction to the easy paths and I'm going to make it hard for dev to fail. That's where I'm going to start with first. So what you were talking about. The... All right, static analysis is super valuable. No developer is going to... Devs are going to focus on building their own features. Let's get a static analysis tool so that when they do a check-in or when they do a private build, they get a bunch of free feedback instantly. Right? So those type of assets. The other thing I'm going to put into play is I'm going to remove every stabilization period and every friction point I can from check-in to prod. Because the biggest part of the problem here is the people side. Devs have been used to not being accountable for bugs in their own code and it's time to get rid of that. So look, dev, when you think it's ready, check in. Knock yourself out. I'm going to add no prevention model between their check-in to going to prod. But, and if you're afraid of the risk there, cool. Let's talk and figure out what you can do to solidify that. One, our devs, to varying degrees, are actually pretty good at... They're moderately good at testing also. They don't have... They all write unit tests. So we're pretty far down that road. But one thing you... You started with an assumption that for maybe a more junior modern tester is worth pointing out is before you use... One prerequisite to going down this path is, what's your life like? And again, nobody's afraid of like, well, if they're doing all the testing, what am I going to do? But I imagine... So what happens? Imagine your developers are really good testers. Then what's your role like? So far, nobody is freaked out and thought, well, I don't have anything to do. Because there's all kinds of stuff to do. Friends making a funny face. I don't understand the connection between what I just said and what you're doing. It is, but I'm starting with why. Okay. To quote Simon Sinek, I think this works really well here. Why? Is talk about why you want the developers to be better testers or reflect on that. So if the developers are all much better testers, what does my world look like? Think about that first and then figure out what you want to do. You kind of, inherently because you know that, you skipped bringing that up. So I wanted to bring it up, even though I did poorly at the beginning. Thank you. I think the things I would focus on, just to sum up from my point of view, is number one, I would... In order to make that team better, the dev team better testers, I would remove my testers from Critical Path. Right? To do so otherwise is just inviting the dysfunction loop to just persist. And that is the number one thing, in my humble opinion, that is the number one thing that prevents success here. It's that intra-team process. So I agree. I want to make sure you hit a little bit the second half of Marcus's question. Oh, sure. Which is, how does this tie into how much separate teams of testers test a product? And as you know, we're not fans of separate teams of testers. But say you have, for example, a large desktop product that ships quarterly, and you need, and for better or for worse, you need that stabilization integration period at the end of the product cycle. So I'm testing at the end before it goes out. I still wouldn't do that. Do I have to have that requirement? You don't have to. Okay, thank you. So, but your statement leads to my better answer for that, which is, over time, if the developers are better testers, the need for that decreases. So the goal then is to reduce that time into zero or as close to zero as possible. Yeah, there is a variant of Kanban that integrates multiple different teams, where you have dev finishes coding in a small story, that goes into a queue, and then test draws from a queue and validates. And in that model, tests often ends up being sort of acceptance testing. And it's still one small chunk that gets checked into main at a time. I would get rid of the stabilization period. I would keep the zero bug goal. I would add a new thing, a new process that says, hey, we're going to integrate into main one thing at a time, and main is always RTM-able. In addition to that, one of my favorite tricks on this is you, is you tell the business team, by the way, we're going to proactively communicate what features are in main every time we add a new one. And the dev, the engineering team, will no longer have any say on ship date. So the business team could ship in a quarter, they could ship in four months, they could ship every day. And what happens is then there's motivation on the engineering team to literally keep main RTM-able all of the time. Absolutely. And of course, living in the services world, that's always the case. But I think a lot of teams who have traditionally done shrink-wrapped slash desktop products forget that, or they think it's okay to ease away from that. But I fully agree. Keep main, keep trunk, always shipable, and a lot of those end-of-cycle problems go away. And just because I don't think we've talked about the separate teams on a Kanban before, the one thing that's really brilliant about this is, so imagine you have a dev work column that goes into a test queue column followed by a test work column. If then it becomes super visible, if that test queue column is growing, so dev is contributing to it faster than test can pull from it. Unlike, so a lot of the problems we've had in the past is, oh, well, I'm getting compressed, and or the ratio discussions, how many tests per dev, blah, blah, blah, blah, blah. What this does is it creates a visibility tool where leadership can look at it and say, right, dev, you need to start helping test push these things through. But it's important that leadership recognizes that, oh. Well, transparency does that. No, we need more testers, which is a conclusion you may make from that. When you and I look at that, well, you wouldn't, but someone may make from that. You and I look at that and go, oh, we need developers to have done more testing so that the time it takes in the queue is shorter for these items. Or even if they've done a great job and they just are accelerating a bunch of features. We need to make it a clear statement that test and dev are specializing generalists. And so at times, because our job is to finish work, not start work, if there's a blockage in the queue, we're going to ask the dev team to switch sides and, yeah, they're going to go slower. But it's still going to help the work move forward. Similarly, like it, and I don't like scrum, as you know, but if you have a sprint model where you start and stop work all the time and you have a sprint model with separate teams, well, a lot of times if you have a one-to-one test to dev ratio, you have a lot of idle testers. Well, that's a perfect time where they should be switching sides and helping to write some of the code to get the flow going. Sure. Yep. So if we could shove in one more quick tangent mailbag, I think we can. Yeah. All right. So I was listening to the online test conference off and on. Danny Fott, one of the three, shout out to Danny, gave a talk on modern testing. It's pretty awesome. It was. And then I also heard a talk from Jesper Tutsen. Jesper on shift left, which is a term I'm not. I get what he means. I'm a fan of Jesper and what he said. This whole shift left concept is, or it's not the concept. It's the, I don't like the word, but I did realize, I tweeted this last night, is that when I type, I use the left shift key almost exclusively. I hardly ever use the right shift key. So that's my new interpretation and final interpretation of what shift left means. But anyway, he was asked whether testers who, air quote, shift left, get pushed back from other areas as they generalize. For example, do ops folks say, hey, this is my area. Go back to testing. You know my answer. I'll let you answer first. Probably. So the next question is, and should you care? Well, but one thing that's true in the, what I think is true with modern testers is, especially compared to the traditional tester, is that conflict between disciplines just does not exist. It isn't a dev versus test thing. It isn't a test versus dev ops thing. It is a collaboration. And for anyone that doesn't get it, you can have a conversation with them. You can collaborate with them and they will get it. I think modern testers don't seek out conflict. Modern testers resolve conflict. That's my thought. And I think part of being, this rule is, and not that shift left equates to modern tester, but from our viewpoint, I think that it makes sense to... Did you see all of his talk? Yes, first? No, just the last half. Okay. Because he tweeted out a shout out to the podcast and I don't know. I don't. I wasn't able to see the deck. Oh, I did see that part, yeah. Yeah, what was the context there? Are we left shifters? Apparently. Okay. So on the dev ops, are people in dev ops going to... Yeah, I mean, yes, they are. Of course they are. Anybody who is solidified in the job and sees it, a whole bunch of untrained people going into that role, yeah, they're going to get a little bit defensive. The thing is... I don't think they necessarily will. You can say they can get a little defensive. It's the job... In my experience, they may or may not. You just have to distinguish between the job and the activity. Right? I have not encountered a dev ops problem space where they were sufficiently funded, where they had all of the staffing to handle the problem that they need to. One of the biggest problems in a dev ops space that I've observed, it's actually very similar, in my mind, to the automation space back in the day, where if you get into this infinite loop where you have leadership who thinks that we need to automate everything, what ends up happening if you're not paying attention is that slowly but surely all of your particular resources are being sucked into maintaining the technical assets that you have built. Similarly with dev ops, brute force, this idea of STEs moving to dev ops, yeah, that's important right now because services, a lot of the tooling isn't universalized, the universe isn't generalized across all of these contexts. There's a lot of great tools, a lot of monitoring systems, but who's looking at it, who's taking action? That tooling hasn't hooked up. It's all context sensitive based off of the service that you're building. So you hire these dev ops people to mitigate via brute force. As you do that, you find that people start then optimizing for, how do we corral these army of manual dev ops folks? Because we need to make that more efficient and that more efficient. And that's where I start actually, I'm starting to see a little bit of the same sort of test as function that we used to have. So I haven't seen that. And I use dev ops as an example, or ops, I can't remember if that was Jesper's example or not, but it's an area where I chose it for my example because I think there's a lot of overlap between a specialized generalist tester role and a specializing tester, specializing generalist ops role. They both probably use and know how to use Docker, for example. I think it's important they do. I think there's a lot of overlap. I am working on partnering my team a lot with our ops team on creating monitoring for quality metrics. And it's a partnership. It's not like, oh, that's my job. It's like, let's work on this together. I think that's what I'm going to do, and you help me. While I have seen ops as sort of the dumping ground that test used to be, I don't think it's necessarily the norm. Anyway, to rewind the stack and go back to the point, sure, there can be some conflict. I don't think it's necessary. I don't think it's rampant. I think that in a team where the specializing generalist quality role could accelerate the achievement of shipable quality, that personal conflict is a bottleneck that we can help resolve. I personally have a lot of interaction with our DevOps here in Azure. And I'm doing a lot of things that one could argue belong in their domain and control. When they discover it, I'm like, hey, you know what? You're right. There's one bit that doesn't belong to you. It belongs to me. But everything else, I'll hand off to you immediately. It's about who can get things done. And the thing that I often fall back on is like, look, I'm in Azure Compute, and my business is asking for these results. So if you're going to build it, great. This is the time we need it. If you can't deliver that to the timeline, I need to build it. This is what's being asked for. I'm happy to work with you as I build it, and then we can construct a mechanism that I build it in a way that then eventually hands off to you. That's fine. Yeah, exactly. My goal is to solve a business problem in the time in which the business needs it solved. Yeah, and I think it should be at a larger scale, is that we need to cut our throats, apparently, is telling me. I guess we're out of time. There's people getting it. All right. OK. We got to go because Brent talked too long. I'm Alan. I'm Brent. And we'll see you next time. 
