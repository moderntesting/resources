Hey everybody, I'm Alan. And I'm Brent. And we're here for episode four of A-B testing, because you couldn't get enough of the first three. We're back here again. And we're already at episode four. Very, very soon we're going to get to the point where I can no longer count that high. I know. I know. What are we going to do? We're going to. Oh, you know what we should have done? It's too late now. We should have counted all of our episodes in binary. No? I have a on my floor mat in front of my office downstairs, I have a welcome mat. And it's the word welcome written in binary. And people stare at it. And they go, what? ASCII or Unicode? The leading zeros aren't in place. So they are 8-bit values. You have to count, because the welcome map doesn't wrap on 8-bit boundaries. So you have to look kind of closely. You have to notice that there's 15 characters in the first line. But it doesn't take a lot of knowledge to go, OK. There's a pattern here. If I break these into 8 bits, I can find the pattern. I go, OK, it all starts. They're all kind of the same. And most people, without even having binary code memorized, I shouldn't say most. A lot of people can figure it out. They kind of stare at it for a while, look at me, and go, is that a welcome mat? I said, of course it's a welcome mat. What else do you think it is? It's interesting, given the context of where you have that thing, and that's at your office at Microsoft, that people don't get the binary code. The funny thing is sometimes somebody will mess with me and turn it upside down. Does that make it an unwelcome mat? Well, it makes it just asky gibberish. Whoa! Yeah, I know. I wonder, as an experiment, can you get the hexadecimal version and see if people get that one sooner? I could. I think we're off track. No, actually, the theme of this podcast is to not be ever on track. And we do that famously. So we decided that this is something that we're good at, and we're just going to measure our ability to produce this by what we're good at. Yes, we are. So we do have, we practice sort of lean coffee sort of approach here. We have a list of topics that we kind of dot voted. But dot voting doesn't really work when you have two people. So I choose which order we go in, and Brent patassively agrees. So the first thing I want to talk about is, I don't think we're going to get another recording in before I head off to the Star East Conference in Orlando. So I have little heads up for in the weird. You know what I'm going to do? I am going to promote this. I'm going to put a link to this podcast in my slides, and maybe we'll gain a listener. Oh, wow. That'd be fantastic. Maybe we'll lose a few. So it's interesting. So you're doing Star East. Yes. I do it all week. I'm like a traveling road show for the whole week. And is this primarily to get a free trip to Disneyland? Disney World. No, you know, here's the deal. And Lee Copeland is probably not one of our listeners, so I can say this. Actually, I love the Star Conference. I've had a long history with them. At Star West last fall, I gave a keynote and a tutorial. When I signed a little paperwork for the tutorial, the fine print that I, in hindsight, was very clear. But that I missed is that when you do this particular tutorial, you sign up to do it at Star West and then Star East in the same year. I'm giving a tutorial on testing, where I talk about whatever I want for four hours. I have a format that actually works really well for that. Yeah, I'll just go ahead and share it. Well, what I did for Star West, I should talk through this. I have a little plan here. What I did for Star West is I took questions at the beginning of the talk. I took about 40 questions just to start off the ballot. What do you guys want to talk about? And I wrote them all down, gave them about five minutes off to get a drink of water. I sort of equivalence classed them into buckets and figured, okay, there's about three or four or five talks I can give. How many people in this room? There were about, oh, 75, 80 in the room. So you're basically doing lean coffee with everyone at the same table. It was great to talk on the fly. And I won't go into the classes to questions. They're all stuff I was comfortable talking about. And then I, at the end, we kind of made sure we covered everything. And it worked out really well, but what was interesting is I had two distinct classes of feedback on that session as well. One was, I love the unstructured format. It was great to get my questions answered and have the context of the tutorial, meet whatever everybody wanted. That was one class. The unstructured format is fantastic. I need more of this. And then some people said, I would have preferred a more structured format. The slides did not match the content because I had to prepare some slides in advance. So I'm not as stupid as I sound. This time I made up slides, sort of matched what I talked about at Star West, assuming the questions would be about the same. And now I'm gonna do the same thing. I'll take questions in advance, all equivalence class them, and then I'll go through my slides in order. And on those slides, or like discussion slides, well, I'll basically do the exact same format, but hopefully anticipating, not that the reviewers, the feedback, I take with all of the grain of salt, but that's my plan. So very free form. I want everybody that comes there, if they're coming to see a stupid talk called Alan Page on testing, the least I can do is try and answer your testing questions. A pot? Yes, a pot. Are you going to a pot? To make it even more interesting, I'm taking the red eye Sunday night, and then my talk is at one o'clock on Monday afternoon. So could you just do your slide deck, just be one slide with a gigantic question mark? I could, but I'm trying to be a little bit more proactive than that. So anyway, I got that thing going on, kind of fun. There is a, one cool thing Lee does is lightning strikes the keynotes, and he gets a bunch of people who are already there, who typically give keynotes at conferences to just each give a five minute lightning talk. Doing that Tuesday night. So I got the tutorial Monday afternoon, I can sleep in Tuesday morning, attend some sessions, and then Tuesday evening for the closing Tuesday keynote, lightning talks. Is this sort of a soapbox format? Soapbox format, anything I want, I'm not going to use slides because, one, I'm not really, I can talk about anything, but mostly because I haven't decided on a topic yet. So I'm going to use this time in front of our massive audience to ask Brent, if I had five minutes, if I have five minutes to talk about testing, what should I talk about? Oh, I should talk about anything I want, doesn't have to be testing. I can talk about the- Peaches. Peaches and peanuts and other P words. Peanuticle. Peanuticle. Based on blogs and based on the podcasts and based on feedback lately, I think probably the most valuable thing is what we've been talking about, which is leading change. And- I thought you were going to say, talk about nothing for five minutes. No, I said most valuable. Oh, gotcha. If I were to pull together that talk, the first thing is, it would be something along the lines of, wake the hell up. Someone has moved your cheese, and you guys should be focusing on how you're going to lead your people towards this new world, towards modernization versus the path of the COBOL programmers. Yeah, I think the challenge there is, people are still eating their cheese from the same place, and even if it's there or not. And I think there's a missing step there of getting people used to, things are changing. How we do testing is changing. Okay, so here is the title of your lightning talk. Test is still dead. Test- Test zombies must die. It's been three, four years since Whittaker did his test is dead thing. And we're both friends of James. We know he added his own, you know, drama and excitement to that. But I was surprised with that, how many people couldn't see past the pomp and circumstance, to the actual parts of what he was saying, which were true and predictive of, in some ways predictive of where we're going now. Yep, but test is still dead. But remember what I said, test zombies must die. Zombies are dead, but they're still so moaning around, causing the old world to still stay around, causing friction towards where we need to go. Zombies are, I almost said brainless, but not well known for their thinking capacity, right? These are the people that we need to flush out of the system, which by either killing them dead or figuring out how to resuscitate them back to life. Yeah, but if you look at the world of software, one thing is looking at the context of where all these people are coming from. You have big companies, you have your Googles and your Netflix and Amazon and Microsofts. And you also have, on the other end of things, you have a ton of people who are making and testing software that is just for internal IT apps. It never sees an end user base beyond a few hundred or maybe a thousand people. So the audience, are these generally folks who are concerned about the role of testing or are they testers? No, it's, star is interesting. When Lee, typically, I've been going to stars off and on for a decade, and Lee Copeland usually kick it off. He asks questions like, how many of you have been testing software for less than a year? And half to three quarters of the hands go up. And usually it's someone just been hired as a tester, just been converted to a tester, just part of a company that formed a test organization. They said, we need you to learn testing, go to this conference. That's a star. So there's some weird context there. Well, you know what? That's probably cheaper than existing models, given the ROI. If companies are insisting on having dedicated folks around testing, then yes, they should spend as little money on it as possible. Well, it's funny you mentioned that. I saw a tweet this week in reference to, again, in reference to James Whittaker's three or four year old talk. And the statement was, I'm gonna paraphrase it wrongly, but it's something to the effect of, companies who advertise that they don't use skilled testers for testing aren't making good software. That's not the exact quote, I'm sorry. But with the point being that you have to, I think the assertion was, that you have to have skilled testers looking at a project, for it to be a quality project. How many testers does a company have to hire to fulfill that advertising criteria? I don't know. I don't know. But the point is that you and I know is for a lot of things, of course you need people looking at things. And of course there's a human factor. I'm not going to dismiss that. But you could actually get a lot of information about product quality, and even how useful people find it, through inference from data. And I think that's the- And all sorts of data, pre-production, post-production, customer data, service data. That is a huge leap that, and you and I both know also, that can be done, that's been done for services forever, but you and I are both, we can do that for connected devices of any kind, anywhere as well. I think that's a leap that a lot of people aren't quite ready to make yet. Yeah, no, one of the arguments I get all the time, but we're a client application. We can't do it. I'm like, why not? Because- Because we're a client application. No, that just means you are slow. Well, usually it's worse. Usually not just client, not just their client application. They go, well, we're really unique, and we have special needs that aren't covered by anything else that's been done and proven correct in the industry. Yes, you do have special needs, but perhaps not the ones you're referring to. I was on vacation last week, just as a quick aside, and then I wanted- We are a podcast full of quick asides. Please continue. I was on vacation last week, and the one thing that I did that was work related is one of the, how do I obfuscate this sufficiently? One of the Apple related groups asked me to come and do a presentation. And I got that question, but we're a client. How do we do this? The only thing that's relevant because you're a client is the fact that you have a slower release cycle. And I'm going to argue that that means that your telemetry, your telemetry investment needs to be even greater because you don't have the ability to quickly enhance your telemetry. Even client teams can go into this world of using the data primarily to derive direction. But if you have a slow release cycle, then that means you need to have a bit more precise data. You have less room, a little less room for experimentation and for your margin of error is a little less, but you can still, there's still a ton of value you can get. Absolutely a ton of value. So the on test things, are you, is there any of your topics that you're willing to talk about here today? Like what do you think is, what was the number one question at West? I can put them into two main groups. And I think my slides are associated this way. One is a lot of questions around testers rolling on Agile team. And you and I have talked a lot about that, kind of where that's going. And I think I'll answer a lot of questions about that and how that should work. Remind me, I had an estimation followup from last week. I'll get back to that in a minute. So a lot of stuff about rolling on Agile team. What does it mean? And I have a lot of, you know, lot of advice there. The other one is- Is it essentially C Lisa's book? I definitely referenced the Lisa Crispin, Janet Gregory Agile testing book are some great things to learn there. It's a great start. But just thinking about things like, I love testers being involved in defining what done looks like and having a say in that. And because there's traps that teams fall into, the fragile kind of things of, oh, test is going to test in iteration N plus one, what was written, was coded in iteration N. And you have stabilization iterations. Those kinds of things, we just kind of just, we talk about why they don't work, talk about some leading change and change management and how you get the team all on the same page. It's the any Agile coach, anybody who's been on an Agile team for a while, it's just talking through the people that are new to the process and kind of their problems and those kinds of things. It's been a while since I've read Lisa's book, but it does remind me of a continual debate between Alan and I. And that's the concept. One thing that he and I both agree with, in an Agile team, there is no world for people who are pure generalists or pure specialists. Agreed. What we disagree on is generalizing specialists or specializing generalists. Generalizing specialists. Specializing generalists. Are you great? What I think you need to do on any team is build the right pieces of the puzzle. You need a lot of things done. You need people that you need, you need backup, you need complementary and both complementary skills and diversification of skills. And Brent's having technical difficulties because he can't stop talking with his hands. So yeah, anyway, a lot of questions about Agile, moving on. Then I get a lot of questions. Sorry, the reason why I brought that up though, is Agile, when you say testers in an Agile team, is that a specialist, a testing specialist inside of an Agile team? Generally a testing specialist within an Agile team, but they haven't quite figured out that transition yet. I think when a lot of teams first moved to Agile, they stick with the strict role definition and role responsibilities with walls in between. The walls in Agile more than any place else need to go away. Yeah. Or blend. Blend is probably a better way because you do have generalizing specialists and that's where the blend comes from. I am my role on an Agile team. I'm holding back from diving into this argument now. You know, my role on an Agile team is I am, I'm the testing and quality and often process and efficiency specialist. Those are things I'm good at. And I think it behooves anyone, if you're part of a team, leverage your strengths, find the things you're good at and use those to make the team better. There's additional challenges. So it's great that we're both in agreement that the pure tester specialist must die. One of the things that happens, if those guys are there, then that encourages the need for us or it encourages the dysfunctional stabilization period. And once that bad boy's in there, you're never gonna get out of Scrum or Fall. And one thing you said, I want to clarify in a little bit, is you said the pure testing specialist. You're right in your mind and you're right in general, but I think what a lot of the meaning of what test is and what testers do is so bizarre, bastardized, freaked out. Oh, I saw a blog post today. Someone said, stop sending me recruiting memos for a software engineer and test. I am a software quality engineer. I go, what the hell does it matter? Titles don't mean anything. I wrote a blog post this long time ago. But the point is that you can say, I'm a pure testing specialist, but in that I look at code flow and process and this and that and you actually know you're actually a specializing generalist or a generalizing specialist. I like watching. Great. Did you just agree with me on the air? My eyes go up and down. So wait, there's even more. After my Tuesday night, so that's, and I'll talk about those. I have some career advice that I like to share at these things, especially for new testers. I have things about how to grow in your career. I didn't plan on talking about those at Star West, but I had the slides in my deck and one of the questions was, what the heck are these slides for? I said, oh, let me talk through those. So I wanna be a little bit more purposeful about it and it went over very well. So I'm gonna be more personal at talking about career advice. So that's on Monday afternoon, Tuesday night got the lightning talk. I get Wednesday and Thursday. I could go to Disney World, but being a 48 year old man going to Disney World by yourself seems a little pathetic to me. So I'm probably just gonna do what I, I'll go to sessions. I will talk to people. I'll probably get a bunch of work done because my day job continues. And then on Friday of that week, Star does the leadership summit, where a portion of the people of the conference hang around for one more day of stuff. And I went last year, I was a member of the panel answering random questions from the audience. And I was impressed by the, say it's a leadership conference. So it's, you never know what's gonna be there in test managers and test leaders, but very mature audience with great questions and great insights. It was fun to be part of that crowd. And I was asked to come back and give the keynote, opening keynote, or actually the only keynote for that talk this year. So I'm talking about leadership on Friday. Test zombies must die. And I think the thing is, and again, it goes back to the, a little back to our leading change, actually a big part of it. And if you see my talks before, they'll reference books I like and books I've read, but some of the main points I'm gonna talk about there are, you can't, you know, getting rid of command and control, you know, software engineers are knowledge workers. You can't manage them like you manage factory workers. My big thing is give people a framework they can work in, let them know the rules of the game, be transparent about what you value, and then get out of their way. So you just listed off two of the three things we talked about last time. You said- Accidentally. People- I can't remember that back that far. You said the people and the processes. And you also got to get the right technology in-house to support those other two aspects. The most important thing is, the most important thing to get right is the people behavior. And making sure that they are working towards the common goal, which I believe is all about people moving in this new world. Our job is acceleration. Our job is to look at what is the behaviors of everything that's involved, whether it be dev behavior, or product behavior, or the customer behavior. And making sure that we're clear on what is the goals that are valuable to the business, and that we're helping to accelerate those things towards that. But it's how you do it that's critical. You can be a leader and say, there's a couple things that happen when leaders try and do this, that don't work. One is they tell people what to do and they don't do it. Well, that's command and control, it doesn't work. Right. And these are knowledge workers. We have creative people, we have people who can figure things out on their own. We need to let them do that. So the things that are very prescriptive and some practical advice there is, one, as a leader, be transparent about what you value. Just make sure people know that they're not, I value output, what you get done, I value that you have business impact, customer impact, and collaboration working with others. Those are the three big things I value people I work with. But it's not just value. You gotta put muscle on it. What do you mean by muscle? So you don't have to fight with it. You just say, hey, I believe in fuzzy bunnies. Well, great. But if you don't give the same people a framework for which you're measuring it, or the definition of success, this is how I know if we have achieved it. And visibly recognize and celebrate when people are demonstrating those values. I don't like carrots and sticks. I believe, I'm a big fan of, if you've read beyond the five minute video of Dan Pink's drivebook, people are motivated by progress. People are, when they see things working, people get excited. You don't need to give them, hey, you got all your estimates right last week, here's a $100 bonus. It's stupid. No, but they want to know that the work that they're doing is contributing to the greater good of the people around them, which is generally the teams and the team's cohesive goal. You just mentioned, like yesterday I had a fantastic event. So I, on a weekly basis, attend, it's essentially an exec status report meeting. I thought you were going to say drum circle, but go on. No. And one of the guys presented a report that says, we are able to measure something. And it is only ever working. All of our customers, only 10% of our customers ever have a good experience. So 90% of the kind of customers are using this particular thing. It's all on the floor. That initiated from the VP a 15 minute praise storm towards that individual for being brave enough to bring in that bad news to showing that they have the technology now to prove that we suck and that it was truth-based. The VP was like, finally, we're able to measure the customer impact of this thing. And that is a massive win. And that is happening. This is the first time I've ever done that. I've seen a VP do this. But I'm seeing the incidence of that type of thing happening over and over again. What's important, green means how close do you map to the truth as opposed to how on track are we or any of these other sort of bullshit vanity metrics? You need to walk the talk. You need to walk the talk. If you say you're going to do this, and I'm sure you and I and many others have been a part of trying to tell people what they think they want to hear versus telling them the truth. And until you visibly recognize and reward is not the right word, but recognize and celebrate, this praise fest was a celebration, recognize and celebrate the culture you want, it's not going to happen. That's a really huge part of that. And this VP, just to be clear, the reason why I went on this diatribe, he has made it very clear, his high order principle is that we keenly understand customer impact, that we keenly understand customer pain. And if that's the things your organization cares about, which it probably should be for most organizations, you have to be transparent and honest about that. You can't go into that meeting thinking, well, I don't want to be the bearer of bad news, because that would make me look bad. So I'm just going to tell, I'm going to gloss over it and kind of find the right way to put lipstick on the pig and make it look all right. That's the wrong thing to do. And the other challenge leaders have, they'll say, Ellen, I get everything you're saying, but I have a hard time getting my management to buy off on this. And I say, yeah, I see Brent about to jump in, but that's your job. You as a leader, you constantly need to manage up, not in a suck up kind of way, but being honest and transparent about what's going on and about what your expectations are. It's part of what you need to do. And I hate seeing, and I see us on Twitter all the time, and it bugs the crap out of me, is constantly throwing, I want to do x, but management won't let me. It would be great if we do x, but our management is not supportive. So what I heard you say, what I heard you say initially is, I would love to remove command and control, but management won't let me. It's a while one loop. And the only way to break out of a while one loop is control out, delete, and kill the damn process. The problem with that guy, so I get those type of things all the time. And I go, OK, here's a couple of techniques you should try. Number one, if your management won't do it transparently, it's probably because he's risk adverse. Everybody is built in different ways. I can kick Mr. Risk Adversus ass every time by showing success. Yes. And what you need to do is, can you fund a small pilot project that will resonate with your management that will say, hey, look, this same team tried it one way. They tried it another way. And look, it's better. Maybe we should try to scale this up to everybody else. The Risk Adverse people, they're always going to leverage what they've known that works. They're going to be fond of best practices. At that level, their decision-making framework is entirely based off of their experience. And you're asking them to improve something they have no experience enough, and neither do you. And also, I can share a story from real life right now. We have at Microsoft a lot of people with a lot of experience at very high levels. And guess what? All that experience is at Microsoft. Where I'm getting to is, I've done the same thing for a long time, and they've had success doing that. And they think the way forward is just to keep on doing the same thing. Because that's what got them there. Must be able to get them there. But that's not always the case. If it ain't broke, don't fix it. But what do you mean if it ain't broke? Well, look at our metrics. They've been green for the last 15 years. How do you know if it's broke if you don't know anything but what you're doing? I think my car is supposed to go rattle, rattle, bang, rattle, rattle, bang when I drive it. Because that's the way it's worked for the last 10 years. You don't know what you don't know. Exactly. One of the speeches that I've been given a lot lately is around a heuristic that I've discovered. And I first discovered it with Agile. And it's just simply this. When you try to deploy a tool or a framework within your current team, and it's painful as all hell, do a quick search to see what the world feels about it. And if you discovered that there are a nontrivial number of people who are very happy with it, then odds are the way it's deployed on your team is wrong. Very well put. Is there anything else I was going to say about? Leadership. Cover Star. Yeah, leadership. I think we covered it. So when does Star become Squar? Scar. Not my decision to make. It'll be fun. I hope to meet some people there. I could bring my stuff and record a little BOTUS edition podcast there. I could grab a random tester and interview him. And then we could over-dub you later going, bullshit! We could actually do a link session. No, it makes for coming recordings. Yeah. So Alan is the technical recording engineer. There's a crapload of credits that we haven't given him here. I am the prettier one. His job is to do the technical stuff. Yeah, that's the way my career works. So anyway, that's Star. It's coming up the first week of May. I'll be in Orlando. If any of our three listeners wanted to go there, is it too late? No, I don't think so. I think you just show up and give them a bunch of money and do what you think. I'll be hanging out in the lobby. So it doesn't cost money to see me. But I don't know how it works out. All right. Oh, I'm going to save. I'm pointing at the Kanban board. I'm going to bring up number two as a mail bed question. So I'm going to save that one. Let's go on to Brett. What have you been thinking about lately? Well, so nothing. Yeah, that's what you did last time. I'm not going to go too deep on this. I'll tell you the type of things that I'm focusing on lately is number one, the current shiny thing that I'm diving into is engagement more deeply. So engagement, I've talked many times. What does engagement mean? Are you getting married? I'm already married, Alan. Thank you. All right, so then what does engagement mean? Engagement is sort of the study of how users behave on all sort of aspects of life. If it's the study of, would it be engagementology? Is it always a la-cheek? I don't know. It could be engagement tree. So engagement is customers? Yeah, so engagement is whether or not your customers, in our context, it's whether or not your customers are having a delightful experience. Like one of my favorite stories is that we learned here years ago there was a product we had, and we compared it to a competitor's product. We'd have customers come in, and they would evaluate both products. Our product, people were 90% successful, more successful on performing the tasks. A typical usability study, we'll walk people through doing things like, hey, calculate one plus one, or schedule a media request, or make a phone call, or save a spreadsheet, things like that. This particular product, we were 90% more successful. People were able to achieve those tasks. But the thing that was fascinating was that 70% of the people in that particular study walked out of the room saying, wow, I have to go buy that competitor's product. That didn't work out very well, did it? No, because the pivot was wrong. We were much more successful at getting work done, but the other product was fun. It was engaging. It made people want to come back. So they were perfectly happy being less efficient on getting the effort done, because they were having a fun time exploring, a fun time figuring things out. That's interesting. Logically, that's counterintuitive. When you sit back and think about what products you like, you go, oh my gosh, that makes sense. I want to have fun using this product, even if it is a spreadsheet app or some business-y thing. That's a great point. One of the topics that I'm beginning to dive in a bit more deeply on is this concept of gamification for engagement. And so one of the things that they found out is people like games, all people. They like games. They like little reward systems. Different people like different games. I'm not super fond of trivia games, but I love puzzles as an example. It's a sort of a scientific study around how to use this concept of games to improve engagement on whatever you want to engage on. After spreading the last five years, or two years physically, on Xbox, obviously there is where those gamer points that are worth nothing, that's sort of the root of gamification. And I've applied that sort of concept on lots of different things. People will do amazing things for points that aren't worth anything. So I also used to work in Xbox. And I remember one of the team meetings I had where I explained that games, one of the patterns that I had noticed, is that games were the inconvenient tax that allowed people to convert dollars to achievement points. Because there was a lot of users who just go and buy the $1 stupid game just to pound through it, just to get the extra 200 achievement points. And Microsoft as a business is quite OK enabling that behavior. Yeah, it's interesting though for engagement. I wonder why more applications, more like line of business and not games, don't have some sort of gamification aspect to encourage usage and learning the product. One of the case studies, so right now there's very few platforms doing this. Badgeville.com is the primary platform for enabling gamification. And one of their major case studies is gamifying knowledge management. So you get points for creating a post. You get points for reading posts. You get points for editing posts. And I said, wow, I wonder if that will change the sort of typical Wikipedia contribution percentages. It's like 1% contribute and 90 zillion percent. Just good math there, Brent. Yeah, plus or minus a few zillions. Well, this is why I think the Stack Overflow and Stack Exchange family of websites have been so successful, is they've taken a question and answer forum concept and brilliantly applied gamification. I mean, I don't know if you know how this works. It's not only points you get for being. You get points for answering questions and doing the right things and having good questions that are upvoted, et cetera. But as you get more points, you get more privileges on the site. So the top contributors become basically the administrators. And as you get farther up, you go, oh, I can now edit posts. I can now look at the, without too much, you get fairly involved. And you start getting the, when you log in, you can see, oh, there's been suggested edits that you have to approve. And it's fun. You get, oh, I feel important now. Now I want to be more engaged. So there's a book that I'm reading as well called a Gamification by Design, which I found there is a website. I think it's itbooks.com. Might be IT Dash Books, where they have the book available for personal use only, but free as a PDF format. So you work at Microsoft. Why are you so freaking cheap? I am one Microsoft, and I will eliminate costs as I see it. Anyway, so one of the experiments that I'm going to be doing, incidentally, I buy all my own books with my own money. So when I get fired, I can keep them. But anyway, go on. I buy all my own books with Microsoft's money so that when I get fired, I will still keep them. Different values. Anyway, by the time that I plan on getting fired, those books will be irrelevant anyway. One of the things that I'm experimenting with, so I don't know if I told you, but I have a confirmed new listener to our podcast. And that is my eldest son is now a big fan. It's mainly for me, I know. Yes, it's all about you. And one of the things that he recognized, because I'm about to gamify our chores system. So right now on my blog, the second most popular blog post on my site is the Canban for Chores. It's a system that works very well. It was inspired by Alan. I only tweeted about it. I didn't blog about it, so I don't have any copyright. It's still inspired. I love Canban for Chores. The kids still love it. But engagement on the Chores system has dropped. You know why they love it? I'll tie it back to my leadership talk. One of the big rules of Canban is limit your work in progress, but visualize your work. And by visualizing your work, you see the progress. People are motivated by progress. People like seeing stuff moved to the done column, whether it's work that we do every day at work or chores. You know, with my kids, I don't think that's the main reason. The main reason is because they get to pick what they want to do. Well, the main reason is exactly that. So me and my wife will put a various number of chores into the ready column, and they get to pick. And so when we batch load the ready column, they're in there. They're like, oh, I want that one. I'm not taking that one. The other thing my kids have learned from that as an aside is how to break down tasks. We'll put even homework into ours, but we'll work together on how to take a long thing, like a book report, and break it down into chunks they can do. I'm going to do this chunk of my homework. I'm going to write down the outline for my report. And then I'm going to go pull weeds or whatever. And then I know I can put it off till tomorrow. I can go through it. I can write the first draft. That's another task on there. But then I can break things down because my kids are a little younger. The oldest just turned 10. And they like to do everything, whether it's a chore or homework, in about half hour chunks. And they want to be done with it and do something else. I'll need to talk to you about that later because my youngest son, I think it's a clever idea putting homework up on that board. Anyway, one of the things I do want to do is gamify the board because now what the kids are doing is there's certain minimums that they have to do to get the rates across. And they've lost their initial glossy feel about how the system worked. And now they just want to do the minimum because it's still a chore. And so what I want to do is use gamification to see if I can't improve their throughput rate on how chores get across. Very interesting. I love optimizing processes, whether it's home life or software. We got about eight minutes left here. I want to get to the mailbag. That was fantastic, Ellen. You're getting really good at this. I practice it on the car on the way to work. So the first one is from my own mailbag. And this is one is addressed. Dear Dr. Brent, Mr. Brent. Oh, it's your mailbag. It's my own personal mailbag. We talked last week about estimation quite a bit. They want to dive deeply into it. We didn't talk about it. You made fun of me 20 minutes last week on estimation. I make fun of you all the time. All right, fair enough. What do you hear the overdubs I add to this one? So I had a mail this week and said, didn't quite say, dear Mr. Brent. But I'll phrase it that way. Anyway, the question was, Alan, get to the point. The comment or question was, I want to have accurate estimates in hours or days for my team. And then because I want to do that because I want to hold them accountable for their estimates. What do you think about that? Brent's boiling. I would like to have a personal one-on-one conversation with this individual. And what would you like to say to them? Something along the lines of WTF. So what I heard you say is, I like how the sun's shown in 1996. And I'm going to continue to pretend the sun shines exactly that same way. I want to have accurate hourly estimates. So just to make this extra fun, can you tell me how long their milestone ship cycle is? When he does planning for how many weeks? I'm expecting something on the order of months. Well, and let me finish my conversation, finish the statement here. His team works on one-month iterations. But he does do his, air quote, costing three months in advance. So to the good employees of that manager, I and Alan both have open positions. And we'd happily talk to you about how the hell to get out of hell. So that is absolutely not helpful. So you're saying, hey, I want strong estimations. I want my guys to know upfront three months of advance things that they cannot possibly know. And I'm going to hold them accountable to their estimations, which means in addition to that, I'm going to hold them accountable to them being on track. So what's going to happen in order for them to have that accountability, that guy is going to have massively bloated estimates. Absolutely. And this is my calmer, yet inspired by the same fire response to this was predicting things is hard, especially about the future. Estimates are educated guesses. And it's not only going to be wrong. It's not only acceptable that they're wrong. It's expected that they're wrong. And you're going to learn from them. If you hold people accountable for estimates, 100% of the time, OK, 99.99% because someone's going to be an outlier, you will drive the wrong behavior, which means extra padding on estimates. Somebody working all weekend because they got their estimate wrong. And just everything will be wrong. It just drives me nuts. So that was anyway. That model isn't encouraging. It is blaming people for being wrong. It's not encouraging learning. I believe in a learning organization where you have these, especially when you're trying to predict the future, it's going to be wrong. And it's OK. You can't hold people accountable for it. Like it's like, I'm going to predict who's going to play in next year's Super Bowl and who will win. And if I'm wrong, Brent has to punch me in the face. Yeah. Yes. That will result in my knuckles being sore repeatedly. This is not going to be a happy event for the person who's being blamed. So I did write a blog around the, it's like you included all kinds of things that I'm allergic to. Number one, the word accountability. Accountability, guys, does not mean who do I get to blame. Accountability means who's going to stand up and take responsibility and fix it. You can't top down, enforce accountability. Number two, this guy wants absolute estimates, not relative. Guys, we don't do. Humans aren't capable of doing absolute estimates to any degree, even on an hourly basis. This is not what we're built for. We're built for relative estimation. We're very good at that. Three, he wants to do three month out plan, or he wants hour by hour estimation for a month long milestone for a project that is likely to entirely change in the next three months. All correct. And he wants it to maintain accurate. And he's not building in a feedback system so that when his team goes into the weeds, his strategy to fix it seems to be finger pointing and not encouraging them to learn what causes the delay differential between their estimation and the actual and improve it. Absolutely. So this is fun. We're not going to get to our second mailbag question. We're running out of time. But I want to get some context on this question in that normally Brent and I sort of talk through our list and what's going to happen. And for this particular one, I said, I'm going to bring up a mail I got about estimation. But that's all I'm going to tell you, because I want to get your real time reaction. And Brent, you did not disappoint. Yes, thank you for that, Alan. We can dwell on that some more afterwards. But we're going to close off for now. Thanks everyone for listening. I'm Alan. And I'm Brent. And I'll see you later. Bye. I'm working on it. I see you sitting in. 
