A B Testing, the modern testing podcast. Join your hosts, Alan and Brent. I am mindless agile robot. I must iterate. As we talk about software engineering, software quality, leadership and whatever else comes to mind. Now on with the show. We're back. Howdy. It's another episode of A B testing. Yay. As we count down as Casey Kasem would have in the seventies and probably longer. I used to listen to Casey Kasem every Sunday night. I did America's top 40 countdown with those little and a mighty six 90 with those stories. I don't know what station it was on here. Anyway, we're counting down to episode 100, but we're not there yet. Yeah. We're a few away. I just made myself feel really old. I can the day the pop music channel was on at a.m. Wowzer. So we're back. We took an extra week off. Travel snow, life, work, etc. Yeah, you you have you've you've had your years worth of change in the last few weeks. Have I? I think so. Maybe we'll talk about that in a. Damn it, Alan. I deserve that. I deserve that. I totally deserve that. That was well, well placed. We couldn't have rehearsed that any better. Oh, my God. Yeah. It's been a hack of the first of the year. Mostly good stuff. Took a week off to go skiing, a bunch of work stuff. I'll talk about in a minute. Kind of hangs in limbo, but mostly I think all good. Hey, everyone. Alan breaking in before we get too far to talk about our sponsor on A.B. testing this week, PractiTest. You may have heard of PractiTest from Joel, who is one of the three or from the online test conference, which is sponsored by PractiTest. Joel himself gave Brent and I a walk through this week of PractiTest. And I have to say, we were both pretty darn impressed about what it is and what it does. As you're aware, there's no one size fits all in testing. That's why they built PractiTest to be a centralized solution that supports testing of different types and sizes, whether it's scripted, exploratory or automated, or whatever they work for you. They envision a place where all testing data will reside so that stakeholders can generate the information required for their needs. Another issue they're trying to solve is that you can create and run more tests and more type of tests, and you end up having a ton of data in the system. Having a lot of data can be harmful if you can't control it. This is why they introduced a revolutionary approach to data organization. Instead of burying the data in folders within folders, they allow users to work with a hierarchical filter trees that are based on the same data you're storing. This enables teams to organize information in more flexible or multiple ways, helping to generate better reports and visibility into product and process. At PractiTest, they believe that a tester's job is not to test for the sake of testing, which is true for A-B testing as well. But to create visibility and provide information that will help the team release better products and do it faster. For that reason, having a platform that supports testing is not enough. So they put a lot of effort into their dashboard and reporting modules to make sure anyone can get the relevant information they need in a format that works for them and when they need it. Overall, I was super impressed with PractiTest, not trying to be everything for everyone and being some huge, massive. I'm not going to throw any names under the bus here, but one of those solutions that tries to do way too much. PractiTest is straightforward, simple and very powerful. Of course, having access to the data and using that to form your reports and make your decisions is a great way to catalog your testing effort. So check them out at practice.com. That's P-R-A-C-T-I-T-E-S-T.com. Thanks. And now back to the show. My pact to not speak my policy, my something to not speak at conferences in 2019 was tried. A game company asked me to come speak at their internal conference in Barcelona. It's like, oh, K-boy-no. And the thing is, the thing is, it's like, oh, my God, I want to go to Barcelona. I'm honored. They want to talk about modern testing. I should go. I should go. And it's not like I just this random like I want to I'm not speaking at conferences and that's the way it's going to go is now, you know, months out from actually going. It's like, yeah, sounds great. But every time I do that, the week leading up to it, I think, why did I say, yes, I don't have time for this? I don't want to go. So I'm going to there's always more important things, too. And I like conferences and. So I had a little tangent in my brain that I wanted to reel it a little bit, but I just taken a break for a bit. I don't know if I'll make it all the way through the year without speaking, but I'm going to give it a shot. Someone. Well, you actually tweeted recently that you've handed the mantle of test conferences back to Whitaker. Yes. Would it? Oh, my God. So our dear friend, I mean that truthfully. James Whitaker posted a tweet how after leaving test conferences, seven, eight, whatever it was years ago, he's going to do some again this year and talk about AI machine learning and how they're applied to testing. And the test community, oddly, I thought a lot of them didn't didn't respond well to that. You watched the replies. I did. People were people are still and I'll get the phrase right here off of what I read on Reddit. But the phrase is people are butthurt. That that that James said testing was dead. And my God, you would think 12000 people were in the audience for his talk where he mentioned test was dead. But no, it was star. It was a couple hundred. And what was lost in that is the message that test. Testors aren't dead. Testing isn't dead. Test, as you know, it is changing enough for a lot of you consider it dead. But God, I just it's just I the king is dead. Long live the king. Test is dead. Long live test. Yeah, I think the way the accurate way you described it just a second ago is test as you know, it is dead. Sure. Yeah. Right. And we've been talking about that on this podcast for well over the last year, exclusively almost. Yeah, I mean, almost the whole time. Really. You're right. Right. One of the interesting I went back, I think in 2013, I commented on this after having a chat with Whitaker when he was still at Google. It's almost if it's not dead for you. What are you doing wrong? Yeah. And because you're still living in also we're preaching to the choir a little bit here because A.B. testing listeners kind of get it. They've they've many of them have come to accept what we mean by principle. Number seven, it's not about test being dead. It's just like you may mature enough. You may not need to dedicate someone to testing. Testing's an activity. Yes. Anyway, and consider the whole shipping system. One thing interesting, and I'm going to forget names here of the three. And by the way, one of the three dot slack dot com is one of the proudest things that has come out of this podcast. Yeah, I love it. And thank you, Percy. Yes. First, kicking that off. And thank you, everyone, all three of you who are in that channel daily asking questions, answering questions. It's fantastic. And just starting discussions that are interesting. Someone pointed out. Oh, oh, oh, oh, oh, yes. Oh, yes. Have you learned about super tacos? No. Yeah, I know. I gave one out. How do you give a super taco? So what I'm and wait, this is a joke here. Oh, to be clear, what we're talking about, if you use slack, there's a service called Hey Taco and Hey Taco gave us a sweet, sweet deal on using their service on one of the three dot slack dot com. And if your company uses slack, I highly recommend it. It's just a fun way to give other people recognition. It's freaking awesome. So we've been giving each other tacos. And then so that's the context of what we're talking about. But I've never heard of a super taco, but now I want to know when you get or give enough tacos, you have the ability to give out a super taco. I have not been tackling enough. I actually gave out the first official super taco. That is fantastic. To Chris Kent's for making the last weeks or two weeks a week before last webinar. So easy for me. That's awesome. Yeah, it was how the webinar go. I want to bounce back to conferences in a second. But now that we're on webinar, we're going to go down that road and then rewind back up to the top. Sounds good. Webinar went well. The thing that I so I have not unlike the podcast, I have not been able to re listen to my own the whole webinar. I'm very clear now on how much I'm editing you do for the podcast. And I greatly appreciate it. I thought I thought you didn't notice. No, this they're literally not exaggerating 50 minimum an episode. To be clear, I take a bunch of my own out as well, but not nearly as much as the Brit a master. Yeah, I need to find a way to fix that because I can only I've been listening to that webinar, re listen to it and sort of 10 minute chunks because that's as far as I can take it before I'm like, oh my God. That's the problem with video with your video. It's difficult to edit out the ums. Yes. Yes. Toastmasters, I did a couple of sessions of it a long time ago and that helped out a lot. They have an off counter and they just count filler words. What's the filler word? Um, uh, uh, uh, uh, uh, uh, uh, um, OK, those aren't getting edited out. So anyway, uh, cool. Yeah, recording available on it's up on YouTube or the AST website. We should probably give them a little plugs and say, uh, spot. You talk right now. Um, damn it. Do you just search for Brent Jensen? Um, no. Though that might work. There are three Brit Jensen's now up on YouTube or at least three. One's a jazz artist who I think is pretty good. And one's a painter. And now there's me with, uh, this webinar. If you search for Alan Page, you'll get a whole bunch of the Supreme Court Justice playing football. So webinar format. I was telling Chris beforehand. Webinar format just freaks me out because it is nowhere even remotely aligned with my style. When I'm doing a presentation, I need, I need a sense of, am I losing my audience or not? It is definitely hard. I've done a bunch of them and I'm a little better at them now and just not caring or imagining, but it is hard to not get. See, if you see confusion or I are nodding heads, those things help. I can, I can tell if they're pretending my jokes are funny. Most are. Yeah. The thing I was thinking, though, next time I do a webinar, what I could just do is I could just sit in my home, record it. There's plenty of apps for that. And then when it's time for the webinar, I just hit play. Then I can practice it 10, 15 times and get it right. Cool. Hey, speaking of webinars, the tangent road will return back to conferences. But speaking of conferences, the I don't know if the dates are announced yet, but Brent and I will be presenting something to be determined at the next online test conference for Joel sometime in the summer. I forget when it is, but we're going to do something and we're going to figure out what that is at a later date. Maybe episode 100. Maybe. By the way, episode I'm never going to get back to conferences, but it's something I want to talk about. So I'm going back to webinars. OK. And then I'll close on this very quickly. And that is if you're the type that tries to fast forward through the banter. I'm sorry that we're still talking and not on. This is important stuff, too. So you may like it. It's interesting on the webinar as well. So I don't know if you are how you deal with this. So that content, right, even when I presented it, it was it was heavily influenced by the Unity deck a year ago when I presented for your team. I modified it a great deal because of what I think the audience wants. But the other thing, too, is I can't tell if I'm too high level or too low level for the audience. I get this is a space where for me, I've been babbling about it. It seems like four years. And I don't know how much of the audience is still at phase zero. Right. I'm like, OK, do I go in deep on how to move data from place A to place B? Or do I just skip over that and say Google it? That's actually a really good point. One of the things one of the great takeaways from my time in engineering excellence, right, studied a lot on how people learn was you need to gauge the audience's level of experience. And speaking in person, like a lot of my talks to start off with, how many of you have blah, a little bit more information than how many of you have blah, blah, blah? And from a few of those questions at the beginning, near the beginning of the talk, you begin to get an idea of where you need to make tweaks later. Yes, that's what I do. So but without that, it's like, here's my wall of information. Maybe it works. Maybe it doesn't. Good luck. I'll see you on the other side. Yeah, one of the most common questions I'll ask, like when I do internal presentations, is how many of you have ever directly worked with a data scientist? Almost inevitably, every time someone raised their hand, I'm the data scientist, the reason their hand for. And so now I just turn it into a joke. I'm like, ah, fantastic. Then none of you are likely to know when I am lying to you. Yes. And I can I can vouch for that. I've heard I've heard Brent use that line. That happens. Yeah. So back up the stack. I think we're done with that. Conferences. Can I go back to conferences? So one thing on one of the three.slack.com you can get an you can get the invitation link at modern testing.org is someone who I've ever forgotten posted a talk, a video of a actually I watched a pretty good talk from JS conference, JavaScript conference. And then Tony, I remember Tony because I met Tony. If I met if I met you, I remember who when you say things, Tony pointed out that he heard me say a lot of those things in same things in Philadelphia at Test Bash a couple of years ago, two and a half years ago. Not that I'm not at all. I mean, nothing I say is so new that I like, oh, my gosh, somebody else is saying this. But the cool thing is that the point I wanted to make that is if there are dev conferences where people are saying a lot of the things I'm saying, that is my next venue. When I get back into speaking, I want to start speaking more at developer conferences about modern testing, about quality, about quality engineering teams and not do that as much at test conferences. Test Bash still has a place in my heart. And I will probably be the only the only test conference that I want to speak at again is Test Bash and online test conference online stuff I'll do till the cows come home because I can sit in my home where pants optional and talk about whatever. But as far as conferences to go to, I really like not just as a speaker, but as an attendee, big fan of Test Bash. But other than that, I'd like to. My future plans are to see if I can talk about some of the things I like to talk about and do it for a developer audience, because I think the world is ready for that. I think not the world is ready for me. The world that the software development world is ready for the things that I've been talking about because they're already talking about them. No, I think I think developer and or DevOps conferences are actually the right place for it to be. Yeah. Right. So we should talk a little bit about the cliffhanger in ninety seven. What happens after modern testing and postmodern testing? Postmodern testing is the answer. You are absolutely correct, Sir. Brian Jensen. Thanks. So I think it's in general, the answer is different for everyone. And people worry about like, what if I do work myself out of a job? My theory, my hypothesis is in my experience, which I'll share a little bit about in a moment, is that in the process of leading a team towards following modern testing principles, there will come a point when. Principle number seven, you may not need a dedicated testing specialist, but along that way, you've learned a lot of other skills. No skills are going to be different for every single person. Talk about the T-shaped persona and breadth. Well, hopefully, I mean, if you just this is why we had the principle or the not principle eight discussion. Right. Hopefully, as you've as you've gone through, you have also invested in in yourself so that you're ready for a postmodern transition. Yeah. And if you think about the things you need to do, there's a lot of leadership exercise, whether you're an individual contributor or in a management position, there's a lot of demonstratable assets from your work. People see if you're have led a team along that direction. I would think that people will be looking for you to do a bunch of different things. I was joking with one of the dev managers in Montreal this week, because I talk a little bit about what I'm doing, but what I'm actually doing hasn't been announced yet. So and it may or may not be by Monday when the podcast goes out. So I'm going to speak a little bit in broader terms. And we'll go into details in ninety nine. But I was joking with one of the dev managers. So do you know what you're doing next? And the reason he asked that is more context. We're doing a reorg. We have a new general manager for our vice president slash vice president for our cloud new cloud division. And we're doing some reorganization as part of that. As part of that, I am eliminating the remaining people on my team. My team, I have gone from a high of just below 50 people in my test organization. Right now I have I just moved my last lead reporting to me to another division and I have 10 IC reporting to me right now. But as part of this reorg, I am removing all of them from my org and dissolving my cost center. Bum bum. Some people may look at that and like I've had people go what the F is wrong with you? You're stupid. People are power. You need your organization. I said and it's tough for me because I yeah having getting rid of my organization is a scary thing. But business wise and my priority is improving the business. It is a more efficient way to run all of those orgs. I have made sure I've done a lot of work to make sure they're going to land well. They're in orgs I trust. I've done all kinds of handoff. It's the right thing to do and it's going to work out. So what does that mean for me? Because nothing's announced. Nobody really knows yet. I've had a lot of discussions with my new manager about kind of what the role looks like. And the joke with one of the development managers in Montreal was which was actually good to hear. He said so what are you going to do? And I said I don't know. I mean I have some ideas but nothing's announced so we're still working it out. I'm probably not going to get fired because if you're fired I will hire you right away. So it's good to hear. It's like it's like I've established that you establish that credibility. So I still defining what the role is. You wouldn't be fired though. Yeah. Well who knows. Laid off. Yeah it would be laid off. OK there we go. We'll talk more in details later. I'm sorry I thought when I made the cliffhanger three weeks ago that a lot more would have rolled through but a lot of things happening going on slowed things down a little bit. But definitely in a much broader scope role across the organization which is good not leading testers most likely leading another part of the organization and in a role where I can ensure that modern testing principles continue throughout the organization which is kind of fun to do from a non QA lead test lead position. I've gone through the stages. It's not the same as seven stages of grief. I went through nervous. Oh my God this sounds really hard to excited. Oh but I can do this too anxious. I want to get started. And then in limbo here for a while I kind of like like maybe anxious and frustrated a little bit on come on. Let's just move forward. But it's all the detail. So we kind of know what the role is as far as position in the organization. But as my manager and I have developed a bit of a relationship figuring out well what other is actually really good. I've never had a position where or a manager where we're not just looking at what do you what's the role you're going to do. But what does that role grow into and how do you and we've even had the conversation because I've done this with testing. It's like what would happen if you if in this role you worked yourself out of the out of the out of a role not a job out of a role and figure out what's next. Kind of making sure it's all set up for those things to work out. So that's kind of and I'm sorry to not have a great answer to the cliffhanger to kind of one of those like soap opera things where you get the cliffhanger then in the next episode you don't really get the answer but you just kind of get strong on a little bit more. That's kind of what we're doing here just so I don't because it would be awkward right. If I said I'm doing this job and then I put podcast on Monday people go people listen to the podcast is the weird thing I've mentioned before about unity is didn't happen at Microsoft. People in my organization people I know listen to the podcast in fact like not just testers the developers someone pointed out like it comes up. Odd it's kind of embarrassing for me because I'm used to having a podcast that nobody actually see often or or who has who whose opinion matters of me. Here's the podcast. Yeah nothing against the three your opinions do matter to me but I'll just see a random direct consequence like I'll be in a discussion talking about some things we're doing and then some random developer and another team will say oh yeah Alan talked about that on his podcast and it kind of freaks me out a little bit. It kind of it's kind of awkward not like it's I don't know why it's awkward anyway I've been talking for a while any thoughts or reactions or questions. First off first off I would like to know the name of the developer in unity that that in a public meeting tells you hey Alan Brent would not approve of that. I'd like to know their name I'll go take them to coffee. Yeah I haven't heard that yet. Not yet but now coffees on the line. Well it's good you broaden that to how do you get that guy Brent to shut up. Could that be a question that would earn them coffee. No OK no. So any any praise for what I am trying to do is figure out a way that I can use the podcast to continue to torture you in your day to day work like. Yeah yeah yeah. You can tell them secrets. I don't really have any though. Alan hates tequila. Whatever you do don't give him tequila. Right. No he does. He hates it tremendously. Yeah I get where Alan's coming from. You've got to think about the business. The thing is is while they're working through plans and I do have a little bit more of a sense than what Alan's describing of what is going to happen to him. I'll tell him I'll tell you guys what I told him which is dude that is sweet. I'm sure it is scaring the crap out of you right now. But once you get in it'll be awesome. It'll just be a matter of time. Yep I think so. That's exactly it. When I first got an idea the scope of the role I was quite scared. I said nervous but maybe scared is right. But I thought the more I thought through it the better I felt and the more I've I've talked to people in a little different way. We'll be able to talk about this a little bit more later but I'm settling into it even though it's not official. And what's really interesting is the reaction from my team as I move them out of their QA roles working for me and to working within their dev teams in a specialist role. And also keep in mind that these people aren't doing test things all day. And in fact we just had our we had these three times a year discussions at Unity called three keys three questions about what is your mission at Unity. What are you doing and how are you doing and just kind of talk about just a reflective discussion around career development what's going on. And as I talk to people the amount of leadership and influence and building and their focus on building a quality culture is like yeah we're fine. And I get from the development leads these guys work with I get nothing but praise and the fact that most of them are doing at the very least things that would be that would fall into the DevOps category and some full on writing features as well as their testing work. The confidence that the teams have with them. I feel really good. There's one person on my team. I'm going to have another talk with them today is a little nervous about the change. But the thing is I'm not leaving the org and the other part is nobody's nervous about oh my God is Alan leaving getting fired. They just they which is weird. I'd like a little bit more concerned over my career given all the turmoil but whatever. I think they feel like I'll land somewhere good and they're right. But really all those ICS on my team only one is a little nervous but I think once the change actually happens and they realize how little things are actually changing because my influence is still there. I will still meet with the dev managers around the org. In fact when we do hiring in these places for hiring more people as a test specialist or a primary test specialist they all be involved in hiring and job descriptions et cetera. So it will be it should be very should be noticeable very little that I'm in a different role and there's no more QA organization in the cloud org. I want to go to a mailbag question I totally forgot to do our last mailbag show. We can't fit them all in or I can I didn't just forget to put something we could. I can't tell that's an I don't have my glasses that Aiden Boyd could be all Aiden Aiden Boyd. Sorry. Question for you Brent. I'll answer to how much weight would you say clear leadership direction holds when it comes to growing a quality culture. Oh I'll probably have an answer for this too. I am fortunate enough to work in a delivery team where I am confident that with time and many robust but constructive conversations we can move ourselves up the scale for most of the quality culture capabilities. However despite the healthy relationships and generally positive attitude towards quality within the delivery team there are no clear company goals or metrics that we can evaluate ourselves against. Is this something we should seek out in order to give us a stronger drive for quality and a clearer sense of direction. Yes. No. It depends. No. This one does not depend. However despite. All right. So I'm reading at the same time I'm glad I loaded it up because I would have remembered three words from that. How much weight would you say clear leadership direction holds when it comes to growing a quality culture. How much. What do you think clear leadership direction means. I think a vision. As the question evolves its goals I think he's saying should leadership provide quality goals that the team tries to achieve. How much how important is that. Yeah I kind of interpret. However let me go to the next thing because I think I interpret this differently. However despite the healthy relationships and generally positive attitude towards quality within the delivery team there are no clear company goals or metrics that we can use to evaluate ourselves against. So that's the thing that bothers me because what I hear is it sounds like he's in a DevOps role. Did I do that right. Yeah I'm not certain what role he's in delivery team he delivers babies. Okay if you don't know where you're going any road will take you there. So yeah you need to have. I am a firm believer that you need to have both qualitative and quantitative definition of success. Right. If you're in a role or if you're in a team how do you define success. How do you how one of the things I believe as a business leader is I'm morally obligated to inform my management when they no longer need my team and should get rid of it. Right but to me it is it is a vision statement it is a goal that helps you to be well efficient lean understand success failure these are all important. Well let's go back to the leadership row of my quality culture transition guide where it starts off with there's no leadership support and then leadership drives quality via metrics and score cards right to leadership sets principles and a clear vision and then the most mature level is leadership can mostly stay the heck out of the way because the team has internalized all those things. Even further I think the final step is leadership agrees with the principles and visions set by the team. Yes so I think leadership needs to be involved I mean you can have some overarching goals it's easy to fall from that third level to that second level. And I want to be careful about talking about leadership versus management here. Yeah. I think we're talking about management. Well we're talking about whoever's sitting the vision for the team. Right. Having a set of specific goals defined for everyone in the company around quality everyone must have 80 percent code coverage. Those things those are more of the how having a high level of code coverage reduces some risk so that'd be a how I'd achieve a leadership goal of we want four nines of uptime this year. We want to lose less than ten thousand dollars on revenue due to site being down. Whatever right. Those are high level goals those things below those a lot of those quality culture bits are the how you achieve that. And that vision can just also help you determine how much weight to put into different aspects of the quality culture. When another thing I reading between the lines on Aidan's post is you know our team has a good quality culture. He doesn't say anything about other teams but I wonder if between the lines and Aidan you can chime in on slack if I'm kind of reading like our team has high quality culture principles but maybe other teams don't I think they don't because leadership doesn't have any goals in place. I don't know if that's the case or not but I kind of felt that a little bit and it can be painful if your team takes a lot of time on the how that quality culture putting a lot of effort into learning and improvement and trying to do everything much better all the time and you have some other teams that don't have those same values it's very easy in a lot of cases to say well this is a leadership needs to establish some goals so we're all on the same page which actually isn't a horrible idea but it's not the only way to get every team on the same page as far as quality culture goes. Can you from your experience can you generalize let's say you take two teams. Mm-hmm. Would you be how would you go about enumerating let's say team A in your opinion has a better quality culture than team B right. Okay. What might be things that you would enumerate like obviously that's going to be a multi scaler or multi valued thing. That's actually largely why I created that quality culture transition guide as a way to to have a conversation not to score a team because I hate score cards that I hate maturity models because they're used to score cards but I use that grid as a discussion point to talk with the team on how they approach quality and what are some areas they may need to improve and would and tied into areas of risk it I use that as a tool a lot in retrospectives as a place to discover places where a team may want to improve so that's I don't know if I answer your question directly but that's well ask ask me again in a different way. So what I read from Aidan's post is is hey there are no clear goals or metrics here right and it's it's because number one we're talking about culture which is hard to measure. Number two we're talking about quality culture which makes it even harder to measure when it comes to measurements though what what people generally do is they they want to get a sense if we can't measure it concretely can we measure it relativistically so that we can learn from others I do agree that I do not like scorecards of this sense if they're going to be used punitively but if they're going to be used as hey on lever number one team a is better on quality culture and lever number two team B right it then it helps to encourage knowledge sharing so that both teams can grow together that yeah trick is to measure it without creating competition or wrongly using Hawthorne's effect yeah I I would definitely solve the problem via sharing and celebrating success to be more concrete let's say team a is fantastic at retrospectives and every single sprint they pick one or two things they want to improve and they get better and I'm going to celebrate that because I want to team B is not doing that I want to see how much that's helping them even if team B is better overall and then team B may have they may have put together a fantastic suite of tools and checks and their CI system date of the art I want to share and celebrate that and get team team a may go oh we didn't know we had that problem but yeah if we borrow this we can do that so I want to actually leverage the strengths of each team to make the other team better I love celebrating success yeah I think it's very motivating I don't think to two agents questions I don't and I completely agree by the way I don't think this is something that you can you can really succeed with with a with a with a hard fast quantitative measure this is does not contradict the statement I would said just a few minutes ago though because you do need you do need that to help guide and measure and and show improvement in a in a in a real sense I'll tell you a problem that I have that's related to something that you just mentioned retrospective so here in Azure we have a big retrospective process not in the sense that we talk about with a small agile team you can think of it closer to like a old-school post-mortem process but that process says in certain instances you are required to file bugs to prevent this from occurring again okay whatever the this is I'm off you skating on purpose and I knee-deep in a lot of these conversations the issue I have though is from a data science point of view I am trying to figure out a way are we just telling people to do activity or is there any evidence that this activity is actually helping right because we don't have so let's say you have a monitor here's an example let's say you had an outage and and it took you three days to respond because you didn't have a monitor for it your customer actually got impacted and called in now you go you build that monitor but that three-day outage is a one-in-a-lifetime event sharks slam into something that caused power to go out right and we didn't detect it right that two months that they used to build that monitor yeah so a quick story that I want to get to the closing here is that's a common problem in engineering off often we will over optimize on the edge case not just for monitoring I saw it recently internally I'm not going to tell any details but a bunch of smart people got together to review how something was going and they started thinking of these bizarre edge cases next thing you know a team is like trying to write a bunch of code to solve these bizarre edge cases it's like stop stop stop stop stop no this is never going to happen it would be why don't we just prevent people from trying to do this stupid edge case versus trying to enable this thing that is theoretically possible but will likely very likely never happen in a million gazillion years the the thing is it's fascinating and to be fair I love their thinking of edge cases because that's that's that systems thinking you know but you have to actually think about it from a risk perspective as well and once you know the edge cases you got to think around okay how do we generalize these things into a different group like I like like the suggestion of oh turns out if we turn off this whole menu item all of these edge cases go away yes there's a couple of feature take backs but but you know no one uses those we need to help the move from testing specialist or risk specialist we need people that can look at the bigger picture that if I risk versus think of new test ideas yes and actually so multiple times on the podcast I've detailed a bunch of insights I've learned when I went to Bing okay there's when you're in that space it's a game changer testing does not make sense in the context we're used to and Bing everything changes every second oh my god are you saying testing is dead moving on oh my god here you here in Azure there is a different phenomenon edge cases suddenly impact thousands of people because things are we are so popular we have so many different widgets running in the cloud and the system is extremely flexible even the case it only happens point one percent of the time is a large number of important things impacted yeah there's a real problem there what I keep thinking through is okay on I always go back to ROI what is if we if we try to define quality or our quality culture we need to invest in terms of some sort of return on investment and so yeah if you can't measure return that at minimum needs to be fixed in terms of metrics and goals for a for a quality culture sure it can be anyway yeah yes I agree one last thing I want to talk about before we go is the upcoming episode 100 of a be testing someday soon someday soon about a month which will as my if I look at my math correctly when I get the date exactly right but will also remarkably happen very very very close to the five-year anniversary of the AB testing podcast of episode one which if you go back and listen is a piece of junk it's awful some people want to go back and start at one for completion and I tell you stop starting the 40s or somewhere we could turn one into like a two-year prequel and just reset the nut up I am toying with the idea of a call-in show for episode 100 I am going to gauge interest among the three I'll also ask on Twitter will try and get a date set of definitely a few weeks in advance I'll work it out with Brent we do a little bit of a longer recording time I think I can pull it off I'm gonna do a little bit of technical work in between but if you're interested in being part of that show please ping me on Twitter ping me an email ping me on our slack team and we'll try and find some times that work it'll be awkward if like only one person dials in but I thought it'd be fun give it a shot I have no idea how it will work it's probably stupid to do such a big experiment for for such a milestone episode but then I could just call it if it fails I can call it 99 a if we're gonna fail let's fail glory you know I was talking to someone on my team recently and he said the team's being really paranoid very caught they're being says being overly cautious about releases and I said that team that product they need to screw up more and he nodded his head like is the the test room a team yeah so they need to the testing specialist who's actually doing mostly DevOps work said you're the team is just you're not learning if you're not screwing up mess some things up practice rollback don't be so darn paranoid just f things up a little bit and it got the message I mean it's probably a little bit over the top but but yeah we should screw it up just so we can learn what works and what doesn't yes all right man yes good hanging out with you again I look forward to doing 99 sometime soon couple weeks talk a little bit more about my new role if it's still not defined in two weeks then well we'll be par for the course I've been in limbo for a while trying to get things finalized but hopefully we get it worked out all right I'm still Alan I'm Brent and I'm not Brent and I'm not Alan all right bye 
