Hey everyone. Howdy. I'm Alan. I am Brent. And we're back again for AB testing. Episode 82. 82. And AB testing is a podcast. It is. Because yes, you could tell because you're listening to it in your podcast player. Right. And that's pretty much the only reason why. What kind of podcast is it, Brent? Oh, you're going to make me make up the thing now. The AB stands for Alan and Brent. I am a test director, long time tester, both long time testers. Brent is now a principal data scientist at Microsoft. And we get together every couple weeks and we talk about testing sometimes. Agile software, software development, software engineering in general, change management, leadership management. And recently for the last several episodes, this thing we have called Modern Testing. And we have a set of principles on moderntesting.org. And you can read through those and then send us angry tweets and emails and tell us you don't like them or you love them as we get both. Yes. We have gone through the transition at Microsoft of going through and sort of drastically reducing test specialists. We've gone different directions. One of the things that binds this podcast together is that we're strong believers in quality. And we don't necessarily think that the testing activity, the way it's been done in the past, directly contributes to quality in today's world. So part of the podcast is sharing those insights and helping others to understand what we've learned. So in a way, I think we like to think, I gotta think about that sentence now. I think we like to think that we are on the edge of where software testing, software engineering is kind of pushing beyond where a lot of teams are, maybe not even beyond in some cases. In fact, I'm sure there are at least some listeners, some of our three listeners, because we only have three, who think, yeah, old news done that. I've been doing that for five years. But for a lot of people, it's like, oh, I don't know about that. And then for a lot of other people, that's exactly what I'm doing. I don't know that we're on the edge. Like what Alan and I, when we went through this, one of the conversations we had a lot with others, as well as with each other, is around Microsoft not being a leader in this transition, but a follower. But the thing is, I actually think our listeners, I don't think we have too many listeners that are in that ahead of Microsoft place. I don't think what we, I think we do. We may. But I think the dominant of our listeners are folks who see that transition that Microsoft's already gone through on their way. I don't think, and maybe it's me checking my bags at the door, but I don't think it's, I don't see companies trying to follow Microsoft's engineering practices much, nearly as much anymore as they did a decade or more ago. That isn't necessarily what I'm trying to say. I mean, I don't think they're playing father to the leader with Microsoft, but I think they're concluding much of the same things that Microsoft did at that time. Okay. All right. I'll buy that. So last time we did a deep dive into the first modern testing principle and- Ish. Ish. And we'll talk- Once we got to it. Once we got to it, yeah. And this time we were going to dive into modern testing principle number two, and now a lot of people are mad and turning it off going, what? So I wanted to take a few minutes if it's okay to reflect on what we learned from the feedback from the last episode, delve a little bit into that. And then actually I have a totally tangential question for Brent around data and data collection and being data driven that I thought we could discuss on air because I thought it may be a good on air, like for live on the podcast, because I thought it would be good for others who may be in a similar situation to understand my questions and kind of listening on how we get to some actionable takeaways. Yeah. All right. So what was some of the feedback you saw from the last episode that resonated or how would you characterize the feedback we received? Well, first off, before answering that question, let me answer the question I wish you would ask me. Oh. Yeah. You're still made up. Yeah. I have to say, like the Slack channel is really a useful resource. And that's one of the three.slack.com. Go to angryweasel.com, whackABtesting, and there's a link you can join straight away. I like very much that the discourse on the Slack channel is open, transparent, and not overtly critical. Like I've yet to observe anyone on the Slack channel ask a quote unquote dumb question and then have the other listeners come back and say something offensive. I like that that is sort of a pursuit of what's right. Let me build on that. Since adding the link to join versus send me an email to join, which adds some internet anonymity to the process, we're getting a lot more diverse sets of opinions on the Slack team as well. If you totally don't agree with us, I think totally join the Slack channel and start those discussions because that's how we all learn. Maybe it could be we're all wrong. There's several interesting discourses since episode 81 going on where we see exactly that. One of my favorite questions is, hey, there doesn't seem to be a lot of testing going on in modern testing, to which my statement is right. Yeah, but it's good to call that out because we didn't call that out directly. But that was, I think I answered that one first, said something about that and you plus one dip. But there is nuance that we, I guess, internally take for granted or just didn't verbally say that are actually really good points about the principle. So that feedback is super valuable. And by the way, worth a plug here. Ministry of Testing made a poster out of our principles and even had, I thought someone obviously went back and rewatched my test bash talk, wrote up a little explanation of each principle below it for the poster. And they did a fantastic job with it. I thought they did a really good job. Like, hey, wait a minute. We just plotted out the content for the next seven episodes. At least at this range, if we have to get that deep dive discussion about the deep dive, it actually may go on that way. It may be 14 episodes. Yeah, the one thing I think is an issue, the one thing actually when I first saw it, and they did a fantastic job, but then I felt a little guilt slash concerned because I'm like, well, what happens if we change the principle? Well, at this stage, I think it's tweaking. I think we can make things, like we tweaked the one around, I forget, we did a lot of tweaking between there, a ton of tweaking between the original set and this set. I think we'll continue to tweak at this stage. I don't know if we'll discover a new principle or that we'll discover one of those is, we may discover a new one, but we may, I don't think we'll discover like, oh, we don't actually, we don't value this anymore. This is dumb. We may make some word changes because like, oh, you know what? It's read this way in this context, it could make, it won't make as much sense. So let's rephrase it here so it's clear. That may happen, but I think it'll be close enough. Yeah, I'm thinking through and I think the current principles will be fine for at least a couple of years. Yeah. And the thing I loved about that write up, and sorry, I remember we're talking about before I hit the tangents, hopefully you put a place mark in that, is the person that did the write up, I was just so impressed with how well they were, they're a little paragraph below each principle. They really nailed it. I don't know, I couldn't have written, I don't think I could have written a better summary paragraph after each one. I know I can't. I am often accused of saying in 10 words what could be done in three. Yep. What's the quote from Mark Twain? I was going to write you a short letter but I didn't have time to throw you a long one instead. Yes, that's me. Yeah, the, I think, so my expectations around modern principles are that they'll adapt as we discover, right? I think one of the key things that we're saying is that in the modern testing philosophy focuses on adaptation quite heavily. So there's a bunch of things, like I don't know when modern testing dies out. I am highly confident it will, and when it will, I'm highly confident that both Alan and I will say, yeah, stop following that crap. It took me three, maybe, how long until how we test software at Microsoft, I was officially declaring it obsolete and don't read it. I started asking you that I think about five years after you did it. I think it was less. I feel like it was three or four years, like, okay, great, it's done its run. This is a great document and time. There are some good points in here. I think there's still, when I re-skimmed it, I think there are some good points remain in that book, but as a, it was just a snapshot in time. I think, I think you'll, my hypothesis is that you'll go back and you'll go, you know what, a lot of the philosophy is still publicly true or mostly true, but the tactics and how to get there are just completely about it. Yes, I wouldn't. It's heavily focused on testing being a separate functional organization. Something has to grow independently of the rest for software engineering. So you're looking at from that angle, like no God, no. But some of the things I wrote around community with the test leadership team, actually even that is very functional, but maybe the test architect group, which may be a better example of community and connecting people across the company. And actually, it was probably where we first started talking. And this podcast was actually born out of those discussions when I believe it was Michael Hunter that said, Jesus, you guys, you guys should record this as a podcast. And here we are. You know what it just occurred to me? You could, obviously you don't work at Microsoft anymore. Obviously. I don't think you could ship V2 of the book, but it does occur to me, you could, if you were to take like the overarching goals of each of your chapters and target this as a book where your audience is a developer. It'd be interesting. Because I have a chapter on bug reporting. I mean, some of the things are just... So, I can help you with that one. That chapter is cut. So, that's what... So, yeah, that one may just need to go away. It'll be a bug reporting. Write the bug on a sticky note, put it on the board. Look, you found a bug, fix it. Chapter four. Don't worry. It would be a little bit more common. Decide if you're ever going to fix this, or if you have evidence to fix it now. If you do, fix it. If not... That's actually a good idea. I don't know that I'd write the book, but it'd be fun to go through like, what is my interpretation of this general topic now, 10 years later? Wait... The problems are still the same. The book, I was... 10 years ago exactly, I was writing the book. It's not even that old. It actually... There were three authors on that, right? Well... Who was the third? The BJ. So, I wrote most of it. And not that I counted, but perhaps I wrote 270 pages of the book, and then Ken Johnson and BJ wrote about 70 each. I'm wondering if it would be an interesting episode to get the band back together, because I know Ken... I have not heard from BJ in years. I wouldn't even know where to find him. I'm on his boat somewhere. But we can get the two of the three. It might be an interesting discussion, because I'm pretty sure Ken would also agree that, oh God, no. Don't use that book today. Well, and even worse for Ken, because Ken wrote the article on services testing. Yeah, sir. What was it called? I can't remember what the chapter was called, but he wrote the services chapter. And I think he would love to just burn that entirely right now. All right. You want to get to this thing? I forgot we were doing a podcast. Yeah, so anyway, deep dive coming up next time on Principle number two. One thing we'd like to do on reflection based on some of the feedback is we talked a lot about why we thought the principle was important and tried to explain what it meant for... And why we selected it. Yeah, and understanding like... And some of the feedback we got was around our passion around the topic, and they loved it. And I think we'll have that around all these, because we've vetted them enough that we believe them. But one thing missing that we'll try to do with no guarantees is go a little bit more into the how someone would execute on that principle. What are some examples of things? I had it in my notes. I thought we hit some of those, but the audience was thirsty for a few more. So I think if we can get some examples of how that principle portrays in an action as we go through, I think that may help out a lot. I think as we go through each of the principles, we're still going to stick to what it means. Because the problem with principles is ambiguity. Can we just go what, why, how? What it means? Why do you think it's important? And then how I think is going to be very tactical, and it's going to be very sensitive to people's environments. Absolutely, but we can give examples of our lives. Because I can't jump in there and go, here's exactly, there's no recipe book for modern testing. Actually, one of the things I think, because this piggybacks off of a question Percy asked, and that is, what are you afraid of? What's your biggest concerns with the modern principles? And I'll share with, I'll share you with mine. Mine is we start building blind followers of the principles. Right? It. Yeah, that would be awful. It would be horrible. The most important thing here, I made a career out of people not telling me what to do, because they trust me to figure out what to do. And I've thrived in that. I think that's actually, having some of that is really important to follow modern testing principles, because there is no recipe book. There is no, these aren't technical challenges, these are adaptive challenges where you have to influence culture and be, being a leader is so important and being able to influence people to influence modern testing principles. And it's, there's no, you can't just go do a bunch of stuff and say, I follow these or you, yeah. Yeah, we've, I'm allergic to this concept we've talked a long time ago of test zombies. People in tests and dislike, oh, this, I have looked up the expert and they are the I'm following their thing and I have faith in them. And I don't want people having faith in us. No, no. And yeah, if I ever see anyone quoting or using modern testing principles as a defacto argument with those substance to it, I will come down on you. Because I see that is another testing community where I see this happen all the time. And it drives me crazy where they. Where they go, what Alan says. They don't even do that. They just, they, they parrot the words of their leaders without thinking about the context in which they're being used. And it drives me absolutely nuts. Yeah. I want, I want people to be able to defend or fail to defend on their own. Yeah. This is a, the modern testing principles are for thinkers. For sure. Let's transition into our podcast material for the day. Part two, see if I can put this in context. He started off with a Slack message exchange between Brent and I, and he goes, stop it, stop it. Let's discuss it in the podcast. We should record it. I said, okay. So my question to Brent was roughly, I'm still working on getting, some parts of my team are definitely data driven. Maybe not data centric yet, but the areas that I work on in my job span, a whole bunch of different parts of the company. And then I'm still trying to, and in some places, influencing people that I don't work with directly to use data a little more effectively. And I will talk to them. I'll talk to my peers. I may give little mini presentations to talk about a lot of the things we've talked about in the past with that preamble. The question, a question came down to is I'm turning the corner on getting a lot more teams to start instrumenting their code and measuring customer data. But what I'm finding is for many of the teams, for most of the teams, it's difficult to collect things that are actionable. Many of the things are what Reese and I and Brent for sure would call vanity metrics, like, or at least in some sense, like, oh, we know how many active users we have per day. That's great. What action are you going to take on that? And, and the answer may be, well, if we make a change in the active users goes down, that'll tell us something great. What action would you take? I'm kind of stuck there. And that's, that's even, that's a horrible example. And Brent is just calmly waiting to add all kinds of stuff, but that's kind of the predicament I'm in. I want to, I need some advice on how to get people over that hump from just collecting data, which is great and looking at it. And now collecting data that can be actionable. How, how would, how would you advise me through that? So what I thought we'd do. So ultimately the problem that you have is people are creating KPIs. I don't know. And they don't know how to use them. I don't think they're even creating KPIs there. These aren't even vanity metrics. They're curiosity metrics. Maybe it would be good if we knew this. How many times someone click on this? So they're guessing. Yeah. How many times someone click on this menu? Let's find out. It's like, okay, at least you're collecting. So, so actually that's, that's called exploratory data analysis. There are a bunch of great books and I will tell you, every data scientist will start with two things. EDA, exploratory data analysis. Cause you're trying to, trying to understand. All right. You're searching for an insight. How does the data work? Sure. I get that. That's why I have a question. What questions can I ask of this data? Right. And we've had this discussion before. There are going way back there have been schools of thoughts on only collect data that answers your question versus collect everything. And I am not against exploratory metrics and tracking things because you're curious because you can actually eventually get a lot of information from those. So I have a, I have a process tool that, um, I wanted to walk you through very concretely on the air. Okay. That I think helps to clarify that. All right. So the first thing I will, will do is I'll say, I want you to think of a hypothesis. Okay. A question, but a difference in my mind between a question that you want to ask and a hypothesis, is a hypothesis is a statement. A statement of belief. Okay. So what is one? So I'm thinking about one of the features in Unity and I'll try and anonymize it that all of my Unity listeners, I think one of our listeners is from Unity. Probably, probably we'll be able to figure it out, but we have a feature that enables, guess it's public. I can talk about it. So I'll be less abstract in the past. You've had a thing called the launcher, which, uh, is used to pick like the project you want to open in Unity. And now we have a new thing. We're moving people to called the hub, which is a standalone application, which lets, lets you launch different versions of Unity, which is handy when you're a game developer or as a tab, you get to tutorials as a, uh, another feed there. So just a lot more full featured standalone application. But one of the main things, the benefit is, I'll just focus on one thing. It does is it allows you to launch different versions of Unity. So, uh, we already track a lot of things like number of users per day, per week, all those kinds of things already. But the statement I'll make is there should be some sort of improvement we want. The statement I want to make, and you can figure out how to measure this is using the hub enables our enables unity users to be more productive when using multiple versions of unity. We can't measure productivity. So you're going to help me fix that or, or it could increase customer satisfaction when using multiple versions of unity users to be more productive. And you could change, be more productive with more satisfied. With multiple users. So actually multiple versions, multiple versions. Let me, let me actually suggest a different hypothesis today. Okay. Exactly. Because what I heard you say is, hey, one of the key differentiators between the hub and the launcher is that the hub, uh, will allow you to use different versions. So I would say that it seems like a business hypothesis is that solving the different versions problem is beneficial to your user base. Yes. Right. Now, let us look at from a business point of view, uh, hub, the hypothesis is the hub, uh, reduces or improves. Now what's a business KPI? Like from a business point of view, why do you care about solving this problem? Main reason is it's been a big request from customers and we know from, we know from talking to them that it will make them more efficient than trying to keep track of, you know, a bunch of different shortcut icons for different versions of unity. Okay. So it's a known pain point. It's a known pain point. Yes. But is there any, is there any business KPI that you, that you think, uh, gets improved? Is it, um, uh, hey, we reduce, uh, support costs as a result of this. There could be some hypotheses around, uh, people may be more apt to download the, and the new version of unity. If they know that it's easy for them to keep on using the old version for their existing projects. All right. Let's do that. Let's do that. So that I like that type of, so because of, uh, different versions support, hub will boost downloads, which why is that beneficial? Stickiness. Stickiness gets people using, uh, the new features. Uh, so I'm going to change all of this and say, we'll, we'll boost retention. Sure. That's why you care about stickiness retention. So in general, and I will say this in the podcast, they can get away with this. I think this is pretty common at a lot of companies is that ideally you have marketing or product management, thinking of these business KPIs in advance of what they want to happen, but what they want to happen in with new features. I don't think in most cases, this is, I haven't checked with marketing or the product managers, but often it's like based on customer feedback and we, we have a really good relationship with customers. It isn't just like one loud voice. So I trust that feedback was like, okay, they want that. Okay. We're going to go make that happen. Yeah. It's good, but without thinking about there isn't a lot of, this is probably the biggest change I'll go through. There isn't a lot of thinking about what is the business impact of doing this? That's going to be a big shift for us. So it is important to evaluate why your customers care. I completely agree. Customers ask for a lot of things. You can't invest in the mall. Right. And so as a business. Try as we might. As a business leader, you want to try to say, okay, what's the order? What's the thing that we tackle first in terms of, all right, what's the one that simultaneously. And honestly, I don't think it's hard, but just even in this conversation right here, because I'm making stuff up. You're anonymizing a great deal. This is not unity stance. This is what I can tell from observing this. Yep. So I think there's an interesting growth point, again, not just for unity, because it could be that people have thought about these things. They just communicate them well, but I'll figure it out. But I think this is a common place for, and the reason I'm phrasing it like this, so we can instigate some of this discussion. So it's applicable everywhere. But I think for a lot of companies, a lot of software organizations, putting that little bit of effort into thinking about, well, what should happen if we do this? What's my, I expect retention to go up 10%. I expect whatever to happen. You can actually measure that and see if it happens. And it now actually fully occurs to me that we may be accidentally going into an example from episode one, like how do you tie this to the business ROI, right? All right. So that's the next phase of this. So the hypothesis I've written down is because of the different version support, hub boosts retention. That's our hypothesis. Now, what's the difference between, I'm going to take a segue, the difference between an actionable KPI and a vanity KPI or vanity metric. When the actionable one changes, it tells you to do something. Right. The vanity is essentially what I also call a stoplight metric. It kind of communicates status. Sure. But yet you don't really know what's good, what's bad, what's better, what's right. Right. If you have a hundred thousand users a day, is that good or bad? Yeah. Exactly. But if you're trying to drive adoption and you know that you have, and these numbers are made up, say I know I have a million users, a million potential users, and I only have, and I have a hundred thousand users, that tells me something about adoption. And depending on your, your business, that also still could be good or bad. Sure. Right. Some, some 10%, having 10% of the market is a huge deal. But if my hypothesis is, and we're not, we're not talking about the hub anymore. If my hypothesis is I'm using round numbers for evenness, that at least 90% of our users will use the new feature. And I know I have, I can measure that. So I have a statement, I know I have a million users and I have a hundred thousand. My hypothesis has failed. So I can do it. Maybe, maybe. Okay. All right. So, so I want to simplify the next bit of this. The reason why we care about actionable versus vanity, as you call out, is because we want to have our metrics tell us what to do next. It's about just describing the situation so that we can make decisions. Reece talks about pivot or persevere. Ultimately, all decisions come back, come around to, do we need to change a direction or do we have evidence that our desired direction is still the correct direction? Right. The similarly here. Now with that hypothesis. And for context, as Brent thinks, that's a lean startup by Eric Reece. Yep. I believe it's chapter eight. So really what you're trying to do with this metric is, is select. Have it be able to select the appropriate action to take. Even better. Right. So we have a hypothesis. Now I'm going to ask you to go the next level deep. I want you to think if that hypothesis held true, that different versions boosted retention. Okay. What are, let's say two or three actions that you would want to take? That you could possibly consider taking if that hypothesis held true. And I will give you one. All right. No op. It held true. It's what we expected. It is. That's the easy one. That's the easy one. What action? So here you want to think about if we can prove the different version support boosts retention, then options are, okay, there's something about different version support that we might want to even further add. So new features around just this different version support. Okay. We might have found a very specific focus. If we found that it only boosts retention slightly, right, that might be a no op situation. Okay. So it depends on, it depends a little bit on to what level of true it is. Right. And what I would like to do is I often like to be even more specific on these hypotheses. It will improve retention by 20%. Okay. I don't know if you like doing that or not. That's fine. Because then you can tell if it's like, okay, didn't do enough. So is it, I may have to dig in a little bit. Maybe I have, maybe it's because it's there, but the performance of switching from one to the other is low. So I can, I can dig in a little bit deeper. I'm probably, I probably would be on here. So if what action would I take? I may make improvements to the existing features. I may, or I may continue developing. Further investment. Further investment. Yes. That's a good summary. Okay. So I have, I can do nothing. I can do further investment. If it's true, I wouldn't try to think what else I would do. All right. Let's just keep it there. So it's, if we were doing this for real, you'd probably want to spend a couple hours going through this. Right. No. What type of investments would you, would you want to do? What type of investments would you, would you want to take? How would you decide if you need to tweak it? What if you, the, another thing would be, actually, I'll give you another action. Maybe it only boosts, it did boost, but it failed to hit your 20% because it's only relevant to a particular customer segment. You'd want to figure out if you have two major different problems. Yeah. Maybe, maybe it solves a problem for one segment of users, but another segment finds no use in it. Right. So maybe you want to target, target segments differently. Yes. That's good. One, one cool thing about that chapter on pivot and persevering the Reese book is he goes through about a paragraph on eight or 10 different kinds of pivots, which is actually pretty cool. And it would help a lot if I would have had those memorized or closer in my head in coming up with these actions. All right. Again, this is kind of trying to, to get you familiar with how to use this process tool, which is really sort of a self interview. Okay. Now, if the hypothesis proved false, what are the actions you would do? Double down. Double down. Sorry. I learned that at Microsoft. So one, again, still no op is a possibility here. Right. You've already invested this thing. It proved false. Right. So that could be a situation of, yeah, it didn't change retention in any way, shape, or form. We wasted our time, but there's no, it doesn't seem like there's any harm done. So it doesn't seem like it would be worthwhile to revert. But there's another action. If retention went down, I would revert, but you're going to say the same. I could revert. You could. I don't know how to summarize this. The action I want to take is to understand why, why did my hypothesis fail? It could, it could go back to segmentation again. And maybe I want to look, maybe, maybe I lost retention in one customer segment, but gained it in the other. And the overall was, was even. Okay. So I want to understand that because that, that's going to help drive my action. And actually in the, the one thing I like about this framework is that it forces you to go, Hey, if that does happen, like what am I going to do about it? Because, um, this is sort of one of the things that, uh, Reese also talks about is you want to go, okay. How do I learn from this? Like, how could I, one of the, one of the most important phrases from the lean startup in my humble opinion is the analysis of how the retrospective analysis of how could I have changed things to come to exactly the same conclusion significantly sooner. Yep. How could I have learned that we were wasting our time on day two, not on day 282. Okay. Now the hard part. All right. And, and to be clear, if I was being less abstract and if I was closer to this feature, I think I could do a lot better here. I'm positive. But again, again, what I'm trying to do is get you familiar with the thing. Nope, I'm fine. Now, now we get to questions. So we have this hypothesis. We have a bunch of actions. What are the, what we want to try to do is go, what are the minimum number of questions that if we answered all of them would enable us to confidently select the right action. Okay. Okay. So are you talked about we need to know retention or you want to go? Yeah. So are people, what is retention before and after? Right. What's another question? What? Well, this is of its question, but we talked about market segments a lot. So we'll need to know our market segments so we can help dissect, but you don't want that question here. Maybe. So if, if, let me ask you, if you knew the retention before and after, is that enough for you to confidently select? No. Okay. What question is missing? I need to know, I need a breakdown of which customers or which segments of customers are using it. Okay. So retention by segment? Yes. Okay. I'm going to help you as well. Uh, cause you also need usage by segment because it's entirely possible that you, you could get the answers to both of these and then discover that no one's actually, no one actually use download different versions. Yep. I need to know which, which I would need to know of the users using the hub, how many different versions of the product that they launch with it. Right. Or how many, it really, really should be how many launch three or more. And then what is the segment breakdown of those? And then, uh, potentially how does those segments compared to the other segments that use launcher? Right. The simple question would be of your customers, say on a daily basis, what percent of them fall in the different versions versus the, the total. Yeah. And it could really come down to, I mean, we could rewind this whole thing now that we've talked through it and do a better job, but we could talk about the behavior changes in customer and usage and customer experience we expected moving from one version of the feature to the next version. So you just highlighted exactly what I hope you would say. What I'm doing is giving, creating a framework where you can start with this belief statement, begin to structurally break it down. The point of the questions is this, as I call out again, is how do I confidently select between these actions? And as you're now thinking about it, you're going, wait a minute, my hypothesis isn't as good as it should have been. Sure. Then you start all over again. Okay. Yep. I get it. Now, once you have enumerated the questions you need to ask. Okay. Now it's very easy where you can come back and say, all right, what data do I need? To answer these questions. So I'm kind of designing the data pipeline from the top down, but then when you look at it, once you've gotten to the final detail, you can go from the bottom up and design what work you need to do. The one missing bit that I have seen over and over again, but I haven't found a way to generalize this in this example. Once you've completed this process and you're happy with it, it is fairly obvious generally what is the KPI, what you need to track. So it is an iterative process. You go through it. You're able to find that KPI. You can also start this with your vanity KPI. All right. You can say, hey, if I do this, it'll improve this vanity KPI. Sure. But then when you go through the actions and the questions, you'll find that the vanity KPI often doesn't help you do anything with your actions. And so that will force you to go back and fix it. Yep. No, I like it. It's a good plan. Definitely rings true to stuff I've read from Reese. But fun to walk through the exercise because it gives me, because I want to, one of the things, so I put it this way. I generally using a far less structured informal format can come up with the right things to measure after some discussion. But what I haven't done as well, what I want to get better at is teaching other people to do that. And this is a good framework for that. And in that show, what I'm doing is I'm saying, start with your statement of belief, decide what actions you're going to take if that belief is true, then as well as what actions you're going to take if that belief is false. Then figure out what questions you need to ask yourself to address or to confidently select the appropriate action. And then from there, it's easily what data do you need to answer those questions. Sure. And then iterate. Iterate. All right. That was useful? Yeah, I think so. I'm going to... Okay. I hope that was as useful for listeners as it was for me. If not, sorry. No, I think it was. I think it helps. So we're going to go back into, in two weeks, a proper deep dive with the what, the why and the how on principle number two. Yep. That work for you. It does. It's a date. Okay. Anything else? Nope. All right. We're out of time. So once again, I'm Alan. I'm Brett. And we'll see you next time. Bye. 
