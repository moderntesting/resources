Hello again. Howdy, I'm Alan. I'm Brent. Another episode of AB testing. Another principle being knocked off, episode number 84, principle number 83. No, no. Yeah, we greatly expanded the principles between last time and this time. There are now 83 principles. No, principle number three, as those of you who are just catching up, maybe it's your first episode, AB testing is a podcast. If it is your first episode, please stop and go back to episode number 60. You've missed a lot. But if it is your first, thank you. The context of previous episodes will help. But we are a podcast about software testing, software quality, agile, process improvement, leadership, random topics including squirrels with hats and things like that. And most recently, over the past over a year now, maybe longer, I don't know when we first started talking about modern testing. We've mentioned the word for a while, but we've been talking in depth about something we've been calling modern testing for quite a while now. Yeah, we've been talking about it for a long time. I think it is about a year where we capitalized the letters. We were talking about it for a while when I submitted a talk on modern testing to Test Bash Brighton. Anyway. Yeah, I think in terms of the numbers. I'm so prepared with my facts. As usual, I'm sure. I could be a speechwriter for Donald Trump, where facts don't matter. So back on track, we've been talking recently about modern testing principles. We wrote seven of them. They're still a work in progress, but they've actually remained static for the last several months. It's a pretty good sign. You can read them on moderntesting.org. And today we're going to talk about the third modern testing principle. But before that, let's take a moment to see what's going on in the world of Alan and Brent. It's part of our pitter patter fast forward like five minutes if you don't like this part. What's new, Brent? Are you surviving review season at Microsoft? It's almost over. But yeah, I'm surviving. I will probably be sacrificing another weekend to close it out. So at Unity, of course, we don't have that heavyweight blood pressure inducing ego driven review process. Three times a year, we have a discussion where we have we asked three questions. Why is your mission at Unity important? What are you doing to accomplish that mission? And how are you doing? And you told me. About a quarter of the way through. It is such good conversations and little prep involved, a little prep, but just a little prep and nobody on either side has their blood pressure go up. It's great. I love it. You've mentioned this before, but the one thing that you shared with me off air was that there's no written component to this. Yeah, we don't. Yeah. Oh, so literally when you say conversation, it literally is a conversation. It's a focused reflective conversation. I love it. Yeah, it's pretty great. But I take notes. So I mean, they're just for me. Because my job as a manager, of course, is to help them be successful. In fact, after those three questions, the question I always ask is, what can I do as a manager to help you be successful in your mission? What can I start doing, stop doing, continue doing those sorts of things. And I'll take lots of notes there. And that helps me throughout the year. Until we have that conversation again. And things change direction, mission can change, but it helps me do a better job. I really like that aspect. So there's a language that rolls around with my team. I'm not, I'm pretty sure I've shared this before. I am not their manager. I am their MSP. I am their management service provider. Which means they are my customer more than they are my employees. And so, yeah, I'm really interested. How can I help you succeed? How do I support that? So are you aware of the concept of servant leadership? Yep. So tell us very much like that. I'm a big fan of that. And Simon Sinek for a plug has a great book called Leaders Eat Last, which talks a lot about that. That's very much my view. Yeah. Yesterday, as an example, we had a, my team had a morale event. It's one of those escape room things. Have you ever done that? Oh yeah. I've done a couple of them. Yep. I still have not because there was a logistic error and we thought we had three rooms scheduled we only had two. And I'm like, yeah, I want the ICs to go. My manager was like, but I'm like, no, there's only eight people. They go. Did you go to the one right down here? In Redmond. Okay. There's one, there's literally one walking distance from here. No, we didn't go to that one. I've been to that one. Of course, people listening going walking distance. What? Shut up. Tell me the damn principle. Yeah. I did one in San Francisco. The one in San Francisco was elaborate. We did it as almost my whole ads quality team. So it was, it's made for a big group. There were a lot of puzzles that could be solved independently and they, and they require like people solving two different puzzles to share information. It was, it was like two, two plus hours with 12 or maybe 12 or more of us. It was pretty, it was, it was a ton of fun. They, we didn't, they gave us a little extra time, but we got it done. Yeah. No. So I was actually really looking forward to it. I mean, I knew it was going to be something that I would have to wait for a more relevant to be able to go to it. And now it seems like I'm probably going to have to change teams if I ever wanted. Well, you and I can go. Sure. If you're going to do something with unity in your team, invite me along. I'll be happy to join. Okay. And so the other thing, uh, what about you? What's new? I'm a little tired. I woke up at two 45 this morning. Okay. To take my kid to the airport, uh, for like a five 30 flight. He is going with his eighth grade class, just finished two days ago to DC in New York for a week. Nice. And then I came home, slept a little bit, went to the gym, came home, showered, came here. So I'm, I'm a little bit of fumes here, but, uh, everything else is going great. What's the occasion? Um, eighth grade trip. Is there no point to it? Just, Hey, eighth grade, let's go to DC. No, no, no. It's a massively educational trip. They are packed from morning to night with, uh, all over DC for two days, then all over New York for three days. And then, uh, New York city, New York city. Cool. So they're going to have a good time. He's looking forward to the view from the top of the empire state building, which I've done before. It's a long way down. It's pretty cool. Yeah. I've done the view from the bottom of it. Oh, great. That's a long way up. Yeah. All right. Well, uh, anything else before we get started? Where in the world is the weasel? Like we both, we both did the, uh, Oh yeah. I am doing a virtual weasel now. I'm as I've mentioned many times before, I'm trying to cut way down on the, at least the travel I do for conferences. I'm still doing star. I have some gripes about that. I'll share offline, but I'm doing star and at very beginning of October and then PNSQC, which I'm really looking forward to, uh, doing an invited talk there on modern testing. Nice. It'll be a definite huge evolution or a large evolution from what I gave it. Test Bash Brighton, but something along those lines. And then I'm doing virtual world. I did two, uh, webinars Wednesday, one for the online test conference, one for the test automation guild. I'll be doing another webinar with perfecto mobile on Monday, which you'll probably hear about, which probably over by the time this episode comes out, uh, on continuous testing, which overlaps with our pro with our, our discussion today. And then I just, uh, got booked this week. I'm way, way, way excited, especially relevant to AB testing listeners. Uh, administrative test has invited me to give an AMA and ask me anything about modern testing eight o'clock UK time. That's noon here on the, uh, on the West coast. So at a time when most of our listeners can probably check it out live and ask questions and it is on July 10th, just like I said, I'll put a link to it in the show notes. There'll be a link to it on my blog. You can find that on Twitter, all kinds of ways to discover it should be a ton of fun. Here's what we should do. I will do the AMA and what happens on these things is a ton of questions come in that you can't get to. We could probably shove a couple of mail bag episodes full of those overflow questions. Uh, yeah. Why don't you just, Hey, we can continue the conversation, blah, blah, blah. Join the slack channel mail bag channel. Yeah. Right. Or I'll just, yeah, just, or listen to the podcast and I will, and I will just take all of those, anything we don't get to, I will take as a mail bag fodder. We have right now we have a queue on the mail bag. I think we're like 10, 15 questions behind. We are not, we are not that many behind. We have a question around ethics. We have like, well, I thought we'll have like one other question. No, I don't know. Well, then there's one in my email inbox that I haven't bothered to bring out yet. So maybe, maybe next episode, we should, we should catch up on the, yeah, at some point we should do a mail bag episode and then we can continue trucking on the principles. Oh, and I've recorded a new mail bag sound. One of the things people didn't realize is when I do the mail bag thing, it's live every time that I go add some reverb while I'm editing and I got tired of doing that. So I just made one mail bag sound, a little bit of harmony, just a tiny bit of reverb, not as obnoxious, but I will never have to sing that in front of Brent again. So that's the good part. And that's, we should probably get going. It's caught up on the news. I'm sorry if you fast forward to five minutes and I'm still talking about BSC don't care about, but let's dive into modern test testing principle number three. If that's okay, Brent says not yet. Not yet. So there's one thing. So earlier this week, earlier this week, Alan and I were on a panel for the online test conference. Right. I mentioned that a minute ago. Yeah. And so the one thing that I found fascinating. So Joel gave me the data beforehand and there's one. That's Joel from practice test, the host of the online test conference. There's one thing that I actually found interesting. Where, so I did some, some very lightweight data science. I looked at the, one of the questions were, are you concerned about your job stability? And one of the things that I found very fascinating was essentially there's evidence in the data that, that those folks that are sort of snapping to, I won't say necessarily modern testing. We don't actually care if you follow modern testing. If you, if you pick up the same principle and you think you're following lean, great. That's fine. But those POVs or those point of views that were more aligned with modern testing, they weren't concerned about the job security. Those that were more traditionalist were very concerned about the job security. So one thing I take away from that is our work here that we're trying to do, the more that we can get people to see that there is this other way forward, the less anxious potentially they become. I think so. And one of the reasons I'll continue our tangent here, maybe it's not a tangent. I am really enjoying talking about modern testing at these conferences and bringing it up because there are people who are scared. We can't, I don't want to do that. And there are people that go, I want to do that. And a surprising number of people who are saying, yeah, I'm doing that. And it's like, whoa, this is really cool. I wanted to share something, a private conversation that I had on Slack on our one of the three dot slack dot com channel. If you go to anger weasel dot com, whack AB testing, you can find a link to join yourself, but I'm going to abbreviate and Michael, I apologize. I didn't ask for permission, but I'm probably violating all of GDPR. I will paraphrase here. It says their engineering team is adapting to work in a way that is aligned with the modern testing principles. We're doing some of it already, just kind of naturally, but the head of engineering has told the department to get on with it now as testers. We've been pushing it since your talk at test bash this year and a few minutes I'm showing your talk to the whole department. So that I, wow, it's pretty cool. I said exactly that font. It says the video went down pretty well. One of the leaders just said, this just sounds like common sense. It should just be engineering principles, which we should probably address. But I think that's great. You know what? It should be. They're going to monitor the uptake of things, do some more pairing and told them to stay in touch and we'll, uh, and we'll see what happens. But that's, that's so exciting to hear. Yeah. No, uh, uh, hey, if, if, if we're somehow being the last spark of credibility, that it's getting leaders to, you know, flip the switch. Fantastic. That yeah. Anyway, yeah, let's go. Shall we go modern testing principle number three, if you haven't memorized it already, it says this. We are a force for continuous improvement, helping the team adapt and optimize in order to succeed rather than providing a safety net to catch failures. There's a lot in there. There is. Where do you want to start brand? What does that mean? So we're going to talk about not the what, but also the why and the how, if we are, if we're lucky. If we do, if we do it. All right. All right. So I am going to take it on from the reverse of the principles. Well, so what this says is to me, we are going to favor continuously, uh, improving and adapting above a safety net prevention model. And, and actually I realized I said that very agile manifestly. I don't actually think it's even a favor. Like my preference would be thou shalt kill thine safety net. Right. That that's how I simplify it. The way I put it, we talked a little bit about this, uh, before we, uh, press the record button, but this ties to the last principle. A safety net is a great example of a bottleneck. For sure. Uh, and it also doesn't allow it's, it talks, it builds that co-dependency. We've talked about where like, please, here's my code. Please find my bugs. Oh, here's, here are some bugs. So thank you for finding my bugs. Everybody feels validated. But in the meantime, the customer is going, where's my damn software? There was something I was reading. It was a comment chain around something we've done in modern testing. I don't remember what it was. Maybe it was Twitter, maybe it was Slack, but the comment that I'm just sick of is. Well, you know, Dev doesn't want to do test. Yeah. Oh yeah. And I'm just like, and, and right. It's it's, um, Oh, I remember, I remember where it was now. And one of the things I found, I, I did like one thing that's unique about tests, like, and we shouldn't go too far into this, the safety net here. But one of the things that's unique about tests is, is, uh, how many other software engineering roles are you aware of where leaders in the discipline stand up and say, Oh, we're validated because another discipline doesn't want to do this job. What the hell is that? Right. That's that's, that's, that's not a declaration of value. It's not a declaration of, uh, return. It's it's, um, I, again, I think of your, your, uh, elephant slides or the parade slides, it's not a good way to go. It doesn't define the discipline in a positive way. The safety net aspect, um, in the, the dysfunctional enablement loop, um, is bad for multiple reasons. Uh, as, as Alan called out, it's, it creates a bottleneck. We want to be against bottlenecks. The second thing is there's, there's a lot of new techniques out. And if we're trying to finally move quality upstream, right? We're not in the coaching principle, but you know, um, like all good set of principles, they build off of each other. Um, if we're going to get quality upstream, we have to retrain our developers to succeed without the safety net. And that is in essence, what I view this principle. It's, it's not about getting it right the first time. It's about getting the learnings right on the first mistake. Yes. Yes. Don't be so afraid of making mistakes. Don't yeah, I get that. Um, I've been thinking about this principle and then overall, something I shared with you, I've been thinking a lot about what it means to improve a quality culture, which we'll get to in a, in a later, uh, principle, but something that aligns here, I was thinking about that. And then I, I, I tweeted this, this week, which was sadly too many people in teams think quality is limited to testing. And that ties in here, I think pretty well, because we need to train and help our developers do better testing. That doesn't eliminate our job because there's a lot more to quality, uh, then simple functional verification. Uh, internally, uh, one of the things that I have found very helpful is, um, is the language around code correctness versus quality. And so when I, when I talk to devs, I mean, it it's look dude, code correctness. Uh, I'm, I'm here to help coach you through it, but you own code correctness. You own it a hundred percent, right? I'll, I'll, I'll help you brainstorm. I'll help you think through it. I'll help. I'll introduce you to tools. I may even bootstrap you in a certain direction, but code correctness is what you own. And, um, we all own quality. And so I try to weave a conversation in a way that makes it very clear that in this man's humble opinion, code correctness and quality are entirely different things. And that generally works well. And it feeds into the conversation. Well, wait a minute. What's quality then? Subject to point of view of the customer rate. We've had this. I have found that taking, taking that belief and taking it on and decom, uh, what would be the de-overloading, the overloading the term helps the conversation move forward. Let me do a one last comment on that. I want to dig into a few more details on continuous improvement. We're a little bit in by design, the principles compliment and overlap each other. For sure. Uh, this one's a little bit more in how we coach the team, but I want to share a conversation I had just recently around training developers or helping developers become better testers. I was talking to a dev lead who is very much new to unity, new to, uh, new to, new to Alan is subtle, modern testing pushes, uh, subtle. Uh, I can be subtle. Uh, he was very much in the camp that developers can't test their own code. Oh my God. So they're, they're, they're too biased. And my, my answer was my initial answer was, eh, yeah, probably initially goes, well, they can only make sure it works. I can. Yeah, initially, but you know, I've probably taught 50 developers, probably closer to a hundred, uh, who started off that way. I asked him a few questions I paired with them or, or whatever, and they could learn that pretty quickly. They could learn to think about the ways the code doesn't work. They could learn to think about edge cases. It's not that hard to train as typically happens. I won, uh, because I think developers can do a very good job testing the, you need to give them, this is going to tie back to what you're saying. Let them, they have to try it and learn from their mistakes. Yeah. And continuous improvement is about not being afraid to make mistakes, but embracing mistakes as a learning opportunity. Yes. Um, and we don't do that enough. Uh, and that's getting over that hump is so huge and going, yeah, let's, let's get better at this. So we talk about the safety net, right? And, and literally when I, when I bring this up, um, right there, that here, there's definitely a fear. And this is where I think, uh, tests plays a good role. They are the knowledge experts on testing generally in these type of teams. Right. What, what you just shared is that your, your dev expert, uh, your dev lead or whatever this guy was, um, he expressed a belief that, you know, devs can't test their own code. They're not objective, blah, blah, blah. Right. That, um, that to me says first and foremost, this is the guy who doesn't know TDD BDD. And that would be the first place where I would start. So safety net, right? Where else do you see safety nets? Circus and the circus and the tight, uh, the, uh, for example, the tight rope walkers, okay. Now, and so when I talk about a metaphor, you got to think about devs. Uh, when you're going through this transition, devs are used to having a safety net, you know, they're up on the tight rope walker, they're doing cart wheels and they're falling down all the time. Right. And what test has been doing is, is just building and building and building, uh, even better, uh, safety net. Um, and in, you know, every now and again, a tight rope walker falls, falls off, injures themselves and it's not the tight rope walker, but it's blamed. It's generally the, Hey, we had the safety net. Why wasn't the safety net good enough? Okay. Now, what we're proposing is get rid of the safety net because really what's happening is they're not gathering the skills. So I have a patent for a, um, a test harness. I did, uh, I built back in 1998. I also have a useless patent. My thing is actually still an operation. Mine has probably been infringed upon a thousand times, but nobody cares. Yeah, but I think mine too, but it doesn't matter. But the, this, this automation framework, the way we designed it really made it easy for automation to run and execute. One of the key things about it is it's a harness that can execute any test case. So if you are a dev team and you had, or back then it was a test team. If you had a particular way you did your automation and another team did it a particular way that they did their automation, this one could run them all together as a singular suite. It was also very componentized. So it made it very friction free to do automation fast forward about, uh, 10 years. And what we find is dev stopped using design patterns. Dev stopped even doing code reviews. What dev would do because the automation suite was so robust, they would actually just check it in, let the automation suite run. They would get a result back in like 30 minutes and then they would go and figure out what they're trying to do. Sounds like you built a safety net. I built an automated safety net. Yeah. Right. And so I didn't actually succeed in moving quality upstream and, and making stronger code. What I did is I made a system that allowed them to do whatever stupid behaviors in your architecture that they wanted. Now this is bad, uh, because sloppy architectures can't scale. You can't add new features to code like this with an efficient timeline. So what I'm suggesting when we get rid of the safety net, what keeping the metaphor alive, if you get rid of the safety net, realize that part of your job is to help influence and motivate folks to get better at walking the tight rope. Now they're going to be anxious. So we're going to have to do the equivalent of, well, we're going to have to retrain them how to walk this thing. So to make sure that they have a minimal risk of getting harm, let's lower the tight rope such that when they fall off, because eventually they're going to, there's less consequence. This is where the continuous improvement, stepping out of the metaphor, this is where continuous improvement and adaptation, uh, come into play. Right. Yep. A little bit. I want to talk a little bit more about the safety net. I do want to dive into just really dive deeply into what continuous improvement is and why it's important. Uh, and, and how we drive that, how we're a force, but your talk reminded me of the headline that came out when Yahoo got rid of all their testers. Uh, we talked about ways to move to unified engineering, both Brent and I believe that a unified engineering team is by far the most optimal way to ship almost all software made today, maybe all software made today. Uh, but you can make that transition in a way that does not enable your team for success. Uh, this is a classic example of be careful ripping off the band-aid because it may be a turn again, but the headline from that came out. I, I've saved it and I use it in presentations once in a while is Yahoo's engineers move to coding without a net subtitle. What happens when you eliminate test and QA fewer errors and faster developments, say Yahoo's tech leaders. There's devil in the details, of course. Right. Uh, so what you're saying is they ripped the band-aid on that one. They ripped the tourniquet. They ripped the tourniquet. Um, yeah. The article goes on to say by, um, by just getting rid of the safety net completely devs automatically decided how to be more careful. Uh, they need some shocking. Right. And this is again, again, much like the safety net and the type rope, right. If, if they know that they've been goofing off on the type rope and so you take away the safety net, they go, whoa, whoa, whoa. Right. Now when I do a cartwheel, you know, nine times out of 10, I fall this time on the, I'm going to severely injure myself. And the challenge is continuing the metaphor is maybe they've either forgotten or never learned the proper technique that would enable them not to fall, they're going to need a tightrope walking coach, probably some extra training just to help them at first to make sure that they can succeed and not splat on the floor. Right. And then, um, on the continuous improvement, right. It's, it's, it's not around. We got to escape out of the metaphor, but this, this will be the last one I try. Right. How do we lower the rope? Then how do we, how, so that the risk is lower and get them improving one step at a time, like the way you get across the wire is one step at a time. Right. And that in my view is kind of how this continuous improvement is. We're, we're down there. The net's gone. They take a step and we're like, Hey, your foot's not quite the right place. Adjust that before you take your next step. And as this, as this gets accustomed, as they are used to this becomes a new muscle memory, they're not going to need you to do this right. And now escaping out of the metaphor, continuous improvement. To me, I think of, of, of Kaizen here. Yeah. And we're going to get into that in a minute. All right. But I just had a thought like I'm so sick of the word that the phrase shift left. I think it's stupid. I hate it. Yeah. Why left? Why not right? Why not up or down? Could you quit fondling the microphone stand please? Probably not. Okay. That's going to be great. Sorry for that. Good junk, good junk junk. I'm waiting for the new, like, I see articles a year from now. Help your dev team lower the rope. Hashtag lower the rope. Yeah. Okay. I will do, I'll talk about Kaizen. Self-fulfilling hashtag there. I think Kaizen is a topic we need to discuss today. Yep. But in the context of this question, sir, Brent, what is continuous improvement? Um, I'll read you a definition. No, it's like, it's improving continuously. Yeah. Read the definition. I think you'll like it. This is from the Institute of Quality Assurance news flash. There's a thing called the Institute of Quality Insurance, which is probably like two guys in a garage somewhere, but whatever. Uh, they define continuous improvement as a gradual, never ending change, which is focused on increasing the effectiveness and or efficiency of an organization to fulfill its policy and objectives. It is not limited to quality initiatives, improvement in business strategy, business results, customer, employee, and supplier relationships can be subject to continual improvement. Then the, my favorite part is the last part. I should just skip this right away. Put simply, it means getting better all the time. Yeah. It's not a bad definition. No, but all of the time, right? This, this, this, this is, this is not every six months when we do the two day post-mortem offsite. Yeah. Let's do a post-mortem and we'll make a big list of things that we're probably not going to do. That is not continuous improvement. That's not even improvement. Uh, there's some argument that there's an element of, uh, catharsis. What I found in the old days, when we do those half day, full day, two day, uh, post-mortems after a product, it was really just a nice venting session. Yes. And it was a nice venting. People feel better, not get better. Right. Did we already talk about actionable versus vanity? Uh, I'm sure we hit that last time. Yeah. Or, or in the deep dive on Brent talks to Alan about data. Right. That actually, that is when we did it. So a continuous improvement. Um, I'll tell a story here. So when I first shifted to dev, I was a unified engineer. Okay. We started with scrum. Um, this was at a time where I was not actually very familiar with, with agile. Um, but we started with scrum and we made a tactical error. We had come up with a scenario, um, that actually required us to pretty much change pieces in our entire architecture stack. And, uh, of course, just like with any scrum team, all the bugs got introduced in the 11th hour and we came in the next day and found out. Nothing integrated because we had person a working on one piece of the refactor person B working on another piece of refactor. So we had done this in a two week sprint and it literally took us two months to unscrew ourselves. And after we successfully did that, um, my team was like, Hey, uh, can we do this Kanban thing you keep saying that we're going to do? Cause I introduced scrum to them, uh, because I didn't think that they were quite ready to go the Kanban. But after that, they were like, all right, let's do Kanban. This is stupid. And most teams have a much larger transition to get there. That's great. Yeah. So maybe that's it, you know, just completely abjectly fail, you know, your first two sprints and scrum. But isn't that improvement? Uh, it was, uh, I kinda, you know, hindsight, 2020, I kind of wish it hadn't cost me that two months of unscrewing myself. That's a, that was a high price to pay. But one of the things that we did once we started doing continuous flow, got rid of, got, got rid of, um, the scrum model, one thing ended up happening. So I had a team of seven at that time, uh, with enough, um, tickets on the board. What I found is that every day someone on my team was releasing something. Okay. Cause here it's not, it's not major releases once every two weeks. It's no minor release, uh, every single day. And guess what happened when we found that the whole, we came in the next day and it happened, came in the next day and we found our whole system was on the floor. Guess what happened? You had to undo one change. Yeah. Took like 15 minutes. We, we knew exactly the cause cause there was only one thing that went in yesterday, which we could just readily revert. Whereas the big integration story, we couldn't readily revert that. Right. Um, it was too many different changes all at once. Um, it would be harder. Yeah. Yeah. I think that that's clear to everyone. And one of the things that I realized is our role as testers. So remember this is, this was like the first few months of me being in a dev role, having left test. So I was still very focused on, on, um, my heart was still in test. Um, and I'm like, Oh my God, check that out. I way more efficiently reduce the risk by just moving to a continuous, uh, integration model, right? I didn't have to spend old school ways. I wouldn't, I would have done, um, like in the scrum example here, I did, I released it and everything was on the floor and it took me two months to fix it. Okay. The old school model, I would have still spent that two months, perhaps even longer before the release. Okay. So even then the scrum model in my view was better because I knew exactly what I needed to fix instantly. I didn't have to deal with uncertainty, but then moving to a continuous deployment model, I could do one small change at a time, make sure that the system handles it. And if it doesn't, the, I can revert it very rapidly from a test point of view. Again, if our job is to reduce risks of the business, getting this continuous improvement thing, uh, rolling is a key one. Yep. Yep. You already went into a little bit of why, which is good. I want to dive more of that in a minute, but I do want to talk about Kaizen while we're on the subject. I think you, I think as, uh, Embracers of the modern testing principles, Embracers of continuous improvement, I think it's important to be at least aware of the Japanese concept of Kaizen, which is one of the things that comes out of lean, which is literally, I guess literally like make better, but it, it, it's the Japanese word for improvement. One of the things I like about Kaizen, which is probably important to mention is it is not just about how do we improve the pipelines? How do we improve everything? It could be the way the, everything from the way people are treated to the processes that go through, just looking for those every day, all the time, looking for those small things that can be improved. And because it's from lean, it's a big focus on reducing waste. Waste can be, waste can be resources, waste could be time, waste can be, um, actually overlaps a little bit with bottlenecks. The continuous improvement has a lot to do with reducing bottlenecks. Yeah. But, um, so whenever I get in a conversation, when I'm, when I'm onboarding a new set of people to an agile process and we start talking about Kaizen, we start talking about waste. Right. Um, it's inevitable that I'll have a efficiency versus productivity conversation. So for example, when, when, uh, one of the key principles in Kanban is limit whip. Mm-hmm. And that seems like you're encouraging waste. Right. If I say, Hey, you know what, there can only be one ticket out on a board. That means Alan, you're idle. Right. There's this belief that, wait, I'm not being efficiently utilized. Therefore there is waste. And so when you sit, continuous improvement, right? The other thing to keep in mind is that there's a goal. And then the goal is going to be different based on the context. Right. But in order to improve, you have to have something that you measure and you're going, did that work better or not? In this particular case, it's, uh, it's probably going to be for most of our listeners, it's going to be an element of productivity, uh, because it is our job to accelerate and, you know, productivity is a better measure of that than efficiency, and it's going to be about quality, right? Not co-correctness, but quality. Uh, are, are we helping to attract and hit the customers that we're going after? Yeah. This ties directly back to accelerate the achievement of shippable quality. Amen. Amen, brother. We spent a lot of time trying to figure out what, where do we improve next? What's the next thing we improve? How much do we improve it? How much, you know, what boy really ties back into that bottleneck one from last time, but before we go any farther, this is very DevOpsy, leanish agile. Why is this, why is it important to call this out as a modern testing principle and not just common sense engineering principle or agile principle? What, why is it part of the modern testing principles? There are several agile principles. Um, I think this is important. There's several agile principles that just make sense. Like, um, what you were saying earlier, they, this is just a common sense engineering principle. This one, I would say we call out because, uh, it's a critical process change in, in, in our opinion, that's directly related to the goal of testing. The goal of quality. They, right. There, there are this principle while it sounds like an engineering principle. Uh, and it's because it is. Um, if, if you, if you are helping to lead and push and drive towards, uh, a modern testing environment in your workplace, this one's important. Getting this lit up improves quality and reduces risk for almost no cost. I have a, yes, I agree with everything, but I have a slightly different take on why this is a modern testing. I think it ties back to it. Although we've said before worth mentioning again, that, uh, there isn't really a modern tester role. Uh, there can be a, uh, someone driving these modern testing principles on the team, because probably most of the people listening to this podcast are in or have been in test roles. Uh, you can relate. But I, I, uh, I think this one's important and it's been so, it's been so long. I think if I remember right, I drafted these and then we iterated the crap out of it, or this was part of the original or not. But anyway, getting to my point testers, I think are not uniquely, but well suited, very well suited to drive continuous improvement for a couple of reasons. Uh, two main reasons. One here, you mean from a leadership standpoint. So one is that our job is to see the system as a whole. Yes. That system thinking that because we do that, we can often find where improvement needs to happen faster than anyone else on the team. I think we're, because of our background, because of what we've done, we're very good at doing that. One of the things I ask my team to do, it's worth mentioning, uh, is that I have not like plastered my team with the modern testing principles and said, we're going to do these. Uh, some of my, some of my team listens to this podcast and they, you know, they follow along, uh, but I mainly just try and drive improvement. And of course, uh, I follow modern testing principles as part of what I do, so some of these things come out. Uh, so rewinding the stack. So I Brent's distracted me with his hyper legs. Um, maybe he has to go to the bathroom. I'm not sure. Uh, one. So we see, we're very good at seeing the big picture of the things, how things fit together. We're also very good at asking questions and often implying critical thinking. So we can often figure out what it is we need to do to improve or what the root cause is that we need to improve in order to make things happen. So I think those things drive it very well. And then you might want to add to that, but that's going to tie into the how I want to get into, uh, how do we drive continuous improvement? And I've found my experiences and I'm curious to hear yours as well, Brent, that testers are very well suited often, even above a product or program or a project manager role to drive things like the retrospective that I think are one of the key things that drive continuous improvement. My experience on this front is again, from, um, a unified engineering model. Um, uh, since becoming a believer in this, I have not actually been a part of separate disciplines. I will say that, uh, when I did unified engineering, right again, uh, I've mentioned this before. I had a good split people who came from dev people who came from test. Um, and over time, uh, it was my, my former testers that ended up being the better developers because they, they had the training to think about how do I write the code? How do I validate it? And most importantly, how do I make sure I'm not wasting my time? This is where the customer validation part comes into play. No one, no one wants to spend their time. We've talked about the pink principles. Um, no one wants to spend their time doing activity. They want their effort, their time to contribute to something larger. Right. So one of the biggest things that continuous improvement as an example, a retrospective going with a Kanban board, um, it creates visibility where visibility wasn't there before. And visibility is coming in and small enough chunks where it makes everyone have a non theoretical, um, concrete conversation about, Hey, this is something that's actually hurting us. God, does anyone disagree? Now, how do we go about fixing it? Um, to address your prior question. Um, yeah, I would say from my experience, people in the test world do pick up, you know, answering the end end questions better, uh, at, at retrospectives and really retrospectives about asking the right questions. And that's something that's very well suited for, for someone in this role. Um, but even then it's ephemeral, right? Because eventually, so when I did the unified engineering, yeah, I got a lot from my ex, oh, I'm a specialist. Oh, I can't test my own stuff. I'm like, well, here's the deal. Um, you're welcome to not test your stuff, I guess, and let's, let's see what happens. But I'm, you know, no one's going to test it for you. And you know what? Just takes a couple of mistakes and they realize, you know what? I need to learn this. It didn't, it actually didn't take very long to get those other guys aboard and realizing, so test has this problem of, you know, uh, we have this fanciful, we're the defenders of the customer. Dev has their own thing, right? I'm a specialist. Who is, um, and it doesn't take long where, where things are so visible that that's just like, you can't hold onto that belief anymore. And one thing, uh, worth bringing up here, it's got about five minutes left, but, uh, as a force for continuous improvement, I've seen people fall into the trap of trying to build a large, like test harness weeks of effort, maybe months of effort to try and be a stopgap solution for something or to try and, you know, really things to try and build a safety net versus help the team adapt and optimize, uh, to me, adaptation, uh, works better when it's a shit ton of tiny changes versus one big ass change. You can get a boiled frogs. There's lots of metaphors about this. And also in a culture of experimentation, applying that to improvement where, uh, do frequent retrospectives try and, and actually I'll talk about retrospectives for just a minute. I think, uh, many teams do retrospectives and they're, and they're a event session. Some teams do them and make a big list of things they want to improve. And none of them ever happened. Uh, what I prefer and what I drive teams to do is do retrospectives as frequently as you can handle, uh, minimum of once a week is what I, yeah, I, some of them do too, I prefer once a week. Yeah. Prioritize the top one, maybe two, three at the most things you want to improve and assign an owner, assign someone to be responsible for, for figuring out what the micro improvement is and implementing it and coming back to report to see what happens. Well, that didn't work. We can, we can learn from that. Try something else. Um, I do something similar. So every retrospective we do. So I do two styles of retrospective. One is, um, sort of a weekly retrospective. And we have another one known as the starfish model that we do once a quarter. The starfish model allows you to see sort of the long-term patterns, whereas the weekly retrospective generally is very tactical. Right. But every retrospective ends with four bullets at minimum under two categories. What are two things that we're going to celebrate? Hey, this really worked well. We very happy about this. And what are two things we're going to change and change starting tomorrow? Yeah, I like that. And one thing I do, but didn't mention is you do celebrate, do you think about the things you do well in addition to the improvements? Well, it's important because you want to think about the whole system, right? You don't want to change something because you see a negative impact and then, and then neglect to think about it. You don't want something to go away because you've neglected it. You want to recognize that it's a good thing and keep it happening. Right. Besides retrospectives, what are some things, uh, we can do to some activities? How else can we drive continuous improvement? So you mentioned one thing that I'll, that I'll state, uh, continuous and adaptive improvement is we haven't hit adaptation, I think enough in this conversation, but, um, as you were talking about doing these small minor changes, I actually even further recommend that you do it as vertical slices through the whole architecture. Right. So a small vertical change through the whole architecture is better than, oh, we're going to do one small release of this bit of the architecture. One small piece of this bit of the architecture, because it's as essentially whatever is the experiential component of your system, that often is where the money is in terms of understanding, are you delivering quality? Do you need to adapt? So the, so there is this tendency a lot of the times to overspend on architecture. When I go through planning, I do planning in two month milestones and I very specifically limit, I will spend no more than 30% of my funding on architecture on just pure, we're going to, we're going to make the database faster. We're going to refactor this thing. Cause that doesn't help you adapt. Right. Sure. You can argue, Oh, well, you know, two milliseconds is better than 10 milliseconds. Sure. Right. But right. But you know what? Humans can't perceive less than a hundred milliseconds. So why do I care? Well, maybe mere mortals like you. Sure. So I think chunking in vertical slices is also a good way of doing, uh, continuous, uh, integration. Right. We probably could do half an episode just diving into applying vertical slices in different ways, but we're not, yeah, we're not even talking about like how you set up your, your, your GitHub instance. Cause there's another, you know, how you set up your project pipelines also helps a great deal. Yeah. For sure. For sure. Oh, oh, here's one of my favorite ones on, on that particular one. You create a, your master tree. You let the business team, Hey, business team from here on out. Do you have PMs at unity? Yeah. Yeah. Okay. So the PMs own telling the business team, what's the latest thing in master. Okay. And engineering has no say in when any build from master releases. It's it's a pure business decision. So what that means is if engineering pushes to master, it better be good because business can release it whenever they want. What that, and what that does is it, is it kind of forces on engineering. Hey, I don't know when they're going to release this thing. They can do anytime they want to. I better make sure master is always clean. Master is always RTM. That sounds like the example of taking away a safety net. It's taking away a safety net. It's continuous improvement. It's adaptation, but mostly taking away the safety net. Yeah. And the thing we're not going to be able to get into all the examples that we're going to wrap up here, but there are literally hundreds, probably thousands of ways they're all going to depend on context of ways to improve the business as small, small, moving to smaller batch changes versus big integration points, moving to trunk based development, moving, which is similar to a model you're talking about. Those are all changes that will optimize your, your team. And like I said before, you can't just go, okay, we're, we've been doing big branches tomorrow. We're doing trunk based development. One of the things we need to be able to do is figure out where we're going. I want to go here. What's what are the series of small steps I need to take to get there? And that's super important and something probably worth elaborating on in the future, but we're running close on time. Do you have any final words for the day, Brent? None for the day. None for today. We'll catch the rest tomorrow or in two weeks when the next episode of AB testing comes out. Uh, one sec. Thank you for listening this far. We appreciate it. I hope we, uh, were able to answer some questions and not answer more questions than we created regarding principle number three. That's always the goal of ours, but I don't know. I like the bag. I like to think it is. Yeah, we're gonna, we're gonna have some serious mail baggage coming up. That's it. Brent has just like wave, like cut what? Oh, we're at a time. This is the longest like end of episode monologue you have ever done. I told you I was tired. Okay. Um, I'm still not Brent and I am. Bye. Bye. 
