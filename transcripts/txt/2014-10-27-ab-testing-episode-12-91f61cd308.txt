Hey everyone. Hello. Hey, I'm Alan. I'm Brent and we're back for episode 12 of AB testing. We're almost getting to the point where I'm going to be or not be able to keep track. All right, Brent, stop pounding on the table and get your face in the microphone. That's all I ask. Testing. All right, great. Yeah. Hey, I think we, we get a lot of flack, but I'm getting some flack about us talking about our three listeners. Apparently we have more. We don't know how many more, but you know, we have listeners in Australia. Somebody said, Hey, you have listeners here. So I think we may have more than we think for, including your God. Okay. The indirect advice I got was to stop making references to our, our low listener count. Maybe someone might feel like, well, if there's only three, maybe I'm too many and maybe there's something wrong. I'm not noticing with what the, I've been thinking through like marketing plans for us and to get t-shirts. I'm one of the three and things like that. I don't know if I approve of that advice. All right. Whatever. Okay. Hey. Caps, mugs, watches. Yeah. Great. Doggy vests. Sure. Absolutely. We can branch out. So last week talked about interviews and some junk going on there and then, kablamo, we get mail this week saying, Hey, we're changing the way we're moving jobs at Microsoft. We're not going to make it. I'm not going to totally leak the mail, which I'm sure is already posted on several news sites, but basically it said that moving between jobs at Microsoft is a lot easier. You don't have to be in job as long as let's let people find out, you know, figure out where they can have the biggest impact, which I think is great. The thing missing from the mail I thought was we still, I mentioned this last week, we still at Microsoft, the one flaw we have in our internal interviews left is that we still put people through basically the same interview they had to go through to get hired at Microsoft. And my assumption is we've hired them once. If we want to bring them to their team, they've done pretty well. I want to primarily interview them for, I want to know how they're going to work with others, how they're going to fit on the team. And to do that, I can talk to them, but, and I'll get some good information there. So I can, I can get some good behavioral ideas, but I really, really want to heavily rely on peer feedback and we don't have a great system for peer feedback. There was, so you mentioned there, I'm going to, I'm going to rewind just a second. Cause you mentioned that there was just one thing that you saw missing and that's actually the logistics around the interview process. There was one other thing missing. We'll see if it's as big as mine, but yeah, another one, um, that I saw missing is it didn't seem to address. So let's say you, you, it significantly reduced the friction around, uh, finding and getting a new job. That was fantastic or exploring it. Um, there were other restrictions that used to be in place, that made it, um, really hard short of, of staying in your team for two years. It made it really hard for you to transfer out of that. Uh, and as we've talked about in the past, right? Team fit is really important. Sometimes you don't detect that in, in the interview. And when you get in there and four months later you realize, holy crap, I may not actually be the fit for the culture for this team. You're kind of stuck there for another couple of years until, until you can, go either fix it in that org or go and find another org that has a fit. But the one thing that I didn't see, let's say you go through this new process and you get accepted. I didn't see any limitation on how long your existing team can hold on to you before you transferred. The current process today, you pass an interview, uh, your existing team can only hold on to you for a minimum of a month. I didn't see any discussion of that minimum. Cause so now what I'm worried about, great. Yes, Alan, you can go to this new team. How's 2017 work for you? Oh, I see the, uh, yeah, you pass the interview. I think that may be in the fine print that goes to managers or cause that that's bad as well. All right. That's not, um, I sort of implied that wouldn't be, uh, that wouldn't be an issue, but we'll, we'll see what happens. I don't think that's a huge issue fast forwarding. And first of all, uh, one of the great things to tell people about working at a big company like Microsoft, I guess big strong word, huge, massive, gigantuous, um, is that you can change jobs without leaving the company, which is, I think sort of a good advantage. You can try new things, new products, even new roles at the company. I've always, I guess that's both good and bad because there's a lot of people who are, uh, they're learning has been stunted, uh, working only at Microsoft because they haven't paid attention to anything going on outside the walls of Microsoft. It's a pro and con like you, if you go to a smaller company, let's say you went to Netflix or something like that, um, right. If, if you get to a point in your career where you've, you feel like you've done learning what you wanted to learn there, you really have to leave that company right here. We have people who really enjoy their jobs and stay in that position for, I know people who have stayed in their job for 15 years. I know, I know. And I, there are pros and cons to that too. Cons for me mostly. I couldn't stand that. I can't either. My resume is filled with, um, I'm actually, it's a very proud point for me. I've done just about everything you can. I've done API, I've done UI, I've done service, I've done consumer, I've done, uh, enterprise, uh, servers. It's for me, that aspect of Microsoft is fantastic. I can just, um, whenever I decide to leave a team, I go, what do I want to learn next? Uh, and who's got a position opening that? Very cool. Yes. Uh, so what the, there was a point I was getting to and it was around peer feedback and I want to really, uh, I was brainstorming this with a friend of ours the other day and you know, we've tried the, let me tell you about the peer feedback system we have right now at Microsoft. It's completely ineffective because the way it works is the way it works today is I say, hey, Bob, Joe and Fred, can you please provide feedback to me knowing that Bob, Joe and Fred really liked me and they will say great things about me and my manager has to take that with a grain of salt. Uh, and a good manager may reach out to some other people and ask them for feedback and it's if they feel like it and it's not a great system for getting objective feedback. So the idea that came out of this brainstorm, which I, I've had several brainstorm on this in the past and one was for those who had been around the internet for a long, long time in the nineties or mid mid to late nineties, there was a site called Am I hot or not? And, um, the idea was you see pictures of people, you'd rate them and then you'd seen an appropriate picture and close web browser and hope you never came back to the site again. But the idea was over, it was crowdsourcing whether you were hot or not moving forward, not what we're doing here, but the, my joking proposition that started from was am I a good employee or not? Where people see me a little bio about me and the people I've worked with and they go, yeah, I like working with him or no, I don't. Um, not the plan I'm suggesting someone is coding that right now in their office. Yes, it's the miracle. Uh, but what if, and I'll throw this out to you Brent, uh, what if you were required to submit, not just solicit feedback to submit feedback, you're required. You need to give feedback on five people a quarter or 10 people a year, some, some number that could increase as your level band increases or could stay flat, but you have to, you're required to give feedback to a minimum of N number of people, uh, per year. And even better, you know, how, um, uh, in any social networking site, review site, you can, or Amazon reviews, you can like someone's comment or give a thumbs up or thumbs down as a manager reading the feedback. Uh, I want to rate you as a person who gives feedback by saying this feedback was useful. This feedback was not useful. So the one aspect, I'm trying to encourage, uh, the goal here is to get people to submit feedback, uh, that, and not just for quantity, but for give valuable feedback that evaluates their peers, people they work with, and also, um, want to be viewed as a good feedback giver. Uh, so that's the idea I'm floating around in my head now, not that I'm in charge of HR. Any of this will happen, but I, but, but the last thing I'll say is I have a good track record of throwing out random ideas, which aren't really that random and, uh, seeing some of them adopted, it's probably pure serendipity. In fact, it's entirely pure serendipity, but there's my, there's my plan. Brent, throw in your ideas. So the, your idea is we have some sort of peer feedback site someplace. Everyone has to contribute to it. So instead of me requesting from others, I have to pick, uh, and number of people and provide them feedback. Correct. And I assume this end would not be in my directs. Correct. Okay. Um, and then other people, let's say for me, other people who have chosen me to, to give me feedback, um, they, they can come in and they can see other people's feedback. I don't know if they can or not. I don't think so. You mentioned this sort of plus one or no, that's, that's for the manager. That's for the, so your manager sees your feedback and they go through it and this person says, Brent's great. He's got a nice smile. And, and the manager has to fill out this. Was this feedback helpful? And he says, heck no. Then someone gives a lot of more, a lot of better constructive feedback or positive feedback. It's like, this gives me a good idea of how Brent works. That's, you know, that's thumbs up. That helps the manager, but more importantly, my idea is once you get a collection of feedback, that's ongoing about somebody, I, my hunch is if I can read that, I have an idea of how they work and a little bit more inside of them than a review and rewards and those things. Yeah. It reminds me a lot of the, the college interview process we used to have here where we would send out interviewers and then, uh, I, I haven't done Cal campus recruiting in quite a while, but the recruiters themselves get rated on their ability, uh, to get, to pick flybacks that actually get accepted. That could work. The, there's a lot of subjective bias in that system though. The first off, like the, the, the manager, let's say you come in and you say something that's totally viable, but I could care less about your principles since they contradict mine. That's totally fine. The manager, I still want that feedback and the value in that more value. Look managers, we have bad managers discarding feedback all over the place, but the value in that feedback in aggregate is that, uh, over time I can get a better picture of how this person works for future managers. So I'm also thinking of this in sort of the data-driven engineering culture, right? Cause we'd also have you presumably sending feedback to multiple different, uh, employees under different managers. Sure. And then we'd have the manager's ability to rate feedback and be able to compare, um, hey, Allen's feedback turns out this one guy consistently hates feedback of this sort. Uh, this other, I think there's some valuable data mining you can give. I think you can look at, again, in, in aggregate in big data, once you've done this for awhile and you can start to get some really interesting insights. And again, it's not everything and your fit and value to the team and your value to Microsoft at reward time is worth a lot more. I mean, pure feedback. I want to be more of that. But it still has to be about output and technical contribution and product impact and, and those things as well. The thing is, but we do a really bad job figuring out if someone people, if someone is collaborative, someone can work with other people. The, we did a, we did a similar, uh, we did a process when me, one of my peers at the last review where we basically, um, evaluated each of the new three topics, like how well you do results and how well you basically leverage others and proactively contribute to others. And we kind of went very deep in all three of those topics for each of our guys. And we found it using period feedback. We found it very hard. Um, the one challenge is if, if I'm able to select five people or no. So actually I think it's actually to brainstorm further. I think it needs to be both ice. I select five people because, uh, and five people are individuals also have to select five people in which they review. Perhaps. And the primary reason why I would suggest that is, uh, we do have people who like to spend their time in their office. Uh, we also have, like, you don't know what you don't know. You're right. So you do need to, there's a chance with that program that there'll be people who don't get feedback at all. And this is a case where, especially in the short term, not going to be overwhelmed by too much data. I think if you gather those and, and, oh, more work to do, like, look, it doesn't take it's, it's minimal, but over time, as we get more of this data, we can figure out, Oh, this, this one of these doesn't work. We're going to stop doing it. Or one of these works. Well, it could be that you can use the data to help tweak the system. I have an improvement suggestion. I have, I already have 10 in my head from this conversation so far. So go ahead. The, the, so an individual, let's say it's n right. The, the number of feedback that they have to fill out, it could be ladder level. It can be whatever at some point in time where the, that poor soul doesn't end up with a feedback burnout. Right. We don't want to, we don't want to get it to the point where they're like, I think you build a code. I'm just going to copy and paste what I said for Alan for this guy. No. And let me, let me pause there because that's a definite trap to fall into because we do feedback one time a year when I get this pile of requests and, and you know, I let mine pile up and I do them and then I put them in there. The idea of my original idea of, you know, me choosing to give feedback is a sentence in, in the moment, sort of in the moment, like, well, Scott really helped me figure this thing out and I couldn't have done it without him. And it was outside the scope of his job. I'm going to give that feedback. I'm going to just chuck that in there somewhere, whatever, whatever the mechanism is, but I'd rather do it in the moment than one time a year when I've completely forgotten about that event. So now I have two ideas. The idea on this one is let them select a few of those folks, but the rest is selected for them via, you know, could scratch your email, it could email the good hierarchy. Yeah, whatever. Right. And then it's, and then it's sort of a random sampling. The second thing, this is my favorite feedback trick. I use this quite often. I don't know if I've ever walked you through it before. This was taught to me by a peer years and years ago. So I'll just run you through it if you don't mind. I don't mind. Okay. Alan. Brett. Step one, what I would like you to do is rate during our last podcast, not this one, but our last podcast, rate your expectations of me from one to 10 of rate how well I achieved your expectations of me from one to 10 in our last podcast. All right. 10 is the most awesome. Yep. What's your rating? Oh, you want me out loud? Yeah. One to 10. Uh, eight. Eight. Awesome. Because I have low expectations of you and you nailed almost all of them. That was sweet. Now that this is the important question, what could I have done differently in our last podcast such that your answer to the prior question would have been a 10. You could have put your damn face closer to the microphone. Okay. If I had, if I had done everything in the podcast and I had put my face close to the microphone, would that have make it a 10? Uh, that would bring it to it. That would bring it to a nine. I've changed it. My first score cause you did a lot. That's a seven. That'd bring you to a nine. Okay. What else could I have done? So when I edit, uh, the less I have to edit of you making weird sounds out of your mouth, weird sounds. Yeah. I'll send you an outtake. That might be why I'm staying far away from the measure book. All right. Anyway, so the, that very quick feedback loop, something like that, I would love to have peer feedback. And in, in addition to that, uh, the, the random selection, you can time box it and say your expectations of this individual in the last three months, what we captured there was an evaluation of and translated to a work situation. It's an evaluation to how perception of accomplishing a task expectations. What's missing. And that's not what I'm worried about. We'll get that. Maybe, maybe I should be, but what I'm more worried about is we don't seem to care enough, careful, let's use my words here about actual collaboration and working with others. And you didn't that, that, that, that, that evaluation didn't capture that. I want to be able to change the, so if you, if you saw what I did first off is I said, give me a reading due to some expectation. Now you can change that expectation. You could say, give me a reading from one to 10 of how well you, you felt I collaborated in the last three months or how well you feel I am as a manager or how well you feel I am as a developer or whatever. Now the brilliant thing about that is it actually doesn't matter. You're going to go all the way to brilliant. Yes, all the way. There are better things than brilliant, but the brilliant thing about that is it doesn't matter what your rating is. What that rating does is it forces you to sort of make your own relative. I think it's a good system, but a different system. Sure. And, and maybe with different input and different results. But the great thing is another great thing about a big company before we move on is, uh, we could AB test not in the, in the Alan Brent testing podcast, but in traditional AB testing where we could run experiments in this division, we're going to do this in this division or this part of this division. We're going to do this. Um, it would be great if we, I want to do data driven HR. That would be awesome. And I don't know why. I don't know that I could get my boss to buy into that. Bet you, your boss would buy into that. There come my boss's boss. Wouldn't yeah. Yeah. But no boss to shield you from your boss's boss. Hey, my, my boss is fully supportive of am I a good employee or not? Alan, do it. Do it. Uh, the other thing, the other thing that you may not have noticed that is something of a huge passion for me, I truly believe that everybody is awesome. But everybody has behaviors that suck, particularly in the environment or in the expectations. And so trying to use peer feedback to tug out what are those behaviors that may not fit well in, in, we talk about systems thinking in the, in the new system that this person is trying to join. Um, I've had people who back in the days, I remember with this one guy who was a tester who hated testing and I sat him down and I'm like, look, you don't like to do any of the things around testing. It wasn't about the activity, but his behaviors show you don't like this. You like PM, go get a PM job. And, and he was a repeat, uh, three Oh, back in those days, three Oh was hugely bad. Uh, with that encouragement, he went on and he, um, ended up being a repeat four Oh four or five employee in that new role. Yeah. There's, uh, I made that invention in the podcast for, but a big fan of the grid that Michael Lott talks about in his fantastic management book called managing humans and the skill in the will quadrant and you have high skill, you have basically can bucket your employees into four groups, high skill, high will high, high skill, low will low skill, high will and low skill, low will. And the idea is that each of those is a different sort of management challenge or different management approach. When you have your high skill, high will employees, the ones that just there, the awesome people, you can't just leave them alone. You got to make sure they maintain challenge and keep them there. You have low skill, high will, like this is like your typical like college hire. I want to do, I want to do a little puppy dog, but get them to make sure they get the right training or, or coaching or mentoring so they can get the skills they need to get up into that high skill, high will. Same thing with the low will high skill. A lot of times like your guy, they're just in the wrong job. Like I'm really capable. I'm really smart, but I don't like what I'm doing or I don't like the product I'm working on. You need to, as a manager, it's your job to get your whole team to try and juggle your whole team into that high skill, high will bucket. Or if they're just low skill, low will, and it's not the wrong job, you just hired a putz, you know, find a way to deal with them in the appropriate way as well. There's another tool that I'm, I'm familiar with. I've talked with Alan in the past, the influencer series. They talk about a similar thing, which is motivation and ability. Right. Motivation. You can get people to change both of these, but a lot of the times it's not worth the ROI. It's important for, and we're off the topic here, but it's important for managers to recognize that you're, you have an employee who just may not be, it may be a great employee, but not the best employee for your team. And if your job is a manager, if they're not cutting it on your team, but you see potential there, it's your job to find them the right fit on the team, on your org or somewhere else at Microsoft. And people, people forget that too often. So what are we concluding on the, uh, I don't know. I'm, I'm, a couple of things came out of that. I'm curious if there are any examples in the industry of data driven HR. So if anybody knows of any of those, I think you brought up an example last time around, uh, no, it was, no, that wasn't data driven. That was data analysis on HR. Google look at, let's look at the feedback we've done over the last 10 years and do some analysis on that. And they said all the KPIs we've been using suck. Well, again, and you can, I'm not going to rehash the last podcast. Uh, I think that we're, I love the, again, I like the direction I was about to say love, but I like the direction we're making some changes. Uh, like with me, I always like to see more, a little more extreme. So I want to see if, and I think peer feedback is one getting that right will help us the system we have now. We may as well not do it. That's, that's, that's my closing. Yeah. It's, it's Pointly. We talked about Google and one of the things, uh, Google does on peer feedback is you can select, um, six people. I don't know. It's a budget. They, they change it frequently, but the peer bonus system. No, no, I'm talking about peer feedback. So this is what I last heard is, is that you as an individual can select, uh, six people very similar to what we have here, but anybody who wants to, this is different from here. Anybody who wants to can proactively add feedback. Yeah. I don't know about that. I haven't paid attention. It just, that one change would be a huge value out here. Let's experiment. Try some things out. Stephen Johnson, one of my authoring heroes was at Microsoft yesterday. I got a chance to meet him and, and get a second copy of his books. I can have one autographed. That's pretty cool. His, uh, sounds like a public service announcement, which it probably is because he did his last book called how we get to now, um, which is based on a nugget from his last book, which I, which I have read like three times now where good ideas come from, which if you've heard me talk at all about testing, I steal liberally from him. I told him that he was cool. He didn't punch me, but how we get to now, he filmed as a PBS series at the same time he wrote it. So he would go out and get the ideas and do the research and they'd film for like 10 hours. Then he'd go back and do a bunch of writing or vice versa. He'd write about something and then they go film it. But there's a PBS series. It's the third episode is maybe fourth. Anyway, it's six episodes, six parts of the book on these great, uh, connections of ideas and innovation through history. Uh, quick story from yesterday. He has a great example of, you know, there was a, I'm going to paraphrase this horribly. That's what I do. There was a printing plant and the ink was running. So they that guy there is trying to figure out how to dehumidify the air and he figured it out. And the machine he made had the side effect of cooling off the room as well. People liked that. I started eating lunch in the printing room because it was nice and cool in there. They maybe were onto something and out of the end, a couple of years later or several years later, they were able to build, um, air conditioning units that would work in a house. And that drove air conditioning. The invention of that, which came out of a dehumidifier for a printing plant actually drove large scale migration of people to areas like, uh, Phoenix, South Florida, Las Vegas, you know, turn of the century, just turn of last century, a hundred years ago, there were like 150 people leave living in Las Vegas. And now it's the fastest growing city in the, like in the country. Uh, and all that's due to the, to dehumidifying. And then he talks about how that goes into how it actually has affected the political climate of political races and, and all kinds of great connections. He's great at drawing these connections. And which makes sense. Cause he's the guy that learned where ideas come from and where do ideas come from, Brent? Uh, unicorn rainbow land. They come from other ideas. And, uh, anyway, uh, I'm a huge fan of his, uh, I think anyone, anyone interested in coming up with new ideas, if you're, if you're fine, have you read Shapiro yet? Stephen Shapiro. Yes. Yeah. He's he's, I have not read Stephen Johnson, but I, I'm all in on Shapiro and it's very much along those same lines. And what I would say with Stephen Johnson, with, um, I have, and to be fair, I've only read like the first half a chapter of the new book. Uh, but where ideas come from, where good ideas come from, if you actually don't care about coming up with new ideas and you're fine, just doing it, you're told, probably don't need to read it. But if you actually want to, uh, figure out where ideas come from and how to come up with good ideas of your own and how to think about that, you gotta go read where good ideas come from. And I've read, uh, he's written nine books. I've written, I've read, I've written, I've read about half. I read everything bad is good for you and the ghost map and, um, and where good ideas come from, which math, I read a third of his book. I'll, I'll, I'll increase that to just under half when I finish, uh, where, um, so, so you guys, you guys in podcast lane can't, can't see Allen when he's thinking about Stephen Johnson, but it is clear. He has a major bromance for this guy. Yeah. Yeah. I'm getting a little embarrassed and little flustered. All right. Hey, um, one last thing before we get going is, uh, maybe one next to last thing, but right. You wanted to talk a little bit about sort of the not invented to hear syndrome, what that means for business. Yeah. So we've talked about this quite a bit. And there's a situation currently happening in my org. Uh, it, it, it frustrates me quite a bit. I'm not gonna, I'm not gonna share it on the podcast, but I'm wondering, maybe Alan, you could, you could brainstorm with me in the time we have remaining today around, how can we accelerate learning in the organization in a nutshell? What we have is we have a well-known bad decision that has occurred in, in business. Okay. Well-known bad decision, a bunch of teams. I would say somewhere around 50 people working hard to replace, uh, the, the assets built as a result of that bad decision. And we're, we're in a phase, uh, where multiple teams in, in a specific context, this is around a data pipeline, which, which I argue is the, the test harness. No, I was thinking the same thing. Yeah. It was, it was the test harness for a long time. And now, now that every team wrote, we have, oh, we have a test harness harness. It's great. Now it's the data pipeline. Yeah. The, the, the difference though, is back in the old school days, if you didn't build a test harness, you still could ship, right? You work 40 hour days. Everyone's bug bashing in the last month. Fun. Yeah. I mean, it's not efficient. It's quite painful, but you could still ship. But in these days, if you don't have a good data pipeline, um, your business is hurt. Period. And so what we're seeing is, is a lot of NIH around data pipes. NIH is not invented here for the acronym, uh, weary. Yes. And, and, uh, there's a human condition where we're prebuilt to, to believe that other people's ideas are stupid and mine is awesome. And I battle that. I, there's a phrase that you'll hear in my hallway quite often that NIH is awesome. Not invented here is bonus. Let's go. Um, anyway, there was a decision that looks like it's in flight. It was made, uh, in isolation. Uh, it hasn't been vetted with all the other executives, but one executive basically very recently has made a decision that it's going to lead us down a path that's remarkably similar to the one we just, um, got ourselves unburied. And not only that, this is much bigger. And, and so what I find myself puzzling is how do we prevent this going forward? Why is this learning at this level so hard? I don't think it's that hard. I, I, I do think about this a lot and I have a great story I'll share in a moment, which will make yours seem mellow in comparison. I think ego tends to Trump invention in non-learning organizations. And what I mean by that is I commonly see, you know, I think non-invented here comes from ego comes from, I, nobody else could come up with a better idea than me. This must be the best idea. I have been here for 20 years and I know. And I also believe that, like I mentioned a few minutes ago from Steven Johnson, good ideas come from other ideas and learning orgs, uh, and another book recommendation, one always knew that the outside edge of my bookshelf is, uh, Peter Zingay's, uh, the fifth discipline, which is a learning organization. I think a learning organization in tech comes a lot from small experiments that fail. Like if I want to have a new data pipeline, I want to think, I want to brainstorm every little bit of it and come up with at least a few, one or two or sorry, not one, at least two or three different options for each part and give, let some people go investigate, not two to three week investigations. I want, I want little short investigations that include an investigation could be, here's an idea how to do it. Let me understand more why this didn't work, why something still didn't work in the past. I think, and that's where those ideas coming together, have those teams get together and do a little brainstorm, a little, little, probably different than a scrum meeting, but a little half hour, like here's what I thought of and let those ideas bounce off each other. And then you're going to come up with something that's a lot better. Uh, I think when you just think this is the right way, I really can't think of another way. In fact, I think this is from Weinberg. I can't remember a bit when I'm thinking for a solution for a really difficult problem. I want to come up with three ideas for it. Cause I don't know, you know, I might think of one and well, this one's the best one for sure. And then I'll force myself. Well, let me think of two other ways I could do this. And often, not always, but often I find something in that second or third solution and, and not just me, but myself, but working with others, because that's how I believe these things need to happen. I got something in my eye. Um, but when you think from the beginning that you have a solution and there isn't, there won't be another way to do it, uh, regardless of whether it's repeats a bad decision you made before, it's probably a bad decision. There's a, there's a couple of things. First off, um, I do the same thing in terms of coming up with three solutions and, and it's very interesting because I, I've actually been teaching people this. I've now taught it four times in the last week and the last time I've taught it to someone who was five years ago. So there's this weird sort of coincidence thing. Basically what I do is when you need to come up with alternatives for brainstorming, solution number one, what is the high cost but low risk solution? Solution number three is what is the low cost but high risk solution? And then solution number two is the one that's medium cost, medium risk. And starting with those things, you, you end up getting the two scales and then, uh, you're able to converge on what would be a good in between that allows us to be flexible. Perhaps one, one technique I use a lot is the, um, look at these extremes for any, any technical problem or any adaptive problem. Look at what one extreme versus the other, whether it's those criteria and others, and then take the best from both sides and figure it out. The other thing that, that I picked up from your, your guidance is it implies that if we could somehow come up with a way to measure the maturity model of an organization around how they are, are a learning organization. Right. There's a heuristic that as you were talking, it reminds me of, they're still very dominant. This philosophy within our company around betting the farm. And I'm like, I've, I've now after seen this for, for several months in the last two years played out, I actually think that that's a heuristic for someone who's low on that maturity model. The, what you're talking about, uh, with the, the, the printer ink not drawing, right? I, I thought of it in terms of Reese's concepts of pivot and persevere. The guys invented something that solved this problem, but they then very quickly understood the behavior of human beings given this new environment. And they recognized, Hey, wait a minute. There's likely a significantly more valuable idea in terms of the usage of this asset we just built. Steven Johnson calls those when you fail to do that, those blind spots. And they're very common. There was a guy ahead of Edison who invented a way to record music on a, on a cylinder. He thought that was fantastic. And it was kind of developed out of his idea that people take shorthand. I'll make this thing that records and then they'll learn how to read the waves in this recording and be able to transcribe it. Uh, he, it didn't even occur to him that people might want to listen to what had been recorded. He could record, but it couldn't, and the fact that it couldn't play back, not even on his radar. And, but I was, I was, wait, wait, what problem was he solving? He wanted to be able to record something so that, and his idea was that well, once it's recorded, people can look at the wave. You can do a little, a little, look at the waves and the wax and transcribe that into words. He thought it would be a new, new, new form of shorthand. Oh, he was trying to, he was trying to automate transcription. Yeah. So he missed and we see that happen. I heard that. I go, I've seen that happen about a hundred times, at least in software engineering where I have this and I see it actually come out of research a lot where the great thing we have super smart people in Microsoft research. And I come up with great ideas. It goes, I developed this thing that does this. And you go, well, that's not interesting at all. But if I take it and then do this with it, now it's super valuable. Thanks. Yeah. Yeah. I've been kind of situations where I've done, I was trying to do this and it completely failed, but I'm sharing and I look at it. I'm like, do you not see what you have? All right. We're almost at a time. Hey, I want to tell my story. So years ago, I was involved. I was actually the principal, the key person involved in trying to acquire some really cool technology from Microsoft. I had talked to the owners of the program. People, excitement at Microsoft was high. Cost was relatively cheap. It's like, oh my gosh, this is going to happen. We're like, it was very exciting time. Started floating it around, you know, and trying to figure out an idea of how much scale we need. And the VP of one major product at Microsoft said he vetoed it. He said, this program, this platform offers features that we may implement sometime in the future. So we don't want it to compete. So I don't want it here at Microsoft. And I said, the deal was done. The deal is off. What? And I have to leave it. I'll leave it at that for now, but it's funny that we may want to implement the features in the company we're trying to acquire. So he said, our product may implement many of these features sometime. So we don't want to have that here because it'll take away from our future growth. Yeah, that was the line. Is that VP still here? I don't know. I don't know. I can tell you though, recently, there's been a resurgence of a huge number of people saying, saying, hey, we should get this product here. And they searched the internal web and they found these documents I wrote six, seven years ago. Say, hey, whatever happened with this? And I sadly tell them probably even a less interesting version of that story. But I think we really missed out big time on a chance to become a learning organization. So that line just blows me away. Our product's going to have this stuff someday. So maybe. So we don't want to spend this minimal amount of money to have it here. Does that product have it? No, absolutely not. Nothing even close. And have you recently done any sort of update check in terms of the status of that company? Yeah. Here's the funny part. Yeah, they're still around. They're more popular than ever. Much harder for us to get it on site than it was then. I still have the contacts. It's funny. I still have the contacts who tell me behind the curtain that, sure, we'll help you get it set up. But there's still NIH around that from the interested parties now. And so even though I honestly could get them here for a price that Microsoft could afford and be economical, given the value. Maybe I missed it. This is a company we're trying to acquire or a service we wanted to bring in-house. A service we wanted to bring in-house. Oh, I hate this. Yes. Anyway, when I think of not invented here, this is a story that grinds me more than any. And I think it's a loss of opportunity. So last time, the time before, I was talking about the third party that we were going with. And we hit some of that when we were flooding the proposal. Because there is another team who's trying to build those services. And I said, that's fantastic. We're going to go with this because we need it now. And here's our design. And we're going to design it such that we can flip the switch. I hope that approach becomes a prototype and a practice we can see more of at Microsoft. But I think you're still in the rare bucket. Yeah, for me, it's just time is of essence. We don't have the time to wait for you to light up this feature. We need it now. And it's cheaper than putting- We don't have to serialize. Right. Right. And we'd- Yeah. All right. We're out of time. So, hey, fun podcast today, Brett. Yeah. Thank you. How do you do today? I'll let you know after you turn the mic off. Hey, everybody. Thanks again for listening. Yep. I'm Alan. I'm Brett. And we'll see you next time. 
