So my name is Matt LeRae and I hate writing tests. And so I thought it would be super ironic if I started a company that was in the testing space. Welcome to AV testing podcast, your modern testing podcast. Your hosts, Alan and Brent, will be here to guide you through topics on testing, leadership, Agile, and anything else that comes to mind. Now on with the show. Hey, everyone. I am Alan. I'm here with Brent. Brent, say hi. Hi, everyone. And we have another guest. We have Matt. Matt, say hi. Hey, nice to meet you. Hey, welcome, Matt. I know what Matt does and where he works, but give us the intro bio, Matt. Yeah. So my name is Matt LeRae and I hate writing tests. And so I thought it would be super ironic if I started a company that was in the testing space. And so the name of the company is SpeedScale. And we use automated traffic replay to validate your software. So you don't have to write tests anymore. And that's what I do for a living nowadays. Can you say that sentence again? Because I don't think on the podcast I have heard a sentence that I love more. Say it again. So I hate writing tests. And so the sentence is, I'll say multiple sentences, is because I'm so lazy and I do not wish to, you know, like write, like do integration tests and like end to end tests and all that. We came up with this idea of recording production API calls, like using it's like a like a distributed wire shark, basically, we take all the traffic, then we massage it into incoming test cases. I mean, we call them test cases, but it's really just incoming user transactions. And then we take the other half of the traffic, the traffic, and we turn it into automated mocks. So that means like, let's say your service depends on the x, you know, Xbox Live API, or, you know, depends on Gmail or something like that. Our service will read what happens in production, and we will pretend to be any of those API's so that you can do like an integration test without actually building an integration test environment or writing tests. And we do that by like recording like wire shark recording what's actually happening. Love it. I'm ready to go into Q&A already. Love it. Love it. Love everything about it. Love, love, love. So let me before you go into Q&A, I want to talk about why this is important for our, I want to actually add some context. And also I want to point out, this is the quickest ever, ever in the history of 164 episodes, the A.B. testing podcast. We've actually got to a conversation about something substantial in the first three minutes. And I am not kidding. Oh, yeah, we had this other agenda. So, uh, no, no, no, no, no, be quiet, be quiet, be quiet. I'm going to go on often in the, the testing world. And there's these different bubbles of where people are. And I talked to Matt a little bit before you got here, right around generalizing specialists and how we sort of saw the move to agile working. I think a lot of times people write tests. Um, there are teams and this hurts my soul that we have teams dedicated to nothing except writing automation and usually writing some UI automation and not being smart about it or converting manual tests to automated tests, which is again, just this is smart. This is like, let's take some stuff we already have and let's turn it into something that's actually even more valuable. I'm going to let Brent ask his questions, but I know Bing did something like this a couple of times too. We had a little bit of things that Microsoft had a little small scale. Like, let me do some traffic replay to try and try and see what Bing would look like if it doubled from 5,000 to 10,000 users. Oh, there's, there's a, yeah, there is, there's a lot on, on sort of traffic replay shadow environments is another thing where, where essentially you have a separate environment where your, your, your proposed code is deployed. And then you just automatically clone the traffic across both environments. There's a lot of techniques here. I don't know. Like I, I think now around, okay, what are the frameworks within Microsoft that's really helping to emphasize this. And I can't think of a single one, right? And it's because it's hard. It's hard and it's hard to make it generalize. And so when, when teams reorganize, things get forgotten, people like, Oh, that was interesting, but we'd have to spend 18 years to port it. It's hard. So great. If, if Matt and team have, have essentially, have you, are you solving this through a SAS play or do you, do you have to install libraries from your team? Like, Yeah, good question. So the, so first thing I'll say is it is really hard. It's harder than I thought it would be. But fortunately, as my co-founder says, I really enjoy pain. I really enjoy the suffering that comes from solving this problem. It just, it just fulfills me as a person. But it is actually really hard. But, you know, it turns out there's like, once you get past a few heuristics, a lot of it isn't actually that difficult. When you get past like, re-signing, you know, JWT's or JOTS or tokens, authorization, authentication, you know, you get into like, being able to change fields and database queries. A lot of the problems like an 80-20 rule, but it is a SAS play, which is the original question is a SAS play. Although we are able to take it on premise later, but right now, because we're kind of small still, we're, we're keeping it in house so that we can see the data and we can figure out what those heuristics and patterns are, as new ones come up. But yeah, it's SAS for now, the number one thing people always don't like is sending data to someone else, right for this. And so we have a data loss prevention thing we do, which is basically like it's like a masking engine. It's just a little, little bit smarter, because it'll, it'll find the same, like not allowed like PII data, it'll say, Hey, if I see Alan over here on an inbound transaction, and I see Alan going to the database, I need to replace both Alan's with the same, excuse me, redacted information, so that it'll play back correctly. And so it's like, if we're mocking it, we're pretending to be the database, the database, meaning speed scale, we'll pretend like we know who Alan is, you know, back and forth. So without knowing, right? Without what? Without actually knowing. So you probably, you probably are hashing the detecting and hashing the string and then using that renaming the mallah, the backward string hash. Okay. Are we going back to the useless stuff at the typical part of the humor went right over your head as usual, Brent. It, it, it did. Do you guys know what a Wiseman score is from the show, Silicon Valley? I remember the name of it. Remind me and I'll remember it. So it's, it's something they made up for the show, but actually is a real mathematical formula. Like it actually works, but they made it up for the show. And it basically is a way of ranking compression algorithms by how they'll play. Of course, of course. Yeah. Middle out's the best, right? Yeah, exactly. Yeah. Middle out's the best. So my running joke with my, one of my co-founders is the first time we did this like data loss engine, uh, or first time we, we, you know, we're sending this data around, we ended up, uh, writing a negative Wiseman score compression algorithm, which if you look at the math and how that works, it means we caused it to get a lot bigger. So that's, that's a, that's a computer science first. Um, our compression algorithm made everything bigger. And the way you got to start somewhere, you got to start somewhere. So if you just start with decompressing the raw data, then, then you'll go in the right direction. You know, I'm going to take that back. What you want to do. Perhaps suggest not. Some other unsolicited advice, take all the parts of the, just take the parts you don't need and delete those smaller or, you know, just, you know, every, every other bit, just drop, drop those. I'm sure it'll just be fine. 50% compression right there. Yeah. You didn't need them anyway. Yeah. It's the extra parts that you get from my Kia, right? One of the things that we've said on the podcast years ago, years and years and years ago. That's before I had a beard. Now, one of the things, one of the things that, that we talked about here is, is one way to sort of merge. This was at a time where people were still trying to get comfortable moving from on-prem to the service space. And they were sort of duplicating their test cases. And they felt that tip meant moving my test suite to the cloud and then monitors and tests were the same thing. And so one of the things, one of the things we talked about here is, yeah, really in this new world, you should be able to use, say, local automation, to sort of drive traffic, which I think you're improving upon that, but use local automation, to drive traffic. And one of the things that we proposed was essentially, but change your validation side of the suite. That your validation should be able to use your existing telemetry and not specific validation steps to identify whether or not you have, let's say, a regression or even a live site type incident. Your thoughts on that and how was SpeedScale advanced sort of those thought processes? You guys, I don't know if you know, if you guys know where it came from. You remember the phrase pets versus cattle? The Kubernetes folks talk about this a lot. Have you guys ever heard that? No, I'm not a warrior. I heard it from Kelsey Hightower, but he credited someone else. I don't remember who it was. And follow Kelsey on Twitter. Kelsey Hightower is awesome. That's where I got it from, is on Twitter. Yeah, he's awesome. He's full of knowledge. But anyway, what it basically says, when you go to these cloud environments, you generally, you stop keeping these carefully curated servers. And instead, like a pet, we just love them, and you dress them, and you give names, and you teach them how to say certain words and whatever. And eventually, you go towards herds of cattle. And they're all going to eventually be, you're not really naming them, you're just kind of keeping them corralled or whatever. And if some of them get lost, it's not that big a deal. And so the kind of the idea is, if you take that to the world of testing, cloud testing, if you lift and shift your entire testing suite from the old thing you were doing, and you move it over to this cloud environment, you're not really getting any benefit. You might as well stay on boxes or bare metal or whatever, you know, VMs or whatever it is. And so that's, you're kind of hitting at the heart of what made us come up with the idea is like, or, you know, the problem is, I want to treat all this stuff as cattle, I want to treat it like it can be blown away, and it's no big deal. And how do you do that? Well, what if I just record a new version out of production every so often, you know, like, so what, you know, like some of our, like, we'll do it for like every hour, for instance, we'll just say, you know, every hour, we're gonna get a new copy of this stuff, like a certain sections of it or whatever. And then, you know, reproduce that and bring it back. And so, so yeah, that's the idea is to bring that automation back to it. And then the other thing is, I, I don't know what you guys experiences like, but I've never been able to keep up with tests, like handwritten tests, even the best handwritten tests or mocks. Have you ever seen, you guys ever seen that work? Where like it stays up to date, and it's not a huge burden on people? Only in places where we have like five unit tests. Seems like there's some downsides to that. Yeah, just, just, just a little. Yeah. The maintenance cost is ginormous. This is one of the places where Alan and I, for example, we were just like, no, handcrafted UI tests should just be banned, just outright banned. And then Alan, I don't know, what is it, but a year where you did, did some deep thinking on this and you're like, Alan shifted. We spent most of our career going, yeah, recording playback, they're stupid. Because they're just a bitch to maintain. Yeah. But if you get rid of the bitch to maintain part, then they become pretty fantastic. Yeah. Just to, to loop, uh, Madden on our thinking, um, the record and playback stuff now has a little bit of ML built into it to adjust for when things move or whatever kind of work. And even if like your selenium test is going to eventually fail this stuff, eventually, no matter how clever you write it, it's, you spent hours or weeks on these tests, they're eventually going to get stale and not work. I would rather just do the record and playback because you can ha if you need that UI test to verify some logic you shoved into your UI, use the record and playback test because it's smart enough to keep working for a long time. And when it finally stops working, you can take five minutes and create a brand new one from fresh data. Yeah. And that people don't like me. Well, you two like me saying that there's some people that don't like that approach. They go, they're holding on to their test automation job, writing selenium all day. Like it's the, I just don't get it to me. It's not a, it's factory work. Basically it's not, it's not, it's not, it's not the love. It's not the kind of pain I'm with you, Matt. I want, I like to live in pain. Other people to live in pain, just me. Yeah. So I'm, I'm far less masochistic, but, but I will say that if it's easy, it's not fun. True. Or if you don't learn true. Right. Right. Exactly. Yeah. Yeah. It was, I was thinking through another scenario on, on that we were just talking about it. It's already escaped me. Sorry. I do want to ask you, Matt, do you have a, an example ready nearby of, uh, a handcrafted cart, cart guitar? Yes. Yes. Actually, Brent is just changing. Yeah. Just, just, uh, just, Brent, while we were talking, Brent decided to read the little bio of that from the website and now wants to take our podcast in the completely fricking different directions. So it's, it's AB testing. It's not just a testing. Uh, it's ADHD testing. You skipped over the bullshit part of the podcast. So I'm going back like ever, ever since I saw his bio, I'm like, yeah. So that was something that I was actually thinking about picking up as a hobby. And then I got a bunch of instruction books and I'm like, oh, this is going to take for goddamn ever playing guitar or carving guitars. Why don't I show you? Yeah. You gotta play, right? It's hanging on the wall. Oh. And, and our listeners can't see this, but just live vicariously through our words. Beautiful. What is the, the, the wood you used? So, uh, this is a, this is like a mahogany, a thin mahogany. Um, I actually bought from a kit, so I don't, I think it's maple. So, uh, I'm not, I'm not an expert on that. Uh, but I carved this, uh, by hand, the speed scale logo, as you can see on my shirt, speed scale, nice. And then I, it was my first experience carving wood. And then also do you, do you play speedy scales? You know, I, I used to play quite speedy scales. Now I sound, uh, more like I belong in like a, like earth wind and fire cover band or something. Like I'm not, uh, it's more of like, let the singers do their thing. I'm not so much a great guitar player. There is absolutely zero wrong, zero with an earth wind and fire. Wonderful music. Just not like super guitar music, you know? I, I dream of being able to be at that level. Yeah. It was funny. I was, uh, I was watching stranger things, uh, you know, in my copious free time. And, um, and they have that, the scene, you may have seen it right where the, the, one of the guys plays master of puppets. Yeah. Eddie. And I felt like I was like, it's been 30 years and I'm finally cool. Finally. Everybody, the song was 30 years note. Everybody hated that song, you know, whatever. We were like the weird people, but now people know what it is. I do want to point out because I have, uh, so many things I've forgotten in my life and Brent's right. The bullshit parts here. I remember the eighties so well, just so well. And that, oh shoot, what year is it? 86 86. It's set on spring break of 1986. That whole, uh, stranger things. And that's no spoiler there. Master of puppets was released in the beginning of March, 1986. So that meant Eddie learned to shred on that song with no tab, no internet in less than three weeks. Impossible. No. Just want to point that out for the cup, for the, for your continuity. And I don't think that spoils anything, but yeah, where were we, right? Oh, oh, um, so a couple of things I want to talk about. Um, I would like to learn a little bit more about how you learn to hate testing. Yeah. Uh, so I've been, I worked in the observability space or what we call monitoring back then for 15 years. It actually started with satellite monitoring. So it was, it was a great job out of college. Uh, and I would go around the world and install monitoring systems for earth stations. So that's like the intent on the ground part of satellites. And then also fiber optic cables. So as a 22 year old, I would go out to these fiber optic cables that would span like run out into the ocean and sit on the beach and install software. There were worse things in life back then. So, uh, but yeah, so then I got into, uh, I was very fortunate to get into a company called Wiley technology that was bought by CA. Um, it was a, the first sort of APM vendor, which meant, you know, back then what it really meant is like tracing, uh, one of the first vendors that did tracing metrics, it didn't do logs, but, um, and, uh, got to kind of learn about how big enterprises, uh, tried to solve problems in monolithic systems. Um, so I don't know how care how much everybody's interested in the deep technical bits, but like Wiley was one of the first companies that did something called bytecode instrumentation, which was you would be able to look inside of a running Java process by losing, like sticking little hooks everywhere. It was almost like, you know, like putting timers everywhere, right? Without anybody noticing. And so, you know, that was great for a monolith because in the monolith world, you know, like getting a stack trace is like King, right? Like that's the, that's as like, as good as it gets, especially if you can get a stack trace with timings, like a flame graph over time. Right. And so we got to do that. And it was, it was fantastic. Cause it was like, all these folks were struggling with this stuff and you'd walk in and you'd be like, here, just install this and they'd be like, yeah, I don't know. No, no, no, trust me, install it. And they would install it. And they'd be like, I can't believe it. I was like, I know exactly. It's amazing. Right. And so we have to do that. And then I kind of watched some of the transition into microservices and that kind of changed things for me where it wasn't like getting the stack trace and the flame graph was what was important. It like inside of a process, it was more like the problem moved to the, to the between the processes. So yeah. Yeah. Yeah. Go ahead. No, no, no, no. I was just guessing. Like that's, that's the nightmare I'm dealing with every day, day in and day out. It's, it's essentially the problems are in the no man's land between components and therefore teams and therefore, right. Trying to figure out how the components are fitting together. And more importantly, how they're not is the problematic aspect. Please. I'm enthralled by your story. Please continue. So when that happened, what, what you needed in a monitoring tool changed. And I think that kind of gave rise to observability, you know, and I kind of stayed in that world for a while. But you know what, like, and I think a lot of like a lot of attractions being made there, some of the new, like the new monitoring tools are amazing, right? The new observability tools, what they can do, open tracing, they're open telemetry and, you know, more broadly, they're putting that into the core libraries now in a lot of the ghost stuff and in Java as well. And so like out of the box, like, let's say you're running Istio, which is a service mesh. And I don't know if everybody knows what a service mesh is, but it's, it's like the networking layer on top of Kubernetes. I'm oversimplifying, but otherwise this will be a five hour podcast. We'll do a whole other three part series on what a service. No, we won't. Like if you turn on some of the observability stuff where it generates spans, which are like where the hops are, if they generate, if you turn that on in some of the service meshes, and then you use the default libraries, right, you get a pretty good view of what's going on, actually, you still need commercial stuff. You know, you still need or not answer commercial, but you at least need Prometheus and all that, or Jaeger zipkin to go and like, put stitch it together for you. But it's kind of an interesting new world to me. And I don't claim to be an expert on all the newest and latest and greatest, because I'm busy with speed scale, but it's a pretty, a pretty interesting world. But it sounds like you guys have been doing a lot of work with that. So what's your, what's your take? I want to talk a little bit about Istio. If, if you can get it up and running, it's, it's, it's a little fickle. There is just a massive amount of information and capabilities in that there's so much you can, it's, it's a kitchen sink of stuff. Yeah. But this kind of gets a little bit into the another question I wanted to ask you about. You got all the way here, although, although I don't know, ever know if I heard your story or if I heard why you hate test. Oh yeah. So, um, uh, I was, I was always in like engineering leadership and then I did a stint in technical sales for like seven years, but I did like, you know, in general leadership. And, uh, the, the thing I hated about testing is like being serious for a second is everybody hates the testers and they're the ones who, who keep you from lighting yourself on fire. Right. Like you're like, okay, I love this code. It's going to be great. It's awesome. I'm ready to send it to production. And then the tester goes and says, Hey, I was messing with this thing. And then it burst into flames and you're like, yeah, you did something wrong. And they're like, no, no, I didn't. Right. Uh, and so, but then everybody hates that, that function anyway. Right. Because it seems like it's a waste. And then the folks who were doing it are sitting here writing all these, like you said, the end-to-end Selenium tests or whatever they've got, maybe it's postman they're just like trying APIs and it's just toil. It's like the definition of developer toil, like just sitting there just grinding. And so what got, you know, I watched all that happen and it was usually about 30% of my team. Uh, and maybe that's cause the products I worked on, but, uh, it was about 30% of my team was just testers. And I thought, you know, this is just inefficient. Everybody hates it. Right. As engineers, we just hate doing it. Uh, we hate, like we hate the bad news that comes from it. So I said, you know, can we, can we, can we automate this thing? You know, and the cool thing was we went and started speed scale is you were talking about Istio a little bit, but it's, it's the whole Kubernetes ecosystem. Like the whole container management thing like makes it possible. Now, like when I said distributor and wireshark, you can actually do that now. And even like five years ago, I don't know of a way to do that at scale, like, you know, at any reasonable scale, you know, to, to do a distributed work. I'm sure somebody can build something. I'm just saying it was like, it was way hard and it's gotten a lot easier because everybody's been pioneering shout out to link your D to not just Istio, but link your D as well. You know, um, those folks, but, uh, um, you know, they're kind of pioneering it. So anyway, that's linker D less finicky, less finicky does a little less little, little smaller. Well, yeah. Well, I have a customer and I won't name them because they will get mad, uh, but I have a customer that says, uh, uh, a friend who says, uh, I said, do you use list, you know, like Istio, like in anger, like in production, cause a lot of folks just play with it and pre-prod he goes, he goes, yes. He says the only way to use Istio is in anger. So, uh, that's the only. So that could be on a t-shirt. So this all leads me to a question I wanted to ask based on, um, just some leading up to this podcast is a long time ago, I used to do a lot of debugging in windows and I was part of my job was to, we ran these stress tests overnight and I was one of the people who got to go connect to everyone's debugger remotely and poke around, see what was going on and either debug it myself or find someone else to bring in. And I really enjoyed it and it was hard. And like Brent said earlier, it was hard and I was learning, so it was fun, but man, as confident and as good as I was feeling at that when it comes to debugging microservices, when, as Brent was talking about before, when the problems happen in the hops in between and trying to reconstruct what's going on as things go from service to microservice to microservice, man, it's a, it's a, I'm good thing. I'm management overhead now. Cause I probably just pull my hair out all day trying to figure out what to do there. So, so, uh, I think you have some, it sounds like you have some experience or thoughts or like to hear like, one, maybe expand a little bit on the challenge and then talk about like how you get better at it. Okay. Sure. So, okay. So the, the, the thing that, um, so in a, in an observability suite, there's like two big areas of things you need, right? And again, I'm oversimplifying. There's a million little things, but like in categories, one is you need deep visibility and you're going to get deep visibility. The easiest one is you're like, you're going to turn on the log monitor. Every turns on the monitor. The next thing you're gonna do is you're like going to turn on stats D and you're going to get some metrics, right? Uh, CPU memory, something put out of your application. Those are like the two easy things to start. If you're really, really, uh, you know, like dedicated, maybe you'll get some traces too, but that's un, you know, more unusual. Um, and so you kind of start there. That's like the beginning of deep visibility. Now, the better deep visibility you get, the more, the more you get into each of the individual boxes in the microservice diagram. So I'll give you an example. Uh, you, are you guys familiar with EBPF? You heard of that technology? I have not. Okay. So extended Berkeley packet filter, and it's like giving Linux a super power. Like if you're running Linux and they're also reporting into windows as well, like you're mentioning, uh, you know, being windows debugger and what EBPF lets you do is view the inside of what's going on in the kernel with low overhead and an excellent security model. So you can go and see inside of an SSL payload as the kernel sees it. Like if you instrument like open SSL library, which almost everybody uses or most languages use, you'll see the HTTP transaction going back and forth inside of it. And so that's like the ultimate deep visibility. Right. And so there's, there's a good product that they're called contain IQ. There's a pixie, which is an open source project. That's really good. And you'll look at how they did everything. Um, but anyway, that EBPF, so that's like going deep into these things, right? Okay, that's cool. So you want deep visibility, right? The problem is the deeper your visibility, the more data you get and the more unintelligible it is. It's impossible to understand. And what you'll see with that, at least I saw with all the, the big enterprises is, uh, the, the alert storm, or as I like to call it, the can filled with coins that's just shakes all day long. You know, you know, the alert thing, it's just like alerts go off. Nobody knows what they mean. So what you end up having to do is create some sort of correlation and analysis engine to go and make sense of that thing. Right. And so what I was going with that about microservices is in the old days in monoliths, the value was all about looking deep, you know, like, like EBPF and all that, and that's still valuable. But in the microservices world, it's all about the correlation engine is how good are you at getting signal from noise? And I think you do some ML work, right? So I don't know if you have some thoughts on that, but, Oh, it did. My, my team is, is deep in that space. But a lot of the code that we're working on, how do we put this? So Microsoft as the, as the stepwise transformed, a lot of the things that we did is we basically lift and shifted server code and kind of turn it into service code. Yeah. Right. And, and we're still in some places, you know, buying down that technical debt and, and, and converting things into microservices and picking it up. Right. So we were at a point right now. So for example, there is a non-trivial open telemetry migration that's underway. It's going to still take multiple years to finally close. But yeah, we, we, there's a lot of tedious work in, in, I view it as tedious because every, every team has produced their own telemetry. We get called in to, to do anomaly detection along the telemetry. So we have to, we have to do everything that you're talking about and, and, and do it kind of manually from a, from a data science way. And each project, because it hasn't been sort of normalized or curated into a common schema is, is basically starting from ground zero all over again. It's painful. Thankfully, that's one of my sister teams working on that problem. My, my, my problem is more in the, in the, in the customers, customer space, trying to understand what causes customers to retain or, or leave. But those two worlds are very quickly merging. And so that's, that's one of the problems I'm trying to work on is essentially, all right, when we see this type of, let's call it signature as a, as a sort of a call flow pattern from the combination of the customer and the platform, can we translate that to an expression of a problem, say that a customer might, might do in their support case report, right? That type of correlation is kind of what my team focuses on. Can we figure out which of these patterns piss off our customers? And do more of those, right? Another, just sort of practical example. I love to hear Matt, your thoughts on this. This is, I will try to anonymize this phrase the most I can. So part of biggest part of my org is just platform engineering. I've made a statement to one of our big service owners that, well, two things happen. One is we'll have incidents, operational surprises, and we spend, I think more, well, in my opinion, more time than we should trying to figure out exactly where the problem is. The result of that is when I talked to the owners of our, some of our bigger services, I will say, and they won't argue with me. I say, I guarantee you right now, something is broken in your, in your microservice architecture. Something is not working as it should. We may know tomorrow. We may know next week. We may never know. It may get fixed by something else, but we just don't know. And it, it all just is making my case for observability or, and how it needs to improve. So that statement is probably true for most microservice architectures, probably in production today. Something's broken now. It's what's, whether you know or not is the challenge. Yeah. Well, the old days you'd say, what changed, right? Everything. And it isn't even that we can have stuff change. It's a little break and nothing changed. Nothing, no code changed, not nothing changed, no code changed. I'm going to give kind of an abstract answer because I've worked a lot on this at previous companies, right? The, but like my thoughts on it are kind of, all right. So in the old days, and this is like updating your thinking for like the new world of microservices or, or the new version of microservices, because if you remember long ago, we've, we've done, you know, if anybody remembers service oriented architectures, we've done some of this before, but in the past, but with microservices architectures. So there's two key things that give you superpowers, right? For, for finding problems to, at least in my experience, and I'm always interested in what you guys think, but the first one is, is you need a set of heuristics that tell you what normal is. And so, and that usually is applied to metrics or log patterns, usually not always, but often, right? And so the simplest version of that is like, you know, it's like baselining, like automatic baselining algorithms, right? There was like a, there was a great company called app dynamics that kind of pioneered this long ago of like finding like dynamic baselines. Okay, cool. That's good. The problem is in microservices, that's it's two variable things change too much because your load balancing, or your, your replicas are scaling up scaling down, like depending on how sophisticated your system is, stuff's changing constantly. So a basic baseline is not going to cut it, right? But you have to come up with some system for understanding normal. The second thing, I think this is the part that people don't understand as well is you have to come up with a graph of dependencies that is continuously updated, and that that influences the normal model. So, like well says like, in the old days is really easy because you'd say, Okay, well, I have a database and I have, you know, I have two or three services, and then I have the user and the load balancer and few little things. And then if I go, if the graph goes down, right, like that's kind of like horizontally, the graph goes across graphs goes down, and I go into the servers and the VMs and the networks and the containers and all the other stuff, right? And you could come up with a pretty good idea of that, right? The issue that comes in now is that with microservices, like, I don't know, have you guys ever seen the Death Star diagrams that were popular a few years ago, where they basically, you know, it's like, yeah, like Netflix has them. And other folks do as well, you, it's very hard to keep that graph up to date. And you can't hold it in your head anymore. At least, I don't know, you can't reason about it anymore. So you need some tool that will, whether you build it, or you buy one or whatever, you need some tool that will figure out what normal is statistically, and then something that will go and overlay that with the heuristics derived from the graph model. And then you start to get somewhere of actually hunting things down, where I see the noise generation happening is where people have one or the other and they're disconnected. So, you know, I'm talking like state of the art, right? Now, now that there's an alternative to that is you get into I mentioned Netflix, right? Netflix has a lot of consistent, incredible volume, but a lot of consistency in the kind of workload, right? Like, it's the stream. So like, that's a different problem. And they have genius people who solve that problem differently. I'm talking the average enterprise that has a lot of weird shaped objects that they're trying to fit together into an online bill pay application or something, you know? So yeah, Netflix has a different quality of problem, I think. But so anyway, I talked a lot there. So I'm going to stop talking. No, it was I had some good flashback there. Everybody used to debug stuff, which can get, you know, it's multi-threaded stuff. And remember taking a lot of notes on a pad of paper, just so I can remember context and try and it was too much to hold in my head. But I could, I could write some cues that would help me remember what a value was on some stack and some other thread or something without scrolling up and losing my train of thought. And I don't have enough paper to do that for microservices. You have to have a system that doesn't. I absolutely think it's, so I've not heard it described so simply. I think that's super elegant. I think one of the things that is sort of inspiring the thought process in my head right now is to what degree are we kind of in this weird, awkward space because people are artificially holding on to the idea that it's understandable, that the complexity of how software operates, that, that, oh, if we just do this, we'll be able to understand it. And I'm now wondering what, what additional advancements would happen if we just let go of that idea. I'm certainly guilty of it. I look at our production system. I know what's wrong. No, you don't Matt. No, no, you don't. Because it's, it's like, it's too complicated to hold in your head. Even our system as a startup, there was 99% invisible. One of my favorite podcasts, I think it, I think it was that one. They, a pencil, a simple pencil. And this is something I think about all the time. Just a simple wooden pencil. There is actually no one that can make those anymore. There's nobody, there's nobody that knows how to build it all. Everything is sort of, there's one person that knows how to build the erasers. There's one person that knows how to assemble the parts together. But the pencil has kind of been something where no one by itself can build it from scratch, from raw material. Yeah. And I find that concept fascinating because you can hold a pencil, you can look at it and go, it seems like something I should be able to build, but no, no, you, you can't. Now you, now you take something to something more abstract, more complicated, like microservices. How do we even find out which microservices are calling which other microservices that goes to your dependency call? Okay. Well, or how about, how about circular circular dependencies in a microservice architecture? So let me just interrupt there and say, that's a solvable problem. We've solved it. We've, it's pretty easy to build from the little show how your services interact. Then you look at this big mess of a tangled spider web and you go, okay, now what? Well, then you have to filter at Y workload, right? This particular thing, whether it's like a particular kind of trend, like user request or something or whatever, you know, you have to zoom in, you know, whatever, but you're right. It's a big ball, ball of ball of wax. I mean, it's a big, all yarn. I mean, it's a big ball of yarn or wax. It depends. You know, you got me thinking a little bit Brent with what you said is that one of the things that happens when you go to these distributed architectures is that software development is like really hard, at least is hard for people like me. So like, it's very hard for some of us, right? And microservices makes it very easy to build software. But then then what happens is all the complexity moves over to Allen's team, the operation platform team, all those guys, it takes all the complexity away, like it makes it way easier to build the software and much way harder to run it. Because like you said, there's no pad of paper, it's too complicated, too many parts. But the development teams love it, because they're like, this is great. All I have to worry about is my own little thing. And it's polyglot. And I only worry about my little, you know, my little loves, I can circumscribe my concerns. It's like, yeah, but now you have a different set of problems. You know, no, it's easy. It's easy for you to produce. As long as you ignore that, you know what, the over time, the amount of time that you're spending actually writing new code versus going in debugging log streams slowly, well, maybe not slowly, increasing. I don't know what the common term is. But for example, in my organization, the DRI, is DRI a common term, a designated person to go investigate? The designated responsible individual. So if your service is out, I assume you, Matt, have some process to handle who's on point to go investigate live site issues, right? That's the, we use the acronym DRI here. When you slowly discover, so you're an executive, Matt, if you were to find out that 50% plus of your engineering team is being spent on just DRI duties, right? You kind of go, well, okay, great. It's easy to produce code, but that's not where the cost is anymore. The cost is in the maintenance tax on shipping the code. And the issue is, okay, no, we got to stop thinking about shipping as code is a singular thing. We got to think of it as, okay, how are we going to maintain it? When some random team reports to us that our random microservice, which then calls into 18 other random microservices, how do we know where the bug fix is supposed to check in at? Yeah, right. That's kind of the problem today. Today, in my view, I want to rewind the stack a little bit pre-brand or I may just delete Brent entirely from that section. I'll think I can figure it out later. One thing you mentioned is like, you're right. My platform team makes sure you have all the stuff it's easy at your service running. But also 15 years ago, I was worried about building testability into products so they could be easier tested. Now it's, I make sure that people can build observability into their service from the beginning and make sure like, just try and give them, here's this nice paved road to walk down. If you do this, here's where you start. And if you do this, you're going to be happy for a long time. A lot of risk is gone. You know, the observability is there. We're going to give you basically the equivalence of stats D or something. And we use Prometheus and some things on top of that. And we're going to just give that to you and show you how to get started there. And you're going to be in good shape or you can kind of go walk off in the weeds somewhere into your own thing and good luck. Yeah. You got me thinking a little bit of a little tactical thing or not tactic, what little things, but tactical things like we're going to standardize the monitoring stack, at least to a basic level. You can do crazy stuff if you want to. Like that has so much value. I'd say actually, uh, Christine Spang at Nylus, she's one of the founders of this company, Nylus, she said something at a conference that stuck with me is one of the biggest bangs for the buck in a microservices architecture is to simply put correlation IDs that traverse every microservice, just simple, like a header, you know what I mean? A header always there. And then the first thing the logging library does is print it with every log message. If you just do that, your microservices architecture is like way easier. And I think that Google did something, some of that with dapper, they all went, of course, way more advanced, right? Or whatever. But like you said, it's like just a few little things can make it a system so much more observable. Nice. So Matt, I saw from your LinkedIn that you, uh, went to school in Georgia. Are you still in that part of the country? I actually moved to the Bay area and, uh, okay. But you moved back. Yeah, I did. I moved back actually right before COVID. I moved back. I like to tell everybody that it was my pre, foreknowledge of what was going to happen in the world, like a month later, because I moved back like a month before it all hit. Uh, but that's not true. That's not true. Uh, that's just the way I tell, um, uh, actually I moved back to Atlanta because my, my, uh, I went to Georgia. I met my wonderful wife who is definitely the brains of this outfit in this house. She's the brains of this outfit. And she, uh, uh, she wanted to be near her family and her, you know, uh, her family. And I'll tell you with COVID changing the landscape, even post, I mean, I don't know if we're post COVID, but it's, you know, it's died down. We may never be post COVID. Exactly. Um, but with it being less top of mind for folks, you know, it's the, the environment's changed a lot. Like, uh, in Atlanta, it's, uh, there's a, there's a startup scene here and there wasn't when I left, when I left for the Bay area, you had to go to the Bay area. If you want to be taken seriously, that's, it's true. Atlanta has a, not just startup, Atlanta has a growing texting. Yeah. You, yeah. Callenly, like I have employees in it in Atlanta. We're building up a whole new development center there. Once a month, I bring up, I bring up a topic to my boss. When are we getting an office in Atlanta? It's a great place. You should come here. It's awesome. Uh, the great, one of the great things that Atlanta is you got Georgia tech right here and you got UGA, which is a pretty good engineering school. Georgia tech is a great engineering school and it's kind of an untapped resource, right? And so like you said, Microsoft's got a, uh, you know, building, you know, across the, you know, over the, over our major highway here, 75 Google's here and I think Amazon is expanding their presence. And you know, the, there's other thing is that there are real legitimate startups. So like, if you think about like where you get ignition and a tech scene, I don't claim to be an expert on this, but like, you know, we're like the fire starts to go a little bit. You need the big companies with all the really smart people, you know, learning best practices, how to do stuff, the Microsoft's, the Google's, the, you know, all those different folks. And then you need kind of like the money, right? Frankly, the, you need the VCs and the money. And then you need kind of, you need just a few people who've gotten that taste where they've had an exit, like full or not an exit, but at least gotten big, like full story or Calendly. Again, these are not, they're not huge companies, but they're unicorns. I mean, they're, they're doing pretty, you know, a sales off pin drop. There's, there's some, I'm here and then you get a taste of that. And with those three combos, people start to go, Hey, you know, I could go do this. You know, why, why would I be the next Jeff Jeff Bezos or whatever, you know, Bill Gates or, you know, it's like, I could do that. And so it's pretty exciting time here. And then when you get enough of these startups and then with attrition, so to start, start crossbreeding and creating new things, it then starts to take on a, its own spirit of its own. Yeah. The VC, the VC thing, that's probably the biggest blocker. I know Bay Area. I mean, it's just chock full riddle with, with tech VCs. I don't know. Maybe, maybe start doing something with Coca-Cola. I'd be something to get it growing. I'll say there's a story from Reed Hoffman in the Alliance. He talks from LinkedIn talks about don't have the world anymore of where we work for one company for 30 or 40 years and then retire people change jobs. And I think change people changing jobs is actually a good thing. I survived to Microsoft for so long for two reasons. One was I changed jobs every 18 months or two years, something I always denied to Brent until he proved me I was, he was right. But also for the first big chunk of my career at Microsoft, there really wasn't another place to go. There wasn't a lot of, or the kind of the level I was, there was, wasn't a lot of other tech besides Microsoft, but now you can bounce all over the place. Atlanta, same thing. There's a, there's a place for you to grow. And I think there's value. I think it's good when someone goes from one company to the other. You don't have to worry so much about, Oh my gosh, we lost all year. Everything we trained you on we lost. Well, you still got value from that. And cause it's a light, it's a partnership. And I think Atlanta is ripe for that. And just, I've met so many smart people from there. I'm just excited to watch it grow. And I want to get the darn unity office in Atlanta ASAP. JR, if you're listening, do it. Come on down. It's great. The weather's not so great right now. That's the one thing I miss about the Bay area. I'm going to, I'm not going to lie. It was, my car said it was 98 degrees and I'm pretty sure that's because it got so hot that the thermometer broke. Yeah. My, my car said 98 degrees here in Seattle today. Okay. Well, there you go. Yeah. It's, it's starting close to that right now. We don't, we don't get the Atlanta humidity though. No, Atlanta's got great weather most of the year, but yeah, there's a few months where it gets a little rougher, you know, but hey, it gets away in the East Bay and California too. So, you know, it's all right, then I, this was actually before we end here, anything else you want to add about the company? Anything else you wanted to shout out today? Are you, are you on Twitter regularly? You want to give a handle there? I can include or anything like that. I'm not on Twitter that regularly, but it's Matthew LeRae. You can find me, you can find my company on Twitter, speed scale, Inc. You can, yeah, LinkedIn, Twitter. Yeah, that's good. All right. Our listeners generally like to stalk people that come on the show. So it's good to give them a few clues to start with. Okay. Then follow me on Twitter because I, I, I definitely, I definitely post there regularly as of the, all the time. I'm religious about Twitter. As of today. Yeah. Right now. I'm going to post right now. All right. Thank you again for being on, being on our podcast today. Hope you had fun. It was very informative for me and man, it got my head thinking harder than it normally does on a Friday afternoon. So thank you. So actually before we close, like for, for our list, Brent with one more tangents with our listeners, like what's the ideal customer to come and engage with speed scale? Right. Obviously you have expertise around Kubernetes. So if you're on containers, you're doing microservices, come talk to us. Yeah. Yeah. What's your pitch? So we help all companies of all sizes, right? Here's the thing. If you got like three, one of three problems, right? Let's say you're trying to move to a container system like Kubernetes, you're trying to migrate there or you're trying to learn about it. You should talk to us because we will help you not like, like have the quick way to test, to validate your software and test it. And so if you're making that migration, talk to us. The second thing is, is if you are fed up with your cloud spend on your, in your, like your integration and development test environments, come talk to us because our little mocking engines, right, are super efficient, too super tight. And they can replace huge tests, integration test environments with tiny little containers. So if you want to save some money on that, give us a shout. And the third thing is, is that if you, if you hate writing tests, then you're, you're definitely our people. And if you're all three, yeah, that's even better. Yeah. Perfect. Thank you. All right. Thanks everyone. All right, everybody. This is Alan. I'll see you soon. Goodbye. Thanks everybody. Thanks, Alan. Thanks. 
