Hey, Brandt, put your phone down. Gonna ride me up a 125. Oh, God, no. Oh, crap. I can't read it well enough. I'm gonna have to edit the whole intro because Brandt is now... Post my face wanted, dead or alive. Wow, this is bad. This is really, really bad. If you have bad hearing, you can't tell your tone to. Hey, everybody, I'm Alan. I'm Brandt. And we're here for episode 55 of A-B testing. 55. Brandt's pretty happy about 55. So, anything exciting with you, Brandt? That wasn't even like a leading setup question. I'm just trying to see what it'll say. No, I have a sad story about a personal failure. Oh, the things I could say right now. Is this a story for our listeners or is this one you want me to rub your shoulders while you lightly sob? So, as you know, I went back to school and I have two classes left. One on graduate level research writing, researching and writing, and then I have my capstone project. Okay. Unfortunately, in the last six months, I don't wanna mention it on the podcast, but you're aware at home, there's a bit of a family crisis. And that's taken a good bit of time. So, I took three days off last week. I had a big midterm proposal due last night where I have to write up a research design proposal for what I'll do in my capstone project. I tried to catch up, read everything. I wrote about 12 pages. And when I sat back about four hours before it was all due, I sat back and I read it with the eyes like I do when my employees give me a proposal. And I looked at it and I said, this is crap. So, I withdrew from the class. I gotta start all over again in September. Summer. Yep. Now, were you also enrolled in the, or these classes overlap? Was that the purely the capstone class? Or that the research class? No, they don't know. By design, you should take them sequentially. Okay, I don't know if I've mentioned this on the podcast before, but I think I mentioned this in a blog post several years ago, but, and I mentioned it every time I go back and visit my alumni, my university, is the methods of research class I took, probably the most valuable class ever took in my life. Probably one of the ones that, it's the class that taught me how to learn how to figure stuff out. Yeah, and that's actually what I concluded when I looked at this. I was busily focusing on producing the paper. And I read about 20 different literature articles. So, I learned a lot about the topic I'm researching, but I realized when I sat back and analyzed it that I hadn't learned about structured research sufficiently. So, what I had produced was essentially a stylized book report. And I said, so you don't go back to school in your soon to be late 40s because you give a crap about the degree or give a crap about the grade. And I don't, and I'm very open and honest with my professors on this. I'm here, I do it to learn. And so, when I realized that I hadn't really, to my bar, learned the topic that I was supposed to in this class on researching, I had learned enough of it to know that it's an interesting thing to learn. There's a lot of black art that researchers go through to find relevant articles. And... Indeed, so I was, I'm old, even older than Brent, and I was in graduate school before really the internet. I mean, internet existed, but my, the only thing I had to research, I could pay long distance to dial up a bulletin board. There was no local bulletin boards where I went to college when I was in grad school. Where did you go? I was in, I was at Central Washington in 1993, 1992? And 1893 is when I got my master's. Central Washington. Yeah. You went there? Mm-hmm. No, in Ellensburg. Okay. But what I learned there that I've transitioned is things like, if you find a relevant article, you look at what it cites, and you figure out, and you start looking at what other articles cite these same things, and what article, if you go read all these articles and see what they cite, and really digging in those extra few levels is where you find the nuggets of really cool information in the insights, which leads to Stephen Johnson's thinking, gives you more ideas to put together and merge together and come up with great ideas. I have been thinking about Stephen Johnson a great deal because I concluded exactly the same thing. Yeah. And there is another quote that, any field of study that you want to master, that if you just read an hour a day on that topic in seven years, you would be a worldwide expert. That's the quote. Given what I just went through in the last few days on that, like I found lots of really great ideas connecting the dots between entirely different topics. My research was on near real-time analytics. I wanted to target it for obvious reasons, for the cloud market, but there are a lot of other industries that have that same near real-time analytics problem and have similar problems. What I found very attractive is analyzing the healthcare industry. So here at Microsoft, when we talk about up and down times, we refer to it as either the server health or the VM health. If you do that sort of mapping while reading any of these healthcare articles, you realize that, oh my God, you could just search and replace this thing and looking for the word patient, put in the word VM, making some certain mappings, like the presence of a certain chemical inside the bloodstream doesn't necessarily map well, but heartbeats do. Yeah. We even call it a heartbeat sometimes. We do. So I read a book on, and I've totally forgotten the book. I liked the book so much that I kept on reading through the appendix. I'm going somewhere with this, 15, more than 15 years ago. Anyway, one of the things that said at the end of the book was, gosh, to the life of me, I've thought about this book a hundred times. I have no idea. I think it was some sort of leadership book. I don't know. But it said, you should challenge yourself to learn new things and subscribe to, and some examples like, read blogs, and blogs are fairly new back then, but they go, subscribe to magazines. And I subscribed for about 10 years to this magazine called Science News. And Science News was this little thin eight page, maybe 12 page, little thing that came every week and eventually came every other week in a little thicker format, not much. But it was just random, it was random little publications from the world of science. These aren't like IEEE type papers, but little, everything from a half page blurb to a two to three page article. And some of it admittedly went over my head. Everything I could understand, I found fascinating and interesting, and there were always some little parallels, oh my gosh, this is just like something from leadership, from software, from testing, from craftsmanship, that I can map to my own world. And one of those examples is pre-Steven Johnson, how good ideas come from, but just one of those ways I was sucking in, I almost used the word alternate, but alternate, the word alternate now means something different in the age of Kellyanne Conway. So, but these different, these different ideas that I could, just sort of merge into the things that I already knew, I found very valuable and very fascinating. Yeah, when you definitely go through the experience and you're cognizant of this idea that new ideas come from old ideas getting together, several times just in the last few days, I just said, oh, holy crap, it just happened. Just happened to my head. Literally 30 seconds ago. It's not being cognizant, being, there's an openness to this, an openness like, I need to look at all this input coming in and at some middle level be able to go, are there new connections of ideas coming in that I need to act on or think about more? Oh, there's. There's definitely a thread spinning in both of us all the time. One of our, and surely we're not the only people who equate thinking to multi-threaded computing, right? No. But there's always one like semi-active thread in the background, kind of looking at what's coming in and looking for those connections. So I chose, when you were trying to find this book title, I chose perhaps unwisely to not interrupt you and be snarky. But my daughter told me. Percy will call you out if you interrupt. Go on. Shout out to Percy. I think last Friday was Dr. Seuss's birthday? Yeah, I think I remember something about that. What is your favorite Dr. Seuss book? You know, it depends on the mood. There's the canonical Green Eggs and Ham, which I read to my kids a few thousand times. Cat in the Hat. I like Green Eggs and Ham better. As do I. There's some good stuff going on there. There's a little bit of a cats in the cradle moment if you read through like, oh, the places you'll go. Really more of a graduation kind of book you give somebody. One fish, two fish, red fish, blue fish. Which is, I think, sort of a precursor or a prequel, if you will, to Green Eggs and Ham. Yeah, I can buy that. So anyway, I would go, my favorites, depending on mood. I'm now trying to resist the urge to spin the rest of the podcast, having an academic debate on which is the best Dr. Seuss. The Dr. Seuss guide to agile software testing. We were having a debate at our house. It was split into two camps, Go Dog Go and Green Eggs and Ham. I took leadership on Green Eggs and Ham. I represented that point of view. For some reason, that Sam I Am line, really, it rings true to my soul. There's also the Grinch that sold Christmas. Again, that's- But because that's seasonal? Right, exactly. You can't have a seasonal be a favorite because, do you pull that? I think you can. I don't know. Well, I think you can, but it's just gotta be fantastic. Like Die Hard is now considered to be a holiday movie. Right? And I'm just like, okay. Stretch. Stretch. All right, shall we do a podcast? Oh, yeah. Okay. How about we start off with a word from our sponsor? Okay. Once again, the folks at TechWell are sponsoring this episode of A-B testing and they have some training coming up. Training all the time through SQE. They are so nice to sponsor us that I'm going to talk about them just a little bit. One would argue that's the definition of sponsor, I suppose. Perhaps. Yeah. Did you know that software testing isn't dead? I did know that. But it is definitely changing. The State of Software Testing Professions Survey published last year by TechWell, participants overwhelmingly agreed. Now again, I'd hate to interrupt my own PSA, but if you're surveying a group of testers, you have a little bit of confirmation bias that we need to acknowledge. Selection bias. Selection bias, correct. Selection bias. Perhaps confirmation as well. All right, I'm gonna start. In the State of the Software Testing Professions Survey published last year by TechWell, participants overwhelmingly agreed that as long as humans were responsible for software creation, the role of the tester will be needed. That makes sense. But the same report- Oh, wait, wait. But the same report also found that today's testing QA professionals are being called upon to have even more technical skills. So far, nothing I disagree with. There's little doubt that the role is evolving. Except for Brent's little interruption here, I'll let him get on with. No, no, no, go ahead. So as part of TechWell's convergence of expert resources for software professionals, they provide premium software improvement training and certification courses through SQE training for testers, test engineers, QA specialists, and more. Whatever you wanna call yourself if you're involved in software quality and testing, they got stuff that will help you out. SQE, if training software testing training weeks, offer up to 10 specialized courses during one week in one location. This includes four Agile testing courses, including an improved and expanded Agile test automation course, new management planning and measurement courses developed specifically for experienced test leads and managers, as well as courses on security testing, mobile testing, and more. Upcoming testing training weeks are planned for Boston, Massachusetts and Chicago, Illinois. You've been to, I've never been to Chicago. I have never been to either of those. I've been to Boston, I love Boston. I've been to Boston in Fallout 4. I did spend a lot of time in Boston in Fallout 4 myself, which was weird, because I've actually walked some of those trails in not screwed up land ever. But that I'm all distracted on Fallout 4, only one of the three plays games, I think. Anyway, if you've made it this far in our ad, A B test listeners can save $250 on any testing training week course purchased by March 31st with promo code 17ABW, which I'll put in the show notes. Again, that's 17ABW. And you can sign up at TechWell, and I will put the link in the show notes as well, but $250 if you're near one of those places. I know we have some East Coast listeners probably get up to Boston for one of these courses. They're very well done and recommended. Star hasn't happened yet, right? Star is always happening somewhere. I mean, there's a West and an East and a Canada. The next star is in May, April or May, I haven't looked in Orlando. There is a chance that you can still go register for Star. Also save a bit of cash with the A B testing promo code. Sure, A B testing. That was from episode 52, I believe. We had that sponsorship. Yep. You can go check that out as well. But anyway, if you are in those areas, want to get a week of training for you or any of your employees, you can use the code 17ABW and get some money saved for you or your employer. Thank you, TechWell. Thank you, TechWell. Now, as a somewhat completely, totally aside. That's so rare that we do an aside or any sort of thing off script in our well-prepared show. So you mentioned that the confirmation bias, selection bias, overwhelmingly, as long as humans were involved, testers will still be needed. So what spawned up in my head was a discussion I had at work the other day. I was listening to the radio, I think it was some NPR show. And a guy mentioned something that hit me pretty hard because I couldn't refute it. And he said, our children, so our grandchildren will never have a driver's license. And I'm like, yeah, that's probably true. But the way things are going, the way trends are going, there is a great deal of things that will be automated. To that effect, I'm wondering, you know what? That world where humans aren't needed for testing may not be all that far away. That reminds me, there's a lot of talk of, I mean, as long as there have been testers, there's been talk of automation taking away testers' jobs. And it doesn't work that way. But I think testing, you really need this quality professional. But this is a different kind of automation than talking about. Yeah, but I saw a discussion just last week on Twitter. Oh, it's because Microsoft wrote, there's some research guys, I didn't look at it. So I'm sure there's flaws with it. Some auto programming AI. You describe what you wanted to program and it programs it for you. I didn't even look any farther than that. Then there was a tangent discussion on automation for this and self-testing software. And it made me think. And I haven't thought this through yet. I should blog about it, but I'm gonna talk about it right now instead. Okay. It's easier. So we have talked before, Teal Shop, about using logs for diagnostics. The logs should tell you what went wrong with the program. If you have really great logging and telemetry, you can tell the failures in the program. You're not gonna identify, just squash the stuff. You're not gonna identify like user experience errors and if the screen flashes pink or maybe you could catch that. But a lot of what's going on in the software, you can tell what's going wrong via logs. Now, we add either some self-execution or some self-monkey test, a diagnostic program that basically executes every, calls everything in the model layer automatically. The logs are generated and you kind of have the edge of self-testing software. Yeah. And you throw in advances in AI techniques. Right, you use AI. You build an AI engine based on actual patterns of customer data. And it's always modifying the way it self-tests itself. That's a heck of a regression suite. Yeah. I, actually that is relevant to a blog you did recently. I was trying to download a data science tool yesterday. And what this tool professes is that you pointed at any data and it will find the interesting patterns in the data and it will find the interesting family of functions as well as their extrapolation for those patterns. And I could see how a tool could do that because that non-trivially that's a portion of my day. You do it enough times just like everything else, you realize this crap can be automated. Yeah. So I think testers or anyone who is afraid of automation is really, really, really, really, really missing the point. I mean, these things excite me. I'm not afraid about them taking away my job because someone still needs to figure out how they work, how to take advantage of them, how to make sure the patterns are correct. There's all kinds of other knowledge work that excites me to do. But I think that, I think embracing this automation and looking at what it can do for us and looking for ways to extend it and get more out of it and let computers do more of the hard work for us is something, it shouldn't be scary to anyone. I think anyone who's scared of it has other job security issues they need to address. So knowledge work, I don't know if there's another realm, but knowledge work is not something that has successfully been well automated in the past. But there is a trend in the last 10 years on knowledge management and knowledge automation. There's a lot of techniques, statistical machine learning, process-oriented techniques around capturing the knowledge of individuals. And once, like I've known, I don't remember the name of it, but Google has been, has a lot of data and they've been doing inference engines on top of that data. And about 10 years ago, they started doing work on turning their inference engines on the output of the inference engines, essentially finding new patterns in the data. There was a very famous example of this with the target and the father and the, where target, right? Look it up listeners. Yep. Where target's AI learned that this father's daughter was pregnant before the father did, right? Yeah, it's a new world. So let me give you another example. There was three or four years ago, could have been longer, I'm old, I forget, but I was driving to work and Amazon was just coming out with their automated item stickers for a lot of their shipping. This is taking away jobs. This is automation taking away jobs from people. I thought, wow, this argument sounds familiar. So here was automation taking away boring work from people and creating some interesting knowledge work because now you have automation to go pick these things, but there's still a lot of analysis on how do I optimize how things are laid out in the warehouse to make life easier for these pickers? How do I make sure that, you know, could I tweak things to make sure that they're picking the right size boxes and all this stuff? So there's plenty of work to do there. What the automation has done, it takes away some of the boring part, which is similar to what we've done in test automation. We haven't automated away our fun exploratory. Let me really dig in and figure what's going on here type work. We've automated away the boring stuff. Well, and again, with current trends, particularly with the onset of Agile, here I'm thinking about combined engineering. Like non-trivially, we've automated away the role because we found better processes, better approaches, the world change. Like there is an approach you can now take in the services world that you really couldn't take in the on-prem product world, right? And so, in my view, the need for this dedicated role has definitely lessened, right? Whether or not testing is dead is somewhat a irrelevant question, but it's... It's definitely evolving. If you don't want it to evolve, don't want it to evolve, or don't feel like it's evolving, you better buckle up and get along for the ride anyway, because it's changing. Oh yeah, I mean, well, if you're a listener of this show and you feel that way, like how did that happen? Again, we have selection bias, preaching to the choir, all that stuff. Hey, you know what we haven't learned in a while? Lots of things. Well, there's one thing in particular I'm thinking of. Okay. You know what it is? What is it? Leading close. Mail back! I was concerned. I'm like, you're not gonna kiss me on air again, are you? Not again. That one time was enough. Yeah, well, all right. So, we have a mail back. We do. Exciting. Exciting. From one of the three.slack.com, our Slack team. Noriam, Kanoriam. Noriam asks. Felipe. Felipe. Question for Brent. Or for BS Data Science, which hasn't started up yet, but question for Brent. Can you share any experience you've had with overexperimenting instead of going with your best foot forward following gut instinct or extrapolating from a small or a non-stat significant population? In other words, not pushing a change that will make the product better because you need to provide evidence of the benefits of the change, even if it takes weeks to produce such evidence. Yes, I can. All right. All right. A successful mail bag question. It'd be great. We should do a mail bag and go, nope, no idea. All right, thanks everybody. All right, so that's the show. Wow, that was a long question with lots of really long words. It's like four lines. But it has words like extrapolating, which is good. Yeah, extrapolating is a great word. Okay, so let's take it. Actually, that word wasn't there, but I just wanna throw that in there. Oh. So his question, what it reminds me of is the old school, hey, we're about to ship. Test and dev are in a triage room. We're all arguing around the bug and no one has any data. Now he brings up overexperimentation. Not really certain what that concept is, but it kind of reminds me of sort of the intuitionist battles that occurred for me back in those days. And a bit of are we overthinking or overanalyzing the problem? And I would say, I kind of take a different approach when I think about it in terms of a test context than I did back in those days. Back in those days, it was, so let's say we found a bug. Really what we're trying to judge is what is the customer impact. Now I would say back in those days, those were important discussions because again, it was an on-prem product world and it was a high time cost if we made the wrong judgment. But so what we did is we judged it by perceived customer impact. Is it gonna be a 2% case or is this gonna impact the 50% case? Knowing what I know now, if this was really important, then we would have handed it off to usability and said, hey, we'll do a fake A-B test. We're gonna give you two versions of the test. One with this, one without. And there'd just be MVP skins of the product so that it's really cheap to produce. And then we just let the usability training come to the opinion. The one problem though that he talks about, the way I'm remembering it was essentially, hey, we're having a big debate. Being blocked by lack of evidence and the time it's going to take to produce the evidence is outside of our window in which we need to close down. For me, it comes down to how do you address two questions. So second half was, for example, not pushing a change that will make the product better because you need to provide evidence of the benefits of the change, even if it takes weeks to produce such evidence. Okay, so not pushing a change that will make the product better. Let's start with that one. How do you know? And the question is, do you use gut for that or do you wait for the data? No, no, but better. As proactively to say that listeners of this podcast know that the producers of the A-B testing podcast firmly state product better is defined by the customer. So pushing a change that will make the product better under that context implies that you have some data or you're just taking a sort of like a an ivory tower point of view intuitionist opinion. Now, so I guess the question comes down to what's the risk of being right? What's the risk of being wrong? And what's the ROI in those two conditions? If given that there's a timeline, what I would say is if the ROI of whatever's under discussion is potentially high enough that there is reason to believe that this is a big deal, then what you do is you spend your time finding evidence. You don't do it as a pure, you don't go down and you don't try to do a full three month data science project on this. You take the time that you have available and you reduce as much uncertainty on this question as you can. So let me. If this is a brand new product that the world doesn't know about, then you're gonna have to make a gut call. And your customers are people that you can't get in contact with or you don't have any yet. It's tough. But this kind of goes back to, let me know if this makes sense to you. When Kemp Beck's first talking about Agile, he was implementing software for the people that were sitting upstairs. He could actually involve the customers daily and get them involved and go upstairs and ask them about things. That's data and evidence that can help you make decisions on what's better. Where data science and telemetry and where data has come in is when those customers aren't in the same building as us or the same state, same time zone. So we collect that data remotely and we analyze it. So somewhere in between is if, the exception is when you don't have any customers yet or it's a secret project, hasn't been announced yet, but otherwise you can contact, maybe you have some early adopter customers. This is easy in Xbox, very easy at Unity also. People want to get on the early, people want to be early adopters. Depending on the product, I think it's easy in all cases. Right, because if someone's a user of your product, you're gonna have some faction of those, some percentage of those who are, yeah, you want me to try something new? Let me know how to get some feedback. I'll do that, sure, no problem, I love you. Yeah, early adopters, they actually, so there was a study on this, and early adopters actually get something out of it. They're willing to take the risk that the product is a little bit crappy to gain the advantage of number one, influencing the direction of the product, as well as already be ramped up on the product when it hits the shelves. But they don't even have to be an early adopter. One of the things I've talked about doing, I've never done this before, is setting up some sort of regular, just call between, like with me, a support person, and some or a few people at some company, and talk about just ideas. And then go, what would you think if we did this? Would that be worse? They go, well, it may, I don't know, it'd be indifferent, or, oh my god, that'd be awesome, or don't do that, oh my god. You don't even have to implement the crummy feature, or the feature that you don't have to even implement the A-B test. You can just, if you have a good rapport there, you can just ask them. And again, you have a little bit of sample size issue with one person, but you get a half a dozen of these going on, you could get that data in other ways than waiting for the long pull of data to come in. As long as you realize that if you're making decisions off of intuition, then there's a risk that you're making a decision off of bias. If you stay objective and realize that what you're trying to do is remove the maximum amount of uncertainty in the time that you have available, then you go after that. There's multiple different approaches. So, as occurred to me, I'm speaking with my data science hat on, but I think the Agile hat is the one that makes the most sense here. It depends on the product, but on my team. My team fixes absolutely zero bugs for purely future-proofing points of view. Like, if one of my reports found a bug and another one of my reports issues and it's code that we've shipped to production, or even if it's not, if, hey, you know what, one of the things that we could do is we could really solidify this widget and harden against this particular concern if you make this three-day change. I'm 100% consistent on, no, we're not doing that. We don't spend, on my particular team, I'm already overloaded in my backlog on things to produce where there is customer demand. I simply guide my team on these ones. Like, great, I love the passion, and yeah, I can definitely see a place where you might be right. So, what I want you to do is the bare minimum is on where this code is. I want it abstracted such that if we, once we learn that you're right or wrong, I want us to be able to very quickly shift to the correct position. In other words, so in Agile, there is this practice that's known as, or principle, known as defer commitment. And what that means is you stall making decisions to the very last responsible moment. You don't go all in on, oh, we had to do it this way because I have been in this industry for 10 years, and I know everything about this. Any time someone plays the experience card, my eyes roll back to my head. Yes. So, you wanna say something? I do, I do. Earlier, I just had a little insight, which may be all full of crap. Earlier you talked about perceived impact of an issue. And it made me think of something we use at Unity, something I've used at Microsoft for, called user pain. Figure out a combination of how bad is it, how many people does it affect, et cetera. You can use the same principle, maybe it's an inverse, but the same principle, as a backlog grooming technique. If I do this item, how many customers does it make happy, and how happy do they get? Well, this is why I brought up ROI. So, you can look at like, well, it makes, well, nobody, but it may make them happier later. Okay, right down at the bottom. Or, you know what I mean? You can look at it the other way, like, it forces you to think about customer value and business value as a backlog grooming technique. So, you just look at an item, it's not making anybody happier, or it's only making one person happier right now. Okay, something bound to be more important to work on right now. Yeah, back in the day, I spent a non-trivial of my time blocking chips. No, this is not ready. Producing a bunch of data, this is not ready. We cannot ship this against the scorecard. Today, I'm 180 degree, no, this thing has to ship. Because we can't, in my view, we can't evaluate properly the quality of the product, the usefulness of the product, if this doesn't go out. I had this on Teams, and I worked on Teams. I had that argument with our VP, and my manager all the time, I said, we gotta ship this from people, oh, it's not ready yet. Said, we're making stuff up, so we don't have an MVP. I said, are you kidding, we're self-hosting on this thing for the last four months? Said, well, it needs to have more features. Said, no, we need to get feedback from people. Which is opposite, like, here I am, the test quality guy going, no, ship it, ship it, ship it. Well, yeah, no, it's absolutely, to me, it's nonsensical. It's going back to the backlog. Like, the way I tell my team I want them to operate, number one, or at least this is how I'm going to operate, number one, I am concerned about business value first. Okay, not realized business value. This means providing business value, not because I said so, or the perception of business value, but having an objective point of view. And I don't care what KPI we measure it by, as long as it's measured objectively. Second, I'm concerned about calendar time. Calendar time, once we've lost it, there's no getting it back. There's no way of fixing the calendar time problem. And then third, engineering time. Those are limited resources. Yeah, I have some levers I can pull. I can, as an old manager of mine used to say, I can order burritos and influence people working late. I could try to hire vendors. I could try to articulate a need for additional head count on my team. There's some levers I can pull on that one. But at the end of the day, even with maximum efficiency, the amount of productivity on my team has a top, has a limit, which is underneath what's being asked of me to deliver. So we need a mechanism by which we tie my team's capacity and produce the maximum amount of business value. So this ROI discussion around bugs, which I'm not certain if that's really what Felipe was talking about, but that's just what it reminded me of, is kind of important, and it's important to try to articulate it from a non-intuition point of view. You don't have to overanalyze, right? And matter of fact, you don't have the opportunity to overanalyze, because if you agree with what I just said, calendar time and shipping are really important considerations. What I would focus on is, again, high order bit, what's the number one, the cheapest way if you were to assign this to somebody and only said, look, I'm only giving you four hours to do this research, but we're gonna spend a half hour brainstorming on how we could spend that four hours to maximize the reduction of uncertainty. Then I would focus my attention, how do we prove if we right or wrong once this thing goes out in the field, and how can we, if our decision we're making today is wrong, how can we make sure that the product will allow us to switch in near real time? Look up, flighting. Yeah, if you have to go by gut, have a way to reverse that decision if you're wrong. Right, and quickly. And if you have to go by gut and you can't reverse that decision if you're wrong, I would argue that you've got problems in your process that you need to solve before trying to solve this one. Yep, agree completely. Anything else, Brent? Nope. All right, this has been episode 55. 55. 55, A-B testing. I'm Alan. I'm Brent. See you next time. 
