Good morning, everybody. Hello, I'm Alan. I'm Brent. Go back for episode 34 of A.V. Testing. Welcome. Good morning, Alan. Thanks for coming out my way this time. Hey, good morning, Brent. It's my pleasure to come out your way. I think loyal podcast listeners, each of the three know that we generally record first thing in the morning when people really shouldn't be awake and to make it more interesting and for added benefits, et cetera. If you're going to read your laptop and not actually speak into the microphone, I'll crush you. I'm getting over a cold, so I may be distracted even more easily than before. But yeah, so how you been, Brent? I'm just fine. Shall we begin? People will be so disappointed if we just begin. All right. So I threw out a couple of questions to internal alias and things that I had observed over the few weeks. And you said you wanted to discuss him here. And frankly, that's why I threw him out there anyway, because I was hoping we could talk about them. So the first one is and I've I've actually blogged about a different flavor of this many, many, many years ago. And I really was just referring to article that Eric Sink wrote even longer ago where he talks about two types of people in the world. Those who know that software ships with known bugs and those who can't believe it. I think anybody listening to this podcast realizes that, oh, yeah, software ships with known bugs. But this is a variation of that. I mean, if you were to talk to someone off the street and say, you know, it's just say, hey, that'd be a stupid question to ask them when someone off the street is like, hey, do you know that software ships with bugs? Of course it does. And anybody that ships software knows we make choices. But there's a flavor of this that I have seen. Can I ask a question? No. OK, OK, go ahead. Yeah, actually, you can. Did this this Eric fellow, did he state this in which decade recent or I believe it was the 90s. OK, so anyway, back then, I think I and a whole bunch of testers would have been indignant with the idea that we ship bugs. We all knew we did. Yeah. But there's a flavor of this. I've seen actually from testers in the industry, as well as from some teammates. I'm going to kind of see if I can weave that story together with some examples, maybe, that are totally almost made up or at least paraphrased because I can't remember them. Hey, folks, software ships with bugs, especially today when the idea is if there's a bug and someone finds it, you fix it immediately. And if there's a bug that is never discovered by anybody, it's not really a bug if and you can. It's a software error defect, whatever you want to call it. But it doesn't affect the usability or the experience of the user at all. If it's never discovered, I would state, though, because I think one way to interpret what you say is this is something new today. And I would argue that's always been true. No, I agree. It's always been true. All right. So I'm not. Yeah. So for example, I remember a couple of years ago, I can't remember who some tester was astounded that I can't remember if it was WhatsApp or Skype or one of those messaging apps when they put in over 32000 characters into their field that the extra characters were not sent. Oh, and you know what? That's a bug. But they were screaming about data loss and which technically it is struck my shoulders and shake my head a little bit. It's funny that the learning from that, I just I knew it was wrong. I didn't know what to do about it. And then let's ignore it. Let's fast forward to today on my team. So we have a team of no testers unless you count me. I don't. All right. So we have these pretty senior PMs and devs who they see a bug and they freak. Even the devs. Sometimes even the devs. Oh, tell me a story about that. That would be great. A dev manager. We're trying to decide if the team if build is we did a big product reset and we just want to get the team dog fooding our product and using our product for communication every day. And I said, we got to go. This one's this one's good enough. It's a little rough around the edges, but we've got to tell the team to start. We can start. And he does like two or three things and one of them doesn't quite work and he goes, no, it's totally broken. We can't do this. And we stop and talk through it. And and we look at the context and the nature of the bugs, the bug. There are all bugs, as you know, are not created equal. There are thousands, if not millions of different intricacies and context and things that go into the bug. How often does it occur? Is there a reasonable workaround? The list goes on and on and on. And what I've realized, what being in software and especially on the testing side for over 20 years is done for me is I can very quickly contextualize any bug. What's contextualized? It's a word I just made up for this. Meaning when I see an error or something go wrong, I immediately start looking at how often would it happen? What's the impact? If this is as bad as they're saying, how come no one else seen it before? Let's look a little bit deeper and figure out what else is going on here. So you evaluate the the risk and the benefit of fixing the bug. I think the risk of not fixing or not fixing now versus fixing later today or tomorrow. Right. Or making a decision based on the nature of the bug. So what I've noticed is I see just there's the from these cohorts who are haven't quite mastered this fact. They see the error, the bug, they have a moment of panic. And when they declare that the whole thing is FUBAR and I stop. And sometimes I know they're getting better at it. I just stop it and I'll just go, what do you think, Alan? And I don't say anything. And they go, let me look at this again, look at it again. OK, so it really it's kind of if you look at it in a way by design and it's it's probably wrong. We should look at it and there immediately as we have a a moment together about it, they can take a moment to realize that it's not that big of an issue or maybe in some cases it is that big of an issue. They may see something that the opposite can happen occasionally. They go, well, that just happens. Hold on a second. This is Alan Page, software psychologist. That the I was going to go for the bug whisperer. But it's an interesting almost human behavior observation exercise. Fun to watch how people react to bugs in software. Why do you think you were reacting that way? Oh, very good, because there's always motivation behind the way people act. One, I think there's a new responsibility they haven't had before to actually care about these bugs and to make decisions based on them, so they're maybe taking it a little bit too. I want to say too seriously, but there's a lot of pressure to make sure they make right decisions based on quality. People are trying to understand that they own quality and this stuff's important. So that stuff drives it. They don't want to make the wrong decision, but in some cases, they're so worried about making the wrong decision that they get paralyzed. In fact, one of them said to me, one of my colleagues said the other day, it's the worst thing we can do is to is to ask the team to dog food this product too early. And I said, no, the worst thing we can do is actually sit on this and never ever have anybody look at it besides us. Yeah, so so it's it's context and it's weird because I'm the test. I'm supposed to be the guy. I'm you know, traditionally the tester is the you know, I don't want to be the gatekeeper. I don't want to be that guy. We know what happens. You see my slide on what happens when you're the guy at the end of the product cycle. We actually podcast this early on. And so one of your prior adventures in your career, you I recall you went and interviewed your manager's peers. Pretty sure this is when you were starting in your very last team. And one of the guys said the goal of QA is to sign off and that upset you greatly. Oh, that was an Xbox. Yeah, we had no longer employed director of development for Xbox. He was talking about roles. And I explained to I thought testing and fit in back home. And testing. Yeah. And he said, no, no. The goal of test is to sign off. Yeah. And the guy was a couple levels above me. And I didn't want to burn any bridges. But the thought you can imagine the thoughts going through my head of what I wanted to say. The thoughts going through my head is imagine the payroll money that Microsoft saved. Now, winding back to your last story, so far my favorite part of your story is what you haven't mentioned, which is. Is that I'm not wearing any pants right now? No. Something else. I was working through a segue to bring that up. All right, great. I guess that will work. No, is the infamous question that was always asked to test in this sort of situation, which is why wasn't this found before? Did that not get brought up? No. It almost seems non sequitur in this case. We have bugs. And my team now, I don't know if I told you this. That's fantastic, by the way. My team, bless their little hearts after all my work, we talked about this. We now fix bugs as we go. So you're down to zero bugs? We still have some previous debt that is being actively beaten down. But the goal is to hover at zero. So your rallying cry is working. I don't know how, but yeah. I think it's the threat of shipping. Well done. You deserve a self pat on the back. Golf clap. All right. All right, so I forgot where we were. Anyway, interesting observation. Wanted to bring it up. I'm still not clear on. So I'm guessing it's somewhere in FUD. But do you think it's something new that's causing the panic, or do you think it's just sort of tradition? So no, a little bit of tradition. But I think more, we have a product and probably some context. So you're going to made up that work contextualized earlier around bugs. Context would be good. So we have, I mentioned our product a little bit of a reset. We're not officially announced. I won't talk about what it is and what those deals are. But we changed our back end infrastructure. So we're slowly getting our site that we had running pretty well back in November, up and running. It's actually running very well. But we have a pretty aggressive ship schedule to start bringing some other teams on later this month, to bring on some external teams and larger parts of Microsoft through the rest of the spring, and then to have public preview, mass availability in June. So it's a very aggressive ship schedule, which on one hand, the art of shipping is so fun. And in my role, I play part tester, part shipper, which are two weirdly opposite hats to wear in the traditional world. So I put on this one hat that says, we've got to fix this stuff before we can go forward. But we have enough people already paranoid about that. So more often than not, I have to wear the ship hat, which says, let's get crap out there so we can get some feedback on it. Let's go fast. As fast as we can, let's be aggressive about getting out to more people and making sure we have the tools in place to learn from their experiences. Because if we don't do that, we're not going to make the rest of our goals. I've noticed a similar phenomenon myself. As I went through the transition, detestifying the way I looked at problems. And I have now found that it's very common for me to take the opposite view I did 15 years ago. No, you can't ship that. I don't want even customers even coming into the room that's dark and with the software running. But now I'm just like, no. Getting customers inside. You and I have advanced down that path a little bit. And a lot of our peers haven't. So I think you saw where my story was going, but I'll finish it for the three, is that there's a lot of paranoia over, we can't go to these people if there are errors in our product. I'm thinking, go to the people. They'll tell you what the errors are. We'll fix them because we're set up to do that. And then we need to get the other feedback. The only time that I am right now, the only time I'm in agreement with, let's say, your peers' point of view, is when we don't have the process and infrastructure in place to quickly react. If we're going to ship a bug and it is super painful, and we don't know, let's say we don't know the bug. This is where I'd become. Oh, yeah. We don't know if we're about to ship something. If we don't have the ability to turn something around within a week, then I'm concerned. No, that makes sense. And also, if you have any sort of, we don't know why. But every once in a while, the whole site goes down. We're not sure why. I think you want understood issues. You still want to keep a tight ship. I don't think you can do, if we still had hundreds of known issues, it's a little more difficult to say, OK, here you 1,000 people go use this, because you know there's 100 issues and so they're going to run into. And I'd rather have them have a small number of issues or some issues that we didn't know about. Anyway, horse is dead and beaten. Speaking of traditions and things. There was this product that Microsoft bought called Yammer that is actually occasionally used to Microsoft. You've heard of it, Brent? I have. I use it. Did your team still use Yammer for reports and things? I've changed teams. So Mike, I did bring it into my team. And I'm pretty sure my last team has abandoned usage. Yeah, as far as I can tell, and this is, I don't spend a lot of time on Yammer, but the primary usage scenario of Yammer by Microsoft employees is to complain about stuff. So my current use of Yammer, we've pulled together a branding site for the output of my team. And when you go to this site, the very first page launches into an embedded Yammer page. So the first thing people see is the communication we have up on Yammer. Interesting. Yeah. Well, a colleague pointed me to a thread on Yammer, which I took part in. But it was, I don't know the context of how it came up. But someone said, the basic gist was, and I won't read it. I'll paraphrase it. It said, can we please have our testers back? No. And the gist was, well, there was a couple of really fundamental flaws. As you know, Microsoft, we don't have any anymore, even Estet's, which does not mean you idiot. I'm sorry, the three-year smart. But when your friends hear that Microsoft doesn't have any testers, that does not mean we don't do any testing. We do plenty of testing. In fact, in some cases, we may do more testing than we did before. Not true across all products. Some people have not made. We've talked about the transition to unified engineering. Some teams have not made that transition as well as others. Blah, blah, blah, covered that. But here's the point. Here's while maybe I'll rant a little bit. Here's the soapbox. All right. If you want your testers back, because you're a lazy developer and you can't make your code work on your own and you need the safety net of a tester to make sure that you feel good about your code, then quit. Don't even wait to get fired. Just either get yourself fired now, HR violations. I have a list of them. I can give them to you. The probably fireable fences. I don't care what they are. Just quit. Leave. I don't want you here. I don't want you in the software industry. Go do something else. Go sell. Real estate. Real estate's on the rise again. Go sell real estate, Brent says. Yeah, I can, sure. Just something else. Don't go looking for your safety net. And you think, well, I'll quit and go to some other company. Guess what? Unless you go into some IT shop in Podunk, Iowa, making $11 an hour, you are not going to get a tester to test your crap. Oh, wait a minute. If we could send them to one of our competitors. Oh, yeah. Oh, actually, for your style, I hear Google is a great place to work. You should really consider it. Yeah. And they'll say, well, Google has testers. It's different. There you go. Go there. You'll thrive. But then there's another reason. So now, I don't want the safety net. It's because our products are low quality. Oh, so you need someone to beat quality into the product, because that's the job of the tester. This idea that testing is where quality comes from is, it doesn't make any sense. How can it make sense to anyone in software? If we would test this, we would have higher quality. There was something that you said on when we were talking about the prior topic. I decided not to bring it up, but I'm going to now. And you said a statement which says, not all bugs are created equal. And it was clear that you were referring to evaluating the impact. But it got me thinking, no, he's right. Not all bugs are created equal. And I'm like, OK, so how is test impacting that? How did test ever impact that? Never did. Never did. Look, Dev douchebag, if you want to bring back testers, first try figuring out what's happening, such that you're creating the stupid bugs in the first place. Right. This whole rant is, there's a side rant coming up. I was talking about testers and quality. And they're just not related. Well, not directly related. Of course, if you test things, you're going to have higher quality. Maybe, if you're lucky. Yeah. Bugs released in the wild are not because of a tester failure, because they didn't have testers. And there are douchebags on Twitter. Did I say, did we just go up to PG? I hate this. I'm pretty sure where I've always been PG. Maybe people who, there are people who make their money on the fact that they need testers. So this idea there are no testers freaks them out. And then every time there's a bug, a press release of a bug, they act like they're going to be like, this wouldn't have happened if that team had a tester. Right, exactly. It goes right back to this, can you look at this bug in context? And they would have found this bug if they had a tester. They wouldn't have released this. And it's like, they're just a bunch of BS. I just don't get people. I am, the longer, and I've said this before, and the three, I do not mean to offend any of you. But the longer I work as an engineer in this new world, the less and less I relate with software testing that I see in Twitter and the BloggerSphere, et cetera. Same here. So you know, my blog site is Testastic, which I love it as a brand name. But I'm just like, every time I go in and I want to post and write it up, I have test in your name. And I'm just like, ugh. I so, and I can feel every time I notice that, I just so, that a big chunk of my self-identification with that brand is just lost. I just, I don't feel it. And I still consider myself a test specialist. There are test, people in the test world who wouldn't consider what I do testing, and I don't care. But I'm a specialist in test and quality. I'm respected for that, and I get paid for that. So that's what I do. But I can't relate to discussions on automation and, and it's ridiculous. I am still quite passionate around quality and how to get there. I no longer have the passion around preventing impact to quality. My passion is much more around how do I minimize the total impact of quality. So the model used to be, oh, we must prevent it at all costs. No. No, if it's cheaper, faster, better, cooler to react to it and that reduces the overall pain, that's a better strategy. I'm done with the month-long test passes before we ship. Yeah, bringing back testers, it almost, to me, hopefully like in the next five years, if a dev says that. So let me put a cap on that, because then we'll go on. My role, I am on this 120-person team, whatever big we are, as the test guy, the quality guy. But I'm not the, and I think having that role, having a quality specialist, a test specialist on a team, I think is good. But I don't do really any testing. Your role is that? I'm making sure the right things are happening, but I'm not your tester. You're a knowledge coach. Your role is to scale the testing skill set within your org. And I think, actually, even with that size, if you were to have more of you in that team, maybe two or three of people like you would be fine. But much more than that, you start to feel and look like a safety net that people would go after. Right. The one thing, too, that I always find interesting, you know how we previously talked about people go, well, if they had a tester, blah, blah, blah, this would have been found, and this would never have shipped. Hey, guys, particularly to those folks who are generally consultants who get paid to rile up testers and send them new projects, I don't know if you're aware of this, but I know hordes of testers who have found that bug and had the decision to ship taken from their control. So no, that's wrong. Your prevailing assumption that somebody didn't know about this before it shipped is not guaranteed. Yeah, and often, I hate, I want to end this subject, but there's context. Often, you may see this bug is horrible. How could they know about this? And sometimes you knew about it, but you didn't realize, you're talking about contextualization. You didn't realize the impact of the bug. Like, oh, this is bad, but it looks like it only happens in this situation, but it turns out it happens in a wider situation or something like that. Yeah, it only happens at midnight on February 29, and only if it's on a Wednesday. Oh, wait, no, it happens all the time. Whatever. Hey, we, thank you, three, one of the three sent us a letter, an email, not a real letter. I haven't got a letter in a long time. I get cards at Christmas, but we got an email. It must be nice still having friends. And you know what happens when, well, if you're not obnoxious and ugly, you get friends. Or one of the two. Yeah. You learned to live with it. When one of the three sends us an email and asks us a question, you know what that means, Brent? I do, but why don't you share? Mailbag! Woo hoo! Love these things. Christopher Rod from Microsoft. One of the three emailed us and asked us. I'll skip all the stuff that says, Brent, you smell bad even over the podcast, which is weird. But anyway, anyways, I have a question for your mailbag. Can you guys talk about your experiences and process you guys go through when doing planning and decomposition for sprints or semesters? I'm assuming you're involved in those types of meetings. Yes. How are requirements gathered, blah, blah, blah. In the past, I've experienced horrible requirements with one-liner storage scenarios, the infamous we're agile, we don't have documentation excuses came up, blah, blah, blah. So I spend a lot of time helping teams groom their backlog and get things in order and watch their crazy attempts at estimation. So I can add some color too, but you've done more of this, Brent, so I'm gonna let you start. Oh, I'm reeling on the agile equals no documentation, but I'm going to do my best to ignore this. It's okay, the three no. Yeah, that's true. All right, so. Anytime now. Where to begin? So I'll just say at a high level, what my team does is we want to, so semesters at Microsoft, I'm gonna start here. Semesters at Microsoft generally mean six-month chunks. I have a question for you because I haven't used semesters before, is that, and I just could be an ignorant part of my Agilish, is that a Microsoft thing or is that really an Agil thing used? No, it's a Microsoft thing. Good to bring up, okay, thank you. I ignore Microsoft-only terms, so I'm happy about that one. So the model that I am now most using, there is essentially three models that I'm familiar with. There's the Kanban model, there's the Scrum model, and then there's the SAFE model. My favorite one is the SAFE model. Really? Yeah. Really? Really. So elaborate, sorry, I'll shut up. Why does that shock you? Are you familiar with it? I'm familiar with SAFE. The SAFE planning model? Yeah. Okay. So when you're planning, there's a couple of considerations. In an Agil world, right, you want to, Alan and I have mentioned this over and over again, you want to create a planning model and a planning cadence that's adaptable, not predictable. Right? Yes. Yes. Now, there's another aspect to this that we haven't talked here, which falls into what I see occurring when people start the conversation around we don't do documentation. Okay, because in addition to, you don't want a prediction-based model, you also don't want an interruption-based model. And loose documentation, no documentation, encourages interrupt-driven processes under the name of agility. So what my team does, oh, I'm gonna go three more principles. Okay, number one, in order to adapt, that means you need to continuously groom your backlog and be able to update it with the latest available knowledge. For execution purposes, you want to have a model that gives the team a longer-term roadmap but creates the most amount of knowledge towards the short-term tactical bits. So to be more specific, Microsoft generally plans at a, not Microsoft, I don't think it's all Microsoft, but certainly my division uses six-month chunks. It's kind of a, more of a predictive model where executives ask us, hey, what are you going to do? Okay, and honestly, what I do is I produce that and say, hey, here's the direction I'm doing and here's what I think is happening. By the way, just to be super clear, as I discover new knowledge, I will adapt and I'll certainly set expectations, clarify when I'm doing this, but you should expect that changes will occur. And all executives I've ever presented do just go, well, of course, that's what you'll do, okay? And then I wanna have a planning cycle such that I'm producing the highest ROI first. Okay, that's the principles that I operate under. To build a roadmap, that's essentially, I'm going to book out three months of effort, okay? And the way I book it out is a month, I'll take each month, split it into two chunks, roughly aligned to two week sprints. I really just, I don't really care. I don't use a sprint model when I execute, so I just split the month in half. Whether or not it's actually two weeks, don't really care. And then for the first month, I will book work at 80% capacity, second month at 60% capacity, and the third month at 40% capacity. Book those three months in advance? Yes. Interesting. And so then the numbers may, if you're gonna do three months in advance, that makes sense, because you have this care of overflow, it takes care of discover to work, which you're always gonna have. That's interesting. I don't like booking three months in advance. But I, and again, understanding that you're not, based on what you just said, you're going to adapt and add and prioritize, et cetera. So anytime you're doing, for me, anything longer than, even a month is hard, but the farther out you get, the less accurate it is. And the more adaptability buffer, I'll call it, you need. So I like, that's kind of an interesting way to do an 80, 60, 40. Right. So you correctly identified why you do the 80, 60, 40. The other thing is, why do I book it out three months? It's, book is how I communicate it to others. Okay. The primary reason why I'm laying out a three month roadmap in that fashion, is so that my team understands sort of the quote unquote, long-term vision. And the point of laying it out for three months is that they can start their work for month number one, knowing full well sort of the work that we're gonna be doing in month number three. And so if the work they're doing in month number one could be influenced by that long-term work, they know that beforehand. All right. So it's sort of a balance between strategic and tactical. All right, so when you get to month number two, do you, does that become the new month number one for the next three months? Do they overlap? That's exactly what happens. So the week before month number two. Sorry, I keep on stealing all your punches. Yes, the week before month number two, we rebuild the plan, right? Because month number one, I needed 80%. Month number two, I needed 60%. If I executed on month number one perfectly and I were just to move the month forward, then I got 20 additional percent of capacity that I need to fill out. So I have a question for you. So how did you come up with the 80% number? That's 80% capacity, right? So what you mean by that is you look at capacity for the team, then you schedule based on 80% of that. Is that correct? Yes, so there's two cadences that we do in terms of backlog planning, okay? Number one, the roadmap building that occurs once a month. There's also a weekly backlog meeting, okay? So we want to be able to continuously adapt to current information, okay? One of the single, in my view, one of the single most important principles to the ability to adapt and keep a sustainable pace is you must have free capacity. So 80% is not a buffer, which is often, is what is considered in the old school world. It's not sort of a, I'm sorry, it's not a buffer, it is a buffer for sure. It's not a buffer of, hey, my team sucks at planning, therefore I'm gonna add a buffer. It's not that. Its whole purpose in life is unbooked time. As I go through capacity planning, like for example, if you were to go to my, I don't think I've erased it yet, if you were to go upstairs to my office right now, you would see that I have the three months segmented, or bifurcated, so I have six segments. And one of the things I do is I then say, okay, so roughly two weeks at 80% means eight days available per person. How many people will I have this time period? I then say, okay, I write, say, number 24, if I had three people. Then I ask them, hey, who's taking time off during this time? That gets subtracted. Then what we do is we look at, there is a pre-planning process, which is around what are the scenarios that have been asked of us. I go through an ROI process to determine which of these things I should prioritize. I do not use just a single stack ranking model. I use what's known as WSJF as a model. I find that extremely effective to rank these scenarios. Then what we do as a team is we take this scenario, even if it's ambiguous. I bring my whole team in a room, we look at the first scenario and say, okay, what do we need to do to achieve this outcome? And then we start discussing what the tasks are, and well, I'll have a bunch of sticky notes. I'll write down the task as it's mentioned, put it on the board, book it in one of the months, and say, how much is this gonna cost us? And we- And you book by our T-shirt size? No, I go, I use Fibonacci numbers and I estimate in days. Okay. Okay, so if it's really huge, and we only use Fibonacci numbers for this because history has shown that as things get bigger, the more likely you don't have a clue how much- Absolutely. So do you have a limit on how long a work item can be? I will only allow Fibonacci number up to 21. Days? Up to, listen, I'm not done. So when we're going through the initial planning, I will only allow it up to 21, okay? But anything 13 or over, when we've identified that it's a 13 or a 21, it gets broken down. Okay, so let me just give you, I have a lighter weight approach we use on our team, which is both good and bad, I think. We also came up with the 80% number, seemed about right. We're gonna do 75, but it seemed, again, we're gonna experiment with it. I like the idea of sort of booking things. We have stuff on the horizon, but it hasn't been estimated or costed at all. So I don't know if it's gonna end up a 60 or not. And again, based on our ship schedule, which we talked about before, we don't have a ton of time anyway to add a bunch of stuff. So that's all the same. We don't use Fibonacci. We do make sure teams break down any items that are, I'm a big fan of the two-day limit for a task. I think even beyond two is when the odds of your estimate being correct start declining rapidly. But I don't make and break down things unless they're five days or more. The thing is for me is I don't prioritize very highly that estimates are correct. No, no, and that's true as well. So we do it, you have to, there's the predictable part where you want to figure out how much work can we get done during this iteration where you have to use the estimations and capacity as a model, remember, all models are wrong, some are useful, as a model for figuring out capacity. Our team is, and I don't think this is a bad thing, and this is where a lot of teams get themselves in trouble. Our team is particularly horrible at estimates, but that just means we haven't learned a lot about them yet. Or you haven't learned how to function without requiring estimates. Yes, correct, either one. There is an amount of predictability that executive leadership usually wants around those estimates, knowing when things can come in. And when you're working on a complicated product where there are dependencies from team to team, being able to estimate to some level of accuracy when a dependent feature is going to be available is important. I want to clarify that very strongly because that's another reason why I do the three-month and I have the, we'll call the adaptation buffer while it scales down. If the direction doesn't change month over month, then I'm able to publish a committed plan, something that my team will commit to if you add up all the numbers, I'm basically taking one third of my team's capacity off the table by doing this model in the three months. So I'm confidently able to commit to what I laid out. If month over month, if the roadmap changes, then the people I'm committing to are involved in that discussion. I think you're, I agree completely. I think I'm talking about different timelines here. We're talking about dependent features being available on a week by week basis or sometimes even more granular than that. I actually, so if you have a team that has a high degree of dependency, then there's another model from SAFE that I much more prefer, which they call the program board, which allows, it's, you can think of it kind of like a scrum of scrums. It's not, and it's super effective unlike, well, that's further proof of why it's not a scrum of scrum. And then lastly, and I'll close out and you can have the final word on this. As my team is working together and we're decomposing these scenarios into features and plotting them out, right? We also write up, hey, what are dependencies? We can move, we shuffle tickets around as a result of, hey, we have to, if there's a natural sequencing, right? Or if we need to get people involved, right? We go, you know what, we need something from that team. You know what we're gonna do? We're not gonna start our portion of this until March, but I'm gonna write a ticket in the start of February. I'll write one ticket for each of these chunks in February to say, reach out to that team and make sure that they're progressing. All right, one thing that's nice or maybe a little bit different on our team is the bulk, we do have dependencies on other server products in Microsoft SharePoint, Exchange, Skype chat services. But for the most part, we're self-contained and we're all on one, about to be two floors of Lincoln Square, so a lot of these things are just conversations in the hallway versus you know, more formal tickets, et cetera. But it's, the challenge is, the challenge can be when you have dependencies, keep everything linked together. One other thing Christopher asked was the role of the generalizing specialists or specializing generalists and how, if we use those skills in these meetings, I want to talk just a little bit about that. So one thing I do in the, when we're talking about estimates of things is just reminding people, remember I'm the quality and testing specialist on the team, is they talk, somebody may be throwing out estimates or something, it's about a day. And I think, and I stop and I give them a look and they go, and they'll look at me and they'll say, Alan doesn't think it's a day, but it's really pretty trivial. I said, there's a world that you came from which isn't the world you're in now. So done, completing the work does not mean it's working as the best I could tell. It means you've written tests for this, not just unit tests, but some integration tests to make sure this works. You have measured code coverage and looked at where gaps are, you've looked, you've added the instrumentation to this to make sure we can monitor how it's working in the field. It's hooked up, if it's part of a performance scenario, that's measured and working and can be tracked end to end. There's a whole list of things that goes into done. And at first, I get answers like this, well, that's why we have the buffer, right? So no, no, no, no, no. It's not that kind of buffer, like what Brett talked about before. It's just unscheduled time. So we need to, when you think about the answer, Christopher's question, one value I bring is helping someone see holistically what it takes for a feature to be done. We had, and this has happened a couple times in my career, once I watched it happen on my team last fall and I will not let it happen again, it was when the feature is done, or almost done for about three months. And I can't handle that. It's too much. You need, things need to get done done now. Done, done, done, done. One thing, one bit of color I'll add to that is my definition of done is proof that it added customer value. That's it, right? No, no, no. Until you've tied the dots together that shows proof that this has added customer value. It's not done, go back to the drawing board, re-estimate. Now, in the waterfall world, that's a pain in the ass. Hey, we only ship once a year in the more agile world, particularly in services. I'm currently helping A-Team that does an on-prem product and they are going to ship monthly. It has to change somewhat how they're gonna view customer validation, but they're going to be able to do it. It's also helpful in terms of failing fast. Like if you've just produced a feature that you find that no customer's using, you can discover that in two weeks versus one year and go, okay, do we not actually understand our model? Because we just spent a month testing this feature no one's using, right? For me, it is so much ROI-focused, it's pitiful. But on the, I, as you know, have gone back to school and weekly using Christopher's question on generalists and specialists. I've learned something interesting that I wanted to share with you, Alan. I can't wait to hear it, Brad. So I'm currently studying business process management. How to optimize processes. It's all about systems thinking and it's fantastic. As I was reading chapter one, they started bringing up the topic of generalists and specialists. And I learned something, yes. They mention these things outside of the AB testing podcast? Outside of Agile even, yeah. Whoa. Yeah. And I learned something that I never knew before. They know when management was created. Do you know when this was? No. Do you have a guess? No. When was the first manager ever? I think you'll find this whole thing fascinating. I don't know. A long, long, long time ago. That's what I thought. No. And thank you. Cause all the other people I've done this with, they answered correctly. And I'm like, damn it. How did your educational system not fail you? No. The first manager was created during the industrial age. I don't know if I... Nope. So I'll give you a name. Fellow by the name of Frederick Taylor. Wasn't like a medieval king, like the manager of his court? He wasn't a manager. He was a ruler, but manager in terms of command and control, like we're used to today, didn't exist. The king is the definition of command and control. I'm going to let you finish, but I don't believe the research. All right, go on. Fair enough. Actually not fair enough, but I'll send you links later. Yeah. Just cause it's written on the internet, it must be true. All right, go on. So the first manager was created during the industrial revolution. Now, the reason why is because prior to that, things operated by guilds where you would have the barber's guild or the shoemaker's guild or the baker butcher and candlestick maker guilds where they would work as a community and they would very deeply understand that whole process of barbarism, but not really understand how things got to them. Now, the guild model, you can view it as sort of a generalizing specialist model. When the industrial age hit, but there was no sort of manager of barbers. They worked together to determine what would set the standard. When the industrial age happened, Ford came in, the assembly line came in, and that's actually when specialists got constructed, at least to the degree that we're familiar with it today. This guy, Frederick Taylor, lived 1856 to 1915, created this topic called scientific management. And his purpose in life was, given this assembly line model, how do we increase productivity? And what they discovered is that generalists in that model completely sucked. That the ability for, since the work was highly repetitive, that it was much more productive to have a single individual do that repetitive task over and over and over again, and then have chains of single individuals doing their own different repetitive tasks. Now, they could also optimize a way, so the dependencies. So if you were imagining you and I were working on the assembly line, your job was to take a doodad and put a bolt through it, and then I came after you, and my job was to put the nut on this bolt. We could put a wall between the two of us, I can still succeed. Okay, here comes a doodad with a bolt through it. Here's a nut, screw, screw, screw. Highly repetitive specialists in that model. Now, this is where managers came from, because now when you over specialize, these guys don't need to talk to each other, and if you change the order in which things are done, it goes out of control. So you needed to have a manager who's controlling who's doing what, when, and how. And as that boosted productivity, management became a science. How to command and control, things like even probably business process management came into fruition. Now, fast forward, thank goodness, fast forward to the 80s. Oh. All right, so what happened then, as they learned that this was creating efficiencies, they began to construct the functional model, where the departments would be invoked, where departments would self-optimize. So now we're seeing, okay, we're starting with specialists, and they're highly siloed. They don't need to talk to anybody, because their work is highly repetitive, and it's the same thing over and over again. Then departments got constructed, or then managers got constructed, then departments. And this created a fractal pattern. And in the 80s, it started to fail, because the number of departments, much like how specialists were not working well together, now departments aren't working well together, but now they have created such an immense system that departments failing to work together is causing failure, causing critical failure. I tried to wind this to the software world in a generalist and specialist discussion, right? And I started thinking through, in our model, right, what is the place where work is highly specialized in terms of the definition laid out here, which is highly repetitive, that doesn't require dependencies? Yeah, I think in that case, in this particular model, the metaphor between factory and software and knowledge work, it's obvious that it breaks down. Right, but I have the generalist versus specialist battle all the time. Whenever I'm onboarding a new dev team to Agile, they tell me, but I'm a specialist. And I'm like, okay, what is a specialist? A specialist is someone with very minimal dependencies and repetitive tasks. And I'm like, I firmly agree that you view yourself as a specialist, but the world that you're in doesn't operate that way. Right. Right, and... I think what they mean when they say specialist is, there's one thing I do really well and that's all I wanna do. Right. But you still have to adapt to be able to do that, and it's not a specialist in the terms you've described today, but within the context of knowledge work, they are a specialist. And honestly though, I think a pure specialist and a pure generalist are not paths for success in the software industry. They're not, and I don't even think a... No, they're absolutely not. I do think more of the guild model is worked better. Right, but once you recognize that really the generalist and specialist, not only is it, I do something really well and that's what I wanna do. Okay. But there's also this idea that's generally around the specialist and I don't wanna have to play with others. Right, there's a siloism that's generally involved. Could be. And what I'm basically stating is, in knowledge work and we're in knowledge work, if you want to go fast, if you want to be more productive as a system, you need to optimize for the reality, which is there are dependencies, there is communication required. And the only way to optimize the whole system is essentially to get rid of the specialists. You have to convert them to minimum generalizing specialists. They have to be able to optimize and reduce time spent on knowledge transfer. I agree. Anyway. And we should talk about that some more sometime, but we are way out of time for today. Yes. All right, everybody, I am Brent's friend, Alan. And I'm Brent. All right, see you next time. Bye. 
