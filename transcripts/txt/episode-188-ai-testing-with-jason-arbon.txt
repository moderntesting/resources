Alan here jumping in off the top to tell you that our best friend of software in the world Zencaster kind of shit the bed on this podcast. I have spent countless hours this weekend stitching together the fragments of audio that it gave me. In the end we lost a big chunk of our guest's audio. I think I was able to string this together a little bit in a way that gives you the gist of what we were talking about and then we're going to follow up and have Jason on a future episode of the podcast. But I think it's enough here to get some conversations going. Hope you enjoy and we'll try it again next time with a different audio software. Welcome to A.B. Testing Podcast, your modern testing podcast. Your hosts Alan and Brent will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now on with the show. Hey everybody. Welcome to the A.B. Testing Podcast. This is my happy jingle. Just made it up. I'm Alan. I'm Brent. And we have a guest. I'd like to welcome, before he speaks, I'd like to welcome President Joe Biden to the podcast. Joe, would you like to say something? Umm... Oh, actually Joe had to step out. So we have a better guest. We have, well, why don't you introduce yourself Jason? Yeah, I'm Jason Arbin and I'm a test nerd. And I listen to this podcast religiously and I get upset when I have to wait more than two weeks for a new episode. That's our fault. Now that's right away. We are 42 seconds in minus the theme song and we have a dig on us already. So I feel good. I feel good. Yeah. We're all about self-deprecation here Jason. So knock yourself out. Yeah. And there's a topic we already discussed we're not going to go into. The one thing I really like about Jason is he's not afraid to, he's to the point, he will say what's on his mind. I think I titled our last Cod, our Cod past, our podcast, all we talk about is AI, which is the new name, which, oh, you know what? Brent change your name to Ian and we can be the AI testing podcast. Or I could just get rid of you and it could be the AI and Brent podcast. Brent you couldn't find the recording button as it was biting you in the pants. Oh yeah. No one wants that. The quality, I mean people already complain about the quality of our editing staff. It will just go straight downhill without you controlling. Here's what I want to do. I would like to get Jason involved and kind of hear his thoughts because he has great thoughts. Jason's been in, it's isn't the Jason plugging. We don't do like, you know, like here, let's interview you about everything you've done in your life. But lately, the last couple of years, you have been deep into something that's become very popular, AI, and voicing thoughts on how AI can help testing. I think we're in a weird spot in the industry and, well, let me get back to that in a second. But I guess the question of the day is, and we can all chime in on this, is what is going to be the impact of AI on software testing? So I guess that's a direct for me. So I'm used to just listening to you guys and not interacting. Although I do yell at the radio or the car radio. Here's our first tangent. One of the things that used to exist when I was studying music was these records called Music Minus One. And what it was, it's like musician karaoke before karaoke was a thing. It was like, it could be a jazz thing. It could be an orchestral piece. And they just leave out one part so you could play along. And we should do a podcast like that, Brent, where we just leave space for people to add themselves and they can just make their own podcast out of us. But yeah, for this one, you're actually live in here. They could do that already, right? They could just... Pause. They could just skip the first 50% of the podcast, which is all the tangential bullshit. Why don't people make mashups of our podcast and mix it in with somebody else's? Anyway, let's rewind the stack and get back to how AI is going to impact software testing. Yeah, so I think there's really... It's a very broad question. I think there's really three cohorts of people that will be impacted differently. One are the folks that are just putting their head in the sand. They don't want to know it exists. They're threatened by it. They're concerned. They don't like change. I think we'll just hear less from them and about them over the coming years. I think it's not that interesting, actually. There's always people that... There's people still red horses around, right? It's all good. The second cohort is a primary set of people that are traditional software testers or people that care about quality. I think that what's going to happen... People are not really ready for... Those people want to add AI as a superpower to make their testing and their work better, faster, smarter, or cheaper. But I think what they don't realize is that this generation of AI is generative by definition. It generates things like test cases and bugs. People are... I don't think... Because I wasn't ready for it either. You get overwhelmed by the amount of data that comes out of these systems. You used to manually create them by writing a bunch of Java code or Python or typing into a test case manager. Now, they're just generated automatically by a machine. Testors will have to deal with this deluge of information to triage it. I think they'll have two big impacts, though. Bear with me. This is just kind of a highlight of stuff. I think there'll be two big impacts on software testing and testers. One is that there'll be more demand for testers that are experienced that can triage issues and tell the robot, yes, no, yes, no, yes, no. But there'll be less demand for basic software testers, like newbies, freshies, and stuff like that, because the machines will just do that work. And they can't really do the triage work. And then there's a third class of folks where teams will... New teams, you start a new team, you start a new app, or something like that, you will just start to use AI first before you hire that human tester. And the real interesting extrapolation and race here is, will the AI get better faster than the demand of those teams that delayed hiring a tester over time? But I think that that's... Those are the major trend lines I'm seeing. So you hit on some things that are very, very interesting and something that you know if you listen to the podcast, but I want to repeat in case anybody's listening to this, is that Brent and I started this podcast because we were seeing changes happening and testing with teams living to more, I don't even call it agile, more adaptive and faster delivery. And what we saw was sort of the same thing. A lot of people just put their heads in the sand and said, this isn't happening. A lot of people were overwhelmed by it and didn't understand how it would affect their job or could it affect their job? What could they do to not make it affect their job? And some people were like, yeah, we're in, we're going to go make this work. And we started the podcast because we wanted to help people navigate what that change was like. And I feel like eight years later, I think we're kind of in the same place in a way. AI is changing. Whether you like it or not, you can put your head in the sand, you can be overwhelmed by it, not sure what it's doing, or you can ride the wave, but it is going to make difference in the way we deliver software, like it or not. And now Brent's gone. So good. It's the. So now it's the AJ testing podcast because Brent puked live on screen. Okay, Brent's back. This is fun. So I'm curious in this case, well, Brent figures out his stuff. I fully agree with those kind of three camps where folks are. And based on those three camps, is it similar to what Brent and I have been giving a few years ago or like what advice do you have for testers right now who are either ignoring or watching or actually, if they're listening to this, they're probably not ignoring AI, but they may not be sure what it's going to do. What advice do you have for them? How do you, how do they, how do they ride the wave versus get beaten down by the wave? It depends on the person. I think they're going to self-select into one of those camps. And I don't think you or I or Brent can do much about it, actually. I think that's probably why I relate so much to modern testing and you guys banter here on on the podcast is that you're trying to help you, trying to talk through it, trying to be honest and vulnerable and trying to get through it with everybody. But I don't think that it helps all that much. I think the people that really need to listen to us don't listen to us. Right, right. And the people, there are people that can, there's a middle, there's a middle ground there where people will listen and go, they get confirmation bias, confirmation bias, or, you know, they know that they're not alone. I think it helps a lot emotionally, but I think people will just automatically self-select into different groups. And I, but I think like you guys have done, you at least give people an opportunity to hear the other side. Um, but I worry about, this is the funny thing. I worry about pushing too hard. Like if people just don't want to do it, you don't want to drag them into a fight that they don't have the energy to deal with. Right. And so it's almost cruel in a way. So I kind of feel like people just self share the information, um, and let people self-select in or out, but at least they should see the, hear the, have a chance to get the information. I think the single most important thing, um, it is very similar to then. Is to as much as possible, get as many people to realize that they have a choice. Right. They can be the butterfly or they can be the wind. They have that choice. Now, if they self-select, they self-select. Right. Um, in my view, if they, the, this is a bit of wisdom. I, I tell my children it's refusing to make a decision is a decision that you've chosen to be the wind. You're going to be guided in whatever direction, the powers that be in your life, uh, take you. Is it the heart? If you choose not to decide, you still made a choice. I, um, uh, got a D in my philosophy course in college. So I probably had not the person asked on that one. I took, I took it 75 years ago. So, um, but that's the important thing to me is making sure that people understand they, they at minimum have that choice. Well, it's, I want to jump in because you're, you're totally right, Jason. It's a weird place where we're doing a podcast and generally the people that are sticking in their head in the sand, they are not actively looking to discover new knowledge anywhere. They're not reading articles. They're not interacting on social media platforms around technology. These, uh, they're just, they're not going to conferences. Uh, they're just not. So you're at those people, the people that we probably need, you know, the most advice and kind of on how to deal with this stuff, aren't listening. And that's always, that's kind of always been our place with our three listeners there largely in camp with what we're doing, or they're wanting to move that direction. So, right. And you mentioned conferences too. Like I, I showed up at two conferences in the last two weeks, the last minute. And it was interesting. Almost everybody was talking or wanting to see something about AI. Then where, you know, a few years ago, I couldn't drag anybody into a talk about it. Now everyone in every vendor has applied AI to the end of their software name. It's kind of like going to Microsoft. Remember when they added.net to the end of everything in the early 2000s? Yes. I worked on windows. So he.net not what about not an ounce of managed code in that thing. And don't forget about active X before that. Oh God. All right. I worked on active X on windows CE. Enterprise for, oh, for business. Sorry. Windows CE.net for business, active X for windows CE.net for business. That's the product killer marketing. Yeah. So I think everybody's wanting, they're kind of curious, but the funny thing is there's like, I'm in different rooms where the focus is on AI. And then somebody, not me actually, would ask kept asking like on these panels and stuff, who here has tried chat GPT? And like, it was like a third of the room and raise her hand. Like my mom has used it. Right. When they know offense to my mother. Um, but she uses it and she asks me about AI stuff sometimes. So if you've got two thirds of a room of people that are in the technology world, right in software, even, um, and they haven't even tried it in the, in the excuse is what is too difficult because it's a chat window. I don't, I don't get it. So I really, I really do worry about trying to drag those types of folks into the conversation. It does come down to that fixed versus growth mindset in a way. And they're just, like I said before, they're, they're just not seeking knowledge for a lot of those test conferences. The vast majority of people have been in testing for less than a year. And their employers sent them, they're saying, please go learn something to make yourself more effective because the employers don't understand testing either. So they're just inherently not curious people. I think I, I'm not willing to, to, I am to pass such judgment, but, but. I mean, that does that same question totally befuddles me. One third is shockingly surprising. Like when you said that I'm like, okay, so my next follow up question to Jason is, Hey, when you went to these conferences, are you finding that they're now sort of old school echo chambers, or is it now more what Alan would just say? And no, it's young people who don't know better. I think it's, it's a barbell. I think you've got the old folks like me. And then you've got the young ones. Middle people are just busy doing actual testing. Um, they're the quiet silent majority. Yeah. But old, old people like you have used Chet GPT. Yeah, but I'm a little, yeah, yeah. No, and, and yeah, yeah. A couple of times. Yeah. Well, I think there are, my, my LinkedIn Chets beg to differ. Uh, I think, you know, my experience is, is that, you know, the other end of the barbell from the noobs, they are often there to try and peddle their wares to the noobs note, not picking on you. I'm picking on the folks trying to sell their, um, consulting skills because they, they prey on noobs. Yeah. I just had a robot. Um, but super interesting. I like, I use Chet GPT probably daily. I'm, I'm far, I'm far smarter with Chet GPT than without. Yeah. Yeah. But that's, that's a, that's a low, it's a low one mark though. Um, dude, you know, what's crazy is that, uh, there's just today, like there was this, you can look on find it on LinkedIn if you really want to, I'll protect the innocent, named innocent. But someone that heads one of the software testing organizations, like the association stuff that, you know, that think big thoughts all the time on testing, um, they were asking, and I, I, I like him as a human being, but he's asking, has anyone tried Dolly three? I'm interested. I haven't used it yet, but I'm interested in what people think about it. What, what kind of a question is that? Like it takes four seconds to try it. I, my, my kids can use it. Um, and like you just wasted. And then the people that reply that say, I haven't yet either, but will soon, it takes longer to write that response than it does. It would be to go generate a fix. Like, but, but that's why, like, but so bringing them into the, into the presence, not even the future is, is maybe a dis you know what, I think that I've, something I've discovered is we haven't talked about AI internally at my company is I think, I think they're a little daunted by it because it seems hard or it seems like it's magic. And I was thinking that the Clark quote, any sufficiently advanced technology is indistinguishable from magic. Then I go back and Brent, do you remember, I brought this up maybe on the podcast before, but I bring this up once in a while, but 10, how what year is it? Maybe 10 years ago, everything was data science, Brent's a data scientist now. But remember when, it's like, Oh, we'll solve that problem with data science. Everything was data science. It was just the new, it was the new magic. We are, Oh, you know what? We'll get a data scientist. They'll figure this out for us. And now today, 10 years later, it's AI. Let's use AI to solve that problem. We can do this AI. Yeah. Yeah. Actually you, you're buried the perfect summary right there. It was that, yeah, it it's intimidating because I think of the last 10 years. What's there's been a huge inflection point last October, right? October or November. Um, because, and I, I spent tons of Google money, uh, and tons of my life, my actual life on this planet, labeling, uh, you know, finding as many pictures of login buttons and search text boxes as I could, and then labeling them. It was very tedious, very expensive, very intimidating. And the stuff would work, you know, 90% of the time when you're happy. Right? Like it was very brutal. Um, but with my fingernails off, but go on with generative AI, it is, it, things have dramatically changed. Like I'm literally probably, I'm rebuilding some of the stuff I've done before right now. And it's with, with no team, like it's crazy, but I'm partnering with, with AI. And so it's, it's, it's, but it, because it's generative. And so it's weird cause you have this huge angst that only this, these super smart people in a closed room at AI at Microsoft or something can, can do this AI stuff in this magic, right? But it's been democratized in the last year so that anyone can get access to it and it's smarter than the even fathom to guess that it is. And they're worried that it's even smarter than they think it is. And so it's just a huge inflection point. So with both, like you said, it's both magical and it's intimidating. I think those things are still conflated with people in their mind. So yeah, I basically just did a long version of what you said eloquently and succinctly. No, no, no. I think what we've said before is I think people, they want, again, I'm not going to throw, try to throw people out of the bus here, but they want it to be something it's not sometimes too. They want it to answer their questions. And for me, it is my constant collaborator. Now, and honestly, right, there was a lot of that back and forth, right? I do see, there are times where I had to sit down even with data scientists and explain to them how LLM works. And there's a particular problem that I now actually even straight up call the genie problem. I think I've talked about it on the podcast. And there's this whole language. So people are getting more and more familiar with it. They have enough exposure to it. They it's being demystified. That said, right, machine and human working together, quite honestly, I think is always going to beat the separate component pieces. Finally, we have an argument or discussion. I actually I'm in my experience, it could be the human operator that's partnering with AI when I like the funny thing about about chat GPT is it's not in a loop, right? It just waits for you. I am the slow part of that loop. I am. And in fact, the way I used I've changed how I use it dramatically over the last six months, I used to think about my existing problems, break them down into pieces like we're told to do in software engineering and then have each little piece implemented by some AI prompt magic thing, right? With enough data. And the funniest thing is I've realized is that if you start with just the top level problem and then have GPT break it down for you, like the human in the loop is the dumb part. Like I like the GPT is unleashed only when I remove myself from more and resume myself from the equation. GPT is more powerful and I'm more efficient in what I'm trying to accomplish. If that sounds too convoluted, it is. But and people don't realize that it's also want to say that, you know, talk about data science or, you know, it was, you know, cloud or whatever these past revolutions have been. And they change the world and people like go, oh, yeah, or test automation. Right. Oh, yeah, there's there's now somebody trying to automate the things that I'm doing. OK, get it. But they have to linearly create them and stuff. But this is a sea change. Like people don't realize or want to realize like, oh, yeah, I can do some software testing maybe. Right. But Congress has hearings because they're worried that it may take over the world and kill everybody or destroy all jobs. And it's a national security risk. And we just had their past legislation to block. Well, it's unfortunately not where we put our smartest people. Right. But but guess who showed up at the hearing, though? The people making these models. Right. So and maybe it's in their self interest and regulated interest to do so. But for the protective modes. But but this said about data science. So it is a significantly different. It is. I think it freaked it freaks people out. The magic of it freaks people out. But people are too afraid of Skynet. And we are so many iterations of AI away from Skynet. Can I can I we could just agree. But by ones don't get exponential growth. Like, Brent, explain the exponential growth. Like, so I don't seem like I'm overly biased. Explain exponential growth by Jason. That's exponential. As always, it's inarguable. No, no, no. So so now I would want to get into sort of discussion around the distinction between an AI and AGI and ASI. AI is how do I put this? They know everything but can do nothing. That is why the human in the loop process or the human and machine working together is always going to outperform. The issue is AI, the AI, like, as you know, is generative. Like, I tell people over and over, you think of a generative model. Think parent. You go to someone's house and you hear the pair go, hello, hello. Right. You think of it. Oh, oh, they're greeting me. No, but from the parents point of view, that's just the noise. It's heard repeatedly. It doesn't know what it's saying. It's the same thing with the generative AI. I have a process, for example, in my team where where I've created a Python tool and I've actually created two agents that actually have a conversation towards a goal. So that aspect of it that you're talking about where where you could have LM work it out with itself. Right. The challenge is, is right now it doesn't have that thinking process around what's the next thing I should do with this. It does now, though. So like there's auto GPT and Microsoft released one a couple of weeks ago. I forget the name of it. It was Microsoft people. They're untrustworthy. That's for sure. We know too much. Could you trust the people that make PowerPoint to do anything constructive? If they put AI in it and then the AI generates the slides. Now, and to be clear, like you just said, auto GPD does this. And I'm just going to say up front, it doesn't. But the way they built it is an over. No, the way they built that is quite smart. So it is it the code that runs out of GPT knows how to drive LLM and take advantage of LLM. To even further make this sort of magical experience where all you need to do is add another little widget and suddenly its capabilities have grown. OK, so I think the key thing about AI is not its capabilities today. So say you grant you everything you just said, just granted all that. The people making these things and finding correlations between the amount of data and compute they put into it in the orders of 100 million dollars per model right now. And I think they think the cost per model might actually go up to 500 million or a billion in two years. But they're seeing an exponential increase in the capabilities of these AIs of even just LLMs. Their growth is accelerating. This is my talking about exponential growth thing. Right. Never get exponential growth. Like we couldn't have dreamed of auto GPT in all of its horribleness two years ago. It wasn't on anyone's radar. If someone thinks it was on their radar, they're lying because then they were just nuts because nobody saw this coming. Not even opening eyes saw the promise of LLMs until it started just doing things. Right. And so I think what people are factoring, I think what you may be the difference between you and I and thinking on this topic is is how fast it's getting better. Like most of these people that want to put their head in the sand, like, you know, Alan was talking about, you're talking about these people that want to put their head in the sand or ignore it or whatever. They find things that cannot do all the time. Right. Oh, it can't do math. It can't add to 20 digit numbers or it can't do whatever. Do you know what it can do now? It can actually you can write an academic formula from a paper, put it on a whiteboard or take a picture of it. It will generate the Python and you can execute the Python. It's an implementation of that that algebraic equation, not just adding two numbers. But these things, these barriers are falling faster than people, like people, people are being proven wrong within like 20, 30 days. Sometimes even like the I saw this. I saw this same phenomenon way back in the day when Google was full on Agile and Microsoft was still doing sort of waterfall big design up front. Right. It's well, real quick, but that was a linear process. I worked on both sides of that phone. Google Google. No, I get that Google improvement. It was a I don't think it was a linear. It was a it was an exponential improvement with a lower exponent. What I do agree with you, what you're seeing right now is a massive growth curve. And the one thing like I think you want to push back on whether or not humans plus the AI. Here's the thing. Here's the thing. I think there's going to be an agreement on the humans that do pay attention and learn to work with the AI and stay abreast of it. Right. Just as you called out where you are successfully building through projects without a team and you were surprised by the fact that you didn't have a team. That's going to be increasingly the case. Yes. And I want to this up my I'm going to I'm going to this isn't going to be a tangent. It's is I'm going to build this into a segue. So AI, I think we can agree AI is going to make software development faster. We are seeing that with co-pilot all over the frickin place. Yes. And again, don't check in code. You don't understand, but it's going to make you faster. It's like remember when like I've written code in notepad. I wrote a lot of Windows code notepad, but think how great autocomplete is. But now we have like autocomplete plus plus. It's it's it's it's a whole new world a million years ago. It's talking to James Whittaker and who we all know. And we're talking about heads up displays and testing and like let's get let's what's the overlay we can get. And let's get let's what's the overlay we can use to to let people know what kind of like we talked about doing it for coverage or for you know flagging bugs and things and and then he went to another small company called Google. And I don't know what he actually did with it there, but I believe he did talk about it at least in the Google Google software book. So thank you. So James is really good. We love James. He's really good. About taking ideas and get other people to do the actual work. So didn't he make you write most of his book? I don't claim any of it. Okay. All right. So the real quick I don't know if that's the answer. I wanted to bring that up as maybe a possible answer, but what is the testers when what is it and when will we see the testers version of Co-Pilot? What is let's let's let's stream for a minute based on your experience or Jason. What is Co-Pilot for testers? The human has to be in the loop because I'm a human and that's my priority. Somehow I put myself in the loop here. No, I'm kidding. A GitHub owned by another company you might have heard of called Microsoft. They have a thing called test pilot already. It's in it's in beta like literally. Look at me. Who's asking dumb questions here. That was not a leading question, but now I want to hear more about test pilot. Tell me. Oh, well, no, it's but this is the thing that nobody can keep up. You know what I mean? Especially when you're wandering around Mount Rainier with a with a beer. I got to say I'm going to interrupt now, but I got I worry because remember when remember when Microsoft added the exploratory testing plug-in to Visual Studio and they called it XT like these are industry calls it ET. They said, Oh, we'll call it XT. So basically, you know, we're going to be doing a test. Let's call it XT. So based on that experience, I'm not confident in test pilot, but I do want to hear more. And that's OK. Like again, even in the more medicines. I'm not trying to convince anybody. I'm good. I'm good with it to be clear. Like if people don't want to think about exponential growth in these things. So the test pilot does it just writes test cases for you and there's a web page it demos it like, but it'll get better. That's the thing I think people don't realize about AI. Like it will get not just better, but far better. Like in, in, in, in, in very quickly. Yeah. And let's, let's, let's bring disturb a little bit. Cause I think generating test cases, I suppose that's interesting if you care about a bunch of test cases, but I wanted to generate, like I want it to prioritize it, like I don't want to know all of them. I want to know, okay, so do these, you're good. Guess who prioritizes all that stuff? The human human has to be a human, you know, right? Right. Doesn't have to be, I was arguing with Wayne, Wayne, the, you know, Wayne. Yeah. But we call him Wayne. You saw him on the love Wayne cause Wayne again, he's super freaking smart and bet, and he is not afraid to challenge. We just hear it. He lives in like the past, the present and the future. And they all can say, um, yeah, exactly. Exactly. Well, when I long term the mystery about, about like, um, because one of my screenshots, I shared something where there was a, I'm willing to pay $18 a month. Cause that's the price of Zencaster, by the way, for a podcast recording platform. Let me record separate audio Tyrax and it actually works every time. I don't ask for a lot. All right, everybody. This has been the AB testing podcast episode 188. I am Alan. I'm Brit. 
