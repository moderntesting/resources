Hey, Alan. How's it going, man? Going good. We are back for a brand new episode of A.B. testing. Episode number 64. Square number. I once again can't believe I made it this far, but I'm happy. And we will find something new and interesting or regurgitated to talk about, which is pretty much the way things work at A.B. testing. Yes. Good to be here. Good to see you hanging out in our hazy Seattle weather. Canada sent all their fire smoke to us. Yeah, it actually caused some disturbance in the Jensen household, because my kids were like, dad, why does it look so foggy? And I'm like, that doesn't look like fog. That looks like smog. And they're like, what? Smog? Yeah, they looked at me like, what? Why? Oh, dad making up words again. I'm like, no, this one's actually a thing. You just don't see it much in this part of the country. No. In our green, green Seattle. Where I grew up, you see it all of the time. All right, I'm heading off to Helsinki next week again. Didn't you just come back from someplace? Actually, I haven't traveled in a while. I go to San Francisco a lot. I don't even count that as travel anymore. I just call that a longer commute to work. And often, most times I go down, I will go down for the day. I'll take the 630 flight. I'll get into the office by 9.15 or so, and I'll spend the whole day there and take a 7 o'clock flight home. It must have been a trip to San Francisco. I vaguely recall you diatribing on interaction with some of our fellow humans on your last flight. Oh, probably. I always complain about my humans. Actually, for the commuter flights, they're actually doing pretty good. I went to Helsinki once already in March, but heading back, having a leads off site there to talk about people, do some planning, et cetera, should be a good time. And there may be some drinking. I've heard that happened in Helsinki. I've heard it happens in Helsinki. But I didn't look it up. I think the day should be about 20 hours long, 22, somewhere in there. Yeah, it helps you go to sleep. All right. Hey, before we get started today, we have a sponsor for this episode. Once again, the lovely folks at Star or TechWell, I should say, who do the Star conferences also have a conference called Agile Dev East. What? Yeah, what? What? And if you are looking for customized learning options to explore test-driven development techniques, tool selection, automation design, to fully integrate testing into Agile's delivery teams, you can attend Agile Dev East, which is the premier Agile event covering the latest techniques and topics to understand the efficiency necessary for project success. And that is this November 5th through 10th in Orlando, Florida, home of Disney World. So is Star renamed, rebranded? No, Star is in May. This is actually Agile Dev East. TechWell does a whole bunch of different conferences. This one largely geared towards Agile Dev. But there's like, I think it's coming up here in my script, but a whole bunch of topics on Agile testing. Love it. Me too. Learn the technical and team skills testers need for success in the world of Agile development. Immerse yourself in hot topics such as Agile and Lean development principles and practices, scaled Agile development, Agile teams and leadership, digital transformation, and more. And once again, stopping for annotation. As I was thinking while I was reading that is, this would be a great conference to, if you're an Agile tester or a modern tester, attend it with some of your dev counterparts. Yeah. The whole team can go. I actually really like that idea. Yeah. As an added bonus. Bonus? Who doesn't like an added bonus? Agile Dev East is co-located with better software and DevOps East conferences. Your one registration automatically gives you access to all three programs. Holy cow. Bring the whole company. What are those conferences? Better software. OK. And DevOps East. And there's so much overlap. We've talked about, there is a massive, especially where I work, a gray, gray, almost white, invisible line between ops and dev and test. So I could, this would be one of those conferences where I would maximize my value by watching like 10 minutes of every talk, skipping from room to room. Hopefully they have them recorded. Yeah. Wow. Back on track here, your one registration, as I mentioned, automatically gives you access to all three programs with insights and solutions across the dev lifecycle to build a customized week of learning that fits you and your organization's specific needs. Explore the program at well.tc, W-E-L-L.TC, forward slash A-B testing. And I'll put this in the notes on the website. And of course, I wouldn't have gotten this far if I wasn't going to mention this. A-B testing listeners can use the code A-B test to receive up to $200 off their conference registration fee, register by September 8th, super early bird deadline, which isn't that early for a conference in November. That's great. For combined savings for up to $600 off. And when you start talking about bringing your whole team, that's a heck of a savings overall. You can tell your boss that's in the whole team and take the money saved. And Alan said you could have that money as a bonus. You did say an added bonus. Yeah, I did say an added bonus. TechWell does not guarantee your boss will give you that bonus, but that is a suggestion from me. Let me know how that works. Again, well.tc, WAC-AB testing. I'll post that later. But that sounds like, I mean, that's a good idea. That's like putting, that's like a food court. I'm going to use this. And thank you, TechWell, for the sponsorship. And I hope our listeners go. But I want to use this topic to this ad, this sponsorship, to in thinking about this show, it's inspired a lot of questions that I thought I could think about myself, or I could just bounce them off of you here today once you stop looking at your phone. I'm just looking at the topics here. Lots of topics. They got lean, they got safe. Scrum, something called Icy Agile. I've never heard of that one. You ever had lunch at Google? Yes. So you have a whole bunch of different kinds of food you can eat. And you can eat as much as you want. This is that for agile. OK. Yeah, the second part of your phrase is a little scary when it comes to agile. Yeah, just fill up a little bit of a, go to the sushi bar, get some of that, and go, you know what? I'm kind of hungry today. I'm also going to go to the pasta bar. And this guy's making fresh smoked teriyaki something. I'm going to have some of that. And because I'm desserts. And yeah. So agile is a little dangerous when you pick and choose a la carte. OK, don't take my metaphor for what it's worth, and then shut up. Shutting up. Shutting up, yeah. So I want to talk about agile and agile testing. One thing I'm all out of order here because I'm thinking while I talk and this shit happens is our podcast, A-B Testing, is as much of an agile podcast as it is a testing podcast, as it is a quality podcast, as it is a podcast about nothing particularly at all. And then every now and again, data. And data as well, absolutely. So we're, which I like. And I am going to break my arm a little bit and pat ourselves on the back because I do listen to every podcast, not in its finalized form, but during editing. I get to re-listen back. And I also listen to a lot of podcasts in my car. My car has TuneIn radio built in, which has podcasts built in. So I can just have my favorites there and listen to them. TuneIn has a podcast here? Yeah. And I listen to a lot of different podcasts about a bunch of different ideas. And as far as production quality and speaker quality and pretty much intelligence quality and everything else, we don't compare at all to things like most 99% invisible. I mean, there's some really well-done podcasts out there. But I got to tell you, when I listen to agile podcasts and testing podcasts, and this is just my ego talking, but I think we have a pretty damn good podcast, despite the fact that we're idiots. It's amazing. I think it's listenable. I think we have topics that inspire thought. And I'm proud of all we've done. I am as well. Yeah, I've listened to several agile podcasts. And one of the things I personally think, I think we're a lot more entertaining. I like to think so. So tell your friends that we're cool. I am now up to this is the awkward thing. I have an awkward point in my life. This never happened to me at Microsoft. Now it's happened at Unity, is I now have at least three people who tell me they actively listen to the podcast. That are employees? Employees of mine. I have employees that, in our one-on-one topics, list an episode of A-B testing where they want to talk about something I talked about on that episode more. So I have to be clear. Did you tell them the mailbag link is right here? Yeah. Talk to the hand. And the hand has an A-B testing tattoo on it. No, I want them to post it on the mailbag link just so that I can help them torture you. Oh, great. And usually it's like, so far it's been, I like that idea. How do you think that applies here? Oh, that's good. So yeah. That's good. Anyway, it's scary and bizarre. And then someone internally asked about, are there articles or podcasts I should listen to and one of them post, check out the A-B testing podcast. No, I don't need more listeners. No. But I don't think I've ever said or I'm not one to guard my words. So I think if I ever said anything, totally controversial. I work with a VP who's a something. One, it's just not true. But two, I think they go, oh, yeah, totally. Like if the thing is, I'll phrase it. I think you have done that one, though. Actually, I have had. There is one VP in particular I'm thinking of. At Microsoft or here? Microsoft. Oh, yeah. Yeah. And that's what I was going to say. It's like, I will call it like it is. And most the time, it's like, yeah, that's true. Definitely, I work with some dipshit VPs at Microsoft. I think that stays in the PG-13 area. So I'm going to leave that in. So yeah. So we're talking about the podcast. It's good. People like it. They listen to it. And then Agile. I'm going to ask you. Before you do that, just to close out the star thing. Oh, sure. I love this Agile Dev conference idea. I really do. I love the idea of people bringing their whole teams. Especially with the three conferences co-located. That's like a conference extravaganza. The topics are interesting and diverse. One of the things I think might be interesting as well is that they had tracks, depending on the maturity level of your team. I think they will have tracks. And one thing I've done, and Lee Copeland suggested this at the conferences. And I suggested, I hinted at it earlier when I said I'd go to a bunch of talks for 10 minutes, is you really do need to vote with your feet. Because you're going to have, I can imagine, as much as 10 or more different talks going on at once, maybe more across the conferences. If you can't decide between two or three or four, it's OK to rotate through and find the one that fits what you're looking for. I think you have to use the power of your feet and realize you're not going to offend a speaker if you step out in the middle to get your most value out of the conference. Completely agree in terms of learning. But I won't say this for too many topics, but for Agile, I will. It's one of those things where you do want to at least understand the minimum number of rules before you begin and follow those rules directly. Yeah, and I agree because I have seen probably one of the biggest mistakes I see in Agile is just doing part of it for Agile. Yeah. Doing part of it or doing a part of it to the extreme, just see every rant I've done on A-B testing on the Agile manifesto. But I love this idea. I love the team inclusiveness. It's fantastic. I had never heard of this before. This is a great idea. I'm glad you approve. I have so many things to talk about. I'm not going to skip that one, but I'm going to go on. I want to talk about Agile testing. What is Agile testing? Testing. Agile testing, to the best of my knowledge, is when you have a separated test team that is operating in an Agile software delivery cycle. You mean like a separate organization? Yeah. That's where I've seen that term used the most often. So I reread Agile testing, the Chris and Gregory book, the first one. OK. To be honest, I reread the first three quarters of the book that I stopped because I had some other work to do. And I had read it, it's probably my third time reading it. I didn't realize how much I'd forgotten and how much has either subconsciously or just by way of work become very relevant to what I do, what we do, what we've talked about on the show. I saw you on Twitter. And I'm going to get to that. So one example is on your definition is in the Agile testing book, Crispin and Gregory definitely advocate the role of the embedded tester on the team. The tester is in the end. So a test specialist. Yes, exactly. Exactly. And I also noticed they talked about the idea that a test organization can be separate, run by a separate person, even though those testers are embedded. And then I was surprised, maybe not surprised is the wrong word, but like I. Flabbergasted? A little bit flabbergasted. Maybe not too strong of a word. But right there on page 60, and I've talked about, and I thought I came up with this organically. And I may have, and I don't know, the brain works in mysterious ways. But they talked about, and we've talked about this, shot, the test team being more of a test community run by the QA director or QA manager, whatever it's called. And they used those exact words in the book. I thought, holy crap. And obviously, their words came out way before I came to this conclusion. But I think it's, I've been talking about this since I joined Unity. I figured this community thing out a month in. Like, oh, of course, this is how it works. And they thought so too. But then did my brain see that once and then make the translation? But I do really feel like it was something I recognized. Because my background for even years before that has been building communities. So I feel like I came to that on my own free will. But very interesting. I love that idea. And I love, I always think about it I always love when I read a book that supports the ideas I already think. Yeah. It validates me. I like to feel validated. That's the last book I read along those lines was the head first design patterns. Because I always designed in a design pattern way, but without realizing it. And then when I read that book, I'm like, oh, this is what I do. And oh my god, there's ways to do what I do better. And honestly, that is the, as far as learning design patterns, that is the number one book I recommend. Yep. It is. I mean, I read the Gang of Four book. Actually. Why? Let me tell you, I can't say read. I looked at all the words in the Gang of Four book. But it could, it was, it's a cure for insomnia. And it's such good information. But it's just I don't read books like that very well. But I enjoyed, I mean, how many times you got a textbook that was sort of a page turner? And to me, head first design patterns, really, it was engaging. It was. I read the whole, I read, number one, it's very rare that a textbook, I read it cover to cover. I read this one cover to cover, and I did it in two weeks. Including stopping, taking a design. There was a tool I was working at the same time. And I would, every time I learned a new pattern, I artificially included that in the tool. In some places, it made a horrible design. But in other places, it was fantastic. Cannot recommend that book enough. On design patterns, and then we should go back to the other topic, but on design patterns, there are only three books you should buy. Head first design patterns. A book by Al Shalloway and his team at NetObjectives, which is called Explaining Design Patterns. Which I haven't read. And then the Gang of Four book. No. And the reason why is, number one, the head first design patterns book is going to tell you why design patterns are frigging useful. Al Shalloway's book, Explaining Design Patterns, is going to show you how to design with them. You have lots of options with design patterns. If you need to connect two objects, should you use the adapter or the bridge or the facade? Al Shalloway's book helps you through that. And then the Gang of Four's book is really a reference manual. This gives you the, for some reason, you need a design pattern beyond the common 20. The Gang of Four book has them. That's it. You should read the Gang of Four book with as much vigor as you would a dictionary. All right, Agile Testing, Modern Testing. I should point out here, and I'll put this on the website. I think I may have mentioned this to you, maybe not. The Chris and Gregory book, Agile Testing, their follow-up, More Agile Testing, which is a whole new book. It's a whole new book. And Gang of All the books, especially the Gang of Four book, which is called, and the Gang of Four book, for those not familiar with design patterns, there's four authors, and it's just called Design Patterns. Yep. You can internet search Gang of Four, it'll show up. There's the O'Reilly headfirst, O'Reilly's publisher, Headfirst Design Patterns, which uses Java as examples, but it's totally readable, totally fun. And the Al Shalloway book called? Explaining Design Patterns. Now, I wouldn't recommend this for the Headfirst book because it's full of great pictures and things. But if you ever like listening to books on audio, our listeners can always go to audibletrial.com, WACAB testing, and get their first free month of Audible for Free, the first month of Audible for Free. Yeah. I haven't promo'd that before, but I'll just throw that in there because we're kind of in book mode. So. And then I guess the last Design Pattern book, because you talked about the Gang of Four book. Do you know where the Gang of Four book got their idea from? Yeah, from, oh shoot, what's his name? The Architect. It'll come to me, give me a hint. His full name is actually the name, the combined name of My Two Boys. Not a great hint for you, I know. That is not a good hint, give me initials. C-A. It's Alexander. Last name is Alexander. Yep. It's Charles. Yep. I do remember it's Alexander. I'm awful at guessing games. Let me give you one more second though. Dunno. Christopher Alexander. Christopher, that's right. Damn it. Timeless way of building. Did you name your kids after the Design Patterns? No, it was accidental. I didn't even know about Christopher Alexander until after my kids were born. Maybe not consciously. Yeah. But back to this idea of, like Stephen Johnson, he talks about how ideas can only happen when there's enough of a background for it to be possible. And then when it's possible, it's very common for them the same idea to pop up in multiple different places. Because it's just, in a lot of regards, it's just a logical conclusion to the two ideas coming together. You. This is Stephen Johnson. Where good ideas come from? You have constantly been involved in testing as well as you have constantly been involved in community building. So I find it not surprising at all that you may have independently come up with the same idea. Yep, totally fine. Totally fine. It's been a long time since I've read the Crispin Gregory book. So when I hear about this talk about community, I constantly think of it in terms of a matrix-style organization. Yes. And that's what they're referring to. Yeah, and I use that explanation a lot. Kind of how we work. So in rereading that book, it's like a lot of head nods. Yeah, embedded, obviously they don't use the phrase, accelerating the achievement, the shippable quality. But there's a lot of test specialists sort of working with that. If they want to, we can make the licensing. Yeah. Not having, and again, that is a book, probably the book on Agile testing, but there's a lot of variations out there on what testing is on an Agile team. So the question I have for you is on this show, we've talked a bit about what you and I are called modern testing. Yep. What do you think some differences may be between Agile testing and modern testing? Or is modern testing just Agile testing rebranded by A-B testing? Well, one of the things that I think is, I kind of wish we had the show notes from. And I have ideas. And by the way, we talked about Agile testing before we talked about it for 10 or 15 minutes in episode eight. Okay. All right. And you know this because of one of your employees? No, no. So the one thing I would say that seems obvious is this specialist versus generalist idea. In my view of modern testing, yeah, we don't really have the embedded IC tester. We would have maybe an IC similar to the role that you served previously. Sort of a, I don't wanna use the word architect role, but something along those lines. So this goes along the lines of on a good, it's an evolution in that way because if I do my job well enough, I talked about this on a Twitter rant as well. The role of, if your role is to be that test coach or the role of any coaching job, really, in Agile coach, test coach, quality coach, we wanna call it is to work yourself out of a job. Yes. So on these, or work yourself out of a role maybe, maybe take on a more of a generalist role. You do it for a larger part of the org. You do something bigger and wider. So that's one good distinction. I thought of two. One is the role may not be needed because the test knowledge, the testing knowledge and the quality knowledge is dispersed among the team. The other thing though that I don't recall from Agile testing, what we're calling in modern testing is we place a heavy focus on defining customer quality through the use of data. And that's the other one I was getting to is just data in general. One thing not mentioned in Agile testing, which is huge to how we see quality being done now, especially in the future is a heavy, heavy reliance on data. Data driven at a minimum, data centric as an ideal. What I recall from Agile testing is still a book on testing. Like what I'm saying on modern testing, it's not really about testing. Yeah. We can look at this as a bit of a maturity model. Agile testing is the book you need to understand how to give you a good insight and understand how to adapt from a predictive test last world into a iterative or maybe even adaptive test as you go world. There's a lot of insights in there into what that looks like. So you read it the whole way through recently because I do remember it covering the iterative, like this Scrum type model very well. I don't recall it covering the adaptive model. Not directly. Not directly. But again, another... Maybe that's in more active testing. Yeah, maybe, maybe. But one thing we've have talked about before, which is an important distinction is this idea that waterfall model, very predictive. Some people are happy with Agile being just a pure, iterative model, but I think Agile done right must be adaptive. So I don't see why they... While you can get stuck in the iterative, I think if you're talking about learning, and actually I wanna pop back to something, adaptive becomes very important. When it's done right, I'll tell you, when it's done right, it is adaptive. And the reason why is because when you've done it right, you've struck on something and you can't scale with the iterative model. You have to adapt. You have to have a mechanism to cut things that aren't worthwhile doing. Let me give you two examples from Unity. On my team. One is I have one lead who very good at Agile. In fact, so good that in one of the teams that he barely works with doesn't have a lead right now. He's not leading the team, he's just leading their, he's just basically playing their role of product owner. Product owner or Scrum Master? Both. Scrum Master. So he's not telling them what to do, he's telling them how to do it. Correct. He's a very good Agile coach. So he's playing that role across the teams. And again, specialized in generalist, he owns some quality stuff, but he's playing the role of a Agile coach. Okay. And just nudging, nudging that word, live along a little bit and good improvements, great feedback. He's helping those teams become adaptive. So I love it. Another example, my most, I won't use the word dysfunctional. The team that needs the most improvement out of all the teams I work with, I had a conversation. Do any of these teams list this team? Maybe. Hey. I am full transparency. I'm not gonna say this team sucks because they don't. Do they have some more room for growth and learning and becoming more adaptive? Yeah, for sure. And nobody anywhere in the org, whether they're test dev ops would argue with me. All right. But the point is I asked this question because I'm some, I kind of dive in and out of being directly involved in the different teams and all in like 12 different products. But I popped in on one of the channels and said, hey, does this team do retrospectives? Kind of knowing the answer based on, you kind of can tell whether a team does or not. And it was great because the answer was, we used to, but no, not anymore. But then I had a few people message me privately and say, no, but we really need to. Can you help start doing these? So they're, each time you wanna make any sort of change, having allies is good, but part of being a learning organization and part of getting to an adaptive state, like you can't do adaptive software development if you don't do a proper retrospective. Because a retrospective is about taking some time to reflect and learn. Yeah. No, the thing that I'm thinking here though, that gives a clue, it's essentially what I find is the most successful retrospectives are the ones where essentially the leadership is disinvited. In order to get there, now this is in the Microsoft mentality where command and control is so pervasive, it's hard to compare it to anything else. And it sounds different than the Unity model. But if they're asking for your help to get there, that to me sounds like a call for command and control to kind of nudge in a better direction. In this role, it's me not as a leader, it's me in the role of a specializing generalist. Just a couple weeks ago, I hit the reset button on my own team's retrospective. We had gotten a little bit formulaic, the team was going through, but the team was pivoting around being polite to each other. And so the really important things that needed to be talked about, like no one ever brought up the elephant in a room, I took back control and started pointing out the elephants. I find that my role, particularly as a leader in this part, is I can build a better team by getting the whole team to heat me simultaneously in retrospective. Now, because I'll go, I'm throwing you all under the bus. And it ended up being, I got on the retrospective, each one of my ACs came after the medium was like, I am so glad that you did that. So in retrospectives, one, there's a couple of observations I've had in my experience with Agile, which is not as extensive as yours, but not bad. Retrospectives, and maybe because I'm such a big fan of the learning organization, I think it's one of the most valuable, not the most valuable practice you can do. And that's probably before- I go back and forth between retrospective or standup. I'm gonna go on the retrospective side for now, but the other thing is, is that there are a ton of things you can do in a retrospective. There's not one true way. There's multiple structured models. And I've found that varying that, everything from, there are games you can do, there are just whiteboard.voting, there are a bunch of different things you can do. And the goal is not to solve every single problem of the team. It's just like the rest of Agile, it's a prioritization exercise. Like what are the biggest levers we can pull now? And surfacing those and coming up with not a- one of the mistakes I see in retrospectives is, yep, these are the problems, let's all think about these and we'll see if they go away, and they don't. You need to go away with an actionable plan. Like what are we gonna do? What are the next steps we're gonna do about these? And what do we expect to see? The feedback with retrospectives is, that's the most common is, hey, we've said this before and we've done nothing about it. One of the goals, so I run, when I do retrospectives, I run two different general models. There's two different structure types that I'm fond of. We do one weekly and then one quarterly. And both have to end with a minimum of two things to change and two things to celebrate. And the things that are on the change, they go on a particular rail in our, if it's work we had to fund versus sort of a philosophical change. It will go on a rail and it'll get funded. What you and I talk about a lot is another evolution of that, where the pure testing specialist may not be needed or is not needed on the team because the team gets that. And actually I have in the dev teams that I work with, there is at least one that doesn't have a tester on the team because of that, they've evolved. And actually two things have happened there. You can look at this, if people that have seen this is that team one, writes very good tests themselves and anticipates where quality issues may happen. And two, they're the most reliant on data. Yeah. So that makes sense. Yeah. It also occurred to me that that may be a sign of the maturity model that we were just talking about, shifting away from iterative to adaptive, right? Because adaptive, in order to adapt, it's about optimizing the flow of knowledge throughout the team. In order to adapt, you have adaptation is about picking the better direction to go. I think you can be... And that requires knowledge sharing. It does. And I think as... But I think you can be adaptive without following like we talk about with modern testing. Oh no, no. But I think it helps with the... I'm trying to generalize. Yeah. Like I think the Scrum model, I think the Waterfall model, I think they solidify. If you have a testing specialist, they kind of solidify those roles. They make it a dependent part of the process. We've talked about codependency quite often. Whereas in the adaptive model, you're like, wait, no. So when I do my presentations on Agile, the term specialist in my mind, I reframe it when I do the presentations. When Dev says, but you're saying I get this from Dev all the time. I'm a specialist and I'm not feeling special in your model. And I'm like, you know what a specialist is in the Agile world? And they look like, what? I said, they're called a bottleneck. Because if there's only one person who knows how to do this stuff, and if that stuff's important, then your bottleneck's behind their ability to get it done. Yeah. Right? In an adaptive world, you really despise bottlenecks. Yeah, absolutely. Yeah. Fully agree. Shall we, anything else on, did we finish on Agile testing? So was it just those two things? Specialist versus generalist and adaptive versus, no, quality versus testing? And are those the biggest difference? Because I do think, because modern testing, when I thought about it, it was very heavily framed in Agile philosophy. I think it is too, but I think the differences are, it's an evolution. I think so. And it's, I don't know if quality versus testing is a given, but I think the two big things are, is that the test specialist may not be needed, and extreme reliance on data. Yeah. I think those, in thinking, in reflecting about what we've talked about in our discussions on air quote modern testing, I think those are the biggest differences in between what we've talked about and where we see testing going, where we see quality going. The Whitaker talk on all this testing is getting in the way of quality. That title keeps on popping in my head. Yeah, when I read that one, that one was kind of a, that helped put the final seal in the coffin for me. I'm like, oh, I completely get what he's saying. Yep. That really added clarity. I think that'll inspire some questions for the mailbag. I'm thinking through the world that we were talking about when we talked about modern testing. I'm thinking the world where it's all personalization, everybody has their own little AI bot running in the cloud. I don't think Agile testing is gonna help in that world. No, no, it has to be, it's a different approach. And there's actually a webinar from the folks at Behave.org, who used to be called Which Test Won? experimentation website on using AI to do experimentation in multivariate testing, which is actually an awkward, an unplanned segue into the mailbag. We have a mailbag question. We do. Which is kind of about this same topic that we somehow evolved into. Which was odd. This time it was really accidental. It was truly an accidental segue. Brent, do you wanna read that question? Yeah, so on the Slack channel, Adrian posted a article. Adrian! Posted an article around something called Bandit Tests. And he asked, is that something you have come across in your job and have found it useful? Have you heard of it? I hadn't heard of it before I looked it up. And to me, and you know more about the things being about the statistician and math thing, it sounds like a sort of a variation on multivariate testing. Is it not? I mean, you're testing multiple things at once and you use the results to drive what you test next. That's the different part. Yeah, it's- So multivariate, as I understand it, is instead of an A-B test, it's an A-B-C-D-E test. The multivariate, the way you phrased it- Really, it's an A-A-1, B-B-1 testing. You're testing multiple things at once. So you can contrast it with A-B testing and multivariate testing. Bandit testing is extremely useful. It is harder to understand than multivariate or A-B testing. But it is something that my team does use in our job and we do find it useful. And we've actually productized our version of it and it's helping to run Azure today. Dun, dun, dun. Did you know it was called bandit testing? You heard about it before? Yeah, actually its full name is multi-armed bandit testing. You'll often hear it in its acronym of MAB, multi-armed bandit. I don't know if I'd often hear that, no? But anyway, go on. But you hadn't heard of that democratization until it came up here. So the general thing on this one is it's, they call it multi-armed bandit because they're thinking about slot machines, also known as the worn-armed bandit. And- Of course. I guess maybe not, of course. That was obvious to me because in the article they had a picture of slot machines. Yes. So the way- I realize that may not be people listening but I go, I don't get why it's called bandit testing. Do you put a mask on? All right, go on. No. So let's imagine that you're a gambler and you have 10 coins and you have three slot machines. And you can put those 10 coins in whichever slot machines you want. But you wanna do it in the ones that will pay you off the most. And so the problem with a multi-armed bandit is number one, you only have a limited number of resources. Like you start off with this 10 coins. You can't test everything at infinite item which is one place where it's different I think from multivariate or AV testing. Oh yeah, yeah, for sure. No. Because they don't necessarily, I mean, yeah, that restriction's there. You still would have an ROI sort of call on those ones but in multi-armed bandit testing is essentially you have a limited number of experiments in which to learn from. And at each experiment, you have to choose between exploiting the knowledge you've already learned from prior trials or experimenting again. So it's multi-armed bandit testing, in my view, makes a lot more sense when running in production. Yeah, absolutely. Right. Doesn't all experimentation kind of run in production? No, no, no, this isn't an experimentation thing. This is essentially a decision framework. Okay. So you can think of it as if we're trying to choose between a best resource for something, we can look at prior history or if there's a new set of resources that come online and we don't know how good these ones are yet, we can experiment with these new ones to see, hey, do we like these new ones better than the ones that we've already been using? So you can kind of think of it as a way to build ML and do sort of a A-B testing type of system online, in line. Gotcha. There you go. So it's extremely useful. It's a slightly different thing than A-B testing. I get it, I get it now. My head's spinning with, as you know, the one thing in this statistics world I've studied a little bit because I've impressed you with some fancy words in the past is experimentation. I think moving towards a culture of experimentation, again, my head's spinning. I realize experimentation is different than bandit testing. What was it called, M-Mab? Multi-arm bandit testing. Multi-arm bandit testing. But I think there is a marriage of ML, AI, and experimentation that is yet untapped, but what I'm thinking of in my head is take a typical experiment. Which treatment is better? Treatment A or treatment B? Typically the way you do that is you route some traffic, some portion of traffic to the new experiment. You look at the result you want, whether it's a click or a signup or whatever business result you want from that, you measure the difference. And you come back and you look at the experiment results and you go, okay, this worked as expected or didn't, let's try something else. But I'm predicting, I mean, it's not too much of a stretch to think of a world where you could be a lot smarter in that sampling of who you route to. You could change based on what responses you got. You could say, oh, you could write some tools and just tell like, oh, in this demographic or in this part of the world or in this time zone. If I show you the ad between two and three, I get a lot more good things, clicks, whatever it is, clicks. There is a lot of, in addition to the just traditional, like do they like it, do they don't like it, is you could use some computer power to figure out why and when or where or how they like it or don't like it and adapt automatically. A lot of the A-B testing that's done even today is about finding the best least common denominator. Yeah. Right, as you were talking, I was thinking of other examples. Like, so for example, is Unity in the cloud? Do they operate in the cloud? Yeah. Okay. That's why I'm employed. Okay. Let's imagine that, so here's an example for multi-armed bandit. Let's say you need to create a new VM in the cloud. Okay. And you had some success criteria, reliability, availability, some other itty, I don't really care. Okay. Now, what you can do, now Azure offers, I think we have 20 different regions in which you could deploy that VM. So which region should you deploy it in? Right, you can deploy it in the one you're already in now or you can deploy it in some new one that you haven't experimented with. But once you deploy it, it goes to prod. So if you deploy it, you could deploy it in the ones that you've already been doing and with a well-known base of the reliability or availability, or you could deploy it in something new that could be worse, but it could be better. And multi-armed bandit testing helps you adapt the right strategy. So you go, okay, I'm gonna put one here so I can start getting information from this one to see if it's better or worse. And then if it ends up being better, then the ML would eventually, as you bring down some of these VMs, it would bring up a few others here. Got it. It's a method of making a data-driven decision. You have a choice to deploy in this new DC, but you wanna make that choice based on data, gives you a safer way to, well, a way to use data to make that decision in the long term. And most importantly, it automates that decision making. Absolutely. Which is like, you don't care. What region is deployed in, that's not something you wanna spend two weeks on, studying. No, absolutely not. Yeah, cool. I think we're out of time. We are. I'll let you go back on vacation. But as always, thank you, Brent. Thank you, Alan. All right, we'll see you next time on A B testing. 
