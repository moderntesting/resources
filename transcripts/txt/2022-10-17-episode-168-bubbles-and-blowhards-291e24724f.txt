If you want to increase velocity, you can't view it as a development task. You got to view it as a quality task. In other words, development and testing of that code is the same friggin thing. Welcome to A.B. Testing podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing, leadership, agile and anything else that comes to mind. Now on with the show. Did you know there's a button you can press to record? I would assume so. It's the A.B. Testing podcast. I am not Brent Jensen. He's the other guy I'm Alan. And I am too. Nice. Hey, you're good on my end. Yeah, I'm squiggly. Less weird. Okay. I, I, I am filled with squiggle and be I'm going to hope. Um, so what we're talking about is we've been using Zen caster for the past few weeks, I get a little better feed from the Brent Meister and all in all, hopefully record ends up in a better sounding podcast. But, um, Brent's not seeing little squiggly's, but I am hopefully we're recording Brent. Yes. Actually, should we stop and test that? No, no, no, we're good. We're good. Cause I'm recording you directly via the magic of USB as well. I have a backup rent. Oh, fantastic. There are extra recordings. You're good to go. I believe in redundancy. Do you know in my backup? So I have, I have a NAS at home, um, or we back up stuff from our computer, a network attached storage. No, I know. I would just like, wow. Okay. Where we back up stuff from our computers and photos and things like that. And then that computer is also backed up to the cloud. So if the house blows up, we still have the main files. I believe in redundancy. Yeah. I had, I was running with a NAS for a while, but then to me, I'm like, okay, carbonite, $99 a year. Here you go. Like carbonite just works for me. Yeah. But I like having the, the onsite stuff is good. I like redundancy. Carbonite may go out of business and where does your stuff go? Oh, I don't know. Then I, then it's kind of like the stuff in your closet. I don't know if, do you have rules around? When your closet gets filled up, dude, I don't have rules for anything. Oh, once a year, I'll go through the closet. It's usually right after Christmas where you know, you got all the new shirts or whatever and you got to figure out, okay, where the hell am I going to put this and it's, it's now at a lease recently used goes to Goodwill. All right. Yeah. I don't have rules for that stuff. I just, it's all, it's all just as needed, just in time flexible. I was talking about rules this week. I'm not going to go into a rule story, but I don't generally follow them. I follow them. I don't, I don't look at me. I listener, I'm looking back at you. Don't look at me like, Oh, Alan doesn't follow any rules. Rules are a great guideline. I think rules, I think they're good. They give me a, a lane to work in, but I don't really walk down the middle of the lane. I wonder whatever side of the rules makes most sense to me in my context. Yeah. I think, I think we're now going into sort of why Britt and Alan are INTPs. Cause I absolutely hold the same opinion. I'm like, okay, well rules are good guidance, but there's a reason why they came up with the phrase exception to the rule. I think I can give you an example just recently from work where I don't think I'll get myself in trouble for talking about this publicly. Although I got to tell you one of the listeners, Hey, Rasmus, I just saw him. He was in the room with someone I was about to have a one-on-one with remotely. He kind of waved to me and said, Hey, Alan, I have to talk to you about your last podcast. And that was it. So I don't know what he had to say about it, but that was interesting, but I think I can get away with this. So for example, I think it's the same at Microsoft. Microsoft does, you have your three check-ins a year, and this is, this is six year old data, but you have a number of check-ins a year that it's Connect or something new, but then one promotion process per year. Or is that not right? Trying to remember how that worked. Well, it doesn't matter. So two, two required Connects a year, two, two formal promotion events, a year in alignment with Connect, but you can promote literally any time of the year. You just go through a different process if it's not part of the formal. And we're kind of in that phase where we have the official process and the unofficial process, but the unofficial process isn't really air quote supported. But it's just a place where I bend the rules. Cause sometimes it's where it's, it makes sense to do it. So I, I will bend rules. I will not break rules. There's always wiggle room. There's always wiggle room. So, um, what do you bet up to, man? Anything exciting? Mariners, Mariners, Mariners has made a big portion of my life recently. Except they lost the last two games. Yes. And sadly two one run games hurts. They both hurt very bad and it's all because of Alvarez. Um, and I don't know how we got seven and let them get eight. The first game like, uh, girl, uh, don't worry listeners. I won't let him talk about baseball for too long. Now I, this is about the end. I'm I haven't looked yet because I'm not very optimistic, but I would very much. Like to see if I can score a pair of tickets for tomorrow. Ooh, you will, you should be able to, I don't, you can pay top dollar online, like on, on what is it? Stub, stub. Oh no. Yeah, you're right. If you can, I've done this in the past, having had or shared season tickets in the past, even for playoff games, um, you can get tickets outside for face or sometimes lower because people are looking and get rid of them. We'll see what happens. Um, I, I, I, I, I, I did a podcast last week. Did you know this? Uh, I did know that you were doing the podcast last week and I'm guessing I learned that you did this roughly 10 minutes into you doing said podcast. So there was an invitation that you were on. I'm not sure what happened there. Like my end looks like Brent Jensen is on the invitation and he's accepted. But you weren't there, but I, um, if you remember Darko from episode one 59 ish, I think, uh, we promised to go back on his podcast and you can just imagine Brent and I went and recorded on his podcast, not released yet, but that Brent was just very quiet. Yeah. We, yeah, we were absolutely both there. Yeah. Yeah. We were both there. Uh, except for more than likely everyone on the, all the listeners know. Pretty good chance. Alan, uh, mocked me on the podcast and I never mock you. No, well, you actually have two personas. Oh, do you think only two? That's great. No, no, there's two in this, in this particular context, one where you're like, okay, this is formal. Brent's my partner. Yeah. This, like if we were going to head to head with someone that was shooting down MTP, uh, yeah, you wouldn't go into the mock phase. But if we're not doing that, it's all bets off. Anyway, anyway, yeah, it was good. We talked about software. It went great. Uh, not posted yet, but you should be subscribing to the semaphore uncut podcast, uh, semaphoreci.com slash podcast or your favorite podcasting host. I don't think that people could like go to websites to click on podcasts, listen to them there. No, they get an app to play their podcast for them. So anyway, go check that out. Our episode should be coming out shortly unless, you know, sometimes I've had, I'm not going to mention where, but I was a guest on another podcast a while back. Well, I did a guest recording for another podcast and I'm guessing that as they listened to it, they realized, yeah, this isn't that good. Cause it never ever got posted. So if, and I don't think that the maker of that podcast, listen, this is our podcast. So anyway, I'm hoping that doesn't happen with Darko and we, and I get posted on his, but I think it will. Yeah. I can't even, I can't even imagine that. Like, like there's like, sometimes people think I'm interesting than they realize I'm not. And even in seven years of podcasting, you have not yet figured that out. So that's okay. Yeah. Fair enough. All right. Uh, let's do a couple topics today. I promise. Again, two episodes ago and then last episode, I promise we would talk about moving from being a tester as many of us think about it today to moving into experimentation, that kind of testing. Oh, also last weekend, I should mention, cause we're going to get to that topic. I hope in about 15 minutes, but last weekend I went camping out in the mountains again, um, did not get eaten by bears. So that was great, but, but it's a great way for me to supercharge my recharging. So, but while I was high up on a hill somewhere, trying to jump over a waterfall or something, uh, my presentation, my recording played for the test flicks event. So this is the one where I, you saw my slides. This is the one where I talked about, uh, the tester. I was this traditional or even modern tester and the tester I want to be, which is I want to experiment and make sure we're building the right thing. The one quote from the Twitter sphere that got repeated from the whole thing was, uh, and great, you're not going to have any problem with this. I don't think, but I said, you can be the advocate for the customer. You can be the proxy for the customer, but you are not the customer. Oh, yes. That was the only thing re-quoted over re or quoted over Twitter. Quitted over Twitter. No, I quoted over Twitter. Really? Interesting. So it's all right. It should be, it should make it to YouTube at some point. You can all check it out, but for now you've said that folks who registered, well, I'll let you know when it shows up. You've said that you said that quote in more interesting ways in the past. I'm sure I have. Yeah, that was very, yeah. You're not the freaking customer. You blow hard. That's the softest version I've seen. I've ever heard it from, uh, from second hand from actually is the second hand. So you are saying second hand of someone quoting you anyway. Doesn't matter. Let's carry on. Yeah. It's carry on. So along those lines, uh, one of the things I do for paint, so I do a five for Friday every week, and this is it again, every, all of my stories ramble. The reason I have five things to write about every Friday. What the hell just happened? So I'm back. Okay. I'll stitch it together later, but this is may go in the podcast with some beeping, but the Zen caster piece, which is good. When you get to the end just crashed in the middle of the podcast. I was, I was about to ask you, okay. Did you figure out what that button did? I didn't touch a thing. All I did, I immediately looked to see. Okay. I wasn't so Zen caster crashed. Uh, we're back now. So the way I end up with topics for five for Friday is probably 10, 15, 20 minutes a day, usually in the evening. But sometimes just when I get a break and I needed to veg, I'll go read crap on the internet. And a lot of it comes from Reddit. A lot of it comes from a half a dozen other sites I go to other slack communities and I find well, glance at a lot of articles and I'll take notes on the ones I think, you know, this is kind of interesting and I'll, and I'll give it a deeper read and I'll find stuff worth sharing. One of the places I go where I have, I have yet to find a post to share on my five for Friday, but it also reminds me of what bubble we're in. Is the software testing subreddit on Reddit. There was a question yesterday. I wanted to share with you just to get an idea of where we are in the bubble topic. Should dev teams test their products and features? Obviously they're in the bubble. I just know that bubble. We're in the other bubble. Yeah. But the fact that this is like a credible question with, you know, a dozen answers lets you know where we are. The answer is yeah. Of course we have an AI. Wait, wait, hold on. How many you said a round a dozen answers for team up votes. Yeah. And I'm trying to think through how many different synonyms for the word. Yes, there are. Yes. Definitely. Absolutely. Uh, there's at least three who replied said, let me know what's wrong with this sentence speaking from the perspective of a dev, I test my stuff before sending it to QA. That's that's nowhere. And I was expecting something dramatically worse than that. No, no, but handoffs are inefficient. Well, no, no, this absolutely. But at least he he tests his stuff before sending to to QA. Although I have not, I mean, I've been in my career for a long time. I've not ever, not ever met a dev that said anything other than that. So of course I test my stuff. Scrolling through here, one person did reply with just yes. So that's good. Anyway, a bunch of other answers, not going to go into them, but just, I see that I go, Oh, that's where we are. Somebody had to ask this question. Oh, I was, I got to it very quickly. What about dev testing other devs work just like code review as a dev. Did you go find this? I did. I don't want to dwell on it because I want to go on to something else. No, but let me, as, as a dev, how feasible do you think that is? Okay. Devs can code review other devs. Why can't they test their, and they can't, I mean, in a way they can. A lot of the devs, one of the things I coach dev teams, I work on with, it's not just like, here's my code, go test it. You're responsible as a developer for code correctness of your own code. The way you test it is, Hey, let's, let's build an app with, if it's an API, let's build an app with our code together on this Thursday and Friday. Let's do something to test this together. Let's, uh, let's do security testing together on this. In that case, testing everybody else's code together, but that's the way I would have, I wouldn't, I would not give devs like, Hey, here's a folder of my code, go test it. Hold on. Hold on. I didn't even hear what the hell you just said because I was reading. You were reading something. And I am getting pissed off. Okay. All right. I already closed the window. I don't even know what's there anymore. I was ready to move on and piss you off even more. Uh, we'll do that in a second. Again, the question is how about dev testing other devs work? Yeah. Okay, great. So dev replied, not feasible. This is not feasible because number one, it would cause an unpleasant amount of context switching between projects and tasks, the hell, like what the hell do you, what the hell does this person think is happening when they hand off the test? It would slow down the development of two too much. And there is, you go look at, Oh, I'm blanking product development flow. Author. Reiner's. Thank you. You go read his book and he shows that that's a bullshit argument. Like you can't, if you want to increase velocity, you can't view it as a development task, you gotta view it as equality task. In other words, development and testing of that code is the same frigging thing. And as well as an accelerate, yet these guys are living in 1998 again. Okay. So let me go on, let me go on. Cause I have another one on this topic and to be clear, I'm going to read a post from says I was tagged into in LinkedIn. Okay. And I want to be very clear. I'm not going to try and hide the author, but I'm not going to tell you who it is first. I don't mean this as an attack, but I do want to get your reaction on about four paragraphs. I'll pause after each paragraph for you. Four paragraphs. How many? Okay, fine. Go. Okay. Just, I'm going to read you. They're short. I'm going to read you like two sentences. I want you to react. We'll repeat that three more times. And then I want to talk about like the bubble and then that's going to transition if all goes well into the discussion I wanted to have around moving into experimentation, a paragraph one shift left is a slogan created by people who don't respect testing to justify the elimination of testers where they do the most good, which is towards the right, or if not that, then it's a slogan that naively asserts that the bugs we want to find will be put in at the left and be easy to find there. If only testers weren't playing video games or otherwise wasting their time while developers were developing. Yeah, that's a stupid rant. And it is, it's somebody who's, who's just getting defensive and, and not thinking through even, even in 1998, right? We had the opinion that bugs are cheaper when they are the most left. Well, that's been, that's been in textbooks since 1972. Let me go on because you're going to let you have a different reaction for each paragraph, but that's all you get. All right. In fact, many important bugs are not put into the code in the earliest hours. I think that's provably wrong. And in fact, many bugs that are put in are expensive to find early while being rather easy to find when the system is more mature. Um, yeah, if you put it together, a bunch of components and didn't do testing along the way yet, you're going to find out pretty quickly. It doesn't integrate. So that's, I think product to get shittier. Like, yeah. Give me that whole paragraph is just pure horse shit. No, it's, it's pure horse shit, but based off of actual and actual truth, right? And it's essentially, yeah. If you, if you build a bunch of shit and then you wait until the last second to string it together, yes, it falls apart. Yep. Next part shift, shift left doesn't mean shift left. It means spread left because we still need to test after the product is built. Since shift left comes along with prohibitions against testing a built product, it also means spread right. Forcing us to test in production. Think they know the author of this. Doesn't let me get to the end here. I don't want it to be a, I want to actually be a respectful reaction to this. No, but the thing is, this is the first time I'm wondering what hole I'm under. Yeah, yeah. That's exactly what I'm going to bring it up. Cause I feel like when I read this stuff, I go, oh my God, last paragraph. Cause I want to focus on the words, not the author shift left is supposed to mean collaborating with developers. This is often problematic, especially when developers don't understand testers and are powerful enough not to have to understand this puts tremendous social pressure on testers to conform to a devs fantasy about what a tester does for a living. I don't know that I would disagree with that last one, right? It just, to me, it feels, so I'm trying to, I'm trying to run a rant filter, uh, uh, on your phrases. Right. When I read that, I'm like, okay, ignoring the rant bed, it kind of sounds like they're talking about the codependency loop that you and I speak to all the time. Obviously the conclusions that they're referring to, I don't agree with. Well, but the phenomenon is absolutely there. Like this sounds, I mean, hell I could have shift left was a thing 20 years ago. Maybe I could have made this ranch 20 years ago, but I don't think I would have because we had testers involved very early in the product cycle, even a Microsoft. Uh, but it just, it feels very 1992. Oh, and, and just, uh, I don't know if it's, if it's helpful to you or not. But Wikipedia says the first reference of shift left testing was in 2001. Okay. Which was in fact, 20 years ago. There we go. There we go. They bring up, they bring up the whole, the whole V curve that we've seen a million times. Yeah. Yeah. So a lot of this is, I don't agree with really any of it in there. Nothing today, especially. And of course everybody's figured it out. This is a post on LinkedIn from James Bach, who, um, you know, I think continues to function in his world. And I think there's a collection of folks who see what he writes. Who's yep. Yep. That's exactly it. And that, and the main thing is, is not pick on, you know, I don't, I just don't care about James. He lives in this bubble of a world, but he's not alone there. It's not like he's preaching to nobody or yelling at the wall. There are, I didn't read the comments. Um, I just looked at what I was tagged into and comment on that last paragraph. But that's for a lot of the testing world is, and you can tell from that Reddit post and this, there is a chunk of the testing world still in the world. We lived in in testing in the early nineties. Well, yeah. But here I, I don't know who's in the bubble and who isn't quite honestly, but I'll just say from my perspective, the Island that they've been living on has been eroding, right? To me, it's, it's kind of now at this point in time, sort of the tail wagging the dog. It's, it's a vocal minority and these moves around software development, right? They continue to sort of threaten to shake their, their worldview. And to me, it doesn't, if someone doesn't satisfactorily answer the question around why is this way better? It's going to continue to happen. You can't get someone to believe something when their incentives are, are, are, are motivators to not believe that thing. I don't know. At this point in time, I'm like up until accelerate came out. You and I just had our experience. We hadn't heard of any sort of formal study being done on this. So to me, accelerate was just a fantastic body of work because great. Someone's finally gone out and done a scientific study on, on what we've observed. That's, that's the huge part. It's not just some business author writing a story about what worked for them. It's based entirely on research. Right. That continues. That research continues every year and it continues to support those metrics. Right. And everything else, you know, yeah. The people holding onto this whole world with, with white knuckles, they would probably be best suited in my mind, right? At this point in time, it's a bunch of, in my view, disciples and acolytes that are keeping this alive. But in the longterm, if they want to win me back over, okay. A place to start is go and read, go and read the, the current research and then start talking about how that research is invalid. Just have a little bit of breakdown on this and then we can probably move on. I want to dwell on it forever. The first three paragraphs are the part that's just in the wrong century to me. It's just, it's based on experience, not on research or knowledge across the software industry, based on the author's experience. That's fair. That's fair for James. I get where he's coming from, from the bubble he's in. The last one is the part that bugs me maybe even more in the last paragraph, whether it's true or not, because it points to a, from again, from James point of view, from the author's point of view here, a continued observance of a lack of psychological safety, which is not, you know, I just will not allow, I will not work on a team like that, that has that sort of devs are powerful enough not to have to understand just, it's like, whoa, whoa, hold on. I just thought of a way I would ever, ever want to work. And if you're a tester listening to this and you feel like you're powerless on your team, that's just, it's just wrong. You don't have to work. You can change your job or you can change your job. These days you don't have to work on a team like that. I at least, and again, you may be in a place where you, that's the only job you, only option you have. Maybe you can't, but I would, I just hate the, to consider the fact that people are working in unsafe environments. Right back on. I completely agree. Like I don't, I don't live in that. Even here at, at, at Microsoft where, where once we were claimed to be the evil empire, right? The, and as you know, toxicity used to be alive and well here. It's in fact, it's one of the main reasons I'm at unity. I just got sick of it. I don't, I don't have that anymore. I'm glad to hear it's going away. So you still have, you know, you still have big, big asshole or you still have assholes who think they're, they're, they're crap don't stink. Right. But that's going to happen everywhere. But what's now different is they don't thrive here anymore as they used to. If you can't collaborate. Yeah, it's, it's the one thing that's actually changed a big deal here at Microsoft. And it's, it's essentially anyone can anonymous, anonymous. I can't say, so I had a dentist appointment this morning. So in my tongue is starting to start. Um, num. Anyone can anonymously report abuse that we used to see all the time. I had HR call me in regards to one of my employees who had, uh, was being berated by a PM and, and an entirely different party said, what this guy is doing is total crap. It needs to stop. And they just set it off to HR and HR came back. Now, thankfully I had it. Every company should have, should have that, should have that safety. I feel like you can speak up that 80 repercussions. Absolutely. So, all right. That's enough of that. You can go read it on your own. I'm not going to link to it, but I just wanted to get your reactions on that. Cause it's the bubble part and there's a safety part, but now, I mean, we're moving on, we're moving on now. I am not going to go another week without getting to the topic where we're going to spend 20 minutes on the next topic. So we're moving on. And I have a story around this because I was around at the big MSFT when they waved a magic wand and they said, Oh, you testers now you're responsible for data stuff and we did, um, what, what did they call them? Uh, something in measures, questions and measures. No targets and measures. I can't remember. Uh, we did some, uh, kind of dumb stuff, but it was a start. It was a start and I knew it wasn't right, but I knew it was a start to get people in the right direction. I'll remember the phrasing in a minute, but let's say I do, I don't care whether I'm exploratory tester. I do. I'm writing automated tests because our developers suck and aren't doing them. Or I am coaching developers how to write better testing wherever you are on that spectrum or bubble. My stance is, I think our stance is it would make sense for you to, it could make sense for you to move into experimentation as well, into the kind of testing that we call modern testing. Now it is modern and it is about testing. How would you make that transition? And I'll tell that I'll tell the story about me in a minute, but how would you suggest, what's your advice to someone wanting to make that transition? So I have, I have, I've written this up someplace. Is it, is it in the unpublished Galazzo book? No. Um, this was, so when I first took on my data science role here in Azure, you were still at Microsoft and it was a, it was a role that was at the beginning of blowing up within Microsoft. Now there's data scientists everywhere. But back then, um, I was on the leading edge of this and others would come up to me and said, Hey, I want to do what you do. I even, I even had a talking deck for this. If you're trying to get into, if you're a traditional tester and you're trying to get into this space, right? The first thing you have to do is get into the production data. You have to wallow in the data and, and look at, Oh, here's a great example. So you previously we've advised people, uh, long, long, long, long, long time ago around how to change their automation. Right. Don't, don't write, don't write the driver code, just write the validation code and have your validation code. Or was that the other way around? No, write your, write your driver code, have it manipulate the product. Then write your validation code entirely off of the telemetry of the product. Don't write your specific validation. Right. So the first step is, is to, to realize that the telemetry can also be used for, for testing. Sure. So I'm going to interrupt because that's, if that's step one, I'm going to interject step zero. Oh, and again, you're coming from a place of a little bit of privilege and knowledge working at a company that's collected customer data, non PII customer data for decades now P zero. Go ask someone, find out what data your company has now about how customers use the software. They may have no telemetry, but they have some sort of business metrics, figure out what your baseline is and figure out what, look at that until you can come up with more questions you can't answer and then figure out, then you can at least know what's there and what you have to work with and what's missing to get started. Then you can move on to Brent step one, maybe, maybe with a step zero a in there as well. You're, you're absolutely right. And like it, I remember having these conversations at the same time. I was having agile conversations, right? I specifically got called into an office offsite years ago and they were the client team and they were like, well, how do we get, how do we get our data? I'm like, well, it's, it's easy. You, you put on a checkbox that says, may we collect data? It's to make your life better and happier. And they're like, yeah, but we only get 10%. Well, yeah, that means you have to figure out how to make effective use of that 10% of the people who do send you stuff, obviously getting data first, right? Then, then to me, and I don't want to make this too long and below the, I'm trying to at the same time answer your question, trying to remember the deck. The next thing is getting accustomed to interpreting that telemetry and the hypothesis of explain customer behavior. What are they trying to do? Why are they doing it that way? Cause here, here we're no longer really trying to validate. How do I put it? This goes to the heart of the thing that you and I keep talking about. We don't give a rat's house about testing anymore. Nope. Here, like in this world where like that's Jeff's job and we trust them to nail it. But now what we're trying to do is, is determine quality, determine, okay, what are the problems that you're trying to solve? Do we find anything that's that's, um, so anomaly detection, I was going to say, that's not really experimentation, but with experimentation, you can take, uh, the cuss or the PM's sort of documents and then turn those into hypotheses and roll them out and measure it and do statistical testing. I mean, it's, it's stuff I do all of the time. I want to, I'm jumping back and forth. Oh, yeah, you are a little bit, but I want to, I'm going to, this is a good time for me to interject and tell my story because you're a data scientist and I'm not, you've been doing this for a long time. And I haven't, but I can talk about the journey that happened whenever it was 2014 or so after Xbox one shipped. And I worked on that stupid science project to put Android apps on windows phone. Uh, one thing that happened at that time was Mike, that was when windows and I was in Xbox, because pulled into windows and then my manager was going to run this science project team. So I did the science project with him. And at that time they basically got rid of testers Estes in windows, but they laid off some, yay, Microsoft. And then they put the rest into these fundamentals teams and some of the dev roles, but these fundamentals teams did one big thing they did was they called it data science. It really wasn't data science right away. It was a crash course for everyone. Some people like Steve Rowe, guest former guest on the podcast will be back again someday, just whole hog into it. Sometimes a little bit too much into like, we need to learn a statistical algorithm said, no, we need to learn how to collect telemetry. But, but again, he got to the right spot. So what happened was I ran data science until we hired, um, so we hired someone who knew what they were doing. I was the data science rep for our group. So I would go meet with the other fundamental leads or data science leads across the cross windows. About 12 of us would go meet once a week and just be completely stupid about what it meant to be dated. How do you, how do you use data? One of the things they wanted to do was take their old test pass rate dashboard and just make it work via telemetry. And I pointed out that maybe, maybe that isn't the thing we want to check. Maybe we want to check on quality or customer outcome. Like, Oh yeah, interesting. So, you know, one thing I can do is be the voice of reason. So the other thing I did, as Brent recalls, is I just read a lot. I just tried to learn as this, the, we talked last time about how I learn. I tried to shove my brain full of stuff about how using data works, trying to get more information so I could speak intelligently about it. That's when I went and gave that talk on AB testing, use a little bit of the language that was great. Uh, that's kind of how I'd start. You just try and experiment. We did this thing. I was trying to remember the name of it earlier, something in measures. It's something metrics, metrics and measures. Maybe it was that, but we tried to do, instead of doing like, here's the test plan, here's how we're going to test it. We looked at here, the outcome, here's what these features do. How will they know we're working in production? And then we'd have meetings to go over these where I don't know a lot about this stuff, but I could raise my hand and I could say things like maybe average is a bad metric there. And we should use percentiles because average is a horrible metric. Generally, Bryce will tell me that I'm completely wrong, but I believe average is generally not the metric you want. So little things like that. And we, and it was still sucked. And we, again, I imagine it's way better today, but you have to start somewhere. You have to be afraid anytime you're trying something new, just be, don't be afraid to suck. Otherwise you're not going to get there. My advice is go figure out what data is there. See if you can collect more data to help you understand whether the feature or product or shipping is being successful for customers and use that, use the data you get to help drive more questions. Today, I'm going to plug Ron Kohavi's book, trustworthy online controlled experiments, because I reread that in preparation for putting together my test flicks talk and just, it's full of good anecdotes. It will, this is the book. If you don't believe me just saying, you have no reason to believe me saying that you can be the advocate for the customer. You can be the proxy for the customer, but you're not the customer. You are not the customer testers. You are not the, you, I could argue maybe you're not even the proxy for the customer, these testers are saying you're the customer. You're not, but you don't have to believe me. If you go read Ron's book and you look at the results of the studies and how stuff works, you are going to walk away going, Oh, you know what? I have no idea how my customers work. I need to trust the data. Amen. No, it's one of my favorite things to do is go up, go to the executive and inform them that the business does not run the way they think it does. Yeah. And it's one of the things that, that does my value proposition. I was thinking through, as you told your story, like, how did I get my role? Right. Essentially what I did, and you kind of were doing the same thing with me cause we've always been leading edge on this one. We kind of saw the beginning of the end of tests. We, we, we knew it several years before it actually landed that it was on its way. The momentum was clearly there. Yep. Um, so I shifted into a sort of a proactive self-defense, right? It's essentially, okay, if this is coming, I have two years. Where should I develop myself? Right. And so what I did is I did exactly what you said. I started reading like crazy, then started what's even more important is used it in my day-to-day life. And if, as I used it, I learned, Oh, okay. I'm not yet at the experimentation, but man, did this say statistical technique really shortcut all of this pain that it used to take me days to take care of. Mm-hmm. Now I got straight to it. Right. And then for me, it's been learn a new technique incorporated into my, my repertoire. And then I get another almost anything, right? Right. That's the way I learned musical instruments. Now that was just, that's why I use the word repertoire. Cause it's very similar to that. It's right. You don't start off playing piano, being a master at Mozart. You, you start off learning, uh, Oh God, what is that one? Done to, uh, the one that goes dumb. No, it's hot cross buns. Right. That's sure. Right. Or, or, or, or twinkle twinkle little star, right? It's something very simple. Although I don't, I do think hot cross buns is like everyone's first song. But just to summarize, so you have to, or to recap whatever, I don't know my words, you can't be afraid to fail or can't just be, don't yourself get stuck. And, and again, no matter what you're doing, we find that the people who are curious, people who want to learn people who are hungry for knowledge, they just do better in their careers in general. And if you want to make this transition, and honestly, I think a lot of people in test rules should, whether you become an expert in doing it all the time, whether it's something you're just able to do and understand how to use data. I think it's critical. I think it's critical for anyone in software, but dive in, don't be afraid to screw up and again, start by asking what data is there. Yeah, totally. I was talking about crash reports today at work and there was a time in my career when basically I was, I was thinking about it in hindsight. I was a full time debugger. I debugged code most, most days every day. I used to, when I was doing campus interviews, I would go and talk to the computer science department, right? Because I wanted, I would go there and say, Hey, I'd like to know who has the reputation of being the best debugger in your, in your depart or amongst your students, cause I would like to talk to them. Cause I, I discovered long ago that test people in tests when I wanted to hire and test people who had exceptional debug skills were awesome in this role. Yeah. Well, the reason I brought it up was when I was learning, when I was debugging full time, I didn't know how to do it. I mean, again, just for context, mostly debugging in windows was done at the assembly level. Um, so I didn't, at first, I didn't know much what I was doing. I could get a call stack. I ended up doing it full time. Cause I learned a lot about it. I did. I would learn about it in a bunch of different ways. One was just watching people. I own the debugger. So it had a remote connection. So I would watch people debug things. Sometimes you could have multiple, multiple people connected. I would watch people debug and say, Oh, that's cool. How did they do that? Or I would, a lot of it was I have, I probably finally gave it to Goodwill, a book on x86 assembly. So I could understand not only, I knew what most the fact, I know what all the instructions do in x86 assembly, but I would understand how many clock cycles they took to understand where for performance issues, a lot of things like that. Or I would, Oh, and one thing I did, which I did when I was teaching debugging, when I taught debugging once in a while was make sure you understand what all the, I knew what all the critical parts of the system looked like when they were healthy so that when they were screwed up, I would know what was different. Oh, okay. That makes sense. So it was pattern matching, right? So you know, that's, that's relativism. And that's what AB testing is all about. Yes. Yes. We want to know what's different. I was using human pattern matching in a perfect world of knowing what I know now and going back then I could write pattern matchers for that. Some sort of fuzzy pattern matching between a healthy system and healthy system. One of the things I was helping a team debug on memory, like I wasn't really helping, I was there trying to live vicariously through my team, but in a memory leak, this is the one I love when I use a teach, if you, if your working set is normally half a gig and you look in your working set is 10 gig, good news, because now chances are, if you look at a random bit of memory, it's leaked memory because most of your memory is leaked. So if you know your project well enough to know what kind of data structures are there, you can look at those constructs. If you're lucky at strings, it's probably not, but you can look at the pattern. And if you know the data structures in your application, you can get a pretty good idea of which data structures are being leaked, you can't like that. But that's, I mean, that's not really relevant in terms of this experiment. No, no, no, I was just thinking about, uh, I was elaborating on the looking what's different from the same. Anyway, a whole bunch of stuff you can try. I encourage you to try it. What else should people do to learn more about experimentation, A B testing or actually practice it? So when I go through it, right? It's all I spent a lot of time. It's important to think through bias here that that comes up quite a bit. Really? I don't even want to bring any of that up because the thing I find the most meaningful is just start trying. Now assume that you're interpreting the data wrong, right? And do your best. Anytime we try out a new technique, try it out and then assume that you screwed it entirely up. Yep. And then do everything you can. This is where you want to read and say, okay, what's the data telling me? Is the data telling me it's no different or it's the same if that's unexpected. Uh, particularly, I'm particularly interested in unexpected patterns. Then that's where I drill down and say, okay, how could we see this result come to a different conclusion? Uh, and, and come to the exact opposite conclusion. And that's, that's all around getting confident and enough in the, with the data to be able to objectively call BS on, on yourself. Yeah. I think it's one I want to close here, but yeah. The first thing you said was the answer to all questions. Hey, Brenton Allen, I want to get, I want to learn how to do X or get better at X the canonical. How do I get into automation or how do I learn to debug or how do I, whatever the answer is you try it. You, yeah. Start you start. And I think a lot of times people are looking for a shortcut, like, oh, all you, they want like, is there a video course I can take maybe, but it may not be helpful, just fricking try it. I would say like, if I don't know which one I favor the most. It would be try it, just start or apply it. Right. To me. So a lot of times I see a lot of people go through and they play with toy data sets and like, look, I made a perfect model based off of this data set that Google publishes that no one cares about, right? It's the, no, no, no. You got to connect the dots between the things that you care about, say, you know, the quality of your product and this technique. That's what I mean by start. That's what I mean by try. That's what I mean by apply. Nice start, man. Try it. We covered it. We got to our topic finally, after talking about talking about it for a long time. Yes. All right. Listeners, I'm going to tell Brent to have a good weekend. This is Friday. If you're listening on Monday, have a good week. You're listening on another day. Have a good whatever. I'm Alan. I'm Brent. Goodbye from AB testing. 
