Maybe the tooling we need around this is the tooling that stops people from being fucking idiots Welcome to a be testing podcast your modern testing podcast Your hosts Alan and Brent will be here to guide you through topics on testing Leadership agile and anything else that comes to mind now on with the show. We're back for another a be testing It's just me Alan wait what just you sorry. I just mean Alan comma Alan. Sorry try that again. It's me Alan and me Brit and we're here together. Nobody no guess nobody else. My dog is asleep on the floor. Yeah my cat empty Your cat's empty. What no, so I I get it. Did I get I get it? Okay, I the cat bed. No cat did I Already show you I did I did last episode. Yeah, yeah It's been fantastic. She no longer jumps tries to jump on my keyboard She's just happy being within arms reach I am getting over a cold. I am going to edit out all of the coughing that I may do but I'm hoping that for the listeners it gives my voice a Texture That will draw you in to the important things I have to talk about Or not or fast-forward you're actually I already sound higher pitched Than I do normally because you are listening to this at 1.5 speed. I'm doing swell I'm eager for the weekend within minutes my of this Podcasts being done my daughter and I despite the damn liquid sunshine today We're gonna go out geocaching and we oh cool. You're both excited about it I did that with my kids a long time ago. It's kind of fun Yeah, hey, but speaking of going out have you seen Dune new Dune? Dune 2 no Dune 2 you got to go see Dune 2 in IMAX. I have not seen doing one yet So much better than the Dune we grew up with Yeah That probably won't take much. I Am going to play Xbox this weekend for the first time literally in months. I got to tell you about my Struggle with starfield Okay is I am in a place now where I do not have enough health nor ammo To get out alive of this building. I'm in but I have to kind of walk my way out of So I think I'm gonna have to go back to a save point well before the building is kind of start a whole big Quest over because there's just no way out I have scoured the building and all I find are more bad guys and no ammo and it's been kind of frustrating I think if I'm a game designer, I would not put myself in that but put my players in that position So I'm gonna give it a shot again It's been long enough now that if I have to go do the whole quest again I won't be mad because I'll forgotten most of it. It's kind of not a great experience. So I'm gonna go fix I In starfield got myself into that situation I Refused to do what you did and I just you know died reload died reload Have already done done that 300 times and I did it enough to manage to squeak through Okay, I will probably try another 50 times before I give up just for the record, but I probably I don't feel confident I am now roughly I haven't played it in a couple of months. I am I think around level 35 the last time I checked in I was I'm at 12. I think it was around 7 but my My birthday was last week in my Oh Happy Birthday Brett Brett's 25 now 25 going on 12 and My eldest gave me Baldur's Gate. Oh, I've been playing that too. That's a fun game, too And that's the other I play three games on my Xbox. I play Starfield Baldur's Gate and FIFA Okay, I thought I felt I was I only am doing to matter of fact I think I told you I went out and bought a whole new Xbox Just to be able to play starfield and Currently that Xbox has exactly two games installed upon it. I Picked up an X. I think last summer It has my whole library on it because why not? I don't have my other one plugged in anywhere I I thought about plugging one of them. I have so many of them now. Anyway, lots of Xboxes. I'm gonna do that this weekend. Oh Oh, I want to talk about how forgetful I am Okay So I had an idea for a podcast topic and Brent has a better one. Thankfully But longtime listeners will know that I often vet articles or our Presentations on the podcast is talking about something that I'm thinking about for a presentation but kind of work through it I'm giving a presentation at another company in May Which is coming rather quickly and it's a it's a company that invited me up to give them a talk pre-pandemic I supposed to give the talk in April or May of 2020 Bad time to travel around the world. I'm finally gonna go back and give some talks and I was gonna talk through them But I heard back from them today asking me for some information about my talk and blah blah blah and blah blah blah and I realized I do not have a copy of the abstract. I sent them anywhere I don't know exactly what my talk is about. So I'm not gonna vet that here on the podcast because Because I'm frankly old and dumb. Well, it feels like a good a good hour or two on the Podcast you and I could just chat GPT it out. Yeah. Oh And we're gonna get to chat GPT in a minute. But speaking of old and dumb This is episode 197 of the AB testing podcast 197 So we are coming up on not only the 200th episode Not all but more but more we're coming up on the 10th anniversary of the very first AB testing podcast Do not listen to it if we were now I have been thinking hard about how we get to 200 to be released on that day and what it means is we got to get 197 198 and 199 out quickly. I think we decided this is 187, right? This is 197 Okay, and I'm Alan and that's Brent and this has been episode 197 of the AB testing podcast. All right Now now let's do 198, okay We're gonna figure it out, I don't know I got time let me know let me know if you want to do we could release Episode one as a recap. Oh No, we could do episode one with like director commentary. Oh for 198 Okay, and and by the way like who invented this rule that episode 200 has to be done after 198 199 Right, right. We could just we could just skip some you're right But I like the idea of MST 3k in our first episode that actually would be fun I haven't listened to it in a long I gotta figure out I am pretty sure I can figure out how to just pipe that in To us and we can listen and pause as we go along. That would be fun Let's actually let's do that. Let's figure out how to make that 199 Okay, actually we'll do that. We get your calendar out figure out how we're gonna shove those in I will make it work If you want to just pick a night, I got nothing going on. Let me know So listeners don't know that Brent tonight we don't talk in between we got it we don't plan We don't have a good things to do Yeah, we should get a we should get an assistant assistant producer that would figure all this out. Just tell us what to do Okay, we don't pay I think we can make this happen All right, the hard part will be We will have to do episode 200 Morning our time if we want to do it on that day Okay We should You know what because that one I probably won't edit so we could If we're going to do it live with guests, I probably won't edit it Uh, i'm going to talk to alan about this Outside of the cast so And if you all want to offer input One of the three slack calm is our slack channel But the way if you're not already there you can go to modern testing org And click the link and joiny joiny joiny and get in there And we're going to do our best to make it all happen and we want lots of guests We're going to have for episode 200 whenever that is we're going to have people join And there's going to be a chat gallery. We can have people just i'll figure it out. We can have lots of people on It's going to be a big party without all the fear of covid and that stuff All right We have done some planning some Gnashing of teeth and fingers and and things and we're going to do the best to make it work But we have more to figure out but our assistant will step in and help out there and let's talk about Something brant you had an idea for a topic and it was five minutes ago. So I forgot what it was. So yeah, so go ahead alan You and I have stop sharing your screen Yeah, all right. I'm gonna try pushing this stop button and see what happens It worked. Okay, great. Yeah, it didn't kill the recording. We're good. Yeah. I'm like, why is it called stop? It should be called unshare. What's gonna happen if I push this button? Whitaker recently wrote a post uh Alan if he remembers will link to it And it is called the resurrection of software testing And I thought it might be fun for us to review slash Comment on it What this is this the same james whitaker who said test is dead? He did He's claiming he's giving credit to his colleague at the time alberto Um, but I heard it for the first time from him Yeah, and a lot of people misinterpreted this Merritt had a great post this week that I mentioned in five for friday talks about people who Reference things in their talk that they have not fact checked it on the full research on so test conferences in my experience are famous for this People telling stories about things that are half true And a lot of the stories about test is dead Missed the point A lot of the test is dead about the same things brent and I have been talking about because your data is there to help us Uh testing as it's been known This is I think that was 10 years ago, right? 15 years ago 14 and 2011 so 13 years ago First gave the test is dead talk Stuff was changing So it was a fair talk to give I think a lot of people lose a lot of the context or haven't looked into why and again as we talked about with um with cat a few episodes ago, it is surprising to me How many testers who tout themselves on critical thinking fail to apply critical thinking when evaluating things they don't agree with but anyway So that all happened. There was some reaction a lot of it misconstrued and then james who is Trying to remain relevant in the world. Sorry james Comes out with an article called the resurrection of software testing where he says, you know what? Maybe it's not quite so dead All right. Yeah The ending of it is software testing may well have died in 2011 But its mindset needs to be resurrected in 2024 The world needs the skills of testers more than ever and researchers need to step up To create similar tools processes and training they did for software so long ago And what's interesting that we're going to actually spend a half hour talking about this post but The first it's not a very long post and the first three quarters of it. All they do is recap albert's test is dead talk And then he says oh ai and then gives uh brent's Paragraph and I think there's a lot of truth to that No, I think there's some truth to what james is saying because ai is Changing the way software development can work A lot of people think of ai as magic Yeah A lot of people blindly use the suggestions from ai without reading them remember my prediction show I talked about As mentioned in the sense then at least twice The ability to read code critically is going to become very important because you have to figure out What's actually happening with this generated code? And if that's and maybe that's a tester mindset In a sense like brent's making a face like he's about to go to town on this really this one paragraph article No, you just said When the way you said the tester mindset triggered Negative thoughts from old podcasts. I get it right When I read this That didn't trigger. So so you you bring in loading that into the brain at the same time. I'm like, oh I'm gonna Cute a little now. Um And I as an aside i'm like, hey, do you have arvin's? Uh phone number on on speed dial like maybe we should pause and call him into this right now because I'd love I think this would be a topic of interest to you. I'll go grab me living in my basement. Oh second living He did something very hand wavy like James super hand wavy. It's canonical with james like he's the master at What's a nice way of saying authoring clickbait titles rhetoric No, like the resurrection of software testing like oh, yeah. Yeah Yeah, and and to be clear we both love james a great friend. He's a funny guy and he's and it's obvious to me what he's Doing here, but he's actually in his and he will tell you That he is not being, you know facetious or clickbaity. He really means it but On the other and he is actually correct. I want to go into why he's correct But there's not enough there there To actually get a full opinion. I think hopefully there's more coming I thought it was funny james posted like this is right from three weeks ago two weeks ago. He posted Any testers on the east side want to get together and chat? And of course, he's going to invite them to his bar where they can buy his beer, right? But he's like get together and I I just replied hey, dude test is dead Did you I did I did I think you replied i'm talking about testers not test words are important so yeah, well, no, it's interesting because I I think we would argue That tester is more dead than test Yeah, I would I'm not going to write the article with that headline but I'm like, you know because I read this line the world needs the skills of testers more than ever So he never he says the resurrection of software testing And I don't know I I don't think he's what what evokes for me is Bringing back right the the singular discipline Bringing I hope not And I want to talk about what what my interpretation of that is and it's kind of riff on that for a second But I do want to bring up a tangent. Oh, no a tangent never Tangent tangent. I happen to be we needed a theme song for tangent except we just I'd run out of copyright money I uh was talking to a internet colleague today Just talking about job searches and giving some generic advice knowing nothing about the person but super nice guy. I think he'll do well I think None of that's important if you're listening And we talk today friday. I actually really think you're gonna do well, but The thing we talked about is something i've noticed because I just I do a lot of free consulting not as an employee of my company just because people say hey Do you got 10 minutes or 15 minutes talk about a thing? And I say as long as you're not trying to sell me something i'd be happy to talk to you The industry has been very nice to me. The best thing I can do is talk to folks who asked for it so What I've noticed and I told him this what i've noticed in having these conversations over the past few months is actually The bubble is moving The bubble is moving on what testing is doing. We'll even put our conversation with brian finster from the podcast into this bucket People are beginning to see that testers on good teams When they exist exist to help developers do their job better And that can be coaching or frameworks things are not there to do the vast majority of testing They may do a little bit of testing because they're really good at it But they're doing it in a way that makes people around them better And I want to come back to that put a pin in that for a conversation on ai And I also talked to a company Uh, a test leader at a large company brent wrote a tangent song and chat gpt. This is great I talked to a manager at a large company who has a test team And my little alarm bells went off. Oh, you have a qa team. Mike also the qa team I said, what do they do? And what he basically he described that I won't use the words But basically they accelerate the achievement of quality. They exist to help make to help with feedback loops Help with the testing part of feedback loops. They can go faster They there was a few cases where they did a little bit of A little bit more than I thought they would but they are actively working on removing that from what they do Okay, so this is a large company's probably had testers for 20 30 years. I am seeing the changes happening I think these changes apply To ai we do not need dedicated ai testers testing ai now. It's funny because Arbin said you and I would one or both of us would both be in qa roles by the end of the year But I have not been interested at all in any of these folks Jobs or positions what i'm interested in learning kind of their journey so I can Try and get stuff a stick in my head I think we need people like and go back to the ai problem It is a parrot as we discussed. It is not magic. I watch self-proclaimed test experts use chat gpt for things or Test chat gpt on things that an llm and generative ai would never be able to do And they point their fingers and say you can't trust this stuff. It's broke and it doesn't work We need people we need critical thinkers who understand how this stuff works So they can one stop those people in their tracks So when somebody it wants to use generative ai to manage their nuclear power plant That's someone steps in so that's not a great application here the right things is good for That's kind of what's been missing and we can go on the little ethics and ai thing We don't necessarily need ai tested We need people who really understand how it works To explain to the rest of us one last thing i'm gonna get off my soapbox because I went to a talk from someone Virtually, of course, I don't leave my house because when I do I get sick I went to a talk Virtually from someone giving a talk on ai or llms or both people don't get the difference and yet neither to this person They were on one hand Ranting about how much they knew and how deep their talk was and while I was watching thinking oh my god, this is all wrong So it's really missing from the industry today to sum up my ted talk what's really missing from the industry today? Is people who understand how llms work? Explaining and hand holding the people who are using them so they use it in the way that's most effective And they don't use it for the things that it's not designed for and complaining You're not going to get me to disagree with that i'm more curious around Like the synops of of the one talk that you talked about Like what was a key thing that this person was wrong on? I can look I probably have notes somewhere. I can look it up They were actually wrong in a couple things Partially right, but also in some places very wrong. No, it was it was So high level that it didn't matter And the low level stuff, but no it went deep because the speaker was very adamant about saying how deep they went But the stuff they went on deep on it was obvious. They did not understand Okay They had no idea what a rag model was but they talked about it like they did but then they described it It was obvious. They did not know what it was for example Anyway, I don't I'm I'm not here to throw internet people under the bus What i'm here to do is say this is an example of where we need i don't even want to call it testing It's an example of where we need critical thinking applied to how AI is used Brent showing me a paper on his desk about how to filter content for retrieval augmented. Yeah I literally just printed it out and just pulled it out of my paper my printer The issue that we're battling is the same thing we've been battling for a long time, you know Ignorance the intellectually lazy is what i'm gonna call them. Yeah. Yeah That's it I don't know how we fix that like honestly lom quite honestly is probably going the other direction Correct, right? It's like oh Oh, we need a jingle We need a little theme song for when we go to the start up the tangent segment. Oh chat gpt Hey, it rhymes cool. Let me show it to allen, right? I spent No time reading it most of my time going what's the prompt? right, uh it theme That's the other thing At work. I play the role that you just Talk through. Oh, maybe you're gonna be a Uh, uh, ai qa Uh, no Yeah, exactly. Yeah, no No, because number one that would make arban correct, uh, and I refuse to let that happen I On a daily basis the last I have had random ICs come to me and they're like hey I have this awesome idea for how to use llm great sit down walk me through it And they're like, oh, yes perfect except for You know the parts where I actually use llm it keeps coming back with weird stuff And then I sit down and I tell them how to use the system With with the endeavors that you do You actually do it Nearly optimal for what is what it excels at right? It's If it's something that's a creative endeavor Oh, yeah, I mean it's it's gonna knock things out like Just as an example today. I have my team has a python package that we had built on top of llm to make our lives easier on a lot of aspects and I One of my ICs had created a unit test that had dependencies And I don't like unit tests having dependencies. So I nor should you So I did depend as sized it And I had some sample content that I use in other things where it's it's quotes quotes from yoda and quotes from mark twain And I just I just said hey Create three new quotes from a fictitious author That simultaneously speaks like twain and like yoda And then write a story that includes those quotes and I better did all right. It did fantastic. Like the the quotes were I'm like, yeah, that's like that's literally mark twain and yoda the story was a little chintzy because It invented a story about a little girl that lived in the village and she found a quote book And she opened the first page and read the first quote and are they cheated? Yeah, i'm like, okay Well, you did what I asked for but that's a boring story. That's what they do. That's what they do But some tester now, here's the here's the difference Some tester will ask you to write that story but write exactly five paragraphs and each paragraph should have 27 words And it's not going to be able to do that It doesn't get that it's not going to do that and they're going to say it's broken We need people because they don't understand how llms and gen ai work, right? So what i'm saying we need in testing is and you like I said llms They are here for now gone for later. We're going to have something else something different with agis or whatever now each different thing that we call ai and even even gen ai is Is it really ai is it really intelligent? Maybe not Right. Absolutely not So what we need for every single iteration of ai I was given our listeners the benefit of the dot to jump in there We need somebody who understands how it works To help figure out how it should be used what we've lost with chat gpt is nobody ever did that Here's a thing you could ask these questions and then 90 of the people use it in a way It wasn't designed and are frustrated with it. I don't understand because it's not hard to go figure out It's it's intellectual laziness as you've called it and it's going to happen because The speed of innovation this area is going to crank like hell And people are not going to be able to keep up if they do not take some time to learn how these things Work before deciding they know how they work There is a couple of things when I when I think about this like I Right as you and I talked about james and james wrote this very hand wavy in my view your view as well And he left it with the final paragraph And and we both know him well enough. He did this on purpose. He left it. We do. Yeah And and what he's going to do is we know james and actually I like the way james does research He's going to pull people in on this. They're going to give him a bunch of feedback He's going to get a bunch of ideas and then he's going to con one of them into writing a book with his name on it Or Yeah, and you get and you'll get to write the forward right, you know He's got an equation that works and i'm not going to fault him for it. I just quite no, no me neither me neither Nothing, but respect because he does he does know his shit and he knows what he's doing 100 unlike some other folks in the industry who just whatever anyway, yeah unicorns and rainbows unicorns and rainbows He says the skills of testers. Okay, i'm not certain what the skills of testers he's talking about here What are what are skills of testers brent? um I guess it would be in this like the ones I would say are are the ability to critically understand what's going on It's not fine bugs okay, it's Tie and connect the dots between the customer's needs And the product I definitely Oh, wow There was a day a day Decades ago where I would have cited. Oh, it's matching to the Requirements spec like you know, i'm long past that It's around connecting to the needs of the customer to this now. I will tell you one Really big I was I was I was in a very high level llm meeting within my company just yesterday um Not yesterday Tuesday, whatever this week. I raised the question Hey Has anyone found a way to? to quantify the number of times a particular qualities uh situation that occurs in llm has occurred And like no, but if you find something I need it And then it went down the line every data science. No, if you find something I need it the biggest challenge Are you familiar with the book? Uh, what is it? Systematics? No, okay. It's it's like one of the primers on systems theory. Okay and In there he has something he calls the law of systematics And that law is the number of problems in the universe Is constant right and you've mentioned this before I didn't I forgot I didn't know what book it was from. So yeah Yeah, and so the problem with llm when you're trying to do it in a non-creative space You have to figure out. Okay. What does it do? Well, i'll tell you It does conformance well Like let's say you have five different help docs written by different people And there's a format a language choices that you like from from on one. Yeah, you just say hey Take this style rewrite these ones It'll do that just fine Well, actually that's a again. It goes back to my I think we need AI Guides what's a fancy word for a guide? guide Okay What a docent near an AI docent to kind of walk organizations through how to use it because time and time again Organizations think that AI is magic And people go. Oh, this is a really hard problem. Let's use let's use magic to solve it and someone needs to say Not only stop them But I actually look at the org and say here are some areas where we really could use AI And make sure it's happening someone who understands You know some companies may call it an AI architect. That's the wrong word too, but somebody It would be more effective if someone made it work correctly. I want to go back to james Arden. I want to hold on. I want to go back to that because I don't actually uh I think the principles of testing that we have been talking about that testers need to serve as a guide Is the way to do it? It's not AI coach. It's an AI. There's the words. It's an AI That's what i'm looking for. AI coach an LLM coach Like someone who can pass on and sprit and spread like seeds to the wind What works what doesn't work? I was gonna say I fully agree and when james says While testing may well have died in 2011. It's the mindset needs to be resurrected in 2024 Testing may well have died in 2011. We need a different kind of person to be doing evaluation coaching testing on AI in 2024 and beyond When the might No, here's the thing and actually this is one of the things with this statement that I I will Adjectly like if you were here, I would look him in the eyes and say james. This is bullshit Because while software testing may have died in 2011 The mindset that was necessary to build software did not still exists And is thriving It's just now we spent the last 13 years Getting devs to understand it better right, it's to me i'm like No, I don't think this I wish I could disentangle with skills attesters when he says researchers need to step up and create similar tools and process and training That part I probably agree with and I think we're probably talking about that. I don't know if it's researchers, but it's there It's not only The guides like you talk about the docent or like coach What this says say is people like me Who who do understand a lot of things about this and by no means do I? I understand more than the common people and I understand that there's a lot more people who understand way more than me What the world needs is a set of in this space a set of tools that make the sticky harder problem clearer Make it avoidable like don't You know what? Here's a tool It's a front end that sits on top of chat gpt And does some nlp on your input prompt To let you to give you a score On the shittiness i'm sorry the quality of your prompt like I could write this I could figure out if this is a question that I think chat gpt will answer Really well or answer like you're asking it something stupid that you'll never get a good answer on You and I use critical thinking to figure that out, but I can quantify that How how would you quantify it? I'm going to use a language model that looks like there. So I know what chat gpt is good at it's good at parroting text and finding similarities and the style of somebody All those things but that's it's not but all those things is a big No, I want to flag the things. I know it's bad at so maybe i'm just going to do it simply from A keyword input file of things like similar it's going to look for words like count or rhyme Or any of the multiplication symbols or all of those things. I know it's bad at and I know it's probably a quantifiable list Uh and use those to help maybe it's harder than I think it is but it's possible It is absolutely harder than you think it is. It's just software Uh, but but here's one of the things that I don't know if you've ever done this um I I told you i've been talking to Employees just about every day for the last two weeks. Okay, one of them is Is is actually a rock star who reports to me, but he's still kind of unfamiliar with llm He's familiar with with neural net. So the other day I was walking him through and I generated a prompt And it gave me an answer and literally what I was asking it to do Is I asked it to write code And I said I want a comment that describes the purpose of this code after the curly brace It kept on putting it before the curly brace And I wanted it after so then I chided it and I said no You put the comment before I want it after And then it says oh, okay, and then it fixes it Then I went back and I did the thing that blew my employees mind and I said Okay Now tell me how I should have Prompted you such that you had gotten it right the first time. I just asked it Tell me what the prompt should have been so that you would not have made this mistake Now oftentimes it gives you advice. It's wrong But nine times out of ten, maybe not nine times seven times out of ten It works Yeah, I wonder if there's a way to use chat gpt maybe because you can modify the prompt Yeah, not in the commercial, but if you just like to open ai Maybe you can work that into the prompt like you can actually If you've worked on the back end of it, you know that you can Change the prompt to make sure it does not give it does not chat gpt as It exists on the web for most people it will always give you an answer even if it doesn't know So it could be confidently wrong, right? You can prompt it to not do that If you don't know tell you don't know no, but that's changing the answer But that you're exactly right though, right? It's essentially So you talked about it's a parrot and i'm glad that that I wonder if because I haven't played with this what I want to know is can I use that same prompt? Can I use some prompt engineering on the sort of in the middle layer like you can do with? Oh, you know if like azure openai can I muck with the prompt there give additional prompts? That would have it Great like rather than confidently give an answer to some math that it can't do If it could reply back and say this isn't something i'm good at Or I don't have a cop my or my confidence level on my answer I don't know now. It's so in my but these are the these are the research tools I want these fancy researchers wittaker speaks of to write yeah, my That for me that one would be a difficult one to do Because and I love difficult because oh it's fine, but the the thing is llm Like I know all it is and I know you know this but i'm gonna remind everyone on in the audience, right? All it does is it given a string of text? It produces the next character Right and then given that new string of text. What's the next character? Repeat until bored. Okay, but I I realized what i'm doing is something I get mad at companies for doing all the time remember when uh One of the iphones like the seven or something it didn't work and steve job said it's because you're holding it wrong Because the antenna was in a weird place, right? I remember that So in a way, I am telling people they're holding their phone wrong. People are using chat gpt They're getting asked. They don't like it's broken And here I am saying it's because you're holding your phone wrong. You're doing it wrong So what I want the tool to do if you're doing it wrong is to tell you you're doing it wrong because maybe that's easier Than have every company having an ai coach to help them from stop being stupid Maybe the tooling we need around this is the tooling that stops people from being idiots gpt And now we're aboard. I just want to be super clear Gpt will never ever ever ever ever do what you just want. However, that doesn't mean That just means no one should be using gpt directly. It should be going through a different endpoint Gpt is just a front end on top of a Really really big llm. Yes, exactly aboard happiness But I look at this and i'm like Yeah To me it it's the same thing if you are building software or a solution that relies on llm then the the developers of it They need to be thinking about What is the quality of this solution? How will will it solve the problems? It'd be interesting to bring Nicole Forsgren on and say Hey, did the principles of your thing? Do you have any opinion? To me, it seems like the very same principles that you're talking about for code Would also apply to prompt engineering. I know you may or may not have studied that but what's your intuition? I'd love to hear her answer to that Because i'm just I have a question for you. Yeah So we've been doing we've been doing a little book club on accelerate at work Have you and we just did chapter three and four today and three is the one on culture And talk about westerum's model, which is super cool I wonder if And you can talk we can both talk about this from a microsoft I wonder if the advent of chat gpt is Re-encouraging the hero mindset Because somebody wants to go solve everyone's problem with the magic of gpt. I haven't seen that I haven't seen that but there is something there is something very relevant that I have seen in that timeline okay, and that is the the proactive claim of duplication of effort in order to Make sure everyone is aware that I have licked the cookie. This is mine right in the the Alan i'm concerned that you're duplicating my team's effort Right. That's just a political ploy Oh sure. Yeah Particularly like i've been getting it a lot lately because I run a data science team that specializes in the nlp And the other team that quote unquote is competing with me is a dev team Right from a knowledge point of view. They're not going to beat my team The only way they win is from a if it turns into an api problem, which Right gpt is an api But it's the quality output of it that matters. So here's my thought so, you know remember back in the day I work with Folks on the microsoft research team on a lot of software engineering studies, but here's the study I want to see It's almost too early to do it. It's way too hard for me to do but I want to find something around the The effect of generative AI on organizational culture Or organizational health either one of those i'll call it organizational health like how is chat we talk a lot because we're adjacent to it About chat gpt helping programmers and helping make software As you know when we've discussed chat gpt and generative AI isn't just for developers That's some pretty good pluses for us and I talk about how I use it in writing and thinking through things But what is that's the thing ai is for everyone gen ii is for everyone What is the effect of that? What what is happening to organizations who embrace? Gen ai who play with gen ai like what is happening there? Are they getting healthier? Are they getting less healthy? There can't be no impact. Oh, there's gonna be impact But I want I don't I actually don't have an opinion of what that's gonna be but it's worth measuring i'll i'll give you I'll give you some thoughts. I have enough thoughts already Here's a few more Okay, i'll squeeze them in number one. Remember what I just said about systematics. Okay. Yes The key thing is that the number of problems in the universe is constant Okay, and when I talk about This this law I talk about like okay when email happened Right when I remember I remember when email was invented Why was it cool? Because we could talk to our friends anywhere in the world And not have to pay for stamps and not have to wait For free, right? As long as we paid our computer lab fee and Then what negative what problem was constructed because we created email a couple problems, okay One big one is you couldn't infer tone or or anything from the email We also created spam emails We also created and again, maybe it goes with the first one with we created internet anonymity where You could be a dick to someone because you're more likely to be a dick to someone because you were you had never met them You'd have a relationship with them. You're talking to people You don't know as much now over email because it's free right and and actually even even More and some may view this as a positive or negative but even if you just talk of you know, limit it to friends and family Right. What did you do? You increase the friction or decrease the friction For you know grandma to send you you know the chain letter that will save Even allies would still will send you crap now right My uncle I have an uncle that every time his pc dies And you've encountered this i'm sure Right. Oh call brent. He works for microsoft. He can be your oh god. Yes Well before email there's no way he would ever have like sent me a letter asking me for that right? Automation reduces friction It doesn't just reduce friction on the bad things it reduces friction So the good things become a lot easier to do but the bad things do too like junk junk mail spam the ability at scale for the nigerian prince to Ask you for help, right? Yep The same thing's gonna happen here and the intellectual lazy are gonna get caught by it They're gonna get creamed one of the issues is Okay, so ai can at scale review let's say Bug reports, right? AI can at scale review it But now you have to review what it reviewed Right. How do you how do you know? Because you know enough that it's a generative AI How do you how do you know it's doing the right thing? And here's the well, isn't that similar to who tests the tests? Yeah Except when it's who tests the tests Uh in the past it's been a human being that you can kind of go. All right. Well Karma will eventually hit the tester who wrote a billion bogus test cases Right. No, I I get what you're saying AI is skynet happens when we begin trusting the AI for everything Well, yeah, and if we if we are about to get even more flooded with AI generated content Right. It's that flood that's going to lead to trust because I just don't have the time to read it. Otherwise 100 million percent. Yep. All right. We should we should cut it there. Yep That was actually a fun conversation. I got a little bit more passionate about it than I thought I would so super exciting Thank you. James Whitaker for your contributions today What was it 197 197? All right. We'll see you next time. I'm Alan. I'm brain order of law I'll peter say 
