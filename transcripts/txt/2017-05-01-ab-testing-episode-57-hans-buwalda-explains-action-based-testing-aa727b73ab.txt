Hey everybody. Hey, I'm Alan. I'm Brent. And we're here for episode 57. 57 at AB testing. Once again, counting down the long, long road to 100. As well as we have our predictions for the end of year episode. Yeah. Wait, it's not the end of the year. It's not yet. We are doing something special here on AB testing, something we've done once before in a slightly different variation. Yes. And special might be sort of short bus special. Many of you, many of the three may remember we had Steve Roe on the show maybe a year ago, a while ago. I don't remember the number because I don't remember the number. No, we have a guest on the show as well, but we are recording remotely. And yeah, I'm thinking you're already doing your pre-editing comments around why the audio is all messed up. No, it's going to sound great. We have one thing to do before we introduce our guest. Oh, OK. Did you know what that is, Brent? I do. You do. I do. There's this handy thing we do every episode. And that is write it up on what we're going to the topics on the board. It is. And then that's so Alan doesn't forget. But this is really exciting. We have this show is brought to you by Cobaton spelled K O B I T O N. You may not have heard of it yet. It's pretty darn brand new. So I have a question for you. Is managing internal devices, those of you testing like mobile devices and exciting the best part of your job? I doubt it. So you should check out Cobaton, your complete mobile device lab solution. Cobaton allows you to manage the devices you own and access the real devices you need from manual to automation testing. Cobaton empowers developers and testers to build great products faster, manage and access internal and external devices all in one place, allowing you to test on hundreds of devices and configurations, identify and resolve issues more quickly with automatically created activity logs, even centralize your testing history to increase collaboration across teams and locations. Cobaton does all of that while minimizing cost and enabling you to get your app to market sooner. Now, if you couldn't tell, I was reading that. But I wanted to jump in a minute and say I took some time to play with Cobaton, as did I. And it's pretty darn cool. It's a lot like, you know, test droid or sauce labs. But to me, I I didn't do a side by side comparison, but they have. I think the I think the technical term is a butt load of different devices, not emulated real devices you can connect to poke around on manually, run your scripts, as well as one thing I like. It's really cool. I'm not sure if any of the other places support this. But if you have some custom devices like some Xiaomi devices from Japan, from China or something, you can plug them in and have them accessible to your team anywhere in the world. That when I looked at it, the first thing I was reminded of was my time in Bing. And I'm like, oh, my goodness, this is like what Selenium was to browser testing. This is is to device. Yeah, seriously, if you're doing a lot of mobile testing, you have to have some sort of cloud solution. And Cobaton looks pretty awesome. If I was in the app development space, I would I would seriously be considering what the price tag is for this. For those of you who have done app compact, particularly device compact, that is a big barrier to entry. Yeah. And it's a shared pool across your whole team. So you can anyway. My view is always try before you buy. So if you visit cobaton.com and we'll put this in the show notes, cobaton.com, whack A B testing as KOB ITO and dot com slash A B testing. You can sign up for a free trial, free trial and start checking it out in a few minutes. So pretty cool. I would recommend trying it out because I thought it was awesome. And I have kind of high standards. All right. So finally, with all that, blah, blah, blah. I am very excited to have Hans Buell on the show today. Hans has been working with information technology since his high school years. In his career, Hans has gained experience as a developer, manager and principal consultant for companies and organizations worldwide. He was a pioneer of the keyword approach to testing and automation now widely used throughout the industry. His approach is a testing action based testing and soap opera testing have helped a variety of customers achieve scalable and maintainable solutions for large and complex testing challenges. Hans is a frequent speaker at Star Conferences as the lead author of Integrated Test Design and Automation. Welcome Hans to A B testing. Hi, Hans. You are not really dead, right? I memorize that, of course. Of course. How are you doing today? Thanks for joining us. I'm good. So Hans, we have to apologize for all the pain and hassle we've put you through. What you guys don't know because of the magic of podcast land is we took up a good hour of Hans time about a week ago, trying to struggle through getting a three person audio recording up on Zencaster. So I wasn't going to throw like a product under the bus, but I guess we just did. No, I'm I'm willing to state that somehow magically it was our fault. But there wasn't too many levers to to pull and too many buttons and options to change on Zencaster. So it's still a bit of a mystery as to why that didn't work. Cool. Hey, I want to get I want to get into the show. And because I've been talking a lot, I'm going to make Hans talk. So we have a theme for the show today, which is the role of test design for successful automation. And I've always been huge on test design. And I think it's far more difficult than most people give give it credit for. So, Hans, I'm going to kick this right off to you. And so given that given that I've already given my opinion on it, but not a definition, what is test design to you? Well, of course, this test design in general is something you do to get good tests. But when I talk about test design for automation, I mean, how you design and design and organize your test to get your automation right. And that's kind of counterintuitive. And when you think about automation, you think mainly as a technical challenge. And you need to be a good engineer to do good automation, and especially your functional testing. But what you do, what I found over the years is that it actually the design of the test have a big impact on how well they can be maintained later on. Now, here, when you say a test design, your test is specifically the test cases. Yes. Yeah, kind of. The way we do it in our company with our method action based testing is we follow a modular approach. So you organize your testing modules. We call those test modules. Think of spreadsheets. And you in the test module, you can have more than one test case that kind of can form a negative to test your system and the test. So we do not necessarily design it and have a test case. First, let me first try to identify those test modules and organize that. Is a module in this light because similar to like a test suite or an organizational construct for your test cases at the I went up and looked at the website and I was a bit confused on the module concept because it's that is also a common term in in development. So I wasn't certain if we talk about this, if you're talking about the test automation side or the first part of organizing and understanding what you wanted to automate eventually. Well, the comparison to system development is actually a good one. When you create a system, when you develop a system, you will organize your code into code modules or code files or whatever you want to call it. And then in that file, you will have a bunch of functions or methods. And it is a class. So what you will not do is put all these functions out there in isolation. You will organize them in a certain way, in subsystems, in modules, etc. And that's what we advocate for testing as well. And you should not see the test cases as just isolated units, which you should try to tie them together into design products, which we call the modules. Now, the test suite is in my model is a little bit different. That is more like how you execute a test. So a test suite might say, I'm going to run this test module, I'm going to run that test module, maybe a third test module, I only run a few of the test cases from there. So that is more on the runtime side. And when you do something like DevOps, that will be, okay, these tests together form my smoke test today. And then these tests are going to be my functional test when I go into the staging environment, stuff like that. I like that. That makes sense. So one thing I want to ask is the theme of the role of test design for successful automation. I think there would probably be, if I were to ask 10 people, I get 11 different answers on what successful test automation was. So what does successful test automation mean to you? Yeah, I agree that I can spend the rest of the hour with giving definitions. But for me, it mostly means manageable and maintainable. So after a year, you should still understand what test you actually have, that should not have run out of control, which is fairly frequent the case. And most of all, if you hit the button, the test should work. And that even becomes more important now with DevOps. If you really want to do something like continuous integration, let alone continuous deployment, your test need to run. And if the test break halfway because of a timing problem or the interface element it can't find, then that spoils the show. Actually, I really like that answer. I think if you were to ask a lot of people new to test automation, they may, that question about success, they would talk about coverage or, actually, they would talk about coverage a lot. And I like the idea of realizing that tests should be able to live for a long time and be trustworthy, be debuggable, be a solid foundation, as much of a solid foundation of the product as the product is itself. So I think it's really cool you have that thought about what success means. I think actually on this one, particularly when he talks about the DevOps world. So he's talking about services type world. Maintainability is absolutely critical. As I've mentioned on the show, in X-Bing when I was, yes, that's right. We've said this before, that's for you on the Slack channel. In Bing, any dev can deploy something within 15 minutes. And having, and they do it independently. A maintenance of test cases is critical in that environment. You have to have your test suite be as adaptable as the product code. Yeah, that's awesome. So cool. There's one other thing, actually a couple of things I wanted to talk to you about. One is I did my homework, I read through your abstracts for your upcoming Star East talks. And I don't know if the A-B testing promo code still works, but I'll throw it out there now in the middle here. But you mentioned specifically action-based testing, behavior-driven development, and exploratory testing. So as far as our listeners go, behavior-driven development, they know what it is. Exploratory testing, they know what it is. I think action-based testing still isn't as well of a known term. I did read up on it, so I could talk about it, but I think maybe referring to you to talk through what action-based testing is and what it does for you would be a better way to do it. Well, thanks for asking. It is based on two principles. One, it's keyword-driven. So when you write a test, you write it as a sequence of actions. That's where the name comes from. And each action consists of an action keyword with zero or more arguments. And you try to write these tests, especially when at the business level, in a way that a non-technical person can easily understand what is being tested. And then the second concept that we just talked about is the modular design. So that combination of the actions and the keywords and the modules, that is kind of the core of action-based testing. And by the way, it was interesting that you mentioned BDD, behavior-driven development. It is very straightforward to make a conversion between BDD scenarios, language scenarios, and actions and keywords. And you can also go back, you can go from keywords back to scenarios. I actually made a tool for that, made it myself just as an exercise, but that makes those conversions. So if any of the listeners is interested in that, they can just contact me, and I'm happy to share that tool with them. But the reason I made the tool is not just for convenience, but just to kind of investigate and show that a test based on actions and a test based on BDD, that they are kind of conceptually equivalent. It is very much the same way of thinking. That kind of makes sense. I never made that connection before, but given when then, does line up with keyword testing. Yeah, and also the sentences themselves. And when you do in the BDD scenario, it's basically a couple of sentences, phrases I think the word is, that those can be predefined. And if I bought 10 sweaters, and then you can predefined a phrase called, I bought X, Y, and then X is the number, and Y is the product. And then you can create automation behind it to exercise that particular sentence. So very easy to convert that sentence into, buy sweaters 10, that's like an action line. And then all you have to do is automate that action. And when you look at it, the organizing in that way makes it more manageable. Actions are much easier to manage and to automate and sentences. At least that's what I think. Is, so many of our listeners are, as Alan pointed out, are fans of BDD and TDD. Is that what you would advise our audience around the difference between, say open source tools like Cucumber and others that enable BDD automation versus an ABT strategy? I would personally, I would want to work with the keywords. It's more efficient, simpler, the sentences, it has a tendency to kind of get out of control and multiple people. And let's say you have a project team with many, many people, different people that come up with different sentences. And to paraphrase you, if you have 10 people creating tests and coming up with a sentence for buying sweaters, there will be 11 sentences saying the same thing. It's very difficult to standardize language. While keywords, and they're almost like function calls, that's fairly easy to organize and to define and to agree on in a team. And then because of that tool, you can easily translate from one to the other. And doesn't even matter what keyword tool you use, and that's a fairly straightforward conversion. So I would personally, if I have to do a big testing project, I would try to write the test and organize the test as keywords when I have to present it to somebody who is not part of the team, then I might translate it into the sentences. One of the things that Alan, I don't know if you know, back in the day, so I did play my hand in ABT. And one of the things that I found that's really nice is it often directly ports into code. And so one of the benefits that I've discovered is when people go read my code, they're not reading Devs, they're not having to follow pointers, right? The methods and the objects used read so very similar to the same test cases that I may have designed in my test architecture doc. Yeah, that is true. In fact, you're doing testing and automation, it is very close to development. And you see that developers feel very at home with the actions because they see it as function calls. And even if you don't want to use a keyword tool and like our own tool, you could just put it in a source file and just make function calls out of it. I think you would achieve more or less the same result. And I do not believe that you have to go to natural language sentences to develop the test. And that's more like to create a negative to share. It's a bit like COBOL and the programming language with COBOL many years ago came to the market. That was even well before my time. The idea was that you would use natural language, English, to create your software. And that it would make it easier to create software. Well, history has started otherwise. And the more successful language now we see C++ and Java, which does not look like natural language at all. Well, so speaking up as a representative of the up and coming data science community, I'm gonna state that I think COBOL was probably a good idea but its time was too early. I do think the advances of natural language programming, I think we're gonna see programming through natural language within our lifetime in a way that actually works. Yeah, I would say you probably want to wait until it actually works. And you can't do more of that. Kind of like with self-driving cars, right? Yes. So love the idea of using action-based testing for functional testing, behavioral testing. Do you use it for other kinds of testing or are there kinds of testing where action-based testing isn't appropriate? Well, I think it works. But the way I always say it, it works for every test that you write down. So there is a very important category of test called exploratory testing. Most of the listeners will probably be familiar with that. And people like Cam Kamer, James Bach, Michael Bolton. That's a different animal. You really interact with the system under test basically from a human manual perspective, usually manual perspective. And you kind of try to learn about the system under test. So you search your way through the system under test and you learn and at the same time, you're going to spot problems. You're going to see, hey, the color of the logo is wrong. The logo is missing. There might be things that you spot that you did not expect to go wrong. So you did not make a test case for them, but still it can be very important items. So that kind of testing, that's generally not something you will do with an action-based model. That does not mean that action-based test could not be exploratory design. When you create and test for a business process, nothing stops you from maybe either talk to the business people, the end users, try to understand that business process. And in that process of understanding, you get ideas for tests. And rather than just follow the steps in the UI. So that there is some similarity there. And I agree completely, yeah. On the other hand, if you have tests like non-UI tests and like embedded software or services, maybe with micro-services, anything like that, that can be done very, very well with action-based testing. Action-based testing is even better in non-UI than it is in UI-based tests. So for that, it works well. I would probably even think that it could be used for unit testing, but I don't see that happening a lot. Most unit testing is just done by developers directly when they create the software. And then especially in agile, you tend to make the test first and then the function on the test. And I don't see that really doing with keywords or actions. So that middle area between the pure unit test on one hand and like the exploratory testing on the other hand, that entire area in between, the action-based testing works very well. What about things like load, stress, and perf testing? Could you find applications for action-based testing there? Yeah, absolutely. And one of the very first action-based projects I did, I was still in the Netherlands, and you guys didn't mention it, I am from the Netherlands. That was for the European Stock Exchange. And there we did a test with 40 laptops just to emulate the trading process. And so that goes very well with actions. You just make the scenarios with actions and then you use more actions to kind of populate the machines and get them started and get the results. So there might be actions like generate load and there might be an action like check response time and with a value of let's say 10, meaning if it is not there within 10 seconds, it fails. So that basically goes very well. All right, that's good to know. I wasn't thinking about that. That's good information. I think one last question, then we'll give you a chance to tell us what we should have asked you. I have a saying I've used for a long time around test design that says, and this actually goes back to a minute ago, you're talking about the difference, like that spectrum of exploratory testing versus what should be automated. And a line I've used for several years is that you should automate 100% of the tests that should be automated. So I'm curious what sort of, and that's the test design challenge or a big part of the test design challenge is that heuristic of what sorts of testing should I automate versus what should I not try cause I'm not gonna get a good return on my investment. So what sort of heuristics you use to figure out what should be automated or how do you figure out like what do you wanna automate versus maybe what you would encourage clients or yourself to not automate? Yeah, that's one of my favorite questions. I always hope that people will ask. Essentially the answer is very simple. I automate everything. That means I will automate every test that I write down. And so exploratory testing is a bit of an exception there. But if I write a test down, even on the back of an envelope, I will typically automate it unless it's really a one-time test. And like in this particular conversation, make sure the audio works. Once it works, we do the podcast and that's it. So we don't have to automate that test. With you guys, this is podcast number 57 and you wanna make it up to at least 100. So maybe you want to kind of automate some of those tests because it's very minimal. Now I have a second point to make there and the question for me is not whether I should automate the test. The question for me is whether I should have that test, whether I should develop the test. So it is not the ROI on automation that counts for me, it's the ROI on testing that counts for me. And in the module approach, that's what I emphasize. Make a list of modules. And like in the ideal world, all the modules that you could possibly make, then be selective about the ones that you actually develop. Once you develop them, because you develop them with actions, with reusable keywords, it's almost hard not to automate them, to just automate them. Does that make sense? It does. Can you go a little bit deeper in terms of how you advise people to proactively evaluate the ROI of the test? Yes, absolutely. First of all, spend some time on it. That's the same with test design in general, just by thinking about it, how do I organize the test? What am I going to test? What is really a priority to test? You already get large, very good results. It helps enormously later on in the automation. Now, in the approach that I follow, it's almost like writing a book. When you write a book, the most difficult part of the book is the table of contents. That's what you have to really think about. And then once you have that table of contents, now writing the rest of the book is, of course, a lot more work. But assuming that you know what you're talking about, it's doable. It's just straightforward doing. It is that table of contents that you really have to sweat about it to think about. How am I going to organize my stuff? So the way I look at test design is very similar. You start with a high level test design phase in which you try to figure out what kind of tests am I going to make? What am I going to need? That results in a list of test modules in my terminology. And then you can make, put them in a spreadsheet, for example, make a column priority. Which one do I do first? Which one do I do last? Which one am I not going to do? Then you can, once you do that, then you can start developing those test modules. And there are always a couple that are very urgent. And there are always many that you don't care about that much. Oh, and another point there, is that's also the moment that you can go to managers or product owners or domain experts, developers, that you can start cooperating. For example, in the realm of Sprint or anywhere else, that you can discuss what are we going to do? And what do we need to do? What test do we need to develop? And when do we need to? So that kind of considerations. So one of the things, I have a few questions. One of the things we, our listeners, one of the things that attracts people to our podcast is we've talked about a concept that I frame as sort of modern testing. And one of the things that I refer to this is that in the last 20 years, some big changes have occurred, right? We've seen in several companies, the individual test role disappearing. I work for Microsoft, that's certainly something that has happened here. We've seen different agile philosophies move forward and multiple different successes in terms of companies producing product using those paradigms. And then even now we see a big push towards services and the cloud. What do you view as modern testing? What are the big changes that you've observed in your career? And what do you think is coming next? That's a broad question. I will try to answer as best as I can. I've seen several things change. Many of those in a meaningful way and maybe some others not so meaningful way. Let me start with the last one. I see testing as a profession. I believe there is such a thing as a good tester, somebody who can find problems, who can try unexpected situations and find bugs that way. Because most of the bugs are not so much coding bugs in my terminology, it's not so much like I specified it and the developer didn't build it, but the developer specified it himself, but he forgot the minus sign or something. Those had bugs happen that are usually quite easy to find and it's not such a common problem. The most of the bugs, the things that we call bugs, is you're not meeting an unexpected situation. Something happened, some input values happened that you didn't expect or you wrote something to the database that was wrong and another module gets into trouble because of it. So unexpected situation. To deal with that, testing can be very helpful. And I believe that is a profession. I don't believe a good developer is automatically a good tester. It is like, and I try to explain it, I sometimes compare it with an airplane. If you are a good C++ developer who develops a control system for an airplane, it does not mean you're a good pilot. If you really think you are, let me know which flight you're piloting, I will take another flight. So it is really a profession. And just like automation is also a profession, a good programmer is not necessarily a good automation engineer. It has, automation is a very specific craft. You have to deal with timing and identification of objects and scheme flows. And it's a very interesting profession to have. Another thing that I've seen changed during my lifetime and that change is continuing is the role of automation. When I started, automation was a yes, no question. What should I automate? And you even asked that question earlier on in this program. But you see a shift in agile and in DevOps. Automation is not a question mark. Everything is done with automation because in DevOps you're essentially automating the entire integration and deployment process, including provisioning test machines and running tests on them. So automation is not really a question anymore. Every project will have automation now. I completely agree. Automation in that context, though, becomes a bit different. You're saying production code has to perform an action and then it has to validate it in near real time. So in that case, what do you advise given your guidance around testing as a craft and Dev as a craft? And I know several DevOps folks who will adamantly tell you that DevOps is also its own craft. What's your guidance? I think they're all right. I think that DevOps is a craft and especially the opposite side. Ops is also a craft, obligations. Planning, production, planning, testing, planning environments, which can be very different for different companies, for different organizations, what that actually means. But it is a craft. It is a different way of thinking than making if statements in a function. That is, it's all craft. And one of the very positive things that currently is happening is that you cooperate. So your foreign teams and those teams cooperate. That means that the developers, the business owners and the testers are in the same team. And you get the best test and the best results when you are in the same team. Because then you can talk to the developers about making the system testable, make sure that there are hooks for timing, for data that you want to display, et cetera. You can work with business owners for the functionality. You can, those business owners can bring you to the domain experts and other stakeholders like auditors. And so that cooperation can lead to very good results, especially if all the team members know their craft and just saying, oh, let's use this mission to test this, let developers do the tests because developers are so smart that they can do that testing thing as well. I don't think that's gonna work very well. I think I wouldn't like to do it that way. And we're not doing that with our own brother. And I think it almost never applies. So I'm looking at the clock. We're running out of time. I think we have time for one more question now. So DevOps, you brought that up. I'm curious now, what's your sense or your guidance on the role of testing in production? What does that mean? Yeah, I'd say testing in production is a very cool concept in which you actually just bring the systems to production and you will do something like A-B testing, not you, Allen and Brent testing, but A-B testing. There is officially no other important version of A-B testing. Okay. And then maybe I stop answering this question. No, no, please do. So yes, A-B testing in testing and production. So what's your sense? That's probably just like how you organize your revenue string and the money coming in, part of it goes to A and part of it goes to B. And so that's the same you do with A-B testing on the production environment. And you typically take a certain slice of users, let's say, Iowa, and then all the users from Iowa get the, all the traffic from Iowa goes through a new version of a module of a component, in the term of a component. And then you see if it works. And then if it doesn't work, you roll it back as quickly as possible. Now it's fail fast and you go back to the old component. If it does work, you switch everybody else to the new component as well. So rather than releasing an entire system at once, you release it component by component by component. So that's very meaningful. Now, one of the things you can do with automation and with test design is that that B component could actually just be the test modules. So rather than put Iowa at stake and let them go through the B channel, you just let your automated test go for the B channel. And if it doesn't work, then you don't put that particular component into production. And now all of this is very different for whatever company you work for and whatever project and whatever system you talk about. And therefore you need to really follow what I call a business or reenter the approach. You really need to look at what does my system do and how often do I need to upgrade it. A continuous deployment is not a must have, it is a nice to have. And many systems do not have to be continuously be released and continuously be deployed. It really depends on what the purpose of that system is. Does that answer your question? Yes, it does. Thank you very much. So Hans, I did mention that you will be at Star East coming up, gosh, is that like two weeks from now? Yes, I'm sorry. Coming right up. By the time I happen, it happens I hope to know. Yeah, great. Is there anything else coming up or anything coming out you want to plug? Yeah, let's see. I'm also gonna talk at Star West with those of you who don't wanna come to Orlando and you can come to Anaheim. We have Disney events in both of them. And if just point out if you wait, A.B. Testers, if you wait on your registration, we will have a discount coming up for Star West registration. And maybe another thing is I keep all my articles on a webpage which is called Happy Tester. And opposite of a sad tester, Happy Tester. So if you wanna read anything I'm talking about, please go to that page. You mentioned earlier Hans, that if folks wanted to reach out to you on ABT, they can. How would you like them to do that? Well, as you probably know, I work for a company called V so if they just do a Hans at V then they will reach me. I will always answer your emails. Don't worry about it. V Right, I'll put that in the show notes as well. I'll obfuscate it enough to keep the scrapers from sending you spam, but I will put that in the show notes along with everything else. So one thing I wanna mention to everyone listening to the show is, other than the specific question about action-based testing, the rest of the questions you answered, you had great deep and well thought out answers for were all general things about test automation. And I think anyone in testing today should be able to answer those questions at some level of depth. That was one thing that really, they made a point that kinda came to me as you were talking today, was that although we were asking you about mostly sort of kinda general level, could be an interview somewhere, but you obviously your experience shines through. You have deep answers for all these things and a lot of experience and it was fun to hear that. So thank you. Yeah, sometimes I allow people to disagree with me. It's hard to believe, but sometimes I do. And the point is not that you need to agree with everything I say when you're in a project. The point is that you think about it, that you make some considerations about testing, not just, oh, it's a side product of the development. Now make it an item, make it something that you think about when you go into a development cycle and that will pay back big time. I can promise you that. All right, well, thank you very much. Everybody, this is Alan. This is Brent. And we'll see you next time on A B testing. Thank you, Hans. Thank you. Thanks for having me. 
