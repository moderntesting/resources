Until you can code yourself into a bot, we'll still need out. But we may not need all those functional button clicking testers. Welcome to AVE Testing Podcast, your modern testing podcast. Your hosts, Alan and Brent, will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now, on with the show. And we're back with Jason. We're finishing up our talk from last week. Let's listen in and take off where we left off. So back to specifically the title thing, which I think is what the revulsion comes from and the cathartic. Go ahead. Brent, stop playing with your desk. I walk, I thought you told him already. We know testing was a dirty word and quality was a dirty word and this debate's been going back like in 2010, right? But I'm telling you, I walked into the Google search building and guess what the titles of the engineers were? Search quality engineers. Now, yeah, just hold on. That's 15 years ago. It was, it was right. It won't be, I'm wrong. But we'll see. I think I'm right. Well, I want to, I want to, I mean, you don't want the, I don't think you're reasoning through it. I think you want the answer to not be that. No, no, no, no. I'm reasoning. So will my role entail the evaluation of quality? If the answer is, it's not doing what I'm saying. I'm going to full on title. If you grep for the string quality, lowercase, the input string, lowercase, the grep match, it will, it will return true. There will be quality in your title. I'm going that far. I want to bring up something I've brought up before. Because can I say one more thing on real quick? Quality is the one thing that the stuff can't do. And when you can do quality, once the machines can actually fully do quality, you don't need humans because the machines can generate infinite variations of the software. If it self avails, who's in the loop? There's not need to be anybody in the loop. So quality folks, the humans still need to define what quality is not necessarily quality. Just to telemetry. Here's a cool thought experiment. Super quick. You take Facebook's app, right? And you just give the code to an AI with a large context window. Just hand wave for a quick second, or it could be per feature. Like we talked about fighting different or A-B fighting and stuff like that. It's pretty relevant. But if you let the machine just permute that code over and over and over and over and over again with co-pilot, right? Create different variations and deploy them and flight them. So you have a thousand flights in production, right? They're like less than 1% experiments. The Facebook app can evolve on its own functionally user interface, like all in all sorts of ways that the code can be permuted. And then that's functional and that's product. So you can think I'm way out there, but I think this is no, oh, you don't know. I don't think you are at all. But then what you need to do, the lever you need to pull is do you optimize that auto generation and feature growth on user satisfaction or on company profit? So a lot of people will do what you were saying earlier on. I totally agree with you. There is this criticism. It's a semi-valid that you optimize for monetization or you optimize for engagement. Those are pretty easy to optimize based on telemetry. Guess what breakthroughs happen. Breakthrough happens with Instagram where they load the image super quick. Like make it look like it was posted before it was. That was one of the magic user experiences of Instagram versus in the Facebook thing. Well, could a machine do that? I think so. But that's where it comes back to the end of the day. AI may be able to start to figure those things out, but until then, it's Alan or a Brent saying, hey, the telemetry says this, the AI wants to optimize this, but guess what? It's not quite there yet. The GPT was missing some elements of Alan's thinking and wisdom. Because you need Alan to look at that and go, yeah, that's not quite right. It's not quite that because Alan's going to adjust that measure and definition of what quality is. That's going to be the most important job on the team because the engineers are fungible. PM is fungible. It's going to be a quality measurement that dictates the evolution of that application. I'm continually tweaking the Alan bot. Let me talk about this path to quality. Last week, Brian Finster, two weeks ago, Brian Finster was on and I fully agree with him. The best way to uncover your quality bottlenecks and issues in your system is just trying to do continuous delivery because all the things that pop up are things you need to improve to get to quality and production. That part's true. That made me remember that working in platforms for the last eight plus years, that when I really got into it, started building dev tools and dev productivity tools and platforms, I realized that I had more control over quality, more influence over quality, more ability to improve quality in that role than I ever did as a tester. I stood up in my car when I heard that, by the way. Yeah, exactly. So let me combine those with the Alan bot and the desire for me to automate my job via AI if needed. And it could be. Again, I'm not going to get stuck on the title right now, but if we're able, if my job is to use, and we've talked and you actually blogged about this after I brought it up on a podcast a while back about the ability to use AI to simulate a CEO. Very interesting. I can, if there's an Alan bot that's pretty close and my job is to tweak it to make all the systems around me work in the pursuit of quality. I guess I don't care what you call me because I'm having a big influence, but there's a lot of levers and things to put together. But honestly, I think our systems are kind of capable of doing it. So kind of scary, kind of exciting to see how this new future unfolds. Yeah, yeah, I agree. And it's, but I think it's directionally correct. The timeline timing, we don't know. But also we need the Allens to define those bots to encode your knowledge and experience into a computable form. Right? Yeah. And even today, like Chad GPT, it's malleable. You can, like I use it all the time. I mentioned on this podcast, a gazillion times I use it daily as a collaboration partner. Hey, I'm thinking about this. I'm phrasing it this way. What do you think? It'll say, great, you've accomplished this and this, you might want to bring up this and I think, hey, good idea. Or, and I'll take stuff it gives me like this, how would I test an LLM? And I will tweak it on my own and feed it back to instead I've made some adjustments based on how I, how I would approach this, what do you think and it'll take that in there. So I think if we can generate that that feedback loop of learning and keep and get it even better at taking that feedback, because sometimes it's really bad at taking that feedback and adjusting. Yeah. And that's why I think that that improves and that's since we chatted last two, I built it also. It was actually sad that I heard, there's some chatter on social media, I like LinkedIn or something. People are saying like, actually, where is social media? Where did it go? Because I left Twitter and where is social media these days? I don't really do anything. Anyway, go on. It's LinkedIn is all I have, which is a horrible social media platform. Yeah. There's only weirdos on there. And people who got laid off with the jobs, which I feel horrible for. Yeah. Get a lot of inbox on that. But here's the thing. I think that is that, so I built this, so I felt bad because people were going, hey, there's all this AI is being applied to test automation. But how come no one's building something for the human? And so I worked on a thing, I called it co-test pilot. I can't believe the URL was available. Microsoft may try to buy it, I guess, but it's called co-test pilot. And it's an extension you install in the browser. And what it does is it just kind of takes the context of the page, throws it through some prompts and then gives you the output. But kind of back to what you're saying is, one is there can be different personas. There could be an Allen bot. I haven't written Allen bot, but there could be an Allen bot in there. But I can look at the page with Allen's context and there's like 20 different ones you can pick and you can get an opinion from it. So there's one that's like as an edge bot, like edge test cases bot. And what it will do is it'll come up with edge test cases. And if it's right 60, 70% of the time, that's kind of useful and kind of interesting to help a manual tester, but help them think through it. Like ideate, like you were talking about with that CEO bot and other things, but help ideate and suggest different type of test cases that it can perform, that the human should perform and make them think about it. But there's also a little tiny text box. And I think it's significant, which is the tester can type into that little text box and say, ignore Ahrefs and really focus on usability, for example. And then when you reload the extension, you click it again, it will take that bias. Now bias could be the Allen bias. It could be Brent's bias or that relates to the business. But I think that's the model that in the near future that testers will interact with AI is that the AI will suggest things and then they can tweak and modify it and present hints, prompt like hints to modify its behavior to behave more like they specifically want it to behave. But they'll get ideas and stuff from AI and they'll be interacting with it. And they can do it today. Let me ask you a question then before Brent takes us on a total tangent with the yet unpublished A-B testing episode. Let's say we get a dozen 25, air quote, expert testers working in expert tester bot.com. And we make sure the bots are available, whether it's one or multiple to do all this stuff. Why do we need testers then? Why can't just developers building code run these bots and use the bots to help them test their code completely? Can't we just put, can't we just switch the industry that way and have 20, 30, 40 people deciding how testing works across the industry and get it pretty close? Certainly, it would be better than it would the good bots. It would be better than it is today as a whole across the industry. It's crazy. We're totally aligned. I did a talk at PNSQC last year, I guess. But I called it testers AI. And I tried to get people I knew that were also cool with their image being destroyed. But yeah, I created a Tariq King bot. I even created, by the way, for fun, there's absolutely 100% a Bolton and a block bot, to be clear, already implemented. And so not all that useful. So they're not in by default, but they're in there. But really, yeah, I think there should be these experts. What I found though, was this when I started trying to implement some of those, is that when you think about it, the experts really only good at the edge cases, like what Brent was doing a second ago. He talks to the Allen bot and the Allen bot's like, say 60% kind of directionally correct. But guess what? Those are high level specific edge case things. And for the most part, that answer is useful. What's the delta between the bot and what the Allen bot would say, that professional super expert delta is usually on the fringes, right? Or some very meta kind of topic, very probably effective and important. But when you're doing like 90% of the testing that gets done and needs to get done, doesn't need the Allen bot or the Tariq bot. And so literally, I just renamed the bots to, now it's the API testing bot, it's not the Tariq bot. Just to remove that extra complexity and drama, and you can still do far more testing than the average API tester would do with a little bot. So part of the problem is because humans, this is interesting, I thought I had this little scheme, if you don't spare with me, there's you'd like this, I had this little scheme where I thought, hey, you create the Allen bot, the Tariq bot, and then you pay him a spiff. So every time they execute a test case somewhere, they get a penny, right? And maybe throw in the blockchain, I don't know. But you record it somewhere that's very authoritative and trusted. This test was influenced by Tariq's bot, and he should get some credit and pay out for it. And that would also encourage maybe them to either promote it commercially or but also help work on the bot, right? To sit down and work with me on defining that bot. So I think that is kind of where things will go. I think in the near term though, in terms of transition, people, that's too much complexity for people, is what I found. They want what does a Tariq bot do? And there's Tariq's opinion about it. And was that the right headshot? Because he's got a very large forehead, it's shiny sometimes, and he really wants to make sure that it's the right angle. You'll notice he always does this little angle with his head. So I want to make sure the bot image and profile picture matches what he wants. And so it gets complicated. There's a couple of people like, oh, I'm going to make a ton of money, right? I'm like, I just experiment, man. I don't know. But it gets, mixing is complicated. So I think, well, directionally, you're right. I just think in the interim, there's going to be, it's going to be a little simpler as well. Yeah, I just think that something else I bring up on LinkedIn quite a bit is people say developers can't test. And I say I've kind of taught hundreds of them to do, do well at it. And some are actually better than some of the most testers I know. And so if you can bot botify my coaching, because it's not like I'm some genius teacher guru. I just ask people to think about things from a different perspective and give them and just kind of push them in a direction. That's botifiable. And in full, full, exactly. So in the fullness of I tried to avoid this, but like, that's what I'm doing at Jackie, that is I told you a little bit last week, and we didn't talk about it much. But like, that's what I'm doing is I'm just having the bots look at the page, figure out what to test, generate the tests, execute the tests, analyze the results, and give you a report. And then that can go to the developer, just not just lost all my testing friends, but that can go back to the developer or the product manager. And then if they want to tweak it, or there's a business context, or the developer knows something about the implementation that the bot could figure out that's risky, or something, we can say, oh, really focus on negative, like it's a bot into focus on negative dollar values on the tip field, text field entry for sure. And then guess what happens? The bots go off, they do it, they take that prompted advice and input and guidance and add those to the suite, and then they get the results back. But what's not in that loop isn't there's no tester in that loop. I think that's plausible for most, for most kind of applications, but there still needs to be the Allen bot for a long time that looks systematic across Unity and all their services, right? And says, hey, like, this is a priority for all these teams. These are the, like, how do we compare against other similar companies, like until that AI is ready, until you can code yourself into a bot, we'll still need out. But we may not need all those functional button clicking testers. Absolutely. So yeah, just me, you will absolutely need something that presents the principles that are the guiding light that define not only the ends, but the means, right? Which is right in terms of like the Allen bot or the Jason bot or the Bach bot. Those are, I don't know, I see LLM doing better than that, right? If we say that ideas come from old ideas getting together, right? Allen is just nothing more than a specific collection of old ideas that are connected in a way that generates a set of principles or philosophy, right? Right. The least idea is you've got, you know, toy to stuff, you blend them, you plan to testing. Yep. So can we have the LLM generate? Can you guys ask it? Like, what's the next cool idea Allen will have? And it may seem never too old. There's never been a cool example for training. Yeah, that's true. Also, damn it. Branches actually. He's our manual tester. So I will say while Brent is typing in, because I'll be able to cut all that out later. Earlier he put up on the screen, he asked the Jason on the expert tester bot available if you have chat chat, GPT subscription thing to generate a podcast script. Oh, it was interesting. Anyway, Brent, what'd you just ask it? What's the next cool idea Allen will have? And as an AI model, I cannot predict specific future caveat, caveat, caveat, however, I could however, speculate on the types of ideas that might interest Allen in his professional interest, particularly in the context of modern testing principles and his current role. Jason has kept this thing up to date. Integrating number one, integrating AI in the testing process is okay. We just talked about that. That's enough. It's creepy. It's AI is listening to the podcast while we record. Right. And the rest ones, I don't know if it's like quality advocacy beyond testing. Quality? I mean, the actual word quality, Allen, that's interesting. Quality advocacy beyond testing, though, which is, I would say, Allen's been certainly doing on this podcast for a decade now. Yeah. Right. Responsible AI, things we've talked about, enhanced user experience testing. Right. So now it's sort of, I think, regurgitating. Which is also the title of the most boring conference talk I could ever give. Enhanced user experience. But you will now, now you have to, now you have to do it. In that past. Yeah. So now you're, you're an LM fan boy then, man. What is this? He is. I get accused of that. You're one upping me. Yeah. Yeah. Brand is like. We have, I, so I think I should this last time. So I run a team that deeply specializes in NLP. LLM is a part of our thing. Okay. I don't necessarily agree directionally with some of the things that you say. Like, I don't, I don't see QA being in my title. Mostly, mostly like there is one path. There is one path where I could see QA showing up in my title. And that is if my exec listens to this podcast, because my exact. And just, and I split the thousand dollars with him. Yeah. My, my exec loves torturing me and I could see him doing that. Brent's title by next week will be QA Ninja. Okay. Jedi Ninja. Just do it. Do it. Where, where I think philosophically we, we, like you are, yes, automate, automate, automate, automate. And I'm like, yeah, I'm all for the removal or existing jobs, right? This is something Allen and I have always been agreeing with is, is that we should be working towards automating our job. My only worry is as we rapidly go towards the singularity, can we, will we be automating ourselves out of a job at a faster rate where society can create those new jobs that are more interesting and more valuable. That part of the ethics is what worries me right now. And in terms of, in terms of, is it AGI coming? Yeah, it's coming. It's shortly followed by an ASI. Does that scare the crap out of me? Yes. Cause I don't know who's going to be the one that particularly the ASI, what we just had a conversation around using AI for quality. And it's around, okay, who gets to decide what the metric is it around user experience? Is it around societal gains? Is it around profit? Right. That person that trains these AI's towards those ends is going to make the difference between utopia and fucking Skynet. Right. So, but by, so I take a little, not offense on it, but like, I just told you, I spent my entire December building a tool to try to help humans and augment them with AI. And then they can make the decision too. Yes. I think, but I think what's going to happen, what's going to happen is this is that I don't think the humans will actually pick up that technology. I think they're just going to, my worry is that the bots are just going to do more because the humans are worried about the AI taking over or influencing them. When it comes to that, that singularity, I don't think I don't, to be clear, I agree with you. I agree with you, but I don't think it's because of that. I don't think it's going to be due to distrust. I think it's going to be due to trust, overtrust, intellectual laziness. People will just, oh, just do the bot do it. I didn't want to do that crap anyway. Oh, okay. Real quick. Then we're, we're actually in violent agreement. But I will say I did write a set of test cases because I'm a test nerd that check for, I don't know if I've talked about it, but ever actually, but, but I have a website up. I can't remember what the URL is, but it's a, it tests GPT and other LLMs for dangerous mode alloys. So I think that's a new form of testing. So I have a bunch of test cases because everyone's like worried that AI is going to take over the world, right? Or have bad, like have very dangerous biases and stuff. And everybody's worried about it. Launch the nuclear war. Yeah. So I did what a tester does and I've written, I had, I used with AI, I partnered with AI, but what generated like over I think like three, 500 different test cases, there are prompts and then verifications of the prompt responses to make sure the AI is not becoming dangerous or, or have horrible or offensive biases in it. And I run that whenever there's a new version of, of AI. So, so that's another example of, I think where testers are even the most needed profession in this. As we, as you were saying, I wouldn't even dare say it, but we just approached the singularity. Someone needs, there needs to be a watchdog, there needs to be monitoring of these systems. And it's not just how did it, did it click enough buttons or have enough boundary value inputs into a search text box. It's, does this thing going to go awry? How do we test to make sure that it doesn't start undermining society, right? And how do you test for that? And the funny thing is there's this guy, Elieizer, I think is his name. And he's like the one that's on all Ted talks saying AI is going to kill us and stuff. I've been peeing him in the background. I'm such a nerd on, on the Twitter or the X and saying, hey, like, do you have test cases? Like, how can we test for it? Like, what would you worry about? He just hasn't responded at all. And his talks, he says, there's no way to test for these things. I think there are ways to test for it. So, so it's, I think we need to have tests for the tests. Another layer of that on functional tests. How do we make sure that these things are correct and good and complete? But we also need another more meta set of tests and monitoring to make sure that the AI is not going awry, doesn't do Skynet and so forth. And I think that's the actual edge and frontier of, of testing and, and quality. And, you know, it's crazy enough that it's concerning enough that it, these, these things appear in, you know, at the UN. I hear what he's concerned about it. I hear what you're saying. And actually, but I don't know, testers will stand up. I don't think testers will stand up. But people with the title quality will, I think, I think, I think it's going to be a different set of folks that take over that the evaluation, quality and testing in that case, right? I don't think it's going to be the, the, the testers of the past coming back. I think it's a new group of people that honestly, I think my intuition on this is going to be a wacky combination of systems thinkers and philosophers that is going to be driving this because it's, it's not going to be about true or false anymore. It's not going to be, you got this right, you got this wrong. LLM is going to know facts way better than any human. Dang it, Brent, I hate to agree with you, but I think, I think I do. But some of those people will be former testers. I'll turn that in. But yeah, I hate to agree with Brent too. It's, it's really kind of wrecks my whole weekend. I should imagine being me like how tortures. Yeah. Oh God, I couldn't, I couldn't stand the smell. I agree with Brent. I think it's, we're talking about quality value fit when the things you're talking about Jason, not, there's a lot of tests is going back to the LinkedIn thing I read earlier. There is a focus with a lot of the testers today on making sure it works correctly. That it doesn't have a recall class, but this functional correctness, largely functional correctness, which I think AI's consult with coaching with bots, etc. And then, and developers can do that testing. But I do think it's interest. I thought about this years and years ago, like at Microsoft in like that 99 or 2000, I thought do we need a true quality assurance role, which is about the things you're talking about? Should we make sure we've built the right thing? Should we make sure this is a, this has fit, it's kind of half product role, etc. That it made me think of the whole, the Bolton post on testers get out of the quality assurance business. But the problem is we need somebody in the quality assurance business to make sure that whatever we're building with LLMs or not, is providing value for the customer has fit for the customer. And that feels more like a product role, or maybe what Brent said around philosophers, than it does a test. Well, I think I think it's actually, it reminds me a lot of the relevance role, like at Bing and at Google, like as what is relevance, you have to define it and then quantify it, but it's a very nebulous thing. Because, you know, what is the best search result for 5 million people you search for Bush, the string Bush, do you want the president? Do you want the band or do you want the plant? So it's a hard problem to solve, right? It's very complex. So, but if you do look at this, this is I think critical today in the AI world is the philosophers, guess what, Brent, I will say this, the philosophers and even the product guys, they really suck at being testers. So if you look at even papers, I did a rant post or something a couple months ago, like, because what's coming out of Stanford, these evaluation suites for AI and ML coming up, they're horribly executed, they're tests, but they're horribly executed, like the actual implementation, like there's tons of false negatives and false positives in these things. And it's really horrendous and concerning, because this is what's being used to defend humanity from AI. But so I think there's a lot of Allen's and Brent's influencing those people to make sure that all the lessons we've learned over 20 years in, in basic testing is applied, but the conceptual things that are being tested are probably like, more relevance, more nebulous things, like what you're saying, like fit, and overall, you know, kind of purpose. They're horrible testers. I don't, I don't disagree. I just think that they are going to be the more well suited testers of the future. Okay, because I don't actually think like I was thinking about a problem that I have just, just this week, right? I'm asking it to, to summarize and just to say, summarize this long email chain. Okay, and then there's some conditions I want it to remove. I want it to honor privacy conditions. Okay. And it consistently screws up on that. And it's one of the issues with with prompt engineering, that's, that's actually known is that if you give it too many instructions, or even without realizing it contradictory instructions, it then kind of makes its own call and goes forward. Right. And so I'm like, but if I tell it, gives me a result, and I tell it, hey, tell me what instruction of mine you just violated, right? It realizes it near instantly. But because I forced that reevaluation, and then I'll just say do it again. Right. And in some regards, if a testers, if the testers of the future is just like, all right, tell me what principle you just violated do it again. Right. That's readily automatable. The thing, it is this whole things, automating this, it sounds too hard. This is why I think, again, the system thinker, philosopher type, you know, maybe chess masters is another one. It's testing things directly. Oh, they got a fact wrong. That's, that's gonna go away pretty quickly. Right. It's going to be, god, I hope so. That's gonna go away. It's gonna go away pretty quickly. It's going to be okay, but now it's combining these random facts that we don't know, because it's a black box into this directional element. We're going to need to be able to think multiple steps ahead to find errors in its thinking. It's going to be, it's not going to be fact errors, it's going to be logic errors. But in it's not going to be as simple as A plus B equaling C when we intended it's going to be no, I combined A, B, C, D, E, F, G, and the rest of the alphabet together and have come up with this, with this logic error. And we're just not going to be able to see that if we don't constrain it. So Brent is very just long windedly said he's all prepared to train his bot. I was thinking the same thing. That's all that's all. Yeah, that's all out. Okay, the Brent bot is coming. Jason, anything as we near the end of time here, anything else you want? I know we know about Chequi AI, which probably got cut off last time, but talk a little about that. What else should people know about you? How do they follow you? How do they avoid you? Things like that. You talk to James Bach about how to avoid me. Yeah, I'm working. So actually, just on LinkedIn, I blog this kind of stuff and think about it and share, just to share and get other conversation. There's more conversation out there. I wish more people were engaging, especially I also really, truly wish that manual testers will engage and take this opportunity to engage with AI to help them to augment them. I feel like I'm not trying to, I guess I'm trying to be whining about it, but I feel like I've tried and like there's been like, you know, a couple hundred signups, but not thousands, right? And not super deep testing or great feedback on the thing. But this is an opportunity for people to, to make for their manual or exploratory testers, which are millions of them, to engage AI and see if you can help them in their job and, and to power super power, power up. But the things are moving fast. So I'd leave everybody with, again, I think we're dealing with exponential technologies. I would go so far as Brent is saying there's singularities coming, but he's crazy, LLM fanboy, but AI fanboy. You don't think a singularity is coming? I would never state that publicly. Okay. Which is not an answer to my question. It's just, what do we do in the meantime? Right. But yeah, but I think it's an exponential technology, which means it will be a nothing burger and kind of just a tease for a while. And then suddenly it's going to be boom in your face and you have to confront it. There's no avoiding this exponential technologies. So I just leave people with that. And invite them to actually. And I will probably, and Brent too, even more so Brent, like on these, these predictions of the future and stuff, we, this, what sucks about predicting these things is that, um, is that you are wrong until you were finally right and nobody cares anymore. So, um, it's, it's kind of a not so good position to be in. And remember, I, if you go back on the internet, I've been talking about how AI is going to be coming for an exponentially and have a chart from four years ago at some star conference, like keynote saying where I think things are going to start to be automated by AI. And guess what? We're in the middle of basic security and functionality being automated by AI. That's we'll see if that materializes in 2024. But it's, uh, and that's based on, like you said, like with recent stuff, that's based on just curves, wilds predictions, which have been right on for like, Oh, I don't know, 25 years or something like that. And so, um, yeah, so people should be ready. And, uh, if you bury your head in the sand, it just could be a Saturday wake up call, but it'd engage. I think I just encourage people to engage it. Try stuff out. Cool, man. Thank you so much. Uh, Brent has been, uh, showing off some things from the, uh, the extra test we bought in our screen, but Brent, I'm going to encourage you to share those maybe in a channel on our Slack group, one of the three dot slack.com. You can go to modern testing dot org to get an invitation. I think they're kind of fun. I think they'd be fun for the community to look at as well. So we'll get those posted up. So, uh, that is, that's the end of our time with Jason Arvin. No, no, next time, next time you talk to another GP teapot. That was funny. When I was in the car, I was like, Oh, I guess that's why they didn't invite me back. Uh, cause you're talking with them with the bot instead. Yeah, I think what happened is then Castor listened to be complaining about it and said, screw you. I'm going to make your life horrible. And it did. It did. It was horrible. So, uh, that's it. This has been episode one 94 and I'm Alan. I'm Brent and I'm Jason. Thanks for a geek note. All right. We'll see you next time. 
