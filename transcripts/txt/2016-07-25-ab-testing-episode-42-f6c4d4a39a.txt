Hey, thanks for listening to the show today, this is Alan, and before the show starts I wanted to warn you that we had some audio issues arise during editing. I lost a microphone, had to try and make up for that with some level changes, and with that I got extra sounds of Brent breathing, some microphone rumbling, and in the end a lot of clicks and pops in the last third of the show. Good content, I hope you enjoy. Thanks again for listening. Hey everybody, hello, I'm Alan, I'm Brent, and it is episode 42 of a video. I'm going to be testing, how about that, 42. Yes, and unlike the usual reference to 42, this episode will not be the answer to life, the universe, and everything. No, it'll be the answer to, hey Alan, why angry weasel? I'm going to kick off episode 42 with some good old fashioned venting. Yes. First of all, Brent. I understand the angry part, and I know you've told me the story approximately 50,000 times, I don't get the weasel part. There is a story on, hey, I resemble that comment. There is a story I've posted on the long, long story, I'll give you the paraphrase version, but if you go to AngryWeasel.com, the sponsor of AB testing, if you go there, there is a story about the weasel or something like that along the banner at the top of the blog. It talks about the fact that many, many, many years ago, I was playing in this rock funk band and we were trying to decide on a band name. One time we had some temporary names, one of these bands had a different name for every show for a while. Then we're talking to someone after a show and they're telling a story about how they were driving through Germany and got some car trouble because they're stuck by the side of the road. Someone pulled over to help them, this kind of strange German, this is right out of a weird movie. Stephen King. Not maybe Stephen King, maybe more Kevin Smith. But they started talking about the weasels and said, beware of the tooth of the weasel. It's not a very good German accent, but they heard the story about the tooth of the weasel and they talked about Angry Weasel and that's it. That's it. That's the band name, Angry Weasel. Even the band said, yeah. Our guitar player said, no, I don't like it. I said, screw you, I am getting angryweasel.com, I'm going to keep it. I owned angryweasel.com for about three or four years before I finally moved my blog over to angryweasel.com. It was originally going to be a band name, I just took over the name and have a logo now and everything, that's where Angry Weasel came from. So nothing to do with Pauly Shore? Nothing to do with me actually being angry except for today. Because let me tell you just a couple things. It's been a while since I talked about dumb things Microsofties do. Alan, soapbox, box, box, box, box. So this is not really it. No sound effects. So Brent and I are meeting two floors below my floor. Why? Do you ask? Oh, why? Because the meeting rooms on my floor are all booked. Even weirdly, we're in the middle of a move and there are only 10 people on my entire floor, yet our meeting rooms are booked all morning. Why? Because people, who I won't use adjectives to describe, we have a lot of Microsoft employees just decide to have, it would be fun to go have a meeting in Lincoln Square so they come over to Lincoln Square now meetings. Okay, fine, done it before. And then we have other people who book meeting rooms like recurring meetings, big long meetings and just never show up. So you can't actually book the room. Understand, there's 10 people on the floor, the meeting room right next to my office that I want to use all the time. I can never use because somebody I don't even know or work with or have seen before takes up claim for a four hour meeting in that meeting room. Do you have, you think you have enough clout to take that meeting room offline? I can't, but I do have enough clout to, there's a, I try not to do this because I can, but I can actually, one thing, career advice for everybody in the world, career advice for everybody. If you have an admin on your team, be nice to them. Oh gosh, yes. Because they actually run the organization. So I have enough clout to go to our admin and say, you can do this at Microsoft. You can say, can you please restrict meeting rooms on our floor to our organization so other teams can't book them. They can do that. They can do that. How? It's just a, I'm going to have to go talk to my admin. Yes, it can be done. I can't have it for myself, but I can have it for my org. I've recently, recently moved buildings. We haven't done a podcast in that one yet, but there is only one conference room on my floor. And for the longest time, it was locked out. It was the, it, it was under exact lock out. Yeah, that was cool. All right, so rant number two. I think the secret to dealing with the meeting rooms is just have no meetings. Honestly, it's a really a first world problem. It's really, I mostly use meeting rooms for podcasts. We do our stand up just next to our desks. When you come to our building the next time, I'll show you my office. Coolest, biggest office I've ever had. And now whenever I have like team meetings, I just do it in my office. I'm just angry as a weasel because- I'm really feeling the anger emotion. Yeah, I was. I wanted to use the room and I thought I had a book, but I guess I did. And the guys that were in there were kind of dickish, so they kind of pushed my buttons a little bit. Hey. Hey, so that room next to you? Yeah. I have a suggestion for you. Yeah. Hang up a scrum board right on the wall outside of that conference room. And just do spontaneous and very loud stand up every half hour until people start unbooking that room. We're out of stage at our product where I kind of knew this would happen. And I haven't played the game if I told you so. I'm playing the game of dig ourselves out. Where things were going swimmingly and we are pushing towards a big milestone and our GPM, GPM and our ship room meeting said, really important we have these features done for this milestone. And I said, whoa, whoa, whoa, whoa, hold on a minute. The past tells me that when you make statements like that, people will shove in stuff without doing their due diligence about quality. Just those features need to be done. It's critical for the product. And I said, OK, because it wasn't a fight. What are you going to say? I wanted to have it. It wasn't going to go anywhere. I believe in discussion and even disagreement as a method of getting to an end. I think it inspires creativity in this case. There wasn't anything to do at that point. So anyway, what happened is we have what I'm calling feature jam. And we just took a snap of our product has been getting progressively worse. Shocking. I'm not shocked. All I can do is do my best to make sure we get out of the hole. But it is we took a snap. I'm a little angry to start off with this morning because we took a snap last night from our sea environment to our pre-production environment, where normally we can sit for like a day and be OK. It's good. Let's stop this to broad. It is for the last three weeks, especially this one. Just awful. I've just I'm really frustrated and almost embarrassed about how every time I see people make these type of mistakes, right? The other mistake that I see is that they don't use it as an opportunity to to make this a learning org. Right. So what are you doing to make sure the next time doesn't do this shit? That is absolutely what I'm doing. So lots of stuff going on. Trying to do a lot of the work to make sure some of these things don't happen again, as well as just there's two things lacking, I think. One is lack of developer testing, even trying their thing out before they check it in, they're getting better at that. And where the learning is going to come from is in an org that up until the pressure kind of hit was a very open org with lots of communication. Everybody knew what was going on in the last, I don't know, some amount of time, month or so. As the products become more real and more something that we're going to ship to real people in a fairly soon timeframe, the pressure has made them silo a little bit and that communication has gone away. I think the key to being successful, you cannot ship a product as often as we want to ship without very transparent communication across the org. So we have teams breaking other teams with their changes, feature teams breaking other features with their changes. I think primarily due to lack of communication, hey, I didn't know you were going to do that, or why did you do that, or should have had a discussion about that, sorts of things. So those things are happening, they would always happen, but they're happening noticeably faster, in fact, exponentially faster than before. So to me, that's the key part of the problem. That's the key thing. So that's a good thing. That we're trying to, I believe that communication could solve the majority of the issues that I have seen over the last few weeks. Your story reminds me of how I execute my Kanban board on my team. We have these tickets, as the three well know, I haven't had a bug database in years, we have these tickets we call swarm tickets, and periodically we go through a period where essentially all the work that we're doing is nothing but swarm tickets. Swarm tickets, when they come up on the board, they're treated like bugs, and when they come up on the board, we have to knock them down in 48 hours, and that means we're not doing feature work. So you can, for those who are familiar with the concept of bug jail, it's like we have a zero bug bug jail. One of the nifty things as a tool of having these tickets in this process and a very visible board is that we can very readily see when we're in the midst of what we've now come to term as swarm storm, where it's essentially we're going on for a large period of time and all we're doing is knocking down swarm tickets. And one of the insights that I learned from the author of Personal Kanban, Oh yeah, Benson, Jim Benson. Jim Benson, I always think of Jim Henson, but that's not the right guy, was that too many swarm tickets is actually a sign of going too fast. And I have used that over and over again to slow the team down, do the proper root cause analysis, do the proper retrospective to figure out what shortcut have we been making that we should not be, and fix it. And then almost always that causes a near order of magnitude boost in productivity. And it's counterintuitive, but once you work through what the board is doing is telling you what the crappy bottleneck on your team is. But if I can be briefly optimistic, I would say that when we dig ourselves out, the team will be better than they were before. Cause now there's a lot of learning from mistakes as we all know, and we're doing a lot of learning right now. Yeah, I'm not as optimistic as you are because I've seen broken window theory happen over and over again in exactly the same situation. I've seen it too. What I've seen is the GPM gets too much power, and then your next six month milestone, what he's going to communicate is, this is how we ship product. This is just the world we live in. Devs should buck up Buttercup and figure out how to cram in these extra features and quit screwing the customer. The thing I hate about this power struggle, cause the way you tell the story, and I hope it's not true, but the way you tell the story is you got like an old school GPM, and those guys get reviewed on their ability to cram in additional features. So let me add something now that I'll probably edit out. Alan here, we cut that part. All right, back to the show. I think all of the three are on Twitter. And if you've been on Twitter, you've noticed over the last week or two, a lot of talk about the context driven testing community. I have this weird take on context driven testing. I'm not against it. I'm all for it, but the thing is with me is I'm a little bit removed from the conversation or the concept, because I don't really believe in context driven testing, not directly. I do believe, in fact, the difference is I believe that I don't believe in schools of testing. Sure, there are people that get paid to push buttons all day, and you can say they're the factory school or whatever, but I just believe that all good testing and all good software development in general and all knowledge work, anyone who is good at any sort of knowledge work is context driven by necessity. So I don't understand the need to say, well, I'm in the context driven school. Just like, what the, I don't even get it. So just the whole concept alienates me. So it's hard for me to figure out where I stand in this conversation. I have my opinions of James Bach. Again, I can talk to him. There's there have been psychological studies. Obviously, we don't prep on this show. So I'm not going to give any references that shows that as your EQ or is your IQ goes up, your EQ goes down. So it's very, it's not uncommon where super smart people, they're extremely logical, don't have much room for compassion in some regards. Right. What the point is this way is I choose to ignore James. I know he's deaf. I read his blog post, but I just kind of ignore him. Is he is his loud, not physically loud, but is his leadership damaging testing in any way? Is question A for you, Brent? And B is by not standing up to him more and not involving myself in the discussion more, am I damaging testing? I think on the CDT front, right? The whole school's topic in from my world view is done, is over, right? It doesn't make sense. The I also strongly disagree with the content, the CDT, the context driven testing. Because testing in my view is context driven. It is entirely redundant having a separate school. That one now famous slide that we've talked about, right? James is potentially attacking the agile movement. It's one of those things where that movement that is successful, it is helping businesses build value. And my point is, my view is his leadership, if there are people kind of viewing James's role in the testing community, quite honestly, it used to be important. Its importance has shrunk dramatically. I think people like Elizabeth Hendrickson and Lisa Crispin are much better leaders for testing in this new world. And I think the importance of his content has shrunk to, I don't know, cult size. It is, and I sort of, I don't know if we're ganging up on James, but I'm glad we're talking about him in this context. Because I think his, to me, his influence on testing has been diminishing for some time. The size of the population of testers that can benefit from RST, Rapid Software Testing, his primary source of income, as far as I can tell, is still a huge pool out there. But it's not the people I hang out with that's not diminishing. So I, like people say stupid stuff on the internet every day. Every day. When James says something stupid, because he has a larger following than most of the other people saying stupid stuff, does it make a bigger difference? Part of me says I should just call him out when he's being, when he's out of context. I don't have time for that crap either. My advice to you, Alan, is, right, you're already doing it. Look, I've coined what I call Jensen's law of politics. And it is essentially this. He who is defensive loses always. And his response, his abrasiveness, in my view, is an act of defensiveness. That last slide is an act of defensiveness. It is... It challenges his world. It challenges his world. Some people get defensive by getting defensive, and others get defensive by going on the attack. He's more on the attack style. We have been very good on this show, as well as before we were doing the show, of aligning with the momentum of the future, future, future, future. Right? Well, let me interrupt there. And I think we're fairly decent at it. I think so too. And I think that one thing is, this is going to sound weird, but by those of us that look at the future and adapt, we talked about software being predictive, iterative, adaptive, adapting to what's needed for the context of shipping software is more context driven than a lot of what happens in the CDT air quote community. So those of you, and I'm just going to add this right now, those of you right now getting tired of the boom, boom, boom in the background, Brent is like fondling the heck out of his microphone stand and won't stop. It's awkward. It's... Sigmund Freud is like watching right now to see what happens. And I'm freaking out a little. All right, where were we? I'm sorry, Dylan. So the one last thing on the subject is there was some talk about James and... Whoa. James and Michael Bolton wrote a paper on the context driven... context driven testers guide to automation or something like that. I was a reviewer on it earlier, almost exactly a year ago. I blogged about this. My name is listed as a reviewer. I didn't know it. The paper is my feedback is mostly grammatical and flow and like, no, this isn't what you called it, etc. And then there was some feedback around the approach used. What that paper is about, and I'll summarize it, it's about how we use some tools to help us test this product, which I think is a good thing to do. I use that all the time. It wasn't a paper about test automation. They use some tools to help them test the product. They didn't actually automate anything of value. So it's... If you want to learn about test automation, not the paper to read. But it's a fine paper on, oh, hey, we use some random tools that weren't quite appropriate, but we made them work to test some random software to show that we could do it. Yeah. I mean... So I... And this paper was published a year ago? No, I reviewed it a year ago. I think it came out like three or four months ago. Maybe six months ago. Three or four months ago, a paper that describes essentially a manual STE world that, hey, look at the nifty tool that we just learned about that makes our life easier. Three or four months ago? Really? Yeah. Going back to your last question, Alan, my advice to you is just take the high road. These guys are just on the wrong path. I think so too. And it's just a waste of your time. Right. So I think the answer is I should just ignore them. I'm not doing a huge disservice by not standing up, but I probably will if I'm ever bored. Yeah. Your last blog post seemed to be about this drama. Yeah. And somebody called you out and wanted you to push back. Oh, Chris McMahon said, hey, you're reviewing this paper. What's your take on it? I said, I don't think it's a very good paper. I reviewed it. Sure. So is Chris asking you to take Michael and James on? I don't know. Chris and Noah Sussman, two people who I respect the shit out of are both they have both been attacked directly by James. So they're and I'm full of support of them. I would support them. So I will do I will do what they much as they ask me to do within my ethics and time. So, hey, we have a few minutes left. Let's talk about something you're doing like data driven change. So I don't know if we have enough time to really flesh out this topic. Thanks, everybody. Yeah. But let's give us an overview. It'll inspire some questions that we use those 43. So one of the the just another shout out those who are listening in and want to be attached to more of the A.B. testing community. Right. We do have a Slack channel. You can you can direct message per Z P.E.R.Z.E. I think you and I are admins on the site, too. Yeah, but I like it. Percy as our admin. So I'm going to keep advertising you per Z until you you you cry uncle enjoying one of the questions that has popped up on the slack channel quite often is hey, great. We're big believers in this data driven culture, this data driven change. But how do we do it? I think that's probably a full episode. Alan recently went through a similar thing. I don't. I did. You did in my view where essentially you had a PM who was the way you described it initially. You had a PM who was building yet another. The metrics scorecard. Yeah. Stoplight scorecard filled with entity metrics, none of which are useful. And that's how you initially described it. And then we talked later and he's like, oh, no, this guy's on the board. So what was the difference for you there? Like initially you're like, oh, my God, is a PM. This is what he's going to build. Yes, there was a lot of talk about the default built. Start talking about building dashboards from telemetry. The default they seem to always fall into is, oh, let's record how many people use our feature. How many people click this button? That's great. But you don't learn anything from that. There's no insights from that other than vanity stroking. So I was worried there was a lot of talk about this. I went and talked to the PM. I said, hey, I didn't want to tell him he was wrong. I just want to figure out where he's coming from, blah, blah, blah. Turns out he actually has a good background in experimentation and he knows that those metrics have some but little value. But he also knows he's a good PM because he knows that at first to get people to shut up, he was going to give them what they wanted, which were the dashboards that show their little vanity metrics and when they go, oh, look, I have a dashboard. But in parallel, help make sure they build dashboards that show customer value and business value and get some insights as well to eventually be used instead of usage metrics have limited. I won't say no value, limited value, but he is actually on the board as far as how for two things I wanted to follow up on. One, building scorecards, for lack of a better word, dashboards that show business value and customer value of a feature. And then also did not blink an eye when I said I want I believe in experimentation culture where everything's an experiment. He nodded his head said, yes, that's absolutely where I want to go to. And when I get that agreement, I go, okay, we're cool. Move along. I have other things to worry about. I have confidence in you to take this forward. So it was going into discussion. I was worried as hell coming out. I said, okay, he's got it covered. Time will tell whether I have false confidence or not, but I feel good about it now. So the PM is building the scorecard. It's a good thing. Because the first thing that you're trying to do is create an asset that drives the business. Create convergence where everybody's eyeballs are looking at this, knowing full well that in a few months, you're going to push, you're going to self push back on the metric that you built. Because oh my God, I just realized there's a better one or oh my God, I just realized it's wrong. Okay, the first the step one is to create a site that people converge towards. Then the next that that creates a vernacular creates a language creates something that people are concerned with. Okay, you use the motivation that they're already used to and people are already used to you can't ship until the scorecard shows all green. Okay, then what I will often do is create a companion site that's a bit more actionable and then link these two together. So you have this one scorecard that shows all these yellow numbers. Then you create this other companion site where people can drill down and see what's known as Pareto order. What's the biggest bang for the buck that we should go after in order to turn this green? The challenge with your strategy that this PM is executing is that you constantly have to change these KPIs if you want to get there. I don't one of the things that scares me about that is I don't want to over index on the on the stoplight, red, yellow, green dashboards. To me, in my experience, insight is addictive. Yes. And by that I mean to explain Brett gets it but the moment like you look at how they could use like it. How much time they push the button. Okay, oh, great. It was 500 years today 600 today. That's great. It means nothing. There's no insight from that. But when I can start to connect some things go okay, this number went up here. The same time we added this feature at night. And I look at some different metrics together and I go, ah, ha, well, people find an insight they go, oh, I want to find another one. It's awesome drug. Oh, it is. So I want to get people to a stage where the dashboards or whatever met however they're looking at the metrics, I'll stick with the dashboards for now, but it can lead them to insights. Because right the vanity metrics, there's no insights that come from those. But if I can find a way for them to get the right data in front of them, where they can start to find first find insights and then search for insights. This doesn't require business intelligence require data science or statistics necessarily. Brent's rolling his eyes a little bit to be some of the more advanced stuff, air quotes again, yeah, we'll take that. But any PM without a math degree should be able to any engineer any knowledge worker without a math degree should be able to look at some set of data and begin to find some insights. And once you find insights, you will begin to search for insights. And once you search for insights, your path to the dark side is clear. I mean, the good the forest the good side. So I recently pulled together a deck on how to build a data centric culture. And there are three phases I would say a team goes through. And the first one is data oblivious and data oblivious essentially means I've talked to actual customers, I know what they want. Anyway, oblivious, then what happens? Then it's data centric. And that's really what you're building when you build this scorecard. Yeah, sure. It's okay. Now we need to have data driving the course of the business and it's here driving with data centric is different than data driven. I assume you're going there. Yeah, data driven is the last one. So the one of the key markers, I like how you frame the story. But one of the key markers that when you're in a data centric is you're starting to see insights being built and those insights becoming addictive. And an insight is really nothing more than an unexpected relationship between two different data sets. That feels good. I mean, it comes with euphoria to a little bit, a little bit of if I had a meeting, if there's a scientist in the three, could you please do a study on what happens chemically in the brain when you have an insight? It's you found a discovery. It's it is fully addictive. It's like I get to play Easter egg hunts all day long with the data. What is lacking though is great, you have an addiction around insights. But what is lacking is which of these are interesting. I like the idea of that insights can be interesting or not interesting. And it's the amount it makes you go, huh, or huh. For me, the you can tie it back. But what really drives interesting is how easily and readily you could connect it to a useful business KPI. And a useful business KPI is essentially does it help me grow? Does it help me earn more revenue? Does it help me reduce costs? Does it help me reduce time? Those so if you can connect the dots to something that the business as a whole cares about. It's interesting. I have seen. So for example, last week I was sitting with one of our interns, and he drew together a history and talking about data science stuff like 80% of what I am doing nowadays is just pulling together histograms and interesting ways and looking at the patterns. I know how to analyze histograms. A lot of PMs don't like when should you look at the outliers? When should you not disclosure? I don't either. Yes, teach me. Okay, happy to do this. I've seen histograms. I don't know if I can get an insight from one. You'll be surprised. Once you play with histograms and you learn how to use them. I'll give you an example. A lot of the histograms are head tail curves. You can do a simple transformation of a head tail curve to turn it into a normal distribution. And that normal distribution will give you a much greater detail of insight than the head tail distribution. The other thing with head tail distributions, generally there's between two to five times additional whatevers depending on what the head tail distribution is measuring in the tail than in the head. But almost everyone focuses on the head. So that's one of the reasons why I will often convert it to a normal distribution, a bell curve distribution. Because it eases the seduction around, oh we must focus on the head. They're the customers that everyone cares about. Where was it going? Hit a pothole. Oh, so this intern drew together a histogram and he found that 80% of the results he was looking at was divisible by five exactly. And he's like, does that bother you? And I tortured him on a bunch of questions around. So one of the things for me, it is immediately initially interesting when it doesn't line up with your expectation of the data. What makes it super interesting, I guess, to try to disambiguate the term of interesting I used before is when you find a pattern that you can connect the dots to a business KPI. But the first thing is if the data shows something that you don't expect, that is something you should follow up on. Time box it. Don't get, because the other thing with these insight addictions, it can be a rabbit hole. It can be something that gets sucked you into a spiral of figuring out, well why? Well why? Well why? You always have to be able to have something in the back of your head of if I answer this question why will I care? There's more on this topic. I think we can go into deeper, like the Slack channel. One of the things I think our listeners have asked for is, hey how do I get the execs to stop being stupid? Oh, let's do a whole episode 43 on how to stop people, including execs, from being stupid. As you know, I don't know if you, I'll mention this again in 43, but I once, been about 10 years since I first did this, but I once described my job as I stop people from being stupid. I do less of that now, but it's still something I've always done. Why do you do less of it now? That's a good question. Let's explore that. I'm pretty certain the amount of stupid people have not gone down. Maybe I just do it so subconsciously now that I don't notice. Alright. That's what we're talking about. Thank you for listening. If you've made it this far, yes thanks. I'll thank you anyway. I'm Alan. I'm Brent. We'll see you next time. Bye. 
