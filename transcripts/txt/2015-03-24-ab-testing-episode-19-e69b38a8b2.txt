Hey Brent, are you ready? I'm ready, Alan. Let's go. Alright, we're back. Nineteen. Episode 19. We are so prepared. This is the A.B. Testing Podcast, and I'm Alan. I'm Brent. And now we can have our talk. Wow, we started the ADT thing even before the podcast this time. Well, yes. New record. Hey, speaking of ADT, hasn't it been like almost exactly a year since our first podcast? Almost exactly. We have like two weeks. So oh, okay, not yet. So no cake today. Don't send a cake care of Alan Page through the OA Microsoft Redmond Washington. Right. Don't do that. But instead send the Don Perrion to Brent J. Microsoft Redmond Washington. Or Don Julio. Actually, yeah, both will work. We will do, how about 8 a.m. tequila shots on our anniversary? Why not? Why not? We'll do. We'll do. I know we'll do every time we go off on a tangent. We'll do a shot. We have a whole 45 minutes to fill. I'm not certain that we'll do it. Hey, because of our anniversary, I got to be a little serious here for a second, Brent. Are we going to turn on the mood music? Well, no. I believe in sharing everything. And I want you to know that I stepped out on you, man. That's okay. I've been doing the same for you for years. Well, I did. I recorded another podcast with someone else. Cool. I know you want me to be all. What? What the hell? What did you do? Don't you value our podcasting? So what was it all about? Joe Colantoneo has his testing podcast. Had it going on for quite a while. He has more episodes than us, so I got to give him some credit. His podcasts, I haven't listened to all of them, but they're mostly interviews. He decided to interview me a week or two weeks ago. We talked a lot about test automation. I just talked. He thought there were some good nuggets in there, but I just blabbed about a lot of stuff we've talked about here and in the A word, my Lean Pub book, which is closing in on 1,000 downloads and very happy. I'll put a link to it in our show notes. You can't get enough of me. Imagine that. People are going, no way am I listening to his voice any more than I am now. But if you're if you have maybe, I don't know, maybe my family or something, maybe can anyway, long lost cousins, friends on Facebook. I guess so. I guess so. Anyway, that bully from high school. But I my thoughts on software testing are yet in another blog. So was it just general, high level? No, no, no. We talked about. People should test things. You know, do you want me to just take the rest of the podcast and just repeat it word for word? No, I want you to do like the top three bullets. We talked a little bit about like as a teaser. We talked about false positives and how people don't take them seriously. We talked about we spent an entire episode on that once. Yeah. Just some things to do with test automation. We talked about GUI automation and how I think it's mostly dumb. I'm sorry, mostly dumb is the wrong word, mostly ill advised and not worth the effort, but how to make kind of make it worth the effort if possible. And talk about model view controller, etc. Some things like that. Just, you know, stuff I've learned or made up or copied over my vast career in software. Yes. Hey, do you, since we're doing promo, did you want to do another star Canada that's coming up, right? Yeah, I got my, my mail saying turn in your slides by May something and, and etc. Yesterday, but which I, I, this year I was so on the ball. I started making up my outline and putting together slides like a month ago. And then I did it for like a day and then I stopped. So I still have all the work to do. It's in my head. I know what I want to do in my head, but I got to kind of put it all together. And then I do this really crummy thing where I don't like it. That conference is what my slides, uh, a month in advance, because I don't really have it nailed down till like an hour before I do my, does that have to be final slides or the same thing? You know, I'm not a tweak them. No, I always have to send a update because they never match what's in the book that they publish. Uh, I think that's fairly common. I get it kind of close, but there is a lot of editing that goes on. What I submit to the star folks is this is absolutely what I'm going to cover. Not necessarily how I'm going to cover it or the order I'm going to cover it, but this is the story I'm going to tell. So there's value in the slides. It's not like there's, if you just heard me talk and didn't look at the pretty pictures behind me, you probably get, uh, you wouldn't know the slides were different, but some people can get really, um, sounds like they, they want the month so that they can do some logistics, like printing out crap. Right. Right. And absolutely, that absolutely didn't need that. So it's a, it works out. I think it's okay. They've never had them complain about me missing, uh, my slides not matching. So it's okay. Just kind of the way I do things. Uh, I do want to get it started though, before, uh, work gets a little bit more hectic, but anyway, it's only five weeks away. Our Canada third week in June, I'm doing a tutorial and a keynote and, uh, lightning talk keynote, uh, all packed in there. I'm doing like the, um, I think my tutorial is Monday or Tuesday. I think it's Monday. My keynote is the next morning. And then I'm doing a five minute lightning talk as part of lightning structure, keynotes that afternoon. So lots of Allen. And then I disappear. We should, we talked about this last time. We should talk about whether or not we want to do a, uh, an AB up there. We kind of talked about it. And then, uh, if you're there, are you going to go? If we're going to do an AB, yeah, I'll be there. All right. What were we talking about? Should we get to our agenda? I don't even have my, um, glasses on passport. So I'm like, if I'm going to go to the Canada, it's going to be worth my while. All right. What's topic number two, Alan? Uh, what do you say? Oh, on the board. Yes. Yes. What? So, uh, believe it or not, I sent mail to Brent. Yes. This week with a question, I'll send it to Ken Johnston. People are going, who is Ken Johnston? Ken Johnston wrote, uh, a couple of chapters of how we test software at Microsoft and he's, uh, we still cross paths quite a bit. Yeah. He's also a very big, uh, uh, presenter around the world. Not quite as, I don't think he does it as much as Alan does, but he goes around the world for whatever that means he's, he's a, he's a blogger. And, uh, he knows a lot. He has a, I think he's, uh, uh, uh, there's so many tangents. I want to go out of my head right now. He's a former manager of mine. Is he of yours as well for a manager of mine? I don't know. You worked for him. Yeah. That's funny. Uh, you know, a great mind on, uh, sort of the data world testing and production, data, different quality, et cetera. Anyway, my question was mail bag. No, it doesn't count. Uh, so I'm looking at, and I hate the word dashboard. I called it dashboard in the mail and I think it set off some weird connotations, but so we'll call it scorecard. So I'm looking at some crap on a webpage that represents, uh, our product usage through data, which scenarios have been covered, whether errors we've seen, whether a scenario is completed successfully. It's a nice view looking, it tells, looks in across our product. It shows what people are doing and where they're running into problems. It's a nice view into the product. Okay. And I'm looking at it. I'm thinking this is really cool, but there's so much of the story that's not being told. So the question I had for Brett and Ken, I'll call, I'll call Bren or Ken Brent. I don't know. I'll think I need a name for you. Kind of like, uh, I don't know some hybrid name team awesome. Yeah. Um, let me thank no. So my question to them was somewhere along the lines of this, look, I have this data, give us a lot of information. I'm realizing more and more that it doesn't give me all the information I need. And there's some things we're not going to get through, uh, product telemetry and metrics, uh, measurements. Um, I don't know. There's a lot of things about risk. I don't know. We're still, we find a lot of bugs internally and we have, which aren't represented there, there could be bugs that we know need to be fixed. The customers aren't running into could be, um, some of us just out, you know, looking at outstanding work items for a particular area, uh, looking at other risk measures like, uh, scenario coverage or people are, are we getting people are completing these scenarios that we have measurements for, but are they, do we have the full coverage, not code coverage, but full. Do we feel like people have done this scenario enough and enough and then the permutations we care about it. There's four different ways to do something. Have they done it all for the telemetry that you're looking at is, is it represent sort of telemetry from some form of testing and production? So, well, we are so we're so private and small right now that our only users is our internal team right now. Okay. But at, but we're getting very close to going to a much wider audience. And my worry is when we get there, we're going to get a bunch more data. And I don't know whether that, when we look at that data and aggregates from a larger number of people, will we will that data on that web page give me a good representation of how our product's doing and how, you know, any risks we have about shipping, et cetera. I feel like there's a lot more information I want to see as part of that. And I can't decide whether it's two different reports, whether I find a way to merge that in there or what. So having been in, you've been in services a lot longer than I have and Ken as well, my question was, do you, when you're looking at services that you're doing testing and production or data driven quality on, what sort of data do you look at that's not customer data and how do you incorporate that into the views into product risk and quality? Can I take this a different way? Absolutely. Until I say no. Okay. Fair enough. Because the way you describe dashboards seems relatively common. Right. The issue is what's not clear is you're doing a dashboard that produces information. I got to tell you every time you say dashboard, it's like, not the right word, but go on the chart. Anyway, go on. So once you go to, there's a term that I'm fond of. It's not mine. I wish it were. A guy described most dashboards. He's a vocal leader, I would say most likely on your side of the point of view, but what he describes is that most dashboards are just a bunch of data puke. Yeah. Right. A phrase I use quite often. I love data puke. Is data rich, data rich information or insight poor? You know what you, I'm going to interrupt for just a second, let you go on, because that kind of hits on it. I feel like the information on our current air quote dashboard is valuable, but I want to see more without puking on it. The primary issue is when people put together dashboards, it's very easy to sort of barf a lot of numbers into a website. Yeah. Huh? But they got the cart before the horse. So when you, when you go into the very basic thing on, on business intelligence, data turns into information. Information turns into knowledge. Do you know the difference between information and knowledge? Yes, I do. What is it? I'm out. I'm going to let you go on and tell me. All right. Uh, hopefully I'll pass my own test. My, my quick answer is, is I'm going to take, I thought where you were going is I'm going to take action on the knowledge. I can't really take action on the information until I process it into something that I could do something with. Knowledge is actionable information. It's, uh, hell yeah. I know, I know. Crap. The difference between street smart and book smart. Book smart people are dumb. Maybe that was a little bit too general. I don't know. Book smart people, they know a lot of stuff, but they can't use any of it. Street starts people know very little stuff, but they can use everything that they got. We could do a whole fricking podcast on this. Easily the, but knowledge is useless without action. So yeah, you have a bunch of stuff that you can take action on, but the whole point of these things is to be able to decide what action to take. So the biggest problem with a lot of these dashboards is they're, they're putting together a bunch of data. Actually it's more information than data, but putting together a bunch of information and not very clear around upfront, what sorts of actions people want to be able to take. Um, and then, you know, the term, uh, actionable versus vanity metrics, much of these dashboards are nothing but vanity metrics that, that really is there's a hope theory behind it that says, Hey, I hope one day that if I get enough information on this board, somebody smarter than me will be able to triangulate on something actionable. Yeah. Yeah. Where does the line come? Cause I've had this argument about vanity metrics before. Where does the line come between a vanity metric and, uh, often it's terrific or I'm going here. Someone wants to take what I think is clearly a vanity metric and tell me it's something it isn't. For example, okay. Test pass rate. Uh, test pass. How about, how about number of tests run? We ran 2 million tests. Fantastic. So what they want to convey with that is we have great product coverage cause we ran 2 million. How, how expensive we are. But absolutely anyway, means nothing. I don't care whether you've ran one test or 10 million tests. It's not, it doesn't matter to me. I, so I'll take a more practical point of view than that. Oh fine. Be practical. Right. Um, the fact that they ran 2 million tests, uh, likely means something, but they're not making it clear what it means. Right. I find the odds of someone running 2 million tests and that, that isn't something useful, very low, but exactly how useful and what's missing is completely unclear. There's no knowledge there. Right. Absolutely. It's the same thing, uh, with, um, test pass rates. Hey, a hundred percent test passed. If I just communicate that, um, you don't know if that's 2 million test cases or two. Same thing if I tell you 50% of our test pass, but only ran two and one failed because of a product, but it's also the same if you say, uh, Hey, customers are, are hitting, uh, this crash. This, this is the number one call stack in our service. Okay. So what just because it's the number one doesn't necessarily the, the, the general rule thumb at, Hey, you should fix the number one call stack. But what if the number one call stack is only really impacting say 2% of your customer base and only for two minutes, maybe it's not worth the time to fix that call stack. Right. This is how you convert it. The way I try to take these things is try to convert it into something that's sort of an ROI, um, or even a risk to return by, by turning it into a ratio versus just a bullshit number. Uh, you, you increase the odds of it being actionable. Right. And that's exactly what I'm looking for in this view into our product. I want actionable data. And I feel like I have that now, but it's not the complete list of actions. So it doesn't tell me that I have a whole bunch of bugs to go fix here. And there's risk inherent in that that's missing from my view. I, let me, let me get on a soapbox for a second. Weird never happens a first ever. One of the ABs is going to get on a soapbox. Yes. Oh, the soapbox. This was actually my, my, this is a big part of my last, um, what are those things that we write blog posts. Thank you. In, in Microsoft, there's actually two and beginning to be a third sort of. Train of thought of how to produce, uh, these, these, uh, visualizations or tools for, for working with that just occurred to me. Maybe I could be the first to invent the context driven school of data. Um, for those, for those of you not in the room, uh, the look I gave Brent, I'll just say was interesting. I didn't tell him he was number one. No, but I sure felt it. But I did age him a few years. So number one, and this is what you just told me is people just make up numbers and put it on a website and then pet themselves on the back data puke. I love that term. I'm going to use that a lot. Data puke number two is the GQM crowd. Oh yeah. Okay. Where you have a goal and you come up with your questions, then you come up with the don't measure it. If you don't know what question you're going to answer. Yes. Um, I've actually had a chat with one of the, the, the, the people who wrote one of the chapters and I'm like, I think it was well suited for. Through one of the what chapters. And so there's a, I forget the book. We know all of the authors for this book, but, um, chapter seven. Oh yeah. Yeah. Okay. Right. Chapter seven in this book is all about GQM. Right. I actually, um, we know all the authors and I got to meet Vic Basili, the guy who invented GQM once. Oh, I, I thought so. Name drop pal. Alan wins again. Boom goes the Alan might the, I actually think they're missing a step. So I don't think, cause goal is ambiguous. And the way I, I tell people to, to think through it is first come up with your hypothesis and your hypothesis is a statement, not a question. It's a statement of belief. I believe that this scenario will do something. Okay. And that something when, when you're doing it at this level, state that hypothesis in terms of a business KPI that the business values, I believe this will increase engagement or adoption or, uh, customer acquisition, or it will push down, um, uh, D sat things along those lines, tie it to something that's valuable to the business. And then once you have that hypothesis, enumerate the set of actions that you would take assuming that that hypothesis were true or that hypothesis turned, um, proved false. Okay. So you, you state a belief. Then after that you go, if I were able to prove this one way or the other, what would I do enumerate those actions? Once you have those actions, then you go to the questions, but the purpose of the questions is to confidently select the correct action. You now want to ask a set of questions that go, once I had these answers, how do I know which of these actions to take? That is a, uh, it's very similar to GQM, uh, but significant enough in the differences, they should probably write that down sometime. Cause it's good. Yeah. It's as I just mentioned, it's in my last blog post. So if I'm a step on my soap box for a moment, uh, and we alluded to this. I have one more thing by the way. Oh, what? Tell you what I will pause my soap box. So that I can finish your thing. Let me have closure, Alan. Please. Oh yeah. Whatever. Go ahead. Then of course, after you have those questions answered or after you have those questions enumerated, then you go and instrument what you need in order to answer those questions, this allows you to avoid the, the instrumentation bloat that seems to be happening over and over again, because now you know exactly what you're trying to prove. You know, the set of questions. There you go. There's room for a little bit more there. I think the answer is somewhere between only measure what you're only measure, what you have a goal or a question for and measuring everything. The answer is somewhere in between those two. So that's a good point. I suggested it allows you to do that. I don't think that's necessarily a good idea. Cause even like there's a chance that you're not asking fully the right questions, right? Because it, the whole, very good chance. The, the whole point to this, there's this awesome data science term called uncertainty. And when you enter into the data science, just a stupid buzzword. Uh, it certainly has the risk of it. Uh, before, so one of the, where I really fell in love with the term uncertainty when I was reading Doug Hubbard's writing on risk and measurement. Doug is fantastic. So it's Doug, a data scientist. I think in today's terms, he'd be, he'd be viewed as that. I think it's anybody who actually thinks about numbers sensibly is now a data scientist. Anyone who thinks about it and can act on it. Sure. Anyway, go on, go on. And then I still have myself box to get up onto the, although I think he'd probably call himself an applied statistician. Yeah. Um, which data science now incorporates the, um, you totally got me off on the weeds. What the hell was I talking about? I don't know. I'm gonna get up on myself. I don't care. So talking about GQM remind reminded me of something in general and all, I'll, I'll talk about GQM zealots, uh, and abstractly and, and figure out where I go from there. This is not a planned speech. I have no words in front of me, but the people that I know that. Adore GQM are the ones that say, if you don't have a goal for it, you do not measure it. There is no, no middle ground. You were talking about context earlier and boy, sometimes can't you really step back and think that there's another way to do something that's similar and follows your principles, but can give you more value. And I see this damn thing happen in software all the time. It's like, Oh, so-and-so I respect so-and-so. And so-and-so said this, I'm going to do things exactly as they said and see things no other way, no matter no, no matter what anybody else says. And they do this and people will say things that contradict so-and-so and they go, Oh, you must be wrong. Cause you don't believe in what so-and-so is doing until so-and-so changes his mind. They go, Oh my God, he changed his mind. This is genius. And this happens in software all the time. And it happens in society, right? You know what shepherds and their sheeple. It is. And so my advice to the world is don't be a sheep. Think for yourself. If you find that you're thinking and writing, um, it doesn't matter. I have people I follow and admire, uh, people like Doug Hubbard, but I don't take everything he says and treat it like I must behave this way. I, I, ideas come from other ideas and the way I try and provide value to my team and to society is by taking the ideas that I hear, that I hear and read and study and listen and adapt them to whatever it is I'm doing. And so whether it's GQM or whether it's herd and sheep, it doesn't matter. My soapbox leadership tip for the day pulled randomly out 30 minutes into this podcast, maybe 20 after editing is think for yourself. Be your, be your own person, be your own software engineer. All right. I'm off the soapbox trust, but verify GQM kind of kicked that off. It's like, cause I know people who are real zealots about that. And then it made me think of other, other situations in life and in software. And it's like you, it's okay. It's okay to admire people. It's okay to be a leader. It's okay to be a follower, but no matter what you have to think for yourself, there, there are a large number of people who, who really prefer, um, everyone's wired differently. There's a large number of people who really prefer, right. A structured formulaic way to succeed. And particularly in things like data science, where it's really, even though it's been around for decades, right, because of how rapidly it's, it's growing in and really how few people really understand it. Um, it's, I think it's fair to say it's, it's, it's entered into a new stage of being nascent, right? Um, so we're going to find people, some people, uh, data science is absolutely a buzzword. Oh, it's just this magical statistical thing. Like one of the things that I'm, I'm seeing throughout Microsoft right now is that there doesn't seem to be a distinction between data science and machine learning. This is wrong, but, um, people have learned that those two words often go together in the same sentence. Right. Um, on, I remember what I was going to say before, like one of the big secrets about being able to use data and convert it into action is you're really trying to, um, piece apart uncertainty. The uncertainty is basically as, as you've talked about it over and over again, armor's rule is it's basically a method by which you turn the unknown into the known, it's a suitable method for discovering what you don't know. You don't know. Right. And, um, you'll love this cause I'm sure it's just a matter of time before it, it, it, um, it's been around for a very long time, but I don't, it's not a term that's common within Microsoft and I'm imagining it will eventually be EDA exploratory data analysis. Yes. Alan is already from his body language. He's already looking forward to XDA. You know, um, no, actually the thought that went through my head was I was talking to some, uh, uh, colleagues, gosh, almost a year ago now. And we need to turn this around and answer my question too, but we're talking about dashboards and viewing data and, and it was very much a traditional let's look at data the way I've looked at test pass rates and, and it was very prescribed and straightforward. And I asked what I thought was a very simple question that, uh, I was just surprised that given the people in the room, it hadn't been seemingly considered, but I said, what about exploratory analysis of the data? I can't now that words in my head, not sure what I use, but I said, sometimes I just want to sift through data and flip things around and look for insights, how are we going to do that? Plank stare. Well, you should talk to so-and-so and see if that's possible. I'm like, uh, I think it's going to be kind of, and I'm, I am not, nor will I likely ever be or admit to being, or use the word data science to describe myself or what I do. But I'm a generalist and I do know how to use data. Maybe not to the degree of someone like Brent. But, uh, to me, it's just, that's basic. I, I have to be able to do that to make sense of the data, to figure out what it means is how I get knowledge from it. Have you ever heard of the banana experiment? Um, I think I saw a video once. I'm pretty sure it wasn't that. Oh, okay. Yeah. Um, banana experiment in a nutshell, a bunch of scientists put a bunch of monkeys in a, in a, in a cage, a, in the head of ladder and a set of bananas on top of it, um, anytime the, a monkey tried to climb up the ladder to get the bananas, the scientists would spray cold water on all the other monkeys. They would consistently do this until it got to the point that the, the crowd of monkeys would actually kick the ass of the monkey, trying to get the bananas. Okay. Every time another monkey went up, so they, they managed to change the behavior of the monkeys in the cage. Once they had all of the monkeys doing this consistently, they would take out, they would begin to swap monkeys. Okay. So they would bring in a new monkey, take another one out. That new monkey knew nothing of the system would go after the bananas. The crowd would kick his ass. They continued to do this and, uh, swap monkeys in and out until it got to the point that the original set of monkeys were all gone. So there were no previously wet monkeys in the room. And you still got your ass kicked for trying to get the banana. Yes. And not a single monkey in that room had ever gotten sprayed by cold water. The other thing is, I think you do the same thing with some pizza on that ladder and people do the same thing. Yeah. Well, think of it this way. Um, test pass rates, the traditional exit criteria score cards that we use the ship over and over again. People aren't asking, why are we still doing this? I am. Michael Hunter. I'm asking. Michael Hunter, um, uh, did this great analysis and investigation around some of the statistics used in, uh, one of his last teams and what he discovered as he was going through and he interviewed, uh, I think he said somewhere in between 30 and 50 people and what he discovered is not a lot of people knew why we were using this metric and, and the, the belief in the value of the metric was purely belief there was that the everyone sort of, it was sort of like the, the underwear gnomes. It's actually, if we measure it, step one, measure it, step two, we haven't figured that out yet. Step three, this KPI moves. Right. They, they, are you familiar with the underwear gnomes from South Park? No, but that I'm sure I can watch a video on. They're, they have a three step plan. I step one is steal people's underwear. Step two, they haven't figured that out yet. Step three is massive profit. All right. Whatever. All right. Winding back. Like you're not going to have a successful dashboard with risks or without. If you don't know how do you want to use it? Right. The very first thing. And that's exactly where I want to get to. I want, you know, for me, data driven quality is the data guides my work, not just for the next milestone, not for the next sprint, but not only for that week, but for that day. Yeah. And we don't have that now. I feel like, oh no, we do. We have it. We have it like crazy. Well, my team doesn't have that now. You just told me it does. Well, not in one view. I mean, guys, the word in multiple views, just to be a pedantic ask. Right. Go on. Your team does have it. But you want to guide your work. This, this last bit is what's missing. Yeah. A valuable way. Okay. Right. I have no doubt that you have someone in your team. Not your right. Inuberate call stacks. But why do we care? Everyone's like, what do you mean? It's the number one call stack. Of course we got to fix it. Odds are we do got to fix it, but stop and tell me why. What's your proof? Yeah. What I really want to get to, and you're right, because for me, when you think about data driven quality, what I'll be able to do is have an easy way for me to look and again, you're going to have views that are good for the team. Views that let you really dive in deep and get more information views that let you explore, but what I want to look at is here's some information where I quickly know here are the top N number of things we need to get done today. To provide the most value. That's the key insight. So it isn't just the data data training that I've done, but I've been in services and I've been in agile and right. And one of the biggest problems is we have a limited number of. Is it agile? Just another stupid buzzword. Shut up. Um, and yes, but shut up. The, every team has a limited number of resources, whether it be people, equipment, or time. Right. So what we're really trying to do is, is use our insight validated by the data to be able to in a nutshell, enumerate a backlog of work that's ordered by the highest ROI. Isn't ROI just a stupid buzzword? Okay. Yeah. You're, you're, uh, all right. Is there something more actionable or tactical you think I can help you with on that one? I, I, it's good to talk through it. And it's one of those things where I, you gave me the answer that. Uh, there's probably a name for this sort of thing where I wanted to look for some, what I wanted from you was like some great insight and says you absolutely have to do this. But instead you told me really what I already knew, which is good. It reiterated like, yeah, you're right. I need, this is exactly what I need to do. It doesn't matter how it looks. Uh, but yeah, there are suggestions I can give you, but it's, it is very context sensitive. Absolutely. And as you just got through saying, I don't want to tell you a bunch of things, um, and we, we, we can go talk about it offline, but I don't want to tell you a bunch of things and, and have you or our listeners then view the, Oh, we had this fantastic formulaic equation. We'll just follow it. And in hindsight, you don't want that. And you see that another thing I see often, especially, well, actually both inside and outside of Microsoft is, uh, people want like the solution. But what they really need is how to think about the solution. And we kind of, even though I was, I fell into the same trap, I was looking for, give me an example of what a solution would look like. But instead we talked about how I should think about what that looks like. And that's in the end, more valuable. The solution is in the system and your, your system is likely different than anyone else's or any team you've been on. Hey, can I go off on a random topic, not on the board? Sure. Sure. Why not? Because that never happens. No, why would we do that here? So I ran into one of our colleagues, Steve Rowe. Oh, I think it was last week. He's a listener. I'm a big fan of Steve Rowe. He is. I'm a big fan of every one of our listeners, but, uh, I'm actually happy we didn't get there today. This is going to be a segue here eventually, but he, uh, said something to the effective and I'm sorry for paraphrasing, Steve, that, uh, that our podcast was sort of for Microsoft people, because we do talk a little bit or actually a lot about sort of Microsofty things, but I think a lot of these things are sort of valuable outside of Microsoft, don't you think? It was interesting. So I know in a way I knew he was right, but I wanted to talk about it anyway. We've frame it from, from our experience. We frame it from a Microsoft way. How many years you've been at Microsoft? Uh, 20 now. So about about 40 between us. So we, I think that's, that's kind of where our context comes from. But, um, and, and it certainly is a part of our podcast is discussing sort of the, the, the, the trials and tribulations we are having and trying to, to introduce, um, uh, an injection of the outside culture in, right? Uh, and we have a belief and it's been validated by, by tweets and, and emails and things like that, that other people are kind of going through these same sort of problems. If we look at a customer, I know, cause we, we talked to it, but if we look at our listener base, I'm fairly certain that it's a head tail curve and that most of them are the, the top of the, of the, the head is Microsoft guys. But I actually think we have more listeners outside of Microsoft. That sounds right to me. Uh, so a couple of things I wanted to mention, I actually forgot one of them. It'll come back to me in a moment. Uh, oh, I read an article and I'm not going to go into details. Is it mad? Cause it's one of many, or at least a handful of sort of things I've seen talking about what software engineering or testing is like at Microsoft. And they're all just, it's weird. I read them and I go, well, this isn't right at all. This is sort of skewed. And then unfortunately I see like in the footnotes, they've, their references are like my blog. I go, oh man, I got to be more clear because obviously what I wrote is open for interpretation. Uh, I should probably, uh, respond to some of those in blog form sometime, but it's weird. Uh, it's also the interpretation. It also has a half life, right? A big portion of your book. And I think you blogged on that. Like when did you write your book? Uh, I started at like, well, I started at nine years ago, like 10 years ago, or I wrote the first chapter, uh, about just over nine years ago. When was it available for purchase? Uh, just over about six, just over six years ago. Okay. How much of it is still true today? Um, half, half, maybe half. So it goes from, um, untrue to non-relevant to flat out wrong. And that's, that's probably half to three quarters of the book. Some parts are like some parts I'm still hold up model based testing chapter, I think still holds up. Yep. I'm very happy with that chapter. What percent would you say is not relevant versus note flat out wrong? Um, gosh, at least a quarter of it is non relevant. Probably especially stuff around, you know, how engineering works. The first couple of chapters. Um, there's depends how you look at it. And we've, I don't want to rehash this because I have other stuff to talk about, but some of it's just a lot of the activities I talk about in the book are now done by, um, the development teams, mostly done, but it's still done or not done, but there's the, I don't know. Open source has gotten hugely dominant. So a lot of how to think through the test architecture framework is, I don't know how to break it down. I, there's a, I wrote a blog post kind of walking through the section by section. So I could go look at that and give you a better number, but it's interesting. Yeah. There's a half life. Some of it's quite relevant and I would use today and the half life is changing. Like, yeah, it's just as a, as a, as a, as a thought exercise. If you were to, if you were to, um, pretend like you wrote it or that it got released 16 years ago and it then advanced six, six years, what do you think the relevancy would have been at that new six year point? Oh, it's probably, it's a convoluted way of you asking or implying that things have changed a lot more in the last six years and they didn't 15 years before then is particularly in the test. Yeah. And that's absolutely true. If I wrote it 16 years ago, six years after that, it still would have been highly relevant, applicable. It's sort of a Moore's law of testing. What I think I need to do maybe to get test and software to kick it into gear again and make more massive changes is read another book. Sure. Sure. That's a, the data scientist, air quote in Brent just said, that's not a valid indicator of change, but correlation with that. I just skipped over. I just skipped over to I, one of the things that I think is really cool about you as you've written a number, number of books, I just don't have the patience for that. Like one, one difference between Alan and I, and we may have talked about it before we've, we shared notes on how long it takes us to write a blog. Alan, he and I could write a blog on a similar topic. It will take Alan about a half hour. It will take me about five hours. Right. It, I'm like, okay. So Alan wrote a book, took him about a year to complete it. It would take me about five years using that same metric. Therefore, by the time I was done, it's completely irrelevant. This is kind of like the, like painting the Golden Gate bridge by the time you're done painting it, you have to start over again. Yeah. Something like that. Hey, the reason I brought up this subject was not to yet again. Now, now that everybody's turned off the podcast, rehash, uh, how we can software at Microsoft, but no, it was all about how awesome is Alan? No, um, that wasn't it either. But I, I talked about this a few other times and I think there's a new segment I'd like to bring to the show. Yeah. And I don't quite have the, I'm not sure what the reverb or effect going into it is, but we've talked about, well, I think you'll know what I'm talking. If you're a long time listener, I'll just call this segment dumb things people do at Microsoft and we've talked about some of them before. I won't rehash. You can dig them out of the old podcast, but the one I haven't mentioned yet. So it's sort of an agile, an agile rewrite of Microsoft. Well, I'm working at, I'm just again, Copeland, if you're listening, not Patrick, Doug, the other wrote Microsoft. I'm ideas, the color for your, for your next book on Microsoft. Um, which the other one's now over 20 years old. So do you have a new dumb thing we do? I do. And, and really it's just things that pissed me off. It could be called that, but that's not as important. So, um, I'm in this organization. My team is part of a larger organization and there's a very large distribution list, email list, a couple thousand people, and it's relevant information about, uh, our overall product, uh, discussions around, you know, things that aren't working, et cetera, et cetera. It is a huge list, lots of traffic. I have a folder, thank goodness, where all these things go and I can browse them and condense them and look at the things they think are important in order to gain knowledge, uh, to help me maybe even do my job. Do all of your daily automated status reports go to that alias? No, I don't even know what you just said. I just tuned out partway through the multisyllabic words. The point is, I don't know where this happened to Microsoft culture, but at some point, someone got the idea that, Hey, this thread is no longer relevant to everyone on the thread. So I am going to block BCC, the thread to cut down the recipient list. So the first line of the mail is usually BCC to remove large alias. Well, thank you, you stupid little, you just broke my rules and put that into a folder that I actually read and care about. I don't care about your damn mail. Why do you think it's cool or relevant or interesting at all? Why do you think that you're being clever or smart or anything other than a stupid dip by doing that? They actually, they're there. It's, it's interesting. No, it's not interesting. I've never actually stupid. And if one, every time it happens, I want to reach through my ethernet cable and punch the person in the throat. I have long since given up making rules actually work because I've been, I'm an inbox zero kind of guy. If my, I keep, if I, my inbox is more than like 10 items, I'm uncomfortable, I'm freaking out. So then the, and I have, so I have, wait, my rules work as I have, if I'm on the tour, CC line, or if it's from my boss, it goes to my inbox. Okay. Or my wife goes to my inbox. Good call. Everything else has rules. And then there's, I have this inbox other for things that go to, I don't have rules for, so like my pry two mail folder, pry one is my inbox. I have my inbox other pry two. Um, so I look at that throughout the day. I don't, I don't look at my folders. Um, there may be on the weekends or at night, or if I'm really bored, I'll go look at the things that have been filed or if I, or if I need to dig up information, but I look at my inbox regularly and my, in my other inbox, my pry two inbox, uh, several times throughout the day. So if you break my rules, you are polluting my pry two inbox and bastards, I have a horse. And again, it's probably somebody who's listening. It's going, who the hell cares? Or saying, saying things like, why do you use email? Why can't you communicate in other ways? Because we're backward and we only know how to communicate via email. That's why that. Yammer's starting to pick up. It is. I wish it were. It isn't in, in my world. Well, in my world, you know, we had a group say, we're going to have a Yammer group and their answer after being unable to actually monitor the group because they forgot about it was to decide to move the group back to a distribution list. No, that's because a bunch of people, I don't want to change. I'm afraid of change. That's what I'm talking about. That's what I'm talking about. I actually, so the, the Yammer group we did in my last group, um, I got to the point where I would say, Hey guys, I now I figured out why Yammer has a particular policy. I thought it was just to dog food. No, there's a wisdom to it. So from here on out, anything on this, on this topic, just want to let you know. I am a big believer and I want to be able to use this and help others come by and self-service. So I'm not going to answer anything on this topic unless it's posted to the Yammer group. I told them what my behavior was going to be and I'm going to told them why and because I told them why they were like, okay, fine. Well, Brent, you are, I would consider you an embracer of change. I do. A bulk of the people I work with are so afraid of change that they would rather shove a rusty nail under their fingernail than change. But once I did that, that became the common behavior. So it only took, I only had to reinforce that for about three months. And then it became the new muscle. Did you spray cold water on them? They didn't go to the, you'll go to Yammer. I did the equivalent. Yes. I acknowledged that I am a huge knowledge bottleneck, uh, for your forward progress right now, and I am happy to unblock it, but you have to do this first. All right. Hey, Brett. Yeah, we're out of time. We are. So thanks for slogging through another episode. We're on number 20 next time. I'm excited for that. In number 20 and it will for sure be one year. All right. Uh, I'll bring, I'm not bringing cake. I may bring shots. Okay. All right. Hey everybody. Uh, I'm Alan and I'm Brent. Thanks for listening. 
