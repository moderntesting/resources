Good morning, everyone. Hello. Morning. Hey, there's someone else in the room. There is fire. Let's see what happens. I'm Alan. I'm Brent. And I'm Steve. Welcome to ABS testing. Hey, ABS. We're plastic. Abs. Abs testing. We will improve your abs. Steve Rowe is back for a return engagement on the AB testing podcast. You last heard him on episode 46. Is that right? That is. Thank you very much for bringing me back. He has it framed on his wall, the digital bits, some ones and zeros framed in a gold frame. It's a big wallpaper. It takes the whole wall. Yeah, he bought that himself. Actually, that's kind of a geeky, cool idea. I have a welcome mat with Tangentime. I have a welcome mat with like welcome written in binary. But what if you just, you could make some wallpaper. It could say you could put a book and just decode it to ones and zeros. And have that be your wallpaper for your hex codes. Yeah. Now, then you could sit in there and you could get a table in the middle and you could do on that table. You could build the new $800 Millennium Falcon from Lego. There you go. It's pretty impressive. I saw it at the mall. We were talking about the the Lego Millennium Falcon before we recorded. So I had to bring it up. So I wonder what book would you pick and how big of a room would you need to put the whole book? These are things that I haven't thought through yet. It could just be with binary. It could just be a short passage. You know what? It could even be a digital representation of my favorite picture. It could be Lorem Ipsum. Yeah, that's what it should be. That way you can mass produce it. We have to be custom. We're just making nice Lorem Ipsum generated into binary wallpaper ones and zeros. Someone's going to take that idea and make a lot of money and I'm going to be mad. Custom binary wallpaper. Isn't that like the matrix screensaver in wallpaper form? Oh yeah. Okay, so we have, welcome Steve. We have an agenda, right? Theoretically. Theoretically we have some stuff written on the whiteboard to get through. Brent, why is Steve here? He's our favorite guest ever. He is. Yay me. That's true. That is true. He is our favorite guest ever. So we'll give him that. So I think there are some other reasons. I'll see if I can guess since you're unwilling to take the lead from me. I think Steve knows a lot about applying data to quality and testing. And I also think that Steve just gave a talk on a similar subject at Star Canada. Yeah, one of the themes that we've been talking about a lot lately is more pushing the test industry to get more and more active with their data. Move away slightly or begin to move away from this prevention model and using data to react and drive towards better customer quality. So Steve had did a talk in Star Canada. When was it? Last week? About two weeks ago. Blame Canada. In Canada. I am doing a similar talk for this unknown little company in Bellevue on Tuesday. So Brent's going to this tangent time because Brent's all excited that he's on the guest list. My entire team from around the world is descending upon Bellevue. First people arriving amidst a flurry of snow today, arriving through the weekend. We're going to spend Monday through Thursday doing a lot of work together. We have a few external speakers coming in and most of my leads dried up. So I had Brent come in. Wow, scraping the bottom of that barrel. And Brent's going to talk about applying data to quality. And by the end of this podcast, he may be fired and Steve may be coming in, but we'll just kind of see how it goes. Just let me know. Anyway. Yeah. My manager had a meeting. He wanted to schedule on top of that and I'm like, no, can't do that. Maybe I can do that meeting after all. All right. Steve knows about data quality. Right? Were you going to set him up to ever talk or is he just going to sit there and say, yep, and laugh at me picking on you? I'm like a little sidekick on the talk shows. I just laugh a little bit. Throw some jokes out. Yes. Steve is actually wearing a Robin costume right now. It's a little awkward, but it's adequate. It's actually no, it's actually for a game called hand of the gods. So yeah, which is my new addiction. So so Steve and I chatted briefly. I thought that this was an interesting adjunct to what we've been talking about here. So Steve, why don't you describe number one, what you went to start candidate to to present. And then be interested to hear sort of what your take of Star Canada in terms of. We've been talking here a lot lately around this idea of a modern test team versus a traditional one. Love to hear your insights around what you observed along those topics at Star. Sure. So I came to start a talk about using data in in testing. Like actually a lot of the content that was that I presented was stuff we talked about back in episode 46. If I actually went back and listened to that to remind myself of some of the good ideas that I'd forgotten since then. So I talked about. So are you saying we've talked about this before? I am saying we've talked about this before. Just checking. Drink. Yeah, exactly. So I went to talk about how to use data in your testing and I kind of had to gauge. It was interesting. I didn't know what audience exactly I was going to have to at the conference. So I wrote up a set of slides and then throughout the conference as I saw other presenters and talk to people I tweaked my slides to kind of aim at the right audience. I didn't know. By the way, I love that technique. I use that a lot. It works pretty well. So they ask you for your slides like months in advance, but you present your own copy so you can change them. Actually, something I learned from our buddy James Whitaker is always change your slides. Turn in your slides because they want them in advance, but change them. And then you can say, oh, if you want to copy these latest slides, you can download them from my website that drives traffic to your website. Does it actually drive traffic to your website? If you're James Whitaker, it does. Oh, okay. I don't think I got much traffic. I said you can ask me on Twitter and I don't think anybody ever did. But anyway, so I was talking about using data in testing. So I talked through kind of the history of testing, how we kind of went from manual testing to kind of more automated testing. And then I'm moving to this more data-driven quality or what you guys are calling modern testing. I talked to that. I talked through ways to bootstrap your area into using data. You know, use the data that's already in your log files, then add some instrumentation, et cetera. So I talked through kind of all that and how you get to the point where you can start using data. Because a lot of these people come from a background of very traditional, very manual testing. And so they're not even at automated testing a lot of times. And so it's quite a leap forward. So I was kind of trying to introduce that topic to them and say, hey, this is coming. You know, you should be familiar with this thing. Here's kind of what's going on. The conference itself, there was an interesting kind of tension that seemed to be going on at the conference. The big word in the air was DevOps, which doesn't mean quite what DevOps always means. But it seemed to be more just a bunch of automation. And so there was definitely people like I saw a talk, a very good talk actually, on behavior-driven development, which was really more unified engineering. And why getting your dev team and your test team together and having them start at the same time is a good plan. And if you're in a box software prevention world, it's a great model. Obviously, we think that we're moving beyond that model now. And that's insufficient for most purposes now. But I saw some of that talk. But I also saw the whole, ML is not coming to eat you talk. And I saw a discussion about how none of you are like Netflix and none of you are like Amazon. And so you can't use any of these techniques because you're all special snowflakes and have to do all your manual testing. You know, and so there's this kind of, it was interesting. Wait, wait, wait, wait. Yep. Can you add some clarity around like the themes you picked up or the argument around how ML is not coming to eat you? I can't actually. So the talk that I saw there was the conclusion was literally here's an introduction to ML. And then the conclusion was I talked to my friend who's really good at this stuff. And he said it's not eat, it's not going to come. That was literally his argument. I was, I was boggled, but that was literally the argument. Very, very statistical. So there is an expert who says it's not coming. That was, that was the conclusion of the talk. He didn't really quite describe exactly what ML was going to eat as far as testing. Like, I think it's going to impact testing, but I don't think we're going to get rid of all testing because ML can just, you just pointed ML algorithm at your app and it comes back and says green or red. That would seem to violate the halting principle. So there are, and I'm going to jump in because there is so much confusion mainly among testers. I'll say in the industry to be nice around some around ML. I get everything from, uh, like what you said, it's not going to cut. Nothing's going to happen. I could ignore it. It's fine to thinking it's magic. I can just take my tests. I drop them into this ML app. I guess I say with eyes tweaked and rolled and then the ML app spits out everything I need to know everything about my product is high quality magic happens. And this, so I've heard both sides of that and DevOps is just crazy. I've heard it's not that difficult of a concept to understand, but I guess it's been, the word's been used in enough context that nobody knows what it means anymore. Right. Well, I think there's, there's a, there's a spectrum as far as I can tell, reading the literature, which is not respected by a lot of people talking. So there's a spectrum of operations, right? How do I maintain my service? Right. So there's what they traditionally call service ops, which is I have a bunch of people who are not generally programmers who run around and try with scripting and manual work to keep the service running. Right. I saw a post recently, um, from an unnamed tester talking about how the DevOps basically paraphrased the DevOps concept was stupid because it just meant that dev and operations work together and it should be called dev test PM ops or something. Cause everyone works together. Yeah. Cause that really rolls off the tongue. Yeah. It's not quite the point that's that happens. Yeah. There's just shut up. In my reading DevOps seems to be more like what I perceive as the Amazon model, which is you don't have much of an ops team. You have a dev team and the dev team is on pager duty all the time to keep your stuff running. And I go even farther going into the world of rent and I, and I would say it's a world that favors specializing generals and generalizing specialists. Well, the better people that can do the operation stuff as well as the dev, as well as the QA and the test, whatever you want to call it. I think the issue, like I think I find this dev ops thing, uh, interesting because in the last few months I've seen a high spike in test leaders sort of communicating or implying that that's the direction STEs are going. I think it's, I think people are caught up in the buzzword. People are. And that's, that was, that's my point. Like, uh, I think, I think you're right. Data science, uh, still today in a very high degree is somewhat magical in terms of people's understanding. And I think DevOps is sort of the same sort of thing. I think it's both of these terms have been blown up to some concept that's not real in either case. Right. Well, we should finish the spectrum, right? There's the center of the spectrum, which is what I believe Google uses, uh, which is called SRE or service reliability engineering, which is basically you take a dev and you put them into the ops team and you tell them how you have to spend half your time making the system better. In some ways, it's just like the old world. You had your devs, which is your DevOps. You have your STEs, uh, or your software test engineers, your manual testers, um, which are kind of your ops, your service ops. And then you got SREs, which are kind of akin to your S debts. They're, you know, coding versions of the service ops. Yeah. The, the thing I think about STEs moving into DevOps, to me, that kind of in some aspects makes sense. They're not actually DevOps, but I see a lot of executives saying, Hey, I want to have one of these things too. And what we are observing, I think is an executive point of view that essentially STEs are nothing but manual monkeys. And now we, we no longer need manual monkeys in the preventative world, but we do need them in the service maintenance world. And I think that's not a terrible thought to go about, right? The, the difficulty is this, I think that you, and I noticed this tension at star, you have a certain number of people that come from kind of a dev background, right? Those are your traditional S debts or even devs that are, that are doing some testing. And then you have people that come from the traditional STE manual testing background. And though some of those people for whatever reason, because they, they find it difficult or because they choose not to have no sense of how to program, right? They have no sense of how to, how to do a dev skill. And so they're kind of stuck in a world where they want to find another world that also requires them to have that same, you know, kind of skill set. And without learning data science, which is a huge leap forward, or learning to program Python, which is again, a very difficult leap forward for, to do it competently for, for a certain group of people, they need to find a world to live. And so part of star Canada was DevOps is the place you can go. And part of star Canada was it's okay. ML and, you know, even test automation are not the right solution, right? The people that are, there was literally a statement that said, you know, all thinking happens in testing and not in automated testing. People who do automated testing aren't thinking was the implication. Yeah, that that's Alan's favorite checking versus testing. Stupid semantic arguments that mean nothing except for the people that invented them. So that was out loud. I think there is actually an interesting thing that happens here because it's kind of a semantic argument, but it's not right. And so I was telling Brent about this, there's a, there's a logical fallacy called the mot and Bailey fallacy. So in Europe, they used to have a castle on a hill, right? And that was called a mot, M O T T E, but it's just a castle on the hill. And then around it, you had a much larger area that was fenced off. And that's where everybody lived. And when the enemies came, you know, when the Germans or whatever came to invade your area, you ran back to the castle and you defended your castle. And then when you finally fought them off, you went back to your homes in the wider area. This is a logical fallacy that happens a lot where somebody makes a claim to a large area. And when you attack it, they fall back to a very small area and then assume that they've defended the large area by defending the small area. And so I had a conversation with one particular gentleman who was attacking the concept of automated testing, using this checking versus testing, like check automated testing is only checking and nobody smart ever does that. And so I started challenging him. He's like, Oh, no, no, test tools are fine to use in the appropriate place. Very small defensive situation. But then as soon as you let him go, he was back to like all automated testing is a bunch of idiots. Well, the world is changing. I think people like I think a lot of people and we've not to digress even farther, but a lot of testers, they don't want the world to change. They're digging in, they're afraid of change. Star and other conferences are good for that. They can see what kind of changes may happen. But it's unfortunate they see both. Like here's some changes that could happen. And the best people saying no, no, it's okay. Keep on doing what you're doing. I think star would be better suited. And there was a lot of speakers here that were this way. But I think more speakers should be here's how to move to the new world, as opposed to here's a defense of your old world. It feels to me here's how to dig in and stay right where you are because there's no need to move. You brought up the idea of specializing generalists and generalizing specialists. I did just a few moments ago. And I actually think this type of stress is cause when you have built your career around specialization and then that tombstones. I agree. And the point I would make is you go to these conferences, I think if what you take away is all I need to do is keep on doing what I'm doing because nothing else needs to change. You've blown your opportunity of the conference. Your success anywhere in software, anywhere in knowledge work requires that you continue to acquire new skills and learn new things. It doesn't have to be, I learned how to write in Python, but it does mean like, Oh, I know how to make my testing a lot more efficient by creating a teeny little shell script. It sets up a Docker container with our server on it. So I can do some quick testing much faster than I could before. For example, you need to, there's a hundred examples like that. You need to be able to press F 12 in your browser on your web app and look at the traffic going on. And even if you don't understand everything, understand like, Oh, that's probably bad. I should ask someone about that. You need, you can't just do your exploratory human engaged. Don't call it manual testing because it's, it kills a kitten. Yes. Every time you use the word manual testing, a kitten dies. I've been told that manual testing, manual testing, manual testing, dog guy. So yeah. Uh, down from my soapbox, blood pressure calming to use a logical fallacy. I mean, I can't, like the Martin Bailey is okay. The Martin Bailey, right? The, that sounds like a brand of stout. It totally does. It totally does. Or, uh, yeah. Uh, a Scotch beer. The thing I've, I've heard just over and over again, is that sort of fallacy around the discussion of whether or not test is dead or dying. Right. Um, I, sure. So, so because we can find one example and I would say the medical industry is the most common, commonly used one in, in defense. Hey, if it's essentially, if it can be proven that at least one test job in the entire world is not under threat, therefore all of testing is not under threat. That's I think the Martin Bailey argument and it's wrong. Yeah. That very much is right. Like, I think that all of testing is good. That's your Bailey. That's your large set. All manual testing is good and you attack it. You're like, well, here's what it doesn't need it. Yeah, but it's really needed for the medical industry. Okay. That's true. It's needed for the medical industry. Therefore it's needed everywhere. And that's where the logical fallacy comes in. You've only defended a small piece of turf and you're making a claim that you've defended a large one. And then usually my, my counter argument to this is I, I point people, all right, let's go to monster.com and look up cold ball jobs. Hey, looks like they're still around. So we have what? Multiple millions. What would be our guest best estimate of, of manual testers in the world right now? I don't know. Let's say 42, 42 million. It doesn't matter. It's not a million. I definitely met more than 42 at, at Star Canada. So I think, I think you're off by a little bit. So, so let's just say a million. I have no idea what that would be though. It could be an interesting data question. 20, 30 years ago, about the same cold ball programmers. Now go to monster.com and look how many of them are today. Yeah. They still exist. Does that mean cold ball is a thriving business? I've been studying it at my spare time. I think it's the future. Do you? Yeah. Really? Natural language programming. I'm not sure I like the metaphor. Um, but no, I just feel like a lot of the, the fallacies come from a, this bias. This is, I, I like what I do. I don't want to change what I do. So I'm going to look for it's confirmation bias. I'm going to look for things that support the fact, anything that support the fact that I don't want to change. I don't need to change. Right. And that's, this is natural, right? This goes back a long time in history, right? We have, when I impose a fallacy while I'm having another fallacy, is that fallacy inception? It could be. Okay. Just check it. Anyway, go on. That's going to a really weird spot. Um, so anyway, I mean, we have this, this story back in, I don't remember like the 1500s or whatever, when they started coming up with automated looms, right? In order to do weaving and everybody who had done weaving before was threatened by these automated looms. And so they started in some towns in London or in England, they started smashing the looms, um, in order to defend them because they were, they were bad for the human psyche and you know, and all that kind of stuff. Right. And today we, the guy who ran that was named Ludd and we call them Luddites, right? And that's kind of not been generalized to anybody who's against technology. But really it's the same thing going on. Somebody's trying to defend their, you know, their livelihood and make sure that it doesn't change, but you can't really stand in the way of the freight train of the future coming. No. And, uh, maybe five years ago, six years ago, somewhere back, there's a blog post on this. I did. Uh, I was driving into work and there was a news article that Amazon was going to start use some robots to automate a lot of their, uh, pick selection for shipping things and eliminate some human jobs. And people were calling in just irate. They said, this is ridiculous. Taken away human jobs. You know, this is, and I thought this is the same argument. This is stuff that is brain dead, boring, easy to automate. Let's automate it. Let's use humans to actually think about solving harder problems. When didn't Amazon just announced they have something like 500,000 employees. So clearly it didn't cost that many jobs. I saw an advertisement around it being pick season now. And, uh, they, Amazon's doing ads on TV saying, Hey, it's pick season. We want to hire a whole bunch of folks. They still need season because the robots can't keep up. It's not doesn't make sense to invest in enough robots to, uh, optimize for peak season for pick season for holidays. Peak peak. I think you're right. I think it's very similar in the world of, of testing, right? Where there are things like Brent talked about medical devices and probably planes and some other things where, um, you know, the, the, the economics or the situation require that you go and do this kind of stuff. I don't think manual testing is ever going to go away, but it's going to become more and more limited in where it is necessary to apply. I agree. And then, and then to the people, um, who have neophobia, which is the fear of new allows the fear of, uh, Keanu Reeves. I have that. Yeah. Different thing. Um, those who have neophobia, like even in that world, when it shrinks and, and yeah, it's sure there's going to be a world where manual testing specialists still have jobs, but it's going to be the best of the best who have them. So I look at my team, Alan talks about working in unity. So I have 40 people ish on my team. Uh, I, let me count in my head. I have one, two, just one, Brent, just one, one, two, three, four, five. I think five are air quote STEs, meaning they don't actually, they all know how to code a little bit into a lot. Uh, one of them is a game developer. The value they, they are the best of the best, the value they, they're all fantastic. Well, I respect the other people making, not just like pushing, like, I don't want to equate the manual tester to, or the STE to pushing buttons, but they, they're, they most of them function like little mini test managers for me. They're making sure the whole product was put together, right? They're doing, they're doing all of the organizational work for their feature areas. Pretty much, uh, they're keeping their features and products running. It's a, it's a, it's a not an entry level rule by any means. Well, I think that there's, there's a difference between what that kind of, uh, manual tester does exploratory kind of manual tester and what most manual testers, most of Brent's million, actually ended up doing like most of the million manual testers end up doing the same thing every day over and over again. And it really, it's not a whole lot different than pushing a button and, you know, getting a piece of candy out of the bottom of it. Right. I mean, you're, some people call them monkey testers and I don't mean to demean it, but I think that sometimes you get put in that position where, where that's what you do. I was in a position at one point in time where I spent, you know, all day running the same test that I ran the day before, which are the same tests we ran the day before, just to make sure that the newest version of, at this point, windows 98 didn't break. So let me dive deeper into, I get up on a soapbox again, because it's fun is a big part of their job is managing not in traditional sense. I'll use a different word, uh, dealing, communicating with people are on people around the team, around the company to help unity product overall. You're talking about your, your five SCs. Yeah. Yeah. They do a lot of communication, talking developers, talking to their testers, coordinating. Uh, we don't have very many, uh, what are our PMs called product managers? I was joking with the interview Canada this week. I don't know. I'm just going to call you a PM because I don't know whether you're a product manager, a project manager or program manager. I don't even know the difference in some cases, but we don't have very many product managers. So these SDEs do a lot of that coordination, making sure the right things are happened, making sure the right people are thinking about things. And the reason I bring that up is I do hear again, some of the digging in my heels, uh, test is not changing. Don't worry about it. Don't worry. You worry your pretty little heart. Uh, people, I hear them say comments like, uh, they're very much in this world of, see how you can describe this without sounding too stupid testing as information provider, which we've raked on before. They say, okay, you fat is a problem in your system. That's all you have to do is report that it's at the management now to figure out what to do. One thing, and this is even true at Microsoft, don't, don't pick on you too much, but if you defer every little gripe you have to management to solve the problem for you, like mom and dad, they're not playing right. You have to play together nicely. It's my order. They just be friends. Yeah. Uh, this is one of the places where, I mean, in that particular one, this is one of the places where I firmly stand behind ML is going to eat that job. Right. MML stats work. It doesn't take a lot of effort to automate information providing in that regard. Right. Sure. The, there's going to be, there's going to be some challenge around context sensitivity, but you have the right team coming and put things in place. The PM himself can, can put that to add that additional context. Do not need a full time man or a monkey to do it. Yeah. And I don't think it's actually the ML is solving the problem for you. I think it's that we can build tooling that allows you to self-service much better, right? Once upon a time, you know, every executive in America had a secretary whose job it was to type for them nowadays. Everybody who's even CEOs know how to type, right? Like we've made that job obsolete because we taught it to the, you know, to the other person. I think to some extent there's a job of go get me the data that I need or go find out how the situation is. And the more we can build tools to expose that better, the more somebody in the PM job or the more somebody in the C, you know, the manager job can just go find for themselves what that is. And they don't have to have somebody to go interpret for them. I think there are a lot of unknown unknowns in this, in this growth path where if you don't know anything about how the program works or how to write code, you may not realize that this boring thing you do every day could be easily automated. Where we've grown out of that world a little bit. And now we're into with this new world of data and machine learning. Often we don't know, Alice is really hard to figure out. I don't know how I'm, I'll just, it's just hard. I can't do it, but we don't know. They don't know, or we don't know that, Oh, actually this is a problem that can be solved given the right skillset and tools. So I think we're growing into the world where we need to, or a lot of us need to figure out like when we have these hard problems, there's actually a way to solve them where we might think there isn't a way to solve them. And the thing that I think the rest of us have to be sensitive to is that this does, when we make these easier to solve, we do displace people, right? There's a different kind of job that takes a different kind of skillset. And so some people that were not well suited before now become well suited. And other ones that were well suited to that particular skillset are less well-suited to the new one. Which is another reason to be the specializing generals. Cause as part of my job becomes obsolete, it doesn't matter because I have so many other things I can do when you can only do one thing and that one thing becomes not necessary anymore, you're screwed. But you did that to yourself. Right. And that's why I think conferences like Star and other large test conferences have a very big opportunity, which is to help people cross that chasm, right? Help them get the new skillset. And there were a lot of talks that were attempting to go do that, but there was other talks, even some of the keynotes that seemed to be more like, the sky's not falling, everything's okay, everybody on the other side are idiots. You know, like there's no thinking involved in automated testing. And so you don't have to move forward. And what they don't realize is that if you go back to kind of the history of testing that I gave at the beginning, and that we gave a lot of detail when we talked about it before, in quotes, I really drink. When you go back to that, there's kind of a progression, right? You start at manual testing and then you move to automated testing and then you move to, you know, potentially test writing themselves. But then you move on to some kind of ML statistical based stuff that tells you your quality. And they're kind of fighting the war between the first and the second, not realizing the world's even moved beyond the second one. Like automated testing is not the thing that I think is the solution for most people's problems today. I don't disagree with that. Yeah. In fact, that's actually an interesting insight though. That's what I was thinking through. But let me, I'm going to interrupt because that's, that plays right into something I've told my team before. And I mentioned here before is I said, I want you, the people in QA roles on my team, I'm going to change their titles. I haven't told my team that I don't follow that out or not, but I've asked them to write fewer tests. Please stop writing so many tests, development team help them write more tests, more unit tests, more integration tests. There's just so many other things I want to do other than write any tests. Yeah. You're basically saying, Hey, look guys, very little value proposition in you doing this work for the dev team now. Everybody knows there's more work to do than they can do as far as choosing what to do, deprioritize writing tests. And that doesn't mean that we're saying that tests are bad, right? Right. Exactly. Checks are bad. What we're saying. Shut up. But what we're saying is that automated tests are more efficiently written by the person writing the code. Yes. Yeah. No, but the insight I'm still puzzling through that I think Steve is absolutely right on. Right. I think it's very literally what you're saying is that Star Canada, you observed traditionalists fighting against the modernist testing. Modern guys are going, what the hell are you talking about? You're holding on to 1996 principles and ideals. We've moved two decades beyond that. We don't care about those problems anymore. Whereas the traditional are still going, what are you talking about? Medical still does. Well, we have a bit of a problem because our audience, the three people that listen to this podcast all get that they all know that they need to be learning new things and trying new ideas is how do we reach, how does the industry reach the other people? And conferences should be a great way to do that. But I think you get out what you put into the conference and you have to go into the conference with the idea that you will take away new ideas you want to try. If all you take away is justification that you don't need to change, that's a frightening waste of time and scares me. Well, I think it's difficult for a conference designer, right? Somebody's setting up a conference to figure out what the right balance is. Because if you tell your audience that this job is going away and this conference is designed around a particular job and now there's only going to be a handful of you and so we're going to have one conference of 50 experts once a year, that kind of makes your conference go away. And other people have conferences in other areas. So it can be a little difficult to figure out what the right balance is. Your audience kind of wants to hear their jobs okay. So there's a sense of I want to reaffirm what your current beliefs are. And there's also a sense of I need to challenge. And so if you challenge too much, you'll just cause them to reject you and go to the other place where they're being given defense. If you're a star and you say, all right, we're going to tell you all that you need to change immediately, this is all wrong, people are going to stop going to your conference, they're going to go to whatever the other big testing conferences are. So it's interesting. In retrospect, I see this trend ever since however long it's been. Six years ago when Whitaker said test is dead, is that there's always one talk, one keynote at a conference talking about test is alive and well. And so it's interesting that I think that split might have started then where it's about here's all the way it's changing and not going to be what it used to be. And the half talks going, no, no, no, it's still going to always be the same as it used to be. This is what we need. Have you seen this obscure bug I found in my flight tracking app where it gives me an error message that doesn't make as much sense as it should. This is why we need software testers. Right now, I think that the conversation is going on in the large, right? Because I saw a lot of people at this conference that were responding back to whatever the last star conference was. I don't know if it's star west or star east. West is in the fall. So west must have just happened. So it must have been just west. And apparently the keynote at west was ML is coming to eat your jobs. And so clearly that conference was definitely talking about at least some of this modern stuff. And then this conference to some extent was some of the speakers were a reaction to that, right? And so there was a very extreme statement made on one side. And so that kind of engenders an extreme statement on the other side. And the reality is probably as usual somewhere in the middle. So I've done this. It literally takes 30 seconds on Google Scholar searching for software testing and machine learning to see what people are doing with ML in the test context and what they're succeeding on and what they're failing on. And I will tell you, big hint, they're succeeding more than they're failing on this front. It is for certain building up momentum. If you looked at the rate which studies of these sort are coming in, you can see that they're rapidly accelerating year over year. In my view, it is much like when Alan and I started this within Microsoft, God, four or five years ago, where to us it was very clear, no, this is coming. There's nothing that can stop this momentum. You can choose to put your head in the sand like an ostrich, or you can heed our warning. I now feel like one of those men with the signs in the middle of New York, the end is nigh, right? You got your little bull out on the street corner. Yeah, I heard they're very effective. You could hear, you could heed our warning and start prepping because there is still time. But I definitely see worldwide things starting to shape up in terms of how we saw them at Microsoft just a few years ago. The test singularity is literally just a matter of time, in my humble opinion. I think that's probably over claiming things. I think that a lot of the AI ML people come out and say, it's going to solve all problems everywhere. And I think there are limits to the kind of techniques we have today. There was just one of the big pioneers, I forget the name I was looking for it and I can't find it, who came out not long ago at a conference and said, back propagation and what we call deep neural networks have a limit. And in order to get to the next level, we're going to have to invent a new technique that's not been invented yet. So I think there's- No, they're going to have limits, but I don't see a manual tester being the- The solution? No, I don't either. Solution to working around. And then so we see a momentum around testing effort being moved to dev, what we traditionally classify as testing effort moving to dev. And then a large portion of these hard thinking problems moving to AI. So then the question in my view is, sure, AI is not going to solve everything. Those of us in the business know what it can do really well and what it can't do really well. But what piece of the pie does that leave over for the manual? Is it just pacemakers? Well, I think that the pacemakers are definitely a big part of what's left for manual testers. I think there's two places where manual testing will continue to thrive. I think one is in the industries that can't ship very quickly for whatever reason, either the cost of getting it wrong is insanely high, so pacemakers, nuclear power, airplanes. And then I think that the other spot are places where it's economically- your product is small enough and has a short enough shelf life that it is not worth building an automated system for. Or even small enough and doesn't matter if it sucks enough. I'm thinking of IT internal apps and reflecting on my time at Microsoft using those that quality obviously isn't a high concern. Think of a lot of web apps that show up on an iPhone or an Android or whatever. A lot of those are a low cost, one or two person system. Building up a team to do data to understand how this thing's working doesn't make sense until you get to a certain scale. But I don't think those- I don't think that those are trapping that happens manually. I don't think the teams that you're describing will go with the AI. I think it's more like, again, there's two things that threatening the manual testing. Agile and AI. And I think that small IT shop, they're far more likely to go, no, whatever testing skills our devs have, that's good enough. Ship it. Yes, I think that's correct. What I did see actually interesting at Star, I met a couple of people that were actually devs from a traditional Agile dev shop who were at Star to try to learn how to think like a tester. Which I think you'll see more and more people that need that skill set. Yeah, there should be conferences more dedicated to that. Because I don't think going to a traditional test conference like Star or STP is the best way for a developer to learn more about testing. I think there could be a more dedicated conference. I think the Agile conference is- what's the big one called? Totally spacing. I don't know. Anyway, the big Agile conference, like 20 concurrent tracks, does have tracks on that. But I think that may need to wrap up. One thing I want to point out and solicit information from you guys on is, I'm actually going to Star East in May. I'm only giving a tutorial on tools for testers. This is sort of an extension, the deep dive extension of my technical testing talk, where I'm going to take four hours and teach a room of people or two people, whoever show up, how to use things like Docker, some basic shell scripting, a little bit of PyTest, F12 debugger. I'm wondering now, I don't have anything- and again, this is allotted, this is going to get an overview in a couple hours. Is there anything on the data or ML side I should shove a little demo in of? The hard part is getting a world where they can bootstrap it. The hard part about data is that it requires a big infrastructure. Now, there are things, I'm sure there's competitors to it as well, but there's things like app insights from Microsoft over in the Visual Studio land, where you can instrument your code and it automatically sends it to the backend and then you can use it to analyze data. So showing off something like that would be useful, whether it's app insights or whether it's one of the competitors. New Relic is a competitor of that. So is app insights available in versions of Visual Studio non-Microsofties like me can use, or do I have to have the enterprise version? I have no idea what the licensing for it looks like. I don't think it's actually- I don't think it's Visual Studio, I think it's a separate thing that you guys- Well, take a look at it. I believe. So it works well with Visual Studio, but it's an Azure offering. All right, I'll take a look at it. I may just do some demos. We have a data pipeline that has a tool called Presto on top, which is basically straight SQL queries into your data that are pretty fast. So I could set up something like that. So the advantage of app insights or one of those systems is that you don't have to create the system for your app to talk back to the cloud. You don't have to create the system like the database. You just basically, you put some instrumentation in your code and it magically shows up in a database. And then you can query that database or even use their UI to get visuals on it. And so the hard part is the infrastructure in between, right? There's the magic between my code says, person click this button and something in the cloud knowing that the button got clicked. I actually think- And it solves that problem. The thing I've been thinking around on this topic is the way to enter into this if you're a tester. And I've been thinking through, I'm quite overloaded at the moment, so I couldn't do this probably until, begin to do this until Christmas. But I'm thinking a context sensitive training on EDA for tester would be useful in this context. For our audience, EDA is exploratory data analysis. This is using graphs and charts to understand what your graphically, visually what your data looks like rather than just coming up with summary numbers. So one of the things I talked about several episodes ago, and I'm fairly certain that I could do this, I could probably pull together a whole, at least an hour conversation on analyzing histograms. It sounds thrilling. No, I know it would be boring as all hell, but- Oh my God, I get some popcorn. But histograms- It's almost as good as the new Star Wars movie. Are you going somewhere? Because I have things to say. Yeah, go ahead. Two things. I remember being in, you might have been there, Steve, early in the days of Windows data science, there were a bunch of people got together weekly to try and figure out what the hell it meant. And I remember at first, and it was part of the growing pains, not picking up on it. At first it was like, how can we recreate our old test dashboards using data? And then I remember pushing- What did the dashboards use before? Using customer data versus test data. Got it. Because we need dashboards. And I remember pushing hard on the fact that I don't give a crap about, I give a little bit of a crap, but exploring that data is much more interesting because those insights are addictive and I want to get those insights being able to explore the data. One thing I should mention, one point, different point completely is in talking, I realized app insights, do the stuff, magic is done for you. I may just write a Unity app because we ship what's called Unity Analytics. It's a server, a service customers can use, which is basically that for you. In fact, we'll add app start, app stop, and a few other things automatically for you. If you choose to just click the button for the service, then you can add other things and get all kinds of interesting information and have it show up on your developer dashboard. That's essentially the app insights paradigm. Yeah, that sounds like a very specialized version of the app insights world. Yeah. So I may do that. I'm trying to figure if I want how much of the tutorial I want to be interactive versus demo, that would be a little bit too- I think actually if I'm going to do anything with data, it's probably going to be demo versus interactive. The hard part is getting enough, getting somebody to stand up enough site licenses or whatever it is in order to let everybody access that data, right? It's free. But you'd have to have a lot of instances for them to come access it or something. You could give them access potentially to a demo data source. Yeah, I'll think about that. But I think that's the important part. If you want to show it off, the hard part is not connecting those two pieces. Let somebody who specializes in that do that. The part that the testers need to learn is once I have the data, what do I do with it? Well, the way I would spin it is- How do I look at it? How do I interpret it? That is useful. How can I do testing via this data? How can I tell for a given use case or scenario, how many people are successful doing it when they're not? What errors are they seeing? Then you could talk through a lot of the problems, the fallacies that end up happening in there. If you have two different populations, one's moving really fast, one's moving really slow. If you just look at the average, you'll assume that everybody lives somewhere in the middle where the reality is nobody's living in the middle. Or if it fails in a certain way, you can have survivorship biases where everybody who fails stops using it, everybody who doesn't fail keeps using it. Most of your users are successful, even though most people who tried it are unsuccessful. Yes. That's that path of insights I love to find out. You find a little bit of data that looks interesting and you go, oh, I wonder why that is? Let me poke a little deeper. Oh, well, okay. Now I know. One more question I want to ask. Oh, crap. I love doing that. Mm-hmm. Yeah. And I think there is a space for the dashboards as well. I think you want to understand, at a baseline, am I in general in good shape? And then what are the special questions? I'm not entirely against dashboards. I just want to avoid what I think is maybe a natural progression of if you say we're going to test using data now to replace your test automation dashboard with a similar dashboard showing test from customer data. I mean, that actually is a good start. I will say that's a good start. It is not a destination. But just to see what the difference is in terms of the tool that you're accustomed to before, and but now getting this data from a new source. So that means that people are going to be, they're going to have an old process, but new data. So it should help move things forward. But it's not where you should stop. I think you have to give people a path to the new world. If you just say there's a new world and don't be in your old world anymore, go. It's very disconcerting and people don't know how to move forward. Yeah. Give them a path. That's an evolutionary path forward, not a revolution. I agree. And a totally different situation, dealing with one fun challenge at work now where we have a generic sizing. There's a team, not part of my team, who absolutely knows where they want to go. I am not convinced yet they have any idea how to get there. So it's fun. I like it. Yeah. That can be very disconcerting because it's very easy to give up or to go back because you didn't get there yet. And so clearly it's the wrong place to go, which isn't necessarily true. Well, it's been, do we even announce what number episode this was or were we too excited about Steve being here? We did not. I'm just that excited. We definitely were excited about Steve being here, but this now closes out episode number 70. Episode 70. I'm Alan. I'm Brent. And I'm Steve. See you next time. 
