Welcome to AV testing podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now on with the show. Hi everyone. Howdy. I'm Alan. He's Brit. Yeah. We are back. This is our third or fourth. I can't remember episode during COVID. Are the days all getting blurry for you? Yeah. Everything. Every day is a Tuesday right now. Luckily I have some recurring meetings that drive the business that keep me from, they keep me from forgetting what day it is completely. But yeah, it's a little blurry. I don't know how many of these we've done, but it's going well. It's going all right. I was pretty happy with how the last episode with our guest, David Bishop, went episode before that was retrospectives also on COVID. And then I think we had one or two, maybe just one. I don't know. We've been here a while. I have some skip level employees that I just do once a month, one on ones with. This is our fifth. All right. The data man, data man. I meet with them once every four weeks and we're getting around to our second round of meetings the last few weeks under quarantine. It's like, yeah, it's been a while. One interesting thing. And then we'll probably have a little bit of COVID talk is we did a survey internally and without going into all the details of what our return to office plan is, it's slow, which is good and voluntary, which is good. And people have a choice to come back in some of the early phases. We have three phases of coming back to the office, then a phase where everyone's welcome to come back and everyone on my team, except for one has said, sorry, none of my team and my whole organization has said, we'll come back when it's, you know, code green, when there is no more precautions, when everything's good in the world. And with a few of those people saying, I don't think I'm ever going back to the office. We have some people who are really embracing the work from home and liking it a lot. Oh, yeah. Not me. Not me. Yeah. There are there are those as well. How are you doing on the work from home burnout? Explain. Are you getting fried working from? Oh, I I passed fried like three weeks ago. Same. I have taken a couple half days on Fridays. I'm going to do another one today just to recover. I just saw something posted from Google. They have given their entire company May 22nd off a day, just an extra day off to rest. Yeah. Last. We actually unity has that day as a holiday. Anyway, we always get the when there's a U.S. three day weekend, we always get a four day weekend. So that's a holiday for us as well. What day is that? That's next Friday. So not this Monday, but the next Monday is a memorial day. Then I guess right. You're going camping. Yeah. Yeah. So last week, so I'm in a part of Azure known as Azure compute. And is that part of Azure Windows? I'm just joking with you because is it Windows part of Azure now? Yeah, it is. Okay. Anyway, go on. Azure compute. Is that like a W S compute? It's like a W S compute stuff better. I'm going to let you finish. But just so you know, are you a W S has a good computer? Yeah, I'm familiar with it. Um, anyway, so the executives we have two of them. One that runs the the engineering or the other one that runs the PM or gave us last Thursday and Friday off. This is actually last Thursday was was when I decided, hey, I'm going to attack the blackberry bushes in my neighbor's yard. I got his permission. So I'm just slaughtering them. Uh, ended up wounded myself in the process. Um, I have what's known as trigger finger. Can't really show it to those on the podcast. Um, and it's supposed to go away. Hasn't yet. Uh, it's it's annoying to type with anyway. We hold on those last two days off. It up again. Hold it up again. I want to get a picture. Uh, I gotta do it this way. Now stretching all of my fingers out except for the one that won't go. And it looks a little weird with looks a little weird with your virtual background, but I'll get it in there. I'll ask you in a minute. I'll tell you why I took that picture, but go on. I usually assume it's, uh, some version of you mocking me. Um, yeah, yeah, you gotta, you gotta, you gotta admit that 95% of the time that's that's right on. Um, anyway, so I spent last Thursday slaughtering blackberry bushes and then I spent Wednesday, uh, panicking around. Oh my God, I need my hands from my business. What has happened to my finger. Um, but other than that, uh, the only, the only difference was we weren't allowed to work in some regards. What I would have, what I would have preferred is that they had said, Hey, everyone, you can work, but we're banning meetings for the day. Now they wouldn't have done that because most of my ICs are like, Oh, well, that's nothing. I don't have any meetings anyway. Whereas I have back to back eight to six half hour. Yeah, that's been a killer. I'm, I'm five to three usually or six to three, but yeah, I would love that. That's fact. That's why I'm taking a kind of a half day off this afternoon. I don't have any meetings. I'm taking advantage of that to get some, to just do some housework to get away from my computer for a bit. Yeah. And then there's the other aspect because of, um, because of how long we've been in here. I, and I don't want to go into the full brain study here, but we've now been in this situation. Um, Oh God, I don't know. It's coming up on two months. And after, after 90 days of a consistent pattern, it becomes part of how you operate it. It's, for those who care, go look up how your subconscious learns and the hippocampus and blah, blah, blah, blah, blah. And one of the things that I've slowly seen happen with me is, uh, starting to hate my home office because going into it represents the oppression of meetings. That is fricking interesting. Yeah. The, any, yeah, it's anytime I can get out of this thing. And it's interesting because I, because I spent a lot of time making this some place that I enjoy, but it's right at that's, it's slowly dissipating Fridays. They're good. We do the podcast. That's fun. Uh, I have a couple of, of meetings with my PM partners and, and those have become sort of, um, a, a mutual back each other off of the edge of the cliff. Cause we're going crazy. Cause of this, this meeting games, um, a misery loves companies sort of one-on-one. Anyway, I want to follow up and it's not just mocking you that I love. It's in finding new ways and new venues to mock you, which really delights me and keeps me going strong in this world of code. Right. I do try to give you something new to work with. Yeah. Yeah. And the new avenue this week, it says things that could only happen in the boredom that comes with, uh, what do I do when I'm trying to keep myself away from my computer, but I'm still near my phone with COVID. I want to keep myself young at heart, despite the fact that I'm chronologically older than you, I do try to remain younger. There's this thing you may have heard of it's called Instagram. And I started an Instagram account for the AB testing podcast. And your picture of your finger will be there in about an hour. Why does it take an hour? Because I'm, I'm talking to you. I'm busy for a little while. I haven't posted it yet. I just took the picture by the time you hear this. You, you will understand what the picture is for. Gotcha. Right. Right. The picture will go out ahead of people hearing this. It's kind of like time travel, but it's really just because it takes me some time to make me sound better. And you sound worse before I released the podcast to the public. I understand. But if I remember, right, if I remember, right, if I remember, remember, I, if I will put our Instagram link in the show notes for the podcast release when it goes out on Monday. So today's the 15th Monday's the, that would make Monday the 18th. Yes. Math. So on Tuesday, the 19th, moving through my calendar, I am kicking off the spring 2020 online test conference, free virtual conference, three days. I'm going to be there talking about, Oh God, here he goes again. Modern testing. So what I'm finding is I do not like giving the same talk twice. And when I gave a talk at the test bash conference, which was, I don't know if you got to see any of that, but what a fantastic idea. They did a 24 hour marathon conference. Some people tried to stay up for all of it and watched it all live. They did. I cannot believe how well put together they were. It was really cool. But Tristan, who's awesome, who introduced me was very keen to keep repeating a brand new talk. And it was a brand new talk. Some repeated concepts, brand new talk. I am off on a tangent, but the point is is modern testing is getting less and less modern. Yes. When I first talked about it a couple of years ago, I was afraid that people were going to throw shit at me. And it turned out that they didn't. Some people thought I was weird. Some people said, yeah, we're already doing this. This is really cool. And it's kind of taken on a little bit of a life of its own, especially over, I mean, more and more and more and more. And so I'm not going to explain what it is and why it works, maybe for three minutes, but I just want to tell a lot of stories from what it's meant to me over the last, oh God, well, three years at Unity for sure. Some things I've heard from the three on how they're doing it, what Lisa Crispin is doing with it right now. Do you know Lisa and Janet are going to be giving a talk on modern testing? No. Awesome. So Lisa, Lisa, ping me about that. I don't have the details in front of me and maybe it's not announced. So maybe I shouldn't say that, but I will, but we'll definitely promote the heck out of that. I'm excited about that. There's all kinds of cool stuff going on. And so by the time you hear this on Monday, hopefully you listen to this the day it comes out because then you can set your alarm clock to for Tuesday, which is one day after Monday and Brent will never get a chance to talk. Cause I am full of saying stupid stuff. Anyway, Tuesday, online test conference, yada, yada, yada. Uh, that's it. So Instagram, that's it for my notes. So Lisa, Janet, are they doing that? It was part of what you see. Do you remember the, uh, the post they did on their take on the modern testing principles on their blog? They're doing a talk based on that. She didn't say which conference. They just said they're doing a talk based on that blog post. And they, she was asking for, uh, the short version of the principles that they could use those. Yeah. Which version of the short version? I think I used your, I think we're aligned on the short ones. I think we're darn close. So business first bottlenecks out, always learn and improve lead, equality culture, customer is King data, trumps, intuition. Everyone can test. I think my last one is different. That's all right. Yeah, it's all right. It's the gist and we're not trying to be prescriptive with these. We're just like, here's what we see. And to be fully clear for our own egos, this stuff that we're calling modern testing that is happening, it was going to happen anyway. Brit and I could not, it would not exist on the planet and the same things would be happening. We are two people. We are historians documenting the path of this thing across across the industry. So that's it. Yeah. I, I'm looking at that, looking at my penultimate or my ultimate, uh, my last blog post that was almost a year ago. And that's the one where I talk about the, where I do my final post on the, the modern testing principles. So my last one, the way I pivoted it, you said anyone can test. Yeah. But I pivoted is embrace the fear. Oh yeah. Yeah. That's a little, that's true. A little bit more abstract. Yeah. I see it. Either one. Did you see, uh, Abby, uh, tagged us this morning in a, in a post. Did you happen to see this? Uh, Abby bangs or was looking at a preview of a book on service, service level objectives, which is I'm looking forward to taking a look at it because it's something that, uh, we are prob I'm guessing I will be asked to put together in the near feature. So help is always appreciated. But there's a quote from the book says QA, this is your last stop. This is the one group that will be most concerned about what these changes mean for them. And I don't have the context of that, but she says that it implies testing hasn't evolved past the defensive stance of you can't automate my job away and we'll fight innovation. Wish I could properly disagree. And which is the stance that we take as well. It's a, one of the things I acknowledged in my, I bring up often is that we do live in a bubble as much as we see this happening, as much as it's common sense to what we're doing, there's a large part of the overall software industry that live in this world where test last testing quality in testers is purely an information provider. Testing is something you do if you're not good enough to do the other work in some cases exists a lot. And, and you and I have both seen numerous examples of people digging in their heels and say, no, this is dumb. My job, isn't going away. I'll always be necessary. I saw yet another Twitter feed last week that we explored together. I think it was last week. Oh look, so-and-so company had a quality problem. Their testers must be bad. It's like, that is the weirdest, it's such a ridiculous non sequitur for me to say that. I'm enjoying your snarky comments every now and again on Twitter. I've been trying to keep the snark. One I don't tweet as much anymore because I just don't feel like it. But when I do, I kind of tweet like three snarks against software for every one snark against Trump. That way people, you know, I don't want to be too political, but sometimes he's just so stupid. I have to point it out. We have no choice this year on that one, but no, what the, the thing I've been finding amusing is your sort of backhanded queries around, holy crap, did I just go into a time machine? Is it suddenly 1992? Yeah, I might've said something like that. Right. But that was to be fair, to be fair. I had not had my daily cap full of bleach by then. So I wasn't quite my full self. Yeah. Going back to Abby's comments, right? I have seen over and over and over again, people digging in their heels. However, when the bulldozer of modern change starts heading in their direction, I have not observed a single person who has successfully stopped it. Right. So the people who are digging in their heels, in my humble opinion, they're operating from a purely theoretical standpoint at this point in time. No, no, absolutely not. They're protecting the craft. Yeah. Hashtag sarcasm. Hashtag baloney. All right. Uh, was there anything else I wanted to talk about? Maybe one other, do I want to get in the mail bag? Do you want me to talk about one other thing and get your take on it? I have a random topic that wasn't on our list. So I can't remember. I'm pretty sure in our retrospective, wait, do we do retrospectives before David Bishop or after before my brain is completely food bar. And by the way, by the way I've got his book. I haven't gotten to the meaty stuff. I really want to understand how he measures what we call it vorticity, agile vorticity. Yeah. But it is, it is deeply research driven, which is great. Unlike say Nicole's book, which we also know is deeply research driven. His reads a bit like a doctoral thesis, which is a choice. I'm not placing judgment either way. Uh, but I haven't, haven't gotten to the interesting bits yet. All right. Well, keep us posted as you go along. I will do. I'm sure that's critical. So once, once I get to the section, if I'm able to, um, with, with my background in agile as it stands, if I'm able to sort of translate it to our, our listeners, I'll do so. Excellent. Excellent. I read the coaching habit this weekend. I read, I finished upstream, which is a book by chip and Dan Heath about very little about software, but about putting preventative techniques in early, the, the general upstream thing. Very good book. I like all their books. They did made the stick power of moments. So it was called anyway, I've read all their books. They're all very good. Lots of research in there, but they read pretty easy. Then I read the coaching habit this weekend, which I highly recommend for people interested in not just the test, coach or the test consultant. Cause I think more and more as I hear as I study like that book, uh, there is a difference. There is asking questions and growing people, trying to change your behavior and then giving people advice and pointers and tools. And those are coaching versus consulting respectively. So highly recommend that. And then I'm sure in the retrospective episode, we talked about the concept of psychological safety, right? Didn't me? Yes. It's like this blameless culture. And I wanted to share one thing I do in my job, which is, uh, and I should probably tell a story about this at online test conference is there's this little teeny part of my job where a half hour, twice a week, once for America's once for Europe, I run this thing called the product engineering sink. And all it is is anyone time across our organization, we have about 60 to 70 different customer impacting projects in flight. Things we think are worth tracking at a global level. What I do in this sink is certainly don't go through all 70 lines and talk about what's going on here. What's your status? That's uh, because that would be super boring and super ineffectual. What people do throughout the week is they add projects, they mark projects as either complete, they're done. New projects go into planning and then there's a status where projects could be green, yellow or red. Green means everything's good on track. We are on track for the target date. Yellow means we are on track for the target date, but there is some risk that's worth talking about. We want to share this. And red means, Hey, some stuff came up. The date's going to need to change. Before I used to use Google sheets and I used to go through all the history. It would take me another half hour before each meeting to gather all the things I want to talk about by looking at change history. We recently changed over to air table, which is, this is going to sound harsh. It's a relational database, but it's kind of like an online version of access. Okay. For better or for worse. Uh, and it, but it actually makes it a lot easier for me just to create queries that show all the things I want to talk about. But, and we have this meeting takes a half hour and we're unable to go through all of the delays and we encourage a no blame culture. We really, this meeting is more about reemphasizing our culture of psychological safety and it's okay to be late. We do. If we talked about our P 50 estimates before we have a rule in our org where only engineers can give dates. And so people have freaked out when I said this at the test bash, test bash home and only engineers can give dates. And so, but they're, and we require them to give a P 50 date, the date they believe has a 50% probability of being hit. And we're pretty accurate on that. Half of our projects finish on time or early half finish late. The things that finished late probably finished some of the fish little later than I want, but that's a different, different challenge. But the interesting thing is we've started to get a lot of people visiting this meeting from across the company because they're trying to figure out how they want to run engineering in their orgs. And the challenge I have is this is where I want to throw something out to you is I think the reason this meeting works the way it does and the reason it's effective is cause we already have a culture where it's okay to make mistakes and it's okay to be wrong. I think we don't embrace failure as much as we should, but people are happy to be transparent. They know it's okay to be transparent. They know it's okay to miss something and to report risk. They're very comfortable doing it. Uh, every week, there's probably of those 70 projects, there's 30 to 40 needs to talk about, you know, 20 for each for America and Europe. And what I see is a lot more people marking things yellow because they want to talk about the risk and share what's going on, which I think is fantastic. My worry is, is why I'm not prescriptive about like, oh, here's the thing every company should do because while I said, and I'm on a long diatribe here, I should have written my notes out while it does become a place for us to emphasize our culture. I don't know how you start this meeting if you don't have that culture already. Have you had any, so my question for you finally is what do you think? Do you think you can use this meeting to drive culture or do you think you need the culture first in order for a meeting like this to work? You can do it either way. All right. Next question. All right. Yeah. Would you like to expand before we go on to our, our mailbag? Well, I get to pull out the mailbag sound effect this time. The, so hurry. But it depends on your leader, on your, your leadership there, right? If you, if you don't have the, this culture already in place, then what you end up having is everyone going, Oh, Oh, we have to tell the leader that we're green, even though we're not. And I've seen over and over and over and over and over again, that when the leaders step up and they over, they go over the top, when someone has the cajunas to come in and say, we are red and instead of pointing the finger and going, taking an approach of, well, that's why I hired you. It's your job, not mine. Figure out how to get on red. The, they instead go over the top and they show appreciation for being brave enough to tell them the truth. Yeah. And they do so explicitly reinforced the behavior you want to see. So that's something that I know my boss did before I got there, but like this week, for example, we had something off the rails. It was just a mess. And they're explaining, I said, and the answer was thanks for bringing that up. Sounds like a pretty big deal. Here's the worst criticism we get in there. Let's get the team together and do a retro on that one. That's it. That, that's the worst punishment you can have for, for being delayed. Having to add to your schedule, something that should already be on your schedule. Yeah. Okay. This sarcasm aside. Yeah. It's like, yeah. And it's, there's no argument. It's like, yeah, let's, because like production incidents, uh, errors in planning are things they're prime opportunities to learn. You know, my big thing is I just had a copy. I moved it so I could have a place for my coffee, but I've been rereading the fifth discipline by Peter Sengay. Did you ever read that? Brits turning around, looking at, he has a virtual background, so I can't tell what he's looking at. Now he's disappeared. He's wearing fricking pajama pants, everyone. Like a true, like a true work from home warrior. So yeah, I, the exact book I have, yeah, I have, I don't have the hard to cover. I have the paperback. Really? I would have thought you would have had the Kindle one. Yeah. It's one of the books I bought before cause that book is old now. I bought it before Kindles. Oh really? Yep. It emphasizes a lot about the fifth discipline or about the learning organization and building a learning organization. If I recall correctly, it's one of the first places I read about the five wise 1990 told you it was old. Wow. Still very irrelevant. I did not realize how old this is. I mean, it's interesting. I spent a lot of time thinking through right, how generations progress, right? The, the, in how knowledge progresses. I find it fascinating. Right. There's that concept that if you don't learn from history, you're doomed to repeat it. I'd absolutely think that that's true. But I do see things like, like the fifth discipline being somewhat timeless. And I don't know, should that be worrisome? Shouldn't, shouldn't learning organization, got 30 years after this book. I mean, interestingly enough, the book is, is greater than 50% of both of our ages. I didn't even, I was still in college when this thing came out. And we, we are talking about the dominance of learning organizations and the criticality of it now. And I, I am, yeah, I don't know. I mean, why isn't this second nature, particularly in, in our industry that pogles me. I find that fascinating. Because power trips and command and control still in many places rule. Yeah. But we've said over and over again, uh, right. This is knowledge work. Uh, in, in, in our industry, it is knowledge work and, and knowledge work and learning, uh, go hand to hand in, in why it is still so much of a struggle. Uh, yeah, you may be right. It may also be not invented here syndrome dominant, right? The things that we've, we've heard over and over again, the deadly combination of command and control and experience as a title, I call it. Uh, and that's, I've been here for 20,000 years. So I know. Yeah. Yeah. It's, uh, I didn't mean, I, it sounds like, I feel like I kind of, uh, blew your mind a little bit by reminding you how old that book was. Yeah. I mean, there's a couple. So between that and making funny your finger, I've had a pretty good Friday. I'm glad to, I'm glad to help. Uh, again, it's just another Tuesday to me. There is, there's a couple of books that I look at and I go, this is timeless and will remain timeless forever. That is because the concepts are so abstract. It's, it's really difficult to sort of make it part of muscle memory. And those would be things like, um, Christopher Alexander, uh, the timeless way of building that bad boy is going to be relevant forever. Um, I forget the author, but systematics, anything heavy to do with systems thinking systems thinking just does not come naturally being a learning organization. Yeah. I don't know. Yeah. Yeah. Should we whip through a couple of mailbag questions? And, uh, I need the dust off is a lot of dust piled up on the mailbag sound effect. Okay, go for it. So I'm going to need to polish it a little, but I'll plug it in later. It sounds like this. I don't recall it sounding quite like that. Yeah. Yeah. Cause that's because of the dust. Gotcha. But our listeners will hear a better version. All right. So what's the mailbag question? This first one, first one comes from little Johnny in Madison, Wisconsin. And he wants to make a request. This one goes out to his little sister who's no, uh, this is from already and it says more for Brent, I guess, but I'm going to jump in on this one as well. Okay. How does CD continuous delivery look like in machine learning? What are the fundamental differences with traditional CD? What traditional CD has CD been around long enough to have 10 years. Okay. So what's traditional CD? I think what's the difference if any between CD in a machine learning world versus CD and like a web service world? Um, I would say from my experience, the most critical difference is CD is required for ML. So the issue with ML is it's rarely the case. So here's an ML architecture in a nutshell. Okay. Usually there is a thing called a model, a model. You can think of it as a function that takes certain inputs and outputs, a prediction or a classification or whatever. It's a pure, in the old world, we would have called it, um, belonging to the business logic layer. Okay. But then you need a host. You need something that say a rest API or something that you can receive a call from the outside world, and then pass it to your model and return it back. Now, unless your model is self learning, meaning you can deploy it once and, uh, you've got the great feedback loop in, and it is constantly adjusting here. You could think of, you could think of say search engines, uh, kind of work this way, uh, where they get instant feedback from the customer, uh, a click through is considered positive. There's multiple things that are considered negative. And then that can be, that can inform the models, um, instant next round of behavior, even for that same query. I haven't done it in a while, uh, but when I last work in Bing, you could take a search term and hit refresh sort of 10 times in a row. And at least three of those times would have been an entirely different experience. And that's entirely because it's constantly refreshing. The models that, that, for example, I'm owning right now today, uh, have to be rebuilt, uh, frequently because we have new, we have new data that would affect the performance of those models. Um, an example of the model that I'm working through right now is, is, um, taking, uh, verbatim from customers and trying to guide them to a place where they can, uh, succeed in finding solutions to problems. Um, it's not a search engine. It's more directed than that. However, when, when the, the say Azure has a major outage or a team releases a new product, uh, there isn't enough of that customer verbatim for the model to, to confidently direct the customer towards a new solution. And the solution to that is of course you rebuild the model on a continuous basis so that it relearns the current patterns that customers care about. Uh, the, the benefit of this is it automatically relearns the, often what I will describe it and think of, um, think of any business logic that you may have ever occurred or encountered from dev. That is just the world's most complicated rules engine. They'll call it that it's really just a, you know, 50 times nested crappy if statement, if that was critical decision making process to your business, you would need to do CICD because a lot of that complexity would, would time out quickly. It's no different than, than with, with ML, except ML is much more efficient at automatically rewriting that, that deep nested if statement. Alan, time to wake up. I'm done talking. No, no, no, no. Cause I want to, again, I love that we have different, it's not really different opinion and perspective, but I can give a different example, more context, different context. I dunno, it's like retrospectives we had, we largely agree, but there's some different approaches we have. Uh, this isn't an approach difference, but I can talk about recently, this is going to be a larger story around culture as well. Uh, but recently our machine, someone in our machine learning team approached to me, I run our DevOps team and they said, actually pause that first thing is, the answer is they both should have a, the pipeline is going to be different, the CD pipeline, but because there's different tools involved in deploying models. Okay. That's the short answer. So recently someone from our machine learning team came to me and said, Hey, we need to have CD, uh, for our ML stuff. Can you please build that for us and, and own that. And just like with, but my, my DevOps team, just like a QA team, it's like, we don't, we don't like owning stuff. There's not enough of us. We'd be a bottleneck then and Brent, our bottlenecks good or bad. They're great. Yeah. Liar. Bottlenecks are bad. You really get security for your position. And if you play, bottlenecks are bad, play it right. You can totally triple your promotion velocity. They're fantastic. Oh, so anyway, what's fun is actually, we're actually are working with them and, uh, on building their own pipeline, their own deployments, their own stuff in a way that with abstraction and documentation and resiliency that they can maintain it themselves. Uh, so, which is the right thing to do, of course, but just some context around that. It's, they've seen it as essential. We think it's essential. We're going to invest time in it. Uh, it's kind of fun to see how we'll use, we use Jenkins for our CD, uh, but it'll be fun to see how they end up using it for, for their stuff. I'm pretty excited about that. It's going to be a cool project. And I will let you, if we find out really cool things in the interim, I'll let you know, but yeah, not a lot of difference, but probably worth bringing up. I think cause well, one, they don't have it now. So they survived without it, but it was painful. In fact, the fact that they don't have CD right now is a bottleneck for that team. Yeah. It slows them down. Can become very quickly a bottleneck, um, from right now for me, uh, it slows me down as well. Uh, there's, there's one of my projects where I am, one of my projects where we have the model going and there's a bit of a, of a manual deployment. Uh, so we have it all the way, automated up to staging and testing, but we need to make sure we don't deploy at craft models. So we don't, we don't have it automated. The final push. Totally fine. Oh yeah. Um, but the, the space that we're in, so we do manually push every week, uh, but the space that we're in, uh, that model or the, the customer experience would absolutely benefit, uh, if we could do just out of deploying new models daily, we've, we've gotten actually that's the one small bit of code I've been able to write over the last month. Uh, I took our build process for building these models. It was something that would take literally a week to do. That was main reason why we needed to, to do that a weekly basis, but now I've got it down to nine and a half hours and still have evil plans to, to, to do it more and more better anyway. Cool. Let's do the other question. Sure. Uh, just cause we have a few minutes here and I want to get to this one's from Patrick H. Uh, Hey Patrick, thanks for listening to the podcast. The question is, does the observer effect play any part in instrumentation and telemetry in software systems? For example, the telemetry services slow down my system. So I turn them off or try to compensate by adjusting some settings, changing those settings may falsely contribute to some metric on the business side and affect decision-making. So if I may, summarize that already short question, it's, what's the impact of telemetry on stuff being measured? And you and I both have worked with us a pretty complex systems around this, but I'm gonna let you start and I'll just add flavor. Um, yeah. So I think even, even in modern systems, it can be problematic. The, how ever brute force is so much easier now. Right. Uh, in, in the cloud, right? We can set up a separate networking path, uh, just for the, the, the telemetry, uh, in terms of CPU, um, if I think it's been a long time since I've observed that the CPU is, is, is really being taken up a great deal by the telemetry system. Um, there are obvious solutions around it. Uh, I don't see any of those commonly deployed nowadays in my life. Like it used to be a way where, right? You could, you could set up a config file or your instrumentation systems and be able to turn off or on and, uh, the, the depth level. So you would have a specific, uh, constant strain that was very lightweight. And then you would have a monitor hooked up to it that would fire and automatically turn on the, the, the really deep traces. Um, I don't, I don't see that as a big issue. Uh, now, nowadays, I'm not going to add to that because one, I think you covered it all. And two, I was only half paying attention because of work going on. We're such a professional podcast. Uh, and yeah, so poor little, you said Patrick H, right? He's probably, uh, yeah. It probably doesn't feel as important. We do appreciate the question. And Brent asked, Brent and I have worked with the similar systems, uh, and I will recap what he said based on the fact that I think we think the same on this is that the overhead of those, of that collection done correctly in a way where you just, you're just collecting bits and saving them on a non on the non-main thread. Uh, and then, and then uploading on uploading at opportune times is very, very little overhead. And maybe one thing to add, if you didn't say it, or to underscore, if you did say it is the one thing we used a lot, I remember a lot on when I was working on Xbox one was our ability to turn stuff on remotely. So we had a lot of stuff that turned the collection on remotely. So all the little ones and zeros, like here's a bit, this thing happened. We said a bit somewhere. We may not collect that, but again, the, with a modern CPU, well, we actually knew exactly which CPU was in there and how much overhead it would have. And it was negligible at best. But if we had something interesting, like, Hey, you know what? Only the people on this version that updated directly to this version, we think are having this problem. Let's go turn on extra metrics for that exact demographic. We could do that. And that was fun to do. And it really helped us. That's what I learned how to debug through telemetry, which was kind of fun. So, uh, you can actually collect a lot and not upload all of it, but be able to turn those things on. And that is, I think that's the key to being successful. I talk with a lot of teams on collecting metrics. They go, well, we don't want to log too much. So he's only going to log very, very specific things. I say, yeah, I see why you think that way, but let's talk. I mean, you do and you probably want to, you want to log all the things that you, I don't know, the nice to have. But like, if you take what you need to log and separate it into some of the P zero P one and P two, right? You probably want to want to do up to the P ones because that's going to be the stuff that you, you go, damn, we could totally answer the customer's question if we had actually logged this. Right. Right. The, the P two, there is an idea of going overboard, right? Uh, in terms of what you log, uh, it, I think that's a happy medium, right? Do you like, remember the old days when we would find a regression, find a customer would find some bug and we go write a test to make sure we, that bug would never get out there again. But these days, more often, it's what monitoring, what could we have collected that would help us debug this faster? And we'll go add that and add, start adding a few dozens or hundreds of those things over time. And you begin to build up that ability to debug these things remotely. Yeah. The, in terms of like the, the operational cost of a telemetry system, I don't think that is anywhere close to, uh, the storage costs. Even with storage dropping, right? There's a reason why a big part of the world is in, nowadays are, are described, um, in big data terms, right? Uh, the, uh, it's because we are storing a lot of stuff, like I, and, uh, certainly trending stuff. Uh, for example, one of the feeds I own, one of the most common complaints I get from my consumers is what do you mean? You don't have five years of data. I'm like, yeah, I only, I only keep a year. Sorry, but it's an ROI thing, right? It's it for sure. For sure. For sure. If, uh, I get an ask for going beyond a year once every year, my team, if we needed it, we would use it. We would have already implemented it right at the way I operate that particular data feed is, Hey, uh, this is primarily for my team's purpose. Um, but because carrying is sharing, everyone's welcome to use it. Uh, that gets me in trouble every now and again. Uh, I had an issue just a couple of weeks ago where some team had super BP visibility. Uh, I found out a month after they began that they had taken a dependency on this feed. Uh, and we're, we're, we're trying to use it in a way, uh, to answer questions in near real time. This feed cannot answer. And, and then after they had solidified after a month, they were doing the full court press, you know, uh, we're going to tell the VP on you type of BS. And I'm like, knock yourself out. I'm happy to go and explain to the executive how you wasted a month of your effort that you could have resolved by just asking me a goddamn question in the beginning. Yeah. Let's go have that conversation. Anyway. Anyway, Patrick H, I hope we did a good job answering your question. Uh, let us know if we didn't, you can ask more questions. Uh, you can go to the one, one of the three dot slack.com. There's an invitation link. If you're not a member of the community on the website. And I think that better be it for us today. What do you think? Yes. Brent says two thumbs up. I'm translating for him. Okay. Thank you again. Uh, happy May. I'm Alan. I'm great. Just as always. See you later. 
