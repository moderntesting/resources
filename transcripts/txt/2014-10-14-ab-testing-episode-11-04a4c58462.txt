Hey everybody, I'm Alan. I'm Brent. And we're back for another episode of AEG Test Aid. For those keeping count, number 11. Whoa, 11. That's so cool. We're a prime number. That's awesome. Woo hoo. Alright, so, gosh, we have an agenda. We have a list on the board, so we're working through it. Last week, I did a post on, like I called it something stupid, like the myth of technical interviews. And it was sparked off. I saw a tweet, an article was written about a bunch of big data analysis on Google's interview data. Okay. And they wanted to see what's the correlation of who are the best interviewers at finding good hires, how much does the interview feedback overall reflect their future success at Google. And the answer is there is absolutely zero correlation. With the Google interview? With the interview feedback, like a person that got the highest interview scores ever, and a person who barely got in, there was no correlation between who would do better at Google. Doesn't surprise me. Doesn't surprise me. It didn't surprise me at all. It was like, duh. And then I looked in the articles from a couple years ago, but not that, a year and a half ago, but not that it makes a huge deal, but. So Google has also changed their interview strategy recently. They used to be more of the old school Microsoft, the puzzle questions and such. Do you know which style this was ranking? It doesn't matter. And the point that I made in my post is that your interview, I hate to hope I don't break anybody's heart for those of you that haven't read my blog posts, but your interview isn't going to decide the future success of this person. Your interview is there to make you interview the person to make sure you don't hire a complete total doofus. You know, one of the better managers in my career, one day to sort of talk me off of jumping off a cliff because I was so upset about our interview process. He basically said two things that that he realized that I hadn't realized before. And these two things were both the review and the interview process. Their goal was to solve the same thing. And that is they weren't there to assure good people move forward. They were there to assure bad people don't. Yeah. And that's a much better way to look at it. And then, you know, at Microsoft, we have really I do three types of interviews at Microsoft where we have three types of candidates. I usually do the as appropriate interview, but three types of candidates at the college hire. And I want to make sure that, look, the language you learn in college, you're probably a program in language you're probably not going to be using in five years. So I don't care what, you know, what sort of trivia you've memorized about whatever language you're working with. But I'll make sure you're a good fit. And I look at the college hire. I want to make sure are really I'm making a bet on am I going to invest a year's salary and benefits to sort of audition you? Because are you, you know, are you good enough to get in? Then there's the industry candidate, which have you I want to figure out, have you been faking your way through the industry? A lot of it's the same. I want to make sure a year from now, are you going to be, are we going to be working on firing you or are you going to still be working here doing a good job? And then the one I think we talked, we hinted a little bit about this in a mail from our CEO this summer. There was another hint about this. There's sort of the internal transfer. And where we really screw up at Microsoft is, I think, is that we put, we interview those three candidates the exact same way. If someone's already at Microsoft and they've been here for a couple of years and you look at like nobody's been trying to fire them for a while. The main thing I'm going to look at is fit on the team and fit maybe some fit for the job and are you going to work well on our team? I'm not going to go ask you the same stupid puzzle and programming, reverse the people on a desert island question that I asked the college candidates, it's a waste of time. So I think I hope we get better at that soon. Yes. So the, it's interesting. I, I have different criteria for, for me, a kid coming in from college. I only care about one thing. Yeah, maybe two. The, the two things then would be, can you learn quickly what we're going to teach you over the next year? Yeah. That's each one of the things I really look for is sort of that passion and ability to learn quickly. Yes. And then on the fully agreed, the passion side, can I get a sense of what your work ethic is going to be like? Are you, are you all in on learning how to learn this business? That when you first come in from college, like your role is just to pick it up and, and I'm overusing the word learn, but to learn, similar to an apprenticeship back in the day, we have this ladder level here and I think it's still kind of true, although our view view view process is differing a great deal. It used to be, you got hired in at what was known as a level 59. And I was hired as a level nine. I was hired as a 10. Yeah. Long time ago. Different level system. Yeah. We changed it quite a bit. Uh, but a 59, if you were still a 59, even 60 or 18 months later, uh, alarm ball or alarm bells went off. Or the alarm balls, which are a little worse. They can be, yes. They're, they're spiky and mischievous. Um, because we have a concern that first year is kind of viewed as an investment. That's what I said. It's I said, it's an audition. Yeah. Um, or, or an apprenticeship of sorts. Sure. Uh, apprenticeship is probably better, but yeah. Yeah. Um, you know, the, the, the internal candidate, there's really only one thing I care about, like I, I can, I can figure out what your skills are from, from the, from looking at your past reviews. I can see depending on your ladder level, like the, there's going to be a balance between, um, requirements around communication or requirements around leadership or technical requirements, but I only care about one thing ultimately. And that's group fit. Yeah. And one other thing I realized I would do differently is that, you know, group fit. I, I talked to the candidate about that. Maybe have them talk to somebody on the team and also remember sometimes are, and I'm going to go out on a limb and say that sometimes our managers can get a little bit full of themselves and focus too much on interviewing the candidate and not realizing if you're trying to bring in someone to your team, they need to interview you as well. When I, when I go on a job interview at Microsoft, I want to make sure that I feel like I fit on the team. I feel like I'm going to be challenged and I'm going to learn that I'm going to have the opportunities that I'm looking for. And I think a lot of times, uh, you know, when you look at a internal hiring loop, it's all about, can they pass our gauntlet? Can we make them sweat and squirm? And, and really for someone internal, I think that's just crap. It, there is a situation where it could be non crap. And that is if the team is, is, is in a constant fire drill mode, right? They don't want to hire someone who who's going to consistently crumble under stress. Well, good. Because as I might, as I'm interviewing a team, I'm interviewing to be on a team and I can tell they're in constant fire drill mode, unless they, unless they are hiring me to come in and fix that, it's not a team I'm going to be on. They're not going to, they're not going to pass my interview. No, the, the other thing. Um, so there, there have been several interviews I've been on several. I've passed and several I've failed. Uh, in, in recent past though, um, thankfully due to, due to my experience here, I usually end up being interviewed by the, the higher level guys who, who spend a lot of time doing group fit. And I'm actually appreciative. Um, when I can tell that they're sort of doing a group fit analysis in terms of like the type of questions they ask me, if I ultimately get a, a, a no hire. Uh, I'm very happy for that because that absolutely that's leveraging their knowledge to go, no, this is not the right place. I don't view good guy, not the right guy. I don't view a failing an interview as a fail. I view it as a, whoo, that's a whole lot of pain in my life. That's not going to happen now. Yeah. And I think that's a good way to come out of that. I've, I've experienced that myself. As a hiring manager, if I'm hiring internal, and then we're going to move on from the subject for, I know we have a lot of Microsoft internal listeners, but if I'm a hiring manager and hiring someone eternal from Microsoft to be on my team, uh, again, I'm gonna look at them for fit, but really the way I want to find out if they could do the job is it's not going to come out from the interview. I want to talk to their, you know, this is hard at Microsoft because we have interviews sort of like, just like, oh, I'm leaving the team, but I want to talk to the team they were on. At least, at least their current manager, if not a teammate or two, just to see, uh, what have they done? Are they BS and they're good? And I would expect, and, uh, as my manager would do today, I guarantee it. Um, if I were to interview for another job at Microsoft, he would proactively call the other manager and tell them, uh, I think good things, but you know, he, he watches out for me. He's a very much a, uh, I'm going to make sure you do all right, no matter where you are, and I think, uh, we need more managers like that. More managers who, who, who cover their, their person's butt. It's making, no, it's making sure that, uh, they're about fit, making sure the hiring manager knows what you're good at. If you're thinking to hire this person, here's what they're good at. Here's what they're going to do for you. Here's the things you need to work with them on. Here's, I think that conversation between hiring managers, hiring manager and current manager, uh, not just a quick phone chat. Hey, I'm poaching your dude, but a good conversation around, uh, capabilities, limitations, et cetera is, uh, essential for a good internal transfer. I, so I've, I've been the hiring manager that makes those calls quite often. And over the years, I found it almost totally useless. I think cause a lot of times what I've seen is it becomes a it's polarizing. It becomes a confirmatory chat. Hey, I'm poaching your dude. Anything I should worry about. And they go, no, just take him. Yeah. He sucks. Well, whatever. What, what are you going to do? Well, if I say he's bad, you're going to hire him anyway. It's that, it's that trust and honesty is missing. Yeah. So that ends up being a BS conversation. There's, there's two reasons why I don't really use so much. Number one, we have so much stuff documented in, in the review process. So when I have a candidate coming in and he's interviewing, I get to see the reviews. I haven't, I haven't done this since the review switched over. But the peer feedback, I'm hoping I have access to that. Yeah. Peer feedback would help a lot. I think often you can't, as much as you want to trust, can't much as you don't want to trust talking to that manager. Often it's hard to tell what's going on from the review documents I've seen as well, but the peer feedback, I think is a little bit, we need to take that a little bit more seriously here. I think it's a much better read on who and how a person, who a person is and how they work. I want to talk about just, I know we've beat the crap, the heck out of this subject, but I want to throw a, something I read over the interwebs recently and see what you think. Uh, Brett and I have talked a lot about what the engineering team or as Microsoft likes to call it the combined engineering team, how that works, et cetera, blah, blah, blah. Not going to bore you with a rehash, but I saw somewhere, someone said something to the effect of if you don't have a test team, you're not taking product quality seriously. I'm curious what you have to say about it. You just say a test team, a test team, test team, a team of people responsible for testing quality into the product. I, that last part wasn't there a team responsible for testing. I would say, yes, I agree. We, this horse, this horse, I'm not only dead, time out, time out, time out. You, you agree the horse is dead. You don't agree that to be clear. Do you agree? Are you saying that without a team for testing software in your software development organization, you cannot make a quality product? I would agree that that is the correct principle for all of our competitors. No, not the ones that are beating us. Uh, the ones, the ones that are beating us should absolutely, uh, take this insight and go with it a hundred, a hundred miles an hour. This is stupid. The, the, the, someone's going to do an edit where they make you sound really stupid and it could be the person staring at the laptop right now. This, uh, episode 12, the all outtakes episode. Yeah. Where to start with this. I've, I've begun to encounter, um, more of this. Here's another horse we beat dead. So there was a situation that incurred in my life very recently where, um, my manager recommended someone else come and talk to me, uh, because relatively speaking, I am now an expert in agile. This guy was trying to invent his own version of agile fragile. That's what it's, that's what it is currently. And this guy told my manager, I have been in this industry for 20 years. I know what I'm doing. And this coupled with what you say, right? It reminds me of something that I now call Jensen's rule of politics, which is he who is defensive loses always. So I think the comment that you're talking about is somebody who's, who's either completely not paying attention to the industry or is paying attention to the industry and is being defensive around how to continue to justify their existence. I think that's a lot to do with it. And I also think that once you see something done wrong, and we've talked about, we've beat the heck out of the dead horse of how to do a engineering team wrong where you don't have any test specialists. Um, see if you see that you, yeah, yeah, yeah. But, um, the efficiency gains from having that combined team and having the test specialists who can do the things that some of the really great testers do on an engineering team. Totally essential. But I think, uh, often I see a lot of maybe since I look at testing articles and blogs and, and tweeters most often, but the level of cynicism on, I've seen that not work once, so it never works. And it's bad is just off the charts. Like your iOS, um, bug last last podcast. The, this is look, Alan. I'm looking. I, I think this topic needs to be done. All right. Done with it. Now, I don't necessarily mean here. Like my response to a tweet like that would be good luck with that. Yep. Right. That, that battle is over. You got some guy who's blind still swinging the sword around and it's over. Yeah. And I think, I think a lot of, there's a lot across the industry. People started figuring this out and figuring out what this looks like. And there's the growing pains and why I've seen it not work here. And I've seen this work here and it works for me, but it won't work for me. Um, but I think the battles, it's just the, we're dealing with casualties and growth and people figuring out what their role is in the new world. And that's fine. And, and, uh, I wrote a blog post. Uh, you haven't written a blog post in a long time, by the way. Yeah, I know school, school and work is, is just slowing me down. Um, I have one half written it'll go out this week. Um, so those who, who are paying attention to that other than that one, which I think is our three listeners, maybe, um, I'll have another one out this week. But, um, about two years ago to wrote a blog post and there's, there's a, you can go to monster jobs and search for cobalt, right? There are still jobs for cobalt. So, so people who hold this point of view that there still needs to be a test team. Yeah, that's true. There's going to be companies that, that believe that, but that's going to rapidly orders of magnitude accelerate to be the opposite point of time out. There is absolutely nothing that says you cannot do full on XP with cobalt. The language, the language is not, does not dictate the engineering approach. No, what I'm saying is, is right back when Alan was born cobalt was an important popular language and it's, it's, it's still important, but it is not where people go like worldwide. When I last did this query, there was like 6,000 job openings for cobalt. And there was something close to 30 to 40,000 job openings for C sharp. Um, it's just not the way the, the, the world is spinning. I've been programming in cobalt for 20 years. I think I know what I'm doing. Right. Good luck with that. Fortran. All right. So let's move on from that and hopefully not have to revisit that unless someone, uh, you know, writes something clever in the mailbag. So Brett and I have talked a lot about data-driven quality, data-driven culture. What's your latest thoughts on sort of what, what does that mean to be, to have a data-driven culture on your team? So we are actually, my, my team right now is going through a process where we're trying to accelerate, um, the training of the program management work. And the way I would say, um, data-driven culture, they're there. The way I would address that is those who are still using their intuition. Intuition is, is good, but a data-driven culture is one where people are constantly validating their intuition with facts and using that new knowledge to pivot the direction of either their feature or their product or whatever it is that they're working on. We're going to use facts to make decisions. Whoa. Um, yes. Unless, unless data means something else to you, Alan, what do you think? Um, I'm just teasing you. I want to rewind and then we'll get back to data culture. Cause as you know, nothing is linear in the world of AB testing, at least for Brett and Alan's version. Training your PMs. Yeah. What are you training your PMs on? Oh, this, I don't think I've shared this with you. You'll think this, I think you'll think this is cool. This, this is sort of a progression of how, uh, I think Microsoft is going to start. Is it wax on wax off on your cars? No, no, no. So my team, we have a kick ass VP, uh, who grew up in the startup world. And he was hired into this company as, as a senior exec to help bring that culture here. Um, one of the things that, uh, me and a partner of mine have done in the last two months is we are routing our instrumentation to a third party and this third party site, they have mastered the art of, um, first off their schema policy. Their API is fantastic. And they have mastered their market is usage analytics. So instead of us having to go through and, um, uh, like every other team at Microsoft is doing right now is creating their own data pipeline and creating their own cubes and doing their own insight discovery. These guys know what they're looking for. They have an API that says, send it this way. We will automate it. And, um, our PMs can now go to the site and see all kinds of fun, interesting data that they didn't see before. Now, what we're doing is we're getting the PMs in the room. We're hand holding them through how to use the site and sort of what sort of insights you can pick up from it. And I have never seen such amount of, uh, wide eyed, uh, as I have here, we're like, Oh my God. Right. Just simple things. One of the things that, that, uh, we displayed the other day was, um, our dashboard uses tiles just like every other windows app, um, or our, our app uses tiles like crazy internally to the app. And when someone pins a tile, there's a default size, but each type of tile has a different context. And what we have found is that there are clear, um, behaviors depending on the tile context and the size that, um, certain tiles based on their context, people are growing them or shrinking them, which, um, kind of means that perhaps each tile context should have a different initial default. So, um, I want to touch on a couple of things you said and yeah. Uh, 20 minutes ago, you talked about matching intuition with facts or I think it was intuition. And this is, this is the key thing for a lot of current testers to understand about what moving to a data-driven culture, data-driven engineering means is that forever testers I know are going, this isn't right. Getting suggestions on how this, we should have these tiles be smaller or whatever. And the response has been go back to testing. We know what we're doing. The difference in a data-driven culture is look what the data says, look at the insights I've derived and, and how I've derived them from how actual customers are using the product. Again, testers forever, well, I am the customer, I'm the customer advocate. And as hard as you try, you're just not. But when you get, when you actually look at the customer data and get those real insights, those are decisions you can act on confidently. The intuition ones are harder to act on confidently or darn near impossible to act on confidently. Or even looking at a comment from someone on the forum said we should have tiles go like this. And the answer may be, well, that's just one person. But when you start looking at this information and aggregate along, you know, thousands or tens of thousands or millions of uses, and then it's like, oh, yeah, here's a decision we can make and it's going to make the product better for the customer. And that is the shift for me from sort of the old world of testing to this world of data-driven culture where, again, I'll call it testing, call it, you have program managers looking at this. Someone needs to look at that data and make decisions. So that's really cool. It's very cool. The chart that I displayed for the PMs was Cindy Housen is a leading expert on business intelligence. And she said, forget the website at the moment. But she did a study with 500 plus different companies. And the study showed project success based on what percent was sort of gut field decisions versus data-driven decisions. And they basically, they found that teams that did around, I think the number was 60 percent fact-based decisions and 30 percent gut field decisions had the greatest success and it was significantly greater. That's a really important thing to mention because I think a lot of people will hear my last statement and say, well, Alan's saying that intuition doesn't have value, but it does. It does. And you can, you end up, I think in a way, training your intuition based on the fact and database decisions you make. And then also when you have, like, I think that if you have a hunch, I'll go, I'll just stick with the same example. As a tester, we have, a tester intuition is very strong. I totally believe in it. Lots of stories about this that I won't go into. But if I have a hunch that we can do better with these default tile sizes, for example, I'm going to, I want to try and go see if I can prove my hunch, my hypothesis with, with data. Let me ask you something, Alan. Let me answer you something, Brent. Where do ideas come from? Ideas come from other ideas. Yes. Intuition is just another way of saying a new idea. You're a hundred percent right. Like these new facts are these prior ideas. So when I have a particular intuition and I get a new fact that comes in, I will get a new idea based off of what I currently believe now modified by this new fact. So the two going hand in hand is absolutely critical. In a deck I did recently, one of the things I explained, Hey, if you go entirely intuition based, entirely gut feel based, Alan used the word hypothesis testing, right? It doesn't matter how much you actually believe you're right. But because no human being is able, is capable of keeping all of the variables straight, this creates uncertainty because uncertainty, no matter how right you are, you are, there is always some probability that you're wrong. And when what ends up happening in these intuition only teams, right? They end up being believers towards a path that's wrong. So intuition allows you a pure intuition model over and over again, allows you to go very quickly towards solving problems no one has and spending a lot of money doing so fact based. If you do fact based only, you can succeed on that. But the studies have shown that facts based really what it does allows you to get to what's known as the local maxima. So if you if you think of a an up and down curve on a line chart, right, no matter where you are, fact based decisions, what they basically do is allow you to optimize for the maximum of where you are in that graph. But if there is a better maximum, fact based decisions may not get you there. You need to have that game changing idea come into play that says, hey, you know what, we may need to actually sub optimize for the next couple of months because there's this greater opportunity that that sub optimization will enable us to get to. Got it. Do you? I do. I think I do. One more rewind thing, which I think is worth calling out again, especially for the Microsoft listeners and actually maybe for everyone. You are using a third party. Yes. Isn't that cool? I've been at Microsoft. I've been doing this for 20 years and I've never needed none third party to help me. Isn't that freaking cool? I think it's so cool. We are such non-embracers of any technology, such lovers of non-invented here to a fault to a fault. So I am just ecstatic when I see a team at Microsoft make a good business decision about going, hey, this other company's product. Or this open source project, whatever it is, it's going to help us out. And we can we can use their product for cheaper. We have such and I could tell stories that would embarrass people for making dumb decisions for the next two hours. But I won't because at the end of those two hours, I wouldn't have a job anymore. But we historically into one hour. Historically, episode 12, Alan's last episode. But it's great to see. I think it's really important. I love that our team, for example, is all in on using Git, which Microsoft didn't make. Oh, my gosh, we're using a source control system someone didn't make. And and actually, the reason we're using it is because it's fully integrated into Visual Studio, not an ad, just the fact that the Visual Studio team embraced it first and we just we just piled on. One of the things that also happened, so when we went with this third party, we're trying to light up what we internally call sort of self service business intelligence, where we arm individuals with the ability to sort of do the right thing on the instrumentation and do the right thing on on by the way, totally awesome approach. I see teams approaching data analysis as sort of a ticket system. Like you have a question, give it to this team and they'll go figure it out for you using their special archaic tools and command lines. They'll get back to you with your answer. Right. Six months later. Horribly non scalable dead end solutions. Well, it's it's a challenge to do the this and the self service model, because typically I didn't say it was easy. I said it was the right way. No, no, it is. It really needs. And we've we've encountered this with automation. What's right? What the right way to to staff an automation team purely decentralized or purely centralized? And the answer to that is none of the above. The answer to any sufficiently complex question is it depends. In this case, it doesn't depend. I think that there is a right answer and it's someplace in between. It's basically you target decentralization, but with the central governance authority to make sure people are converting. And we're off in the weeds here, but I hate I you know, isolated or automation teams. This team does the testing. This team does the automation or this team writes the test cases. This team automates so far in the wrong direction that I don't want to start. And so I'm not going to go on, Brent. Yes. One of the new common problems that people are encountering is with the data world is is is GIGO garbage in garbage out. One of the challenges me and my team has been facing is this instrumentation problem like basically because we started with a self service, be I approach, people are interpreting that as in essence. I can instrument whatever I want to, however I want. And one example is about a month ago, we had five different ways to communicate whether or not the call resulted in an error. PM's now going to this third party. Immediately, they they're confused because the scheme is just all over the place and within a week, an uphill battle in terms of trying to get people to converge on a common a common philosophy around schema. And in some cases, an actual common schema just died. Yeah, I'm a huge believer in garbage in garbage out as a way to make change happen. Yeah. Like here's the answer we have based on what we have. You know, this sucks. Well, fix the input. One of the quick tangent here before we go on to the mailbag is I had a maybe I told the story before, but on a previous team was working with a guy and he he wanted to get a bunch of data from the team. So he did what a lot of people at Microsoft do. He sent out a big spreadsheet and said to the whole team, to 100 people, 150 people said, everybody fill your data in here. And he was mad because people weren't filling it out. He goes, I need to get a VP to go yell at him and tell him to fill his data out. That's not the way you do this. You can get Satya this in that email. It still won't happen. It still won't get done. So I told it is here. Let's let's just kind of napkin math. Let's fill it in. Let's go ahead and fill it in. So I don't know the answers. Do you have close guesses? It says, yeah, I can probably guess pretty well. OK, let's fill out the whole thing. Let's send the new mail that says, hey, here's the data I have for the team. Please confirm it's correct. And we'll go on from there. Man, he got feedback. It ended up with a really good set of data for him to work with. But that's that technique works wonderfully. You know, don't don't say, hey, fill in this section. You know, same thing. My boss did the same thing this week. He went last week. There was a slight reorg in our org and he wanted to send out a reorg mail. So instead of saying, hey, each test manager fill in this section, he said, here, I wrote up the following. Go ahead and corrected if you want to say something different. Man, did they change it right away? You can't say this about our team. This will hurt so it's those feelings and whatever. So it's I love that technique for getting people to do things. Yeah, but people definitely prefer to iterate off of a baseline than to get together and create a new baseline. Yeah, crazy. That's what leaders do. They can do the creation. Yep. Hey, we skipped it last week, but now that I'm warmed up, let's do the mail. Nice. All right. That's been four months, three months, man. Yeah. I, I, and I forgot to work on that when I was out. So, um, Hey, a couple quick things. Um, one is I mentioned Microsoft listeners and, uh, Ching Song Yao has been a good, he's been a faithful listener. He's posted on an internal Yammer and telling people listen to our podcast. So, so thanks. Thank you for listening and for promoting us. Uh, yeah, despite our silliness. So that's cool. And then, you know, we joke a lot about our three listeners, but guess what? Andrew, who, uh, I, I met in Australia when I was there, uh, two years ago for the software testing Australia, New Zealand conference, and then, uh, there was sort of an Agilish conference in Sydney. I spoke at, he said all the way in Australia, listen to our podcast. That's really cool. That's I I'm, I'm excited. So, um, so I have no idea how many downloads we have. But anyway, that's, that's pretty cool. Pretty cool. Hey, nobody's written a review for us on iTunes and iTunes. If you don't know what it is, is the really crappy product Apple uses to work with their really cool phones. Is that a good description? Yes. Okay. So you can write a review there, write comments, and we will, um, address you in the mail bag. Someone tweeted me, uh, right when I got back from my extended vacation and said something to the effect of, Hey, what would it be like if you took this sort of vacation every year, like a two month vacation every year, what would it like eight weeks of vacation look like when I first got back, I thought, man, it would be so totally awesome. And the vacation itself would be totally awesome. My answer is every Corona commercial you have ever seen. But, um, one thing I didn't know, this is by far the longest consecutive vacation I took. I took, I've taken two week vacations quite a bit. I took a three week vacation once, but I was gone for about nine weeks total. It's, uh, I'm just now a month back. Sort of feel like I'm, I know what I'm doing at work again. The ramp up coming back after so much time off was hard. Why? Um, well, for a bunch of different reasons. One, the first week I came back, I worked like shorter days because I had not on purpose had some other stuff going on, but I was exhausted after five or six hours in the office. I was just beat. I wasn't used to working. So I, there was a stamina issue. I had to work through it and then, uh, I came back to the same product team, but a whole bunch of stuff had changed. I had to spend two, three plus weeks just figuring out what was going on and who was doing what and what we were doing and what, and why we were doing it and all that stuff. So it was almost like you, you. I'm not rehired into the team. I may as well have joined a new team because it was the same sort of ramp up, except, um, uh, it was weird, maybe even a little harder. Cause I felt like this could have just been me trying to ramp up, but almost its expectation like, well, of course, you know, what's going on. You're working on the team. I said, I don't know what's going on. I'm trying to learn. And I asked, is there a diagram that explains this? No, everybody just knows it. And so that tribal knowledge, that, uh, all that's lost on me, that stuff you get from context from being involved in the team on a regular basis was just gone. So I ended up being the person to ask for, is there a dock on this? Is there a picture of this? Can someone draw me a picture of this? Uh, just to try and figure out what's going on. So the ramp up time was, I think something I'd not want to go through every year. And again, our team went through a bunch of changes. It may not be the same, but, uh, getting used to working was a bigger challenge than I thought it would be. It it's, it's interesting. One of the things that you just shared with me reminds me of one of the consequences of, of the layoffs and you and I have a meeting, uh, I think it's Wednesday talking about, um, the new sort of corporate wide, uh, approved model for knowledge sharing amongst communities, right? The, the communities that we've been, we've talked about several times on, on this podcast are now officially unfunded. Um, I think it, and to be clear, I think that means I'm not sure about, uh, communities you're involved with as our community will continue in a different form with no corporate sponsorship. QTech? Quick. Okay. Long story. Anyway. Yeah. And, um, what? Yeah. You said, what, uh, internal communities we have for, uh, are based around quality. Well, there's three that, that I'm active in. And I know that you're in at least two of them. So I don't know which one you're. Yeah. Um, I don't know either. Yeah. I'm a thought that far ahead, but more stuff to ramp up on. So that it doesn't seem like a complete non sequitur. Like it seems like, um, we still have a long ways to go to sort of, um, breed out the, the tribal knowledge way of doing knowledge sharing. Yeah. We're, we're not good at knowledge sharing. I think, and I think you talked about learning earlier to loop back to that. I think from there you go to how, how do you acquire knowledge? I have a big believer in Philip armor's five orders of ignorance and the fifth orders for fourth order, you need a suitable means to discover what you don't know, you don't know. That is a critical part of knowledge acquisition. And I think good communities are a great way or a primary way even to solve that. One of the, one of the approaches. So we now have a very active, um, Yammer group for my team. I'm going to go totally stock your Yammer group. Knock yourself out. Cause I, I, the only like totally active Yammer group I've seen is the one that complaints about things. Oh, um, there's several that I'm on. It went and totally active is, um, relative. All right. The, um, but one of the words I keep finding myself saying over and over again, once we've lit this up is posted on Yammer, please don't send me an email. And I'm very tempted to take up. Yammer's I now completely get why Yammer's doesn't respond to email. If you send anyone in the Ammer group and email, right? They'll, they'll not do it. Most people view it as annoying, but now that I understand what it really is where you, you capture knowledge. Well, we're not effective. Uh, I hate to pick on ourselves too much, but again, because the company's growing, we've done, I think, maybe I should characterize all of Microsoft. Let me give you a story about our team and we have not grasped the connection, the correlation between communication type and communication and method. And to give you a hint, I went to a all hands meeting, got the whole team in a room and when I have an all, if I was hosting an all hands meeting, it's about interaction and discussion. You want to take a look, you're leveraging the fact, you have the people in the room, let's let's, you know, whether it's roundtables or Q and A or something, let's do something we can't do over another communication mechanism, but it was all information sharing. They could have sent me an email with everything in there thought, wow. And yet then they send email trying to start a discussion. Said, no, it's backward. It's backward. Come on. And to me, it drives me crazy because just so obvious, but the way we've grown up and, you know, again, I'm not sure how unique Microsoft is in this aspect, but we are email centric. I think it's entirely, it's much like everything we've been talking about. Right. When I send an email, I want the answer. It's sort of local optimization. I'm not considering whether or not there exists someone else that might want the answer as well. And how am I helping to coalesce this needed knowledge in a way to make it easier for others, right? And in the past, it's been challenging to do like I have no idea what I know that Alan would like to know as well. Right. But if I start posting the, these things when, when they come up, everyone, every once in a while, maybe I can get Alan to, to recognize, Hey, there's one out of 10 things that Brent puts on his Yammer group is actually interesting. It's worth my Alan's time to scan those emails for, or not emails, Yammer, what it, Yammer posts for that knowledge. Yams. So I don't, I don't know. It doesn't matter. I don't approve. We should probably, we could probably have a much, much longer discussion on, on knowledge acquisition and information sharing. It's when I'm bringing up in, in my internal group, when I bring up sort of the importance of knowledge sharing, it's not inherently obvious. I have to go in and explain it a great deal. And I, I think much of what we've been talking about over and over again, that this is really a systems problem. People aren't really, they're seeing the short term benefit at the cost of the long-term benefit when there are, when there exists today's, today, uh, ways to do both simultaneously. All right. I want to talk more about that, but not today because we're at a time. So if you listened this far, thank you and keep the questions coming, keep the comments coming. Happy to hear from you. I'm Alan. I'm Brent. 
