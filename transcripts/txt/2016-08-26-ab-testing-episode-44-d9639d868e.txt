Hey everybody. Hi. Hey, I'm Alan. I'm Brent and we are here for episode 44. Baby testing. How are you doing Brent? I'm doing swell. All right, great. How about you? I'm doing swell myself. Also swell. We are in Brent's building, the palatial offices of Microsoft Building 22, which I did not bring this up in the pre-show notes like we plan. But this is the very first Microsoft building I ever came to. Really? 25 or more years ago. I did not realize this was a Microsoft building that long ago. It has been a Microsoft building that long ago. I was a bicycle messenger in Seattle. I did not ride my bike all the way over here, but part of our bicycle messengering was involved picking up something in a lawyer's office or something downtown, coming back to the station, getting in a car and driving it somewhere in the Northwest. Was this a Microsoft building back then? It was a Microsoft building. Because the rumors have it that this used to be part of the group help, a group health. No, I remember when I pulled it. I hadn't been in this building probably since then and I pulled it and I thought, no. And you know, believe me, I have read all the studies on how memory is fallible and I could be completely wrong, but I'm reasonably confident. This was a Microsoft building back then. So what we've learned before we moved into this building was that this was an old medical facility. And in particular, floor one used to be the morgue and floor five and six used to be the psychiatric ward. This is like the Arkham Asylum Dork Edition. Right. It could be coming up right. We're on the insane asylum floor right now, which I find appropriate. Yeah, could be. Could be. So shall we get on with the episode? We can try. We can try. So the theme of this episode is data driven. Data driven-ish. Data driven-ish. Yes. So we've gotten feedback from many of the three. I guess that would be. Many would be at least two. Right. Two of the three. That, hey, this data driven culture, this change that you guys have been talking about for a long time is interesting to us. How do we do it? So I thought today has a good theme we should talk about as a tester or even that's we could even potentially go further back and just say as a black box tester, if I run a black box test team, how can I get into the data game? Does that seem reasonable? We can start from there. So the premise is like my org, we don't use data. We don't believe in data. How do we turn the corner? We have I know an executive mandate from now on this org is data driven and then it's just done. Right. By definition. Yeah, that's it. Okay. So all you got to do everybody. Yep. We'll be back. We're going to keep going. So one of the challenges people have by first and foremost, they don't know what it means. There may not be buy off. There may not be data. You may have a culture of where I find pure intuition riddled. You'll find executives or program management. They're all sort of Steve Jobs people just asked them. So these are sort of the challenges that people in this arena are facing. And as a side note, because this show is not a show without tangents to give you the name later, but I was in a meeting once with a a manager who is known in circles for not being particularly awesome. And he I had to bite my tongue because he pulled out the to make his argument. His point was to make it to prove that his point was right because it's what Steve Jobs would do. Which deck of geeky I'm awesomeness playing cards you have you pull out the that's what Steve Jobs would do discussion over. Yeah, there's an equivalent one that I've never encountered that blatant of the card being played, but I've encountered similar ones, right? Like there's a rule in these type of meetings and it's generally I'm making up numbers, keeping the number theme alive, but generally the first person in a debated meeting that claims guys, we need to do the right thing for the customer. The first person to do that is already winning the argument. Everyone else gets on the defense. So it's like the Trump card that wins all the intuition battles. You know what, Brent? Yeah, screw the rest of the episode. Let's make let's make ourselves a engineering game. It's like a card game or like a magic thing, but instead of like the stuff I don't understand, we have cards like but that's what Steve Jobs do. What would the customer do? Oh, plus five. We'll figure out the rules later, but there's there's something lurking there. I haven't played card games in a long time, but this sort like magic the gathering. I used to be all about my I know nothing about it, but a big game player board games, card games, etc. And I'm thinking like the goal is to ship the product and or is the goal to have the customers like it? You decide. I don't know. No, the goal is to get the better review. Oh, shipping the products is entirely irrelevant. Yeah. Okay. Let's rewind the stack. As long as you can blame all the product failings on someone else and you get the better review, it's a win. Brent. All right. Winding back. Yeah, winding back. Once upon a time, we were talking about data and moving and moving your team culture or to use data more or something like that. Remember that? This episode? Yeah. Oh, yeah. So let's do that. All right. As for first time listeners, I'll go back and I'll talk about sort of the three phases of data, data driven cultures of the team. First and foremost, there's data oblivious. Hey, we are populated with Steve Jobs. We don't need no stinking data. It's all driven by intuition and customer feedback. Or we don't need data. I know what the customers want. Yes. That succinctly is the problem space there. And then you'll hear variants of this like, I have done this for the last 25,000 years. Therefore, I know. Absolutely. The Experience Card. In that book that I wrote, I talked about the Hippo and I knew I didn't invent it, but I could remember I finally just figured out recently where I found that out. Roni Kohavi came up with the term, the Hippo, the highest paid person in the organization. Did he really? Yeah. That's like worldwide now. It is. It is. It is. It is. And he has the first document and I weirdly and oddly indirectly also, I was one of the first people to grab onto that because I used it in the book not much after he talked about it. But the point is we've all been in orgs. We've all been in orgs where the highest paid person in the organization, the Hippo, decides everything based on a lot of intuition because they're experienced and they know. So the wine back there is, I'm going to give a more objective reason why that is true. Number one, in a lot of these organizations, they don't have data. Number two, this is a guy who's being paid a buttload of money to be accountable for the decisions. Absolutely true as well. And it can be a risk to these folks to be accountable and I'll make up a term, over empower. I think you're right. I think that's one of the biggest obstacles to moving towards a more data driven culture is getting over that. Not entirely their fault. It's a tough thing to get over. If I was ever the Hippo in the room, I know that I'm stupid enough that I would want to use data. That's just me. I would as well, but for me, it would be critical to use data. The biggest issue is, again, in the market today, you don't have a lot of free time available to be wrong. So you really want to be able to fail fast and fail cheap so that you can quickly adapt and then go into a better direction. In today's market, that's a much better way to mitigate risk. We can probably move on. I think we've beat data oblivious. I think it's pretty obvious to everyone. We will need to talk about how do we get that executive to buy off. But let's first off, to go from data oblivious to data driven, you have to have data. Yeah, exactly. Because it's interesting you brought that up that way. How do you get them to buy off when every time in my career, I've had to convince an exec of some change, you use data to show it? Right. We have a little bit of a catch-22 here. I have been in situations where you have to try to convince the executive or the executive surrogate, which is often a challenge, right, to invest in the data where they don't have any data. And that's a challenge because it's viewed as, wait a minute, I have to deploy a multi-million dollar data engineering stream and spend three years before I even have a data point? No. You don't. But that's kind of their fear. That's their worry. That it's going to suck all of their budget and all of their engineering hours to have a stupid chart. Right. But if they have the stupid chart, they can say they're data driven. All right. Yeah. Brent's making a face, but that was my awkward transition into what does it mean to be data driven versus data oblivious? Data oblivious is, again, you're not relying on data. You're using beliefs and derived beliefs from customer feedback. Data driven is essentially intuition validated by facts. You're using both of them together. You rely on the intuition, but you find ways to prove it before you lock down the action plan. On the data stuff, so let's imagine, let's do the way back machine and imagine we're on a product. Let's make it a non-service product. And we're black box testers. Okay. How do we get into this game? How do we, obviously the first step is getting data. Where can we get data? Subjective data from customer feedback forums and from customer support? Yes. We can still collect telemetry off of our application. It's just slower than with a service. Yes. We have bugs. Bugs are data. Bugs are data. We can measure perf and have perf data. All kinds of things we can measure and have data. So, if we had, so I'm going to step out. So, I was talking about black box testing. If we had a white box testing where we had an automation suite, we have a lot of data potential from the output of the automation, perf test and all that. Stopwatch testing, that's slow. The very first thing on the black box test is you need to be able to figure out how to automate your tests as well as automate your data feeds. Let me back up even more. Just to be clear, black box doesn't equate to manual. True. Because I have done plenty of black box automated testing in my life. Completely agree. From my experience though, a pure black box is a UI tester. Perhaps I'm being. All right. Maybe you should have said manual tester versus black box tester. So, even with manual, right? If you're a manual tester and you're on a product and your product doesn't have some form of a diagnostic feed for dev, then really your product has so many things to work on. It's probably not time to begin this journey. Sure. And to further dig in this tangent, because I think these terms get a little confused, I can do manual white box testing. Yes, you can. That's code review. That's stepping through stuff in the debugger? I've been in the past. I think it was last episode we talked about code generation tools. Yes, it was. It was. One of my first patent applications was a tool that essentially, in my view, was likely the beginnings of reflection in the .NET library, where we had a UI, you pointed it to a comm library and it generated a UI for that API and now you can ad hoc away on this artificial UI. But let's go back to, all right, so we have data, hopefully, communicated by the developers into some sort of log feed. Got it. Right? Okay. So there's multiple places to get data. The other one, four test teams in particular, test teams that have automation. One of my most favored strategies to start getting into this game is change your automation. And you change your automation to, the typical automation will perform actions and then do a validation. Generally, doing a validation via some actions against the product itself, using the product as a deriving state. That validation, I recommend people to not do it against the product, but change their automation code to validate against data emitted from the product. What's the difference? You lost me a little bit. An example would be good right about now. So let's say you're doing performance cases. Okay. Okay. And you want to know page load times. Sure. Okay. So. From my non-service app, from my web app. Yeah, I guess it wouldn't be page load times. So transition times from screen one to screen two. Perfect. Okay. Just keeping you honest. Change your automation. Well, first off, your automation probably already has automated the transition from screen one to screen two. Correct. The practice that I did back in the day, and I haven't been a perf tester for a long time. We're all perf testers. Was essentially you would set up a timer. You would start the timer before you perform action one. And then when you finally did the final action, you would wait until the load event completed, and then you would stop the timer and then you record. Then you do math. Right. You have a transition time. Right. What I'm saying instead is log the load times into your diagnostic feed. Using the same math? No. Oh. So here it's. Oh, I see. So the. You have the developers say, so every time there's a transition, you have the developers log the start and end events. Okay. So that's no longer part of the test. It's part of the. The data. The application logging, the data emitted from the product called logging. Okay. Yes. Now you in your validation of your perf test. So your perf all your automation becomes load generators. That's it. But when you want to go and look at what was the time you have the validation part of your code. Go to the data itself and extract the time from the data. All right. So let me see if I compare phrase this old school was I get the tick count. I get the time I do my action. I wait for it to complete. I get the new time do math. Now I have the load time. I can track that in my own stuff somewhere and then I have load times old school. New school is just write the automation knowing that. The application logs all those times and you can use those times rather than rather than calculate them yourself. Yes. Now in some cases you may need to leverage a hook in the application where you pass a unique ID. Okay. So let's back up a second. Let me just talk about what I understand as the Brent's playing genius here. I'm playing data idiot because I'm pretty good at it. So the advantage here is is now the developers on the project as I see it. Now see data generation about the product as part of what they do. Okay. Step one towards that. They may. I mean in some cases they'll go what just do it in your automation. Right. So the way I've done this in the past is I've essentially taken my guys and they've changed that instrumentation because a lot of dev it can be very reluctant and resistant to adding additional instrumentation that that isn't inherently useful to them up front. They're like wait I got to add all of this code to the product when you could just do it inside your automation without me. Okay. Right. What's an example of a place where the developer may have actually have an answer for this. Okay. I think we're a developer would also want to add logging in addition to performance. What's another place where. Logging like air conditions. Sure. That's what I was saying obviously. Yeah. I'm driving but driving a blank. No I think I think it's actually a good example. I think if you think about any of the things that traditionally gathered for web apps and services via telemetry a lot of that you want to of course avoid the coverage. You don't want to log everything under the sun. Now on line 10 now on line 11 now on line 12. No. No but I have seen teams go over the top logging too much like I'm in this function. I'm calling this function. I'm back from this function. I'm exiting this function maybe over the top. Yeah but there are if there's things you want to know about how the customers use the product you can log this. Yes the key secret here for lack of a better word is number one. If you're a tester and you're trying to get into this game like the first thing adapt what you already are a master of. So you already know how to drive the product. You already know how to test. You already know how to find bugs. But now what I'm suggesting is find bugs via the data being emitted from the product. So if your dev has log streams master the art of using those diagnostic streams to find bugs. Okay change your automation to use that same data feed to generate true false on your test passes. So that is interesting because one of the most difficult things in automation to do is the Oracle. Yep. I've done action A and action B and now C should happen but often verifying C is very very difficult. You just do action A action B action D action F and then analyze the log files to see where the errors may have occurred or if things worked as expected. Yes. So I have a calculator app. I'm going to put right on the spot here. I enter two plus two equals. Do I verify that four shows up in the output or do I just count on the logs to verify that's correct. I would count on the logs. You could. I mean that that's going to cause some consternation because hey well it's entirely possible that they write four in the log and write five in the UI. Right. It is and that's something that this is a shortcut that I recommend taking because the data stuff is far more valuable. But that UI validation leave it to the dev. If if again if you are on my my view is if your dev team is moronic enough such that the there is a actually a possibility that what shows up in the log file is different than what's in the UI. You have bigger problems and that's at least another two episodes of it. I think it's a little bit of a trust but verify because you're going to have it's not like you're good just like automation isn't going to eliminate the needs for humans to look at a product with the UI. You're still going to need to look at it. Yep. Like why did all these actions and the logs don't tell me the screen turned pink. That's okay. You're not going to release this without at least looking at it and and furthermore doing some actual testing on it. Right. And then if you do find that hey I'm in calculator and I add two plus two and it comes out to five logs as four you're going to notice and you're going to root cause figure out who is being an idiot developer on the team. There will be there will be another process right an easy mitigation for that is you you build up your your suite based off of using the logs. You create an adapter to your automation that could actually drive it from the UI and then you infrequently run that UI only pass as a validation to make sure that the logs and the UI agree. I would never do that because I would simply go to the dev team and say look guys if that's something that I need to worry about then I want my test team involved in every one of your code reviews because you're just telling me that your concerns around the quality of your craftsmanship are so low that I need to pay attention to even comments. All right so in a nutshell one step towards data driven is to move the Oracle to the output emitted from the product versus something you try and do in your test. Yes and the reason why we're doing that is number one people already find value in results of test passes and finding bugs. All we're doing is we're making a change in the system to get that value from a data feed. Now the nice thing we were talking about sort of an on-prem product but the nice thing is you build this asset on a service product say on pre-prod. Now once this bad boy goes live now you've already built up a facility to use the data from customer generated actions to prove true and false on your on your tests. Yeah if you can pull this stuff off on a desktop app. Many parts of it get much much easier when you're doing a distributed app, service app. Yeah and if even if it is a even if it is a desktop app right if if you as long as you have some form of a log shipping mechanism once this thing goes live you've already garnered the skills around processing customer-derived telemetry. Sure right you may not have all of your test suite because you know the the instrumentation costs can become expensive as it relates to CPU processing. And one thing I actually don't know enough about Mac and Linux to know I assume they have equivalents built into the system but on Windows you have ETW tracing which it's don't brag a lot about Microsoft but one thing I'll mention it it's a nice it's a nice logging mechanism with APIs that is very performance non-intrusive. A lot of devs will worry about if I'm gonna write to disk all the time our app is gonna slow way down. Right so you want to that may be something you have to consider and look for mechanisms either open source or built in the operating system that allow you to do a lot more extensive logging and then you're probably doing now without an impact on performance. There is a challenge so I do know this there's a challenge with Apple they have very hard to consume APIs to get to think like the equivalent of a crash dump information. Not surprisingly Apple kind of runs their OS like like they do many of their products as a as a walled garden like there's there's act there's places that you're allowed to access in this places you're not allowed to access and a lot of the interesting information is in the places that you're not allowed to access. Linux I haven't had any direct experience with Linux but I got to imagine like Linux being an open source platform I got to assume that they're probably easier than Microsoft on on getting this type of information. Yeah just trying to address a potential feedback bit of feedback you might get like we can't log everything but there are ways to do it that are going to be non-performance intrusive. And you don't need to log everything but these making this move will be logging a lot more than they have before. They will and as they gain success they'll also gain knowledge around what information is useful to log to really move things forward. Right. So what else does it take to be data driven? So that's that's a I love that I'm going to use that what else what what else makes me data driven? So what we've really done here is we just got we got the test team starting to use the data okay for work and value propositions that they're already accustomed to. Is that enough for us to get executive buy-off now? No. No not at all. No but what we've done is we've got the testing getting used to the data okay and we've now mastered using the the the data for our test pass work to derive customer behaviors in a in a very high level way right. As we just discussed that this thing goes out to the world we now can look at the data feed and go oh this is what customers are actually doing on the product okay. In the limited scope as it relates to what we've done to support the the test cases right. Now what we need to do is convert that into new value propositions. I want to talk about my North Star so I'm going to go way to the data centricity thing because it is I think it's useful to say instead of as we walk you know step by step along this path where the hell are we going and the rallying cry that I use for my team is essentially deliver the right knowledge at the right time to the right people that accelerates actionable and valuable decision making. Okay so the first thing that we've done is we started using the data the the first step towards that adriven is create visibility where none exists. So we've created a brief element of visibility around customer utility. Okay next thing we need to really focus on the next two things is using that data in a much more frequent fashion to drive actionability. So now we've been using it to validate the test passes. Yes you can ship no you can't okay. Now we need to go we need to take this data and use it to drive decisions on the product. I'll give you an example of something that that I did when I was in the early phases of this. This was during my time in Xbox. Okay I am. Brett and I overlapped in Xbox for about a month? Two? Maybe two weeks it was really. First first thing when I got there I said this guy's got to go. And it was positive that I was already on my way. Yes yeah um yeah so from Xbox I went to to Bing but in Xbox one of the things that I did there was held my own against the PMs in terms of their intuition. There was this one app that we were doing for the Kinect. No longer remember the name but it was intended for essentially teen literally the target audience was teenage girls and the belief was teenage girls would like to use Kinect and Xbox so that their avatars could chat with each other. So it would be like you and I going into a chat room. Yeah and I wish I could translate a face palm into a podcast comment. Do it harder Brett. Now what I started doing so I didn't own this but I hated it. I thought it was completely stupid so I started asking precision questions and getting data. So this was at at the beginning phases. I was one of the first people on the the Xbox team to use the Microsoft big data store. Xbox had been emitting data to this big store forever. Cosmos. Yeah yeah. But I was one of the first people to actually use it. What Dev had been doing was just stuffing data. PM had been adding sections to their spec that says this is the data that must be instrumented. Checkbox development. But no one actually had any plans to use it. So I was one of the first people there. So is there a step now between data oblivious and data driven. It sounds like there is an interim step data aware. I guess so. I'm just going to add that my model. It's like we're not oblivious like we know there's data. We don't do anything with it but we we know about it. Right. Oh yeah. Yeah. We have data look. It's all instrumented. Terabytes of it. Yeah. What do we do with it. We don't look at it. Oh god. That's too much. Yeah actually and that was part of the problem because it was too much. So we had. Let me not go into that. Sorry. Anyway so I I looked at once we started going to beta. I looked at the data around who's using it. I spent I looked at the org chart of the team that was developing it and came up with a rough heuristic around cost. And I basically showed that this is stupid. We were paying a huge amount of money not counting the technical the the technical assets we had to pull the bear when we shipped this on life site because we're talking about these constant data streams. But essentially we were paying just a crap load of money per launch. So someone launching the app how much did how much did we pay every time they launched the app. I did some basic research. I really didn't understand data like I do today. Like today if I were to go after this it would be a much different story. A bloodbath right. Yeah. But but I was trying to then go OK well what are we going to gain from this. Right fine. We pay a large amount of money per each launch. But that's going to. So when everybody who launches this what are they more likely to buy from us later to. Obviously the app isn't going to convert into profit. What will. Nothing. I went and talked to the team and it was it was from the executive down. So your intuition said this was a bad idea. So you use data to go help prove that point. Right. He ultimately ended up shipping it and then about three months later removing it from the product for the card game we're developing. Yep. There's an I told you so card that you can play at this point. I told you it would fail. I would. I would play the fail faster next time card. That would be my equivalent of that one. Sure. Now. So if we want to get the right information at the right time for action ability. Now what we need to do is think about what does. So let's talk about buy off. Let's spend the rest of the time talking about how do we get executives buy off. We have not the whole rest of the time because I want to get more into what's beyond data driven data centric data centric. This probably another podcast. We only have 10 minutes left. But how do you get by off. So I'm getting data. You're getting data. I'm getting some insights. I feel like hey this is I'm getting the hang of this a little bit. Now how do I get buy off to go even farther from the higher ups who may be leery of what I'm doing. Because damn it Brent. I just want a red yellow green test pass result and I'll make my ship decision based on that. That's enough. You don't. That was my point here at boss. So that's what bosses are accustomed to. Let me ask you Alan. Brent. Where do. New ideas come from other ideas. How they meet and they fester together. They coagulate and they have they sometimes they love each other very much and they make new ideas together. So when two ideas really love each other. Yeah. All right. So let me give you a new way of viewing the the data science buzzword insight. OK. An insight is an idea buried in data. So where do new insights come from data getting together making making making new insight babies making data babies. Right now. So what you have to be able to do is take the data and connect it to something that's valuable to the executive. So an executive says if you can sit down how do you get by off. You go. Hey look at the success we've had improving the the automation the proving the accuracy by leveraging this data. And we did this data investment because now we have visibility to how the product works in a customer reality much better than we've ever done before. Executives will have an intuition view based off of their customer segments. The way I have done this in the past is that you can get the executive to give you a clue around their business KPIs around how they want to how they're trying to sell this to customers how they're trying to grow usage. What are the key goals in his review. These are these are the things that they're thinking about when they lock down control around how they want the product to function. What you need to do your ultimate goal is to say hey I can help reduce your risk and increase your speed towards your goals and you start small. Before you go and present that to an executive you need to figure out a way and honestly you may need to do this via the dark matter factory. It's a term a buddy of mine uses up meaning I just do it and I go dark and I don't tell anyone. Okay right because if you have to articulate if you have to argue the biggest challenge with getting an exec buy off is you're asking them to make an investment and take a risk on something that no one in the organization knows or understands and no executive I have ever met will ever do that. To me if you have to show it's less risky. Yeah and honestly if you've done it right up to this point if you've got enough if you're at the point where you have data permitted from the product showing that you can get insights. Yep. And you can get the wheels turning on that exec just a little bit like so. So what's something you want to know about what's important to you. What do you want to know about the product. They're going to go. Well I want to know blah and blah and blah or A and B and C. We can measure those through data. Let me show you how and then we talked. I'm going to show it to Keeler for me the second here. We've talked about this before but I think insights are addictive and the moment you get someone spinning going I can use an insight data idea buried in an idea buried in data. Is that right. Yep. So once I have one of those it's like oh I want another. So if you can just get them hooked a little bit and which again if you've gotten to the point where you have enough to prove to them I think hooking unless they're actually the pointy here boss from Dilbert is not a stretch. And once you get them hooked you got them on your side. You need all you need is so unfortunately this is context sensitive. So every executive and every business has their own different set of KPIs that they worry about. Yeah. You set up and say hey I want to try something. It's nothing you need to worry about. I'll figure out how to fund it. None of the plan will be at risk. But tell me as it relates to the product what is your top of mind. What keeps you up at night. What are you worried about. OK. It could be I don't have enough developers. Do I have the right features. Am I going to make enough money. Whatever. OK. Hey I can't go fast enough. Generally you can apply this data or the data in some way towards the goal that he's looking for. All you need is one of two things from the data which again by now you've you've learned to use because you've you've changed your tooling to live off of it. You need data that shows he's making a big risk or she or you need data that says hey wait a minute. There is this huge opportunity that the data is showing that we're not taking advantage of. And I find these things are easy. Right in my office right now I am probably 75 percent the way through a data insight. It's one of my favorite things to do is show the executive team that the product does not work the way they think it does with data. And every time I have done that the business has changed. There are often really simple insights that that people beliefs. Let's go with that one. There are often common beliefs. Sorry. Common beliefs within a team. So when you're in a data oblivious there's intuition going running rampant. Right. You've got the Steve Jobs character at the top saying this is what our customers want and you have developers generally marching to that drum because they let's say it's positively the distrust that the PM knows their stuff. So pick one of these beliefs that you personally find likely wrong. Or right. Or right. I find actually I'm going to take that back. That you find likely wrong. Because one of the things that tests are very good at is investigating wrong. Investigating broken. So leverage that as a strength. Then go through and see if you can prove or disprove. Now here like the key principles number one we talked about the very first thing you're trying to do is make the invisible visible. You then need to drive action off of that and you also need to target interesting. Okay. And the interesting is what you're going to target is those things that we just talked about that the executive is going to find interesting. You're looking for things that will cause the executive to realize that he's taking a risk or that there is a better opportunity. Either one of those good. As you've mastered that you're going to want to speed it up and simplify it. They make it so that they don't need you. But that's more of the data driven to data centric transformation. But truth is probably the hardest part of this. How do you make sure you're telling the truth. Even though it may not be the truth people want to hear. At these beginning phases making sure that you're telling the truth and not so coming even to your own subjective bias. Right. Hey PM says this I believe this I went in the data and unbeknownst to you you did what's known as confirmation bias. You proved you were right. And you did it in a way by only loading in the sample set that proves you're right. Right. As you go through this journey developing truth as a key principle is critical because that's how you eventually win on this on this data culture shift and build trust. Okay cool. And that about wraps the A B testing episode on becoming data driven in almost data centric. We'll get that next time. That correct. Yes. All right. Hey everybody. Thank you very much for listening. I'm Alan. I'm Brent and we'll see you next time. Bye. 
