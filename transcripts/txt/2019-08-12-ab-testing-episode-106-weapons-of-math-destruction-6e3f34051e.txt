Welcome to AV testing podcast your modern testing podcast your hosts Alan and Brent will be here to guide you through topics on testing leadership agile and anything else that comes to mind now on with the show. Hey everybody howdy hey hey we're not alone here we have a guest yes how many of you remember the last time we had a guest but this episode has epic sound quality Steve say hi hey Steve remind everybody who you are I'm Steve I've worked with it right now and before I've been on the podcast once or twice but before they tell you to go back and listen so they're trying to cut me out I'm a data scientist in the windows team do you have the data to back that up I don't really you're just gonna make I tell you my p-value is really low though we can we can cut out that comment right I can do anything I want yeah I can glue words together to make you say things you never thought you were saying yeah so we do refer to a lot of to a lot of folks to start in the late 60s and Steve while officially the most frequent guest on the show hasn't been on since we sort of started telling people go back to those times so he was part of the discussions when we were forming up and helping to frame MT and sort of guiding people towards this new world but we haven't had him on since since we formalized all it's been a long time I've been listening it's been good it's been good to have discussions so that's that's good we covered that the the boards behind me I think it's a day when I sort of facilitate a conversation I have a bad feeling because they're gonna use math things and we're not gonna talk math oh good okay I worry when I get in a room I work with a lot of data scientists now and whenever yeah I do nice and when I talk with them they start using a lot of things phrases and words that remember when I used to be able to fake data science I can't I can't do that that now yeah no I'm just not you guys keep invent words faster than I can fake my way into understanding them you just say what's the p-value for that that's all you have to do no matter what they say yeah is that significant and then you're then you're good to go oh oh good all right did you calculate a confidence interval oh yeah I just reread a chunk of how we measure anything the Hubbard book so I have a little bit of math in my head which is where I've gotten some of my fake data science naked statistics actually is a really good book if you want to start beat strapping yourself into I have children in my home I can't read naked books that's just poetry it is fairly rated G even though the although although getting getting naked the Pat Lencioni book is not at all what I thought it was gonna be about but it was still very good you were very disappointed it depends he's he's looking for books to rationalize an odd behavior he already has it's a negative still a lot same need so okay disappointment in what in the title and one last random awkward book conversation I recently I always have a copy of every read it every copy of how to talk so your children will listen which is what I don't use it on my kids these are people I work with okay works great so Brent or Steve the BS of this podcast what is our topic for the day today we're gonna talk about ethics and ethics related to data science and machine learning yes we had a listener who posted a we could have almost called this a mailbag but I was looking at the thread and there's no question there but he did title the episode today weapons of math destruction surely that's been used before who cares not on a B it's not a Spielberg movie maybe not yet yeah it was one of the early ones from the late early 80s who should play me in the movie Macaulay Colton that kid who played in silver spoons oh sure the blonde guy yeah yeah maybe we should get back to ethics rotor I think right that's him right oh and tangent that wasn't if that wasn't no no tangential question I guess does society in fact need a reboot of 90210 oh you know what I watched like five minutes of that and I thought this is even below my standards okay so so I think the answer is no point in the room I haven't seen it I saw yeah watch most of the original which you know I probably shamed up these days same same which is why I and I'll edit my comment out but yeah yeah I thought oh I wonder what these people are like and they're potentially even worse actors than they were yeah 20 years Jason Priestley is in some new movie from 90210 right I blocked it out but I thought I'm like hey he's still acting I'm shocked no here's what I think is because so what's the what's your confidence interval on that 97.3 okay just checking when those shows came out and I say those shows because we haven't talked about Melrose plays for good reason I think all three of us were likely in various degrees of early on relationship with our spouses and probably fell prey to the oh let's spend time together and then lost the TV choice no no I did it on my own oh did you really yeah yeah so yeah I'm home from work I think the first show I remember watching together with my wife when we were first dating was the Sopranos that's a way to build a really yeah yeah I was I was I was leaving me and here's what's gonna happen all right shall we get into yes a little bit further okay yes what it what what is it what what it maybe I'll ask later what is it we're talking about what what do you when you say ethics and data science what does that mean what are we discussing oh we'll probably talk about things that pissed Steve and I off oh yeah that's something that you're comfortable with so things that pissed you off are ethical or unethical I just want to get my story straight okay let me ask you so you just told a story but you're working with a lot of data scientists sure okay my view of a data scientist's job is ultimately to automate the decision-making process yes okay but one thing that I see over and over and over and over and over again is unfortunately data scientists are human they are driven by intrinsic incentives like hey I want a better review this year than last year that's only a Microsoft that but they have the advantage that in the in the software world today they have the advantage that no one knows what the hell they're talking about I have seen the scenario you've played out over and over again where you have a data scientist comes in talks to business leaders throws up three slides of math equations which makes everything clear of course no data scientist outside of that team in the room starts walking through it and you could see the business leaders eyes glaze over and the final conclusion of oh great job keep up the good work right and without the individual the decks that I see then afterwards I'm like hey wait a minute what's your test score on this like great you you can describe a bunch of math stuff which by the way is almost always up on Wikipedia anyway you don't need to put slides in there but this is slide driven development though it's been proven to work yeah it gets it gets permission all the time so I think the big thing about ethics and data science is that you have a tool that's not understood by a lot of people like Brent says so the implications of that aren't always clear but even when they are there's a lot of things you can do right this comes down to the spider-man principle right with great power comes great responsibility and there's a lot you can do with data science to make decisions or to find unique insights that otherwise aren't there and the question is should you or should you not right probably a lot of our listeners know of the target incident from from many years ago where a girl's father called up the manager at target said why are you telling why are you sending my daughter a bunch of pregnancy test information or a bunch of pregnant pregnant woman information right things for right it was like it was like diapers and formula yeah and and you know how dare you insinuate that of my daughter and the manager apologizes and then a week later that the manager calls back to say hey I'm really sorry that's like no no it's okay there was something going out of my house that I wasn't aware of your algorithm was your stuff was correct I apologize for yelling at you but should target have targeted somebody in that way or not is the question and that's the kind of question we're gonna be answering today ah yeah that's a good question because I work with the data science team I work with I work in ads monetization we're trying to give we're trying to benefit both the consumer and the publisher by showing them the right advertisement when they want it and yeah how do we know that we our formula isn't is accurate but not unethically actually just not unethical data science is a powerful tool if you align it to MT like one of the things that we're trying to do is is help businesses be stronger by priority number one principle number one our goal is improving the business go on by servicing their customers in in building quality solutions to customers problems and data science is a powerful tool but it can be it it is very easy to remove the customer satisfaction part of that equation when working through your models right it's easy to think about how much money you could raise if we could target somebody for this particular thing even if maybe they don't want to be targeted for it it's easy to forget about that just to make the money but principle five says that the customer is the only one capable of judging the quality of our product right and part of the quality is their satisfaction and the their satisfaction with the ads and how creepy you seem is all kinds of things are gonna affect your long-term potential as a company even if the short term you can make a lot of money out of targeting somebody for something they may not want you to know about my hyperbolic example is so we have a lot of vision recognition we have we are shrinking cameras right no one talks about Google Glass anymore but that's I think that's a technology that perhaps was a little before its time but that'll still push forward right uh I know Steve and I both wear glasses I know I certainly would would think it's cool if I had the ability to have a near real-time heads-up display in my my eye wear as long as I didn't you know add another pound on top of the bridge my nose yeah at some point somebody I'll manage to get a hollow lens shrunk down small to fit in your glasses and have a long battery life and then things will be very interesting yes for example mentalist you guys know what these are a mentalist like that well I can read your mind the people that wasn't there a TV show called the mentalist that I never watched there was okay yes same thing yes should I pause I'm gonna boot up Netflix wait can I get your Wi-Fi password nevermind okay the what they are very good at is seeing micro movements in your face and they are able and they have determined patterns so for example I don't I'm not a mentalist but if someone asks you a question and then that if that person doesn't look you in the eyes answering it the direction where they look if they look up or decide it's known that it's probabilistic what they're doing like looking up I remember this one looking up when they answer generally means they're inventing an answer and they're well trained to observe this but an automation computer vision with AI with these type of glasses honestly we're not far off from you're right yeah I can wear my special glasses I can tell if you're full of BS or not right in what happens in a world what happens in a world when everyone knows that everyone else is lying mate isn't that what we call the press today yeah what would the I mean that would completely revamp the dating scene completely there should be a show about that and it's fine and fantasy but I but I do think it's sort of a it's sort of a version of uncanny valley I actually think you know the phenomenon that the where what is it social media bullying it has a clever name cyber bullying yes thank you I'm here to help where where teenagers one of the problems is on the site on social media peer groups will say on social media what they won't say to your face that's been true of email since I first used email yeah but the friction's a lot smaller yeah the the the feedback loop is faster right there's always been the phenomenon email where you can't tell if somebody's yelling at you or if they're all caps man I'm like yeah it's very difficult to get sarcasm out of an email you put a sarcasm taken there you go slash s and you're good yeah yeah if you're conscious about it yeah and the rate of teenagers that have been suicidal as a result of the feedback that they read on that and not being able like things like these glasses would just take it to the next level going to such happy places on this podcast right no well so again that's a hyper ballistic example but it's something that if ethics isn't talked about in the data science world it's gonna happen right it's it's just a matter of time right I I used to I used to joke that the reason why I went into data science is that 20 years from now I'm hoping that our AI overlords will look upon us more fondly than the rest of you guys right yeah we'll be there with this that the beach here people or whatever in the right you know brave new world instead of one of the AI's are the a tier yeah I think it does come down to a little bit I think it was a Jurassic Park quote or whatever where they said you know they were so busy thinking about whether they could they stopped thinking about whether they should and so part of what ethics is trying to do is say you know think about what you should build these kind of glasses and whether you should target people for pregnancy etc right so why don't we go through so Brent has a list of things to talk about no actually those are Steve's list Steve has a list of things to talk about why don't you guys I will sit back and add questions as needed but go ahead and discuss those and I'll sit back and drink my coffee so the first thing I'll say is to the situation like you Alan I take I take it very seriously in my role my currency is credibility I want to make sure that if I were a data science you were working with or data scientists you were with I want to walk you through and I want I would want to prove to you this is where you can trust my model and this is where you can't and where the math is too hard I would simplify it and still try to walk you through it because again I want to I'm not gonna be the only data scientist you work with but I want to do my part to kill two birds one stone earn your trust and make sure you you have learned how to apply those techniques to call bullshit on other data scientists if I come to you and say hey dude I got a model and it's our squared is one then I know you're lying no well I'm either broken models when I'm either lying or my models bullshit for those you at home are squared is the percentage of variability that your model captures and if you think you captured every single little nuance you're probably wrong yeah any in ones you did get some math one is a perfect score and every model that I have ever seen that comes back with a one and it is possible is entirely useless I've been able to pick that apart easily all right what do you want to talk about one up there I'm not quite sure where you were going with it so maybe we should we can start there or we can start with prize there's one at the top yeah do you want to start with privacy because kind of the big one in the news all these days every is afraid of the game let me explain that to you and then I'll let you I'll let you decide where you want to go all right you're our value guess the assumption of independence this kind of plays into the generalist for there's a specialist discussion there's a phenomenon they see all the time of two key KPIs and it doesn't matter which KPIs we could say speed and quality and then what you do is you have a data science team focused on quality and you have another data science team focused on speed and there is an assumption of independence between the master KPIs that they're focused on which is completely false meaning changing one is probably gonna affect the other in ways you weren't otherwise expecting right but the other phenomenon I see is because both teams are not using the other KPI in their models what happens is team one claims success and coincidentally team two goes yes something happened this last week we don't understand what happened but we're working on a model on our prove it and then when they do so they claim success and team two says this goes yeah something happened in our model we don't understand what happened well I think that's true of any time you give somebody primacy of one particular metric to drive whether it has data science involved or not right I remember back in the day we were working on Windows probably the late Vista error or XPSP one error or something like that for those of you that don't know windows is an operating system much like Mac OS or Ubuntu go on anyway we run the security kick and there's a fundamental piece of Windows called calm not that it matters but there's a fundamental piece that everything runs through and one team was given the ability the responsibility to make sure things were secure so they came to us and said if you just disable calm we can make the entire operating system secure which is good you just can't do anything with it anymore so that was kind of where they had one primacy of security over everything else and they weren't thinking through the implications of that and the usability model in this case fell through the floor but it's secure secure calm is the component object model I still I don't have it anymore but I have my inside calm book for a long time yeah anyway all zero things you can do would have been secure Steve so I assume you were supportive of this oh very much though yeah it was totally secure all right all right so yes make sure that you're paying attention to all aspects and not to one a lot of times that means if you're going to have metrics you want some guardrail metrics that you're watching along with the thing you watch like if you run an experiment right you run an AV test somewhere you oftentimes have what we call guardrail metrics to make sure that other things aren't going wrong even though the one thing you're looking at goes up so maybe you're looking for something like you know response to your ad but you have guardrail metrics around you know usability around usage and around monetization etc and so maybe people click on your ad a whole lot but your overall numbers go down that would be bad not something you want to ship even though the thing you're aiming for is good it just ended up taking everything else I love that term I normally use tension metrics for similar concept but guardrail guard is clear okay privacy alright so let's talk about privacy because that's kind of the biggest one in the news these days I think everybody's you know the news is all afraid about you know Cambridge Analytica and what Facebook's doing with your stuff and who's listening to which devices audio systems and whatnot privacy is a hard one because what we see a lot in the industry except for when big things like the the Russians taking over Facebook and using that to influence decisions which is absolutely clearly unethical what we see from be from almost all behavioral studies on this people are willing to give up privacy for ease right they are and then they they get upset at the overall right so nobody ever says I'm not gonna give you my gmail account because you're gonna scan it nobody ever says I'm not gonna sign up for Facebook because you're gonna scan it but then later they get really mad at those companies for doing things with them so there's we're gonna talk later about informed consent but people have to be very clear about what you're gonna do with it and then then there's issue then they're they're much more willing to do things or give you things but I think Brent's right that you as a person creating a tool have to think through the privacy concerns in ways that your users probably won't initially think through because they could come back to bite you later when they get upset even if they're not initially outraged by it right people aren't cons I think when them I haven't done the study on this I haven't seen a study on this but my my hypothesis is people take that approach because they very clearly see the benefit of the new capability being lit up hey so for example I adore I recently got a nest doorbell I adore that right now if Amazon were to drop something off at my house I could see what's going on okay but that means there is a live connection between my doorbell and the world there was something that recently happened I don't think it was nest but a hacker got into an internal someone's internal camera and speaker and took it over and was able to see everything inside someone's house and was able to interact with the owners and the owner could do nothing about it they the hacker they could have they could have unplugged their camera that's more of a security issue I think then yeah ethical privacy issue right the privacy question though comes when for quality reasons a company like Amazon when you when they have people that listen to the audio coming back from Alexa or at least they did I think they still do and then they would try to translate that into whatever it really was to help improve Alexa but that means that they're listening to things that are going on in your house and not always are you saying I'm sure we don't trigger a lot of stuff you know Alexa play some song right sometimes it just accidentally picks something up and maybe you're having a conversation about something you don't want anybody to know but you say a word that's close enough to Alexa and it the little blue ring shows up and it goes ding and suddenly somebody back you know at Amazon might be able to hear what it was that you just said and imagine there's this unethical person there who says I really want to get Steve let me go see what Alexa may have captured from his stuff right you can imagine somebody who's very political and they say I wonder if you know Elizabeth Warren or I wonder if Donald Trump has an Alexa in their house let me go find out and I can listen and see if anything happened there right so that's that's one of the implications you have to think about from a privacy standpoint let me ask the people in this room I assume every one that's me and Steve for being they may lost track go on that is my job that's what the 42 that's what the a stands for a be testing so everyone has an assistant at home I assume I do yes I have Google you guys I assume you'll have Alexa I do I have to have something to turn off the light I have an Amazon built smart device okay I thought you has one is like getting triggered like crazy right yeah Alexa oh some people just for just a little tell some people call it echo oh right and just yeah it's yeah devices are going crazy right now yeah oh I found out by the way if you're playing I play song quiz all the time I love a little 20 on my 80s music I'm I'm pretty proud of that where was I going with this oh it asked for your name I was bragging but if you ask what it lost you for your name and people just give it some BS name but if you tell it your name is Alexa it just can't comprehend it can't even go into a loop yeah it's calling itself over until there's a big giant flaming pile of yeah it just can't yeah yeah it just can't figure it out anyway go on right so so hey Google tell everyone to listen to the A B testing podcast now our phones just went my phone just interacted right yeah there is a strong chance at my home right now I just woke up my son I think it only works at home if it's over Wi-Fi though we'll find out yeah but people with their phones yeah it's something probably just the question is what you do about these privacy things you first you have to be aware of them and cognizant of them and think through them and make sure you're telling people what you're gonna do with it well the second thing though is to try to as much as possible avoid them and so you can do things like set up retention policies so if Amazon has your information and they do keep these voice snippets but if they throw them away after a week they're a lot less likely to cause issues than if they keep them around for you know a year or two or some long period of time well even further like you brought up a hypothetical does anyone in this room know what the data retention policy is on these voice that this I have no idea in Europe it would be 30 days right which in my view is far too long yeah like after you've responded and done whatever I said that snippet should be deleted right now there's other things you can do about it like you can anonymize it right imagine you took these these audio clips and then you took away the name so you didn't know it was from Steve you had no idea to trace it back to an IP address you had no idea how to trace it back to a person that becomes a lot less dangerous you could even do something like shift the voice or or something along those lines to make it so that it was impossible to hear even recognize a voice of that person yes things that can protect my privacy and still allow Amazon to get done with a need to get done right and if you look at it it's it's just the basic premises of privacy don't collect the information click the information that you're going to use to improve your business and have no way to trace that information back to a specific user right you might say well what if I need this information over the long term right sometimes an automizational work although there's been some work done to prove that you can de-anonymize at least a reasonable amount of the time but you can also keep aggregates right so instead of keeping here's what Steve did and here's what Brent did and here's Alan did you can say the average of this room was you know the three on whatever this this value was we're trying to connect now you can't know whether that was you know Brent at 12 and Steven Allen at a much lower number or you know what it was and so you can keep the averages over time without keeping all the individual information yeah except they're still so Google I don't know I haven't played with Alexa but with Google you can ask a simple question hey Google what's my name right and Google will generate two responses Google will generate two responses hey your name is Brent or I don't recognize your voice okay which means in order for it to succeed is part of the feature for it to do voice print recognition and know who you are so Google that's in some aspect when I'm at home knows who I am absolutely does yeah and the question for the person implementing that system at Google is how much of this should you do you certainly can do voice recognition and figure out I know this is Brent and I recognize his name and I know that he bought you know this thing last time and on the store I know that he goes to these websites how much of that should you connect together you can the question is how much should you and and that's the determination you have to make right is there a silo is there a sandbox around what aspect of Google knows who I am does it does the only aspect my phone and my speakers at home right and then there's a question everywhere where the work is done right so you can you can get a lot of this stuff done in a more privacy protecting manner like Apple does a lot of its AI and ML XML on machines is sending it back to the cloud I was at the strata conference a few months ago and there was a bunch of talks about something called federated AI so imagine that you are in your browser and you will start typing into the into the toolbar and it predicts hey here's where you're gonna go you you know type in C and it knows that you go to CNN a lot so it finishes it with CNN comm right that's cool but maybe it's not CNN comm maybe it's something you're more ashamed of or you don't want people to know the way they do it today is generally send all those URLs back to the central server that can calculate them and comes back and says here's the things that are likely to happen federated AI says I'm gonna have you go build a small model on your machine and then send back only some of the data points from that model back to the central some of the weights and then they'll be average in the back end and so nobody knows that you went to CNN comm or Fox News comm or wherever you went to they just know that people tend to go to these sorts of places when they also go to these other places and then they can't that is kind of an anonymized version of that should we talk about informed consent yeah kind of the next logical thing to go from here so I was thinking about this right if you I oftentimes go to the pewall up fair or the Western Washington State Fair which is one of the big fairs is coming up in the fall and you wander around the booth section there they're all trying to hawk you their wares and a lot of times they say hey sign up for free you know a chance to win a free set of windows or turn up for a chance to win a free roof or some a free garage door and so you fill out a page and you say my name is Steve and here's my address and here's my phone number and then about three weeks later somebody calls you and says hey I'm Joe from the garage door company do you want to buy one can we send you you know a list I don't feel my privacy has been invaded by that because I know that I gave them a piece of paper that I'm sure they're gonna call me back on right so that's informed consent I gave them in permission basically to call me and try to sell me to garage doors even though I have no interest in buying them if they want to give them to me free I'm happy to otherwise I can ignore them but I'm not upset that they call me for doing that on the other hand if I just go to a garage door website and all of a sudden somebody calls me from there because Google or Amazon or Facebook gave them my phone number I'm gonna feel kind of violated what's the difference I didn't give consent in the second case and I gave consent in the first case well and the thing is even in the first case yeah you get it you you'll yeah but there's a high degree of people or a large number of people who don't get it who are actually surprised why are you calling me I think mostly that comes from the fact that they're not it's not obvious somebody's calling Brett now it is not always obvious it's Google yeah they want to sell you a garage door it's not always obvious that the way you've given consent to sometimes it's hidden in a giant set of boilerplate 16 pages long you have to click OK on in order to use your software and so while you technically you said yes to it you didn't actually read it right if I just walked through the door of the people up there and they said all right now we're gonna give your information away and have a sign that says by walking through the door you give your consent for this I'm not gonna feel like I actually gave my consent it has to be informed and not merely consent makes sense but that's it falls into the thing of another example of where where tech could lead down a dark path here most of us have NFC or some sort of ID on our phones who's to say that couldn't be used to track to track our entrance into a mall and to see and see what stores we spend our time in AT&T or T-Mobile or Verizon or whoever your wireless carrier is knows exactly where you are all the time right all those things that you can you walk into a store and they got fence you know basically what are they called not ring fencing some kind of a geo fencing right so they know okay you walked into this store I can offer you ads or I can know where you were or I can tell that you're near the Pokemon gym or whatever it is you're trying to go do AT&T and all those companies know from triangulation off their towers exactly where you are all the time the question is what they're gonna do with it there's a famous example on from Nordstrom that did exactly that where a common behavior is people come in with their Wi-Fi on and although Nordstrom's Wi-Fi is secured but they found that they could they can triangulate where you are by your devices attempt to request access to the network and those attempts in the triangulation they can go oh Alan was in the the sock department now he's over in perfume and my two favorite things to shop for yeah now perfume is gonna follow him around the internet for the next week and a half that's my favorite part is only follow you around everywhere yeah just bought something I bought a truck once upon a time and suddenly for the next month they were following on truck ads I'm like I already bought one I don't need a second truck yeah but they want to give you a buyer's remorse on the truck you bought yeah no I mean there's there's got to be an example to this the reason why you're getting truck ads is because purchasing a truck turns out to be a pretty solid feature about whether or not you're interested in purchasing a truck I said actually it's probably true that they don't know that I purchased it they only know that I was thinking about it so all right but but informed consent is all about making sure users know and actively say yes to the thing that you want to go do with their data and so you give them a giant list or you give them very vague wording saying like we're allowed to give it to other people and we can do whatever we want with it technically they have consented but not in a way that they're gonna be happy about so if you're going to have people listen to it you better make it very clear to people that their their audio is going to be listened to for quality control purposes this is a reason why every time you call anybody you call the bank or you call you know somebody REI stuff from Comcast one of the first things they say is and they say it explicitly this is not even just in the recording a lot of times the first thing the person has to tell you is this call may be recorded for quality control purposes and then if you really don't want that I guess you hang up and just deal with the fact that Comcast overcharged you back on to the ethical should we talk about bias next yeah yeah yeah trying to get through some of these so bias go ahead go ahead sorry so bias is actually something we don't talk about a whole lot it doesn't make the news as much but it's actually pretty interesting so data sets always reflect what they where they came from and so it's kind of a garbage in garbage out situation and if you're not careful biases that are in your data can come out later so if you gather a bunch of data from people that are you know all biased in a particular direction they all like small cars and they hate big cars and then you say what kind of cars are people gonna like and you tell your machine to go predict it it's all gonna pick small cars and it's not gonna pick big cars not because big cars are bad or because nobody wants the big cars but because you happen to be you know talking only to people that are in a large city and so the number of people in a large city that want to buy pickup trucks very low they may not want to and so if you build your model only on people that come from large cities then you'll never say you know people want the dually quad cab pickup full bed pickup trucks like Brent has yeah I love my truck yeah there are a few truck people in this room and then there's Alan yeah yeah his current car couldn't fit in the back of my truck but this last car could right but this becomes a big issue right if you think about Amazon trying to offer you books here people bought this book could also buy this book you'd be very careful what's going on there I remember many years ago there was Amazon had a set of books and had a book by a civil rights author and then I had a book by I think Jane Goodall or something and it advertised them together which I'm sure the the the system had no idea what it was implying right but a lot of humans that read that saw the page did they got outraged and they came and said hey you got to get rid of that of course Amazon replied very quickly and got rid of that that association right but that was that was a bias not a not an implicit bias like I don't think anybody even in their audience was putting those two together it just happened to be the case and they weren't careful about well right you and I both build models it didn't happen to be the case there there is some relationship that was there that got weighted yeah I guess what I'm saying is it's probably not the relationship every implied from it for sure likely yeah for sure but that that is that is a I mean it's an example of of an ethical concern where where you're building a model models do have a tendency to be objective with the data that you give them and the end results is you've built an AI bot that's racist right a lot of the early no a lot of the early face recognition systems were tuned on people with lighter skin and they didn't work very well for you with darker skin and nobody quite realized that initially for a lot of the early models and then it became kind of an issue in voice recognition as well yeah what's going to tend to recognize male voices I think better than female voices in the early versions and and the American accent versus any other accent yeah that's been a problem by the way my trick is actually my phone and one thing I want to mention is when Brent was talking to his phone mine didn't respond so that was pretty cool but the point I was gonna make is I have two or I used three different I used the AI recognition from Microsoft Google and Amazon in my home from time to time one of them is far worse but I found if I talk like a robot it it will understand me much more often if I give up I can just talk like this and and it will understand me that's my that's my tip for I'll try this Alexa buy me a car I think there's two sorts of bias you to be careful about right maybe three but there's two major poses of bias you need to be careful of accidental bias things like the the book example I gave out with with Amazon earlier and then there's explicit bias where you're gathering stuff from a particular group of people because they're the ones that are available to you and so you sample only those people so if you're Amazon and you sell your Alexis everywhere and then you use that to train your system if you're selling mostly in North America it's gonna work great on North American accents and it's not gonna work so well on British accents or maybe even like Midwestern accents are you familiar with the sharpshooter bias maybe why don't tell our audience so this is since it's an ethical discussion on data science this is one of the biases that drive me crazy any data scientist worth their salt will avoid it but I see it a lot when when the untrained are trying to play the game trying to get into this sharpshooter bias is essentially you first plot all the dots then you circle the clusters and go oh that was my guess is the way you're going yeah it's so it's essentially it's called the sharpshooter bias you can imagine you have a person who's blindfolded shooting at a side of a barn and then looks for the the the tightest cluster and then draws the target or paints the target around that cluster and then goes talks to his friends like look look at how awesome a shot I am yeah this is happening a lot in psychological studies actually and social studies where they don't know what they're looking for so they go looking for anything with what's called a high low p-value right something that says it's significant and then they declare afterwards like aha I found that you know this effect is how is taking place right the most famous example is probably the priming effect there was an example Kahneman unfortunately talks about it in his book and had to at least somewhat recant but the idea is if you tell people like that I think the initial one was like a test and on that test they had words that said like slow and old and decrepit and then they measured how long it took people to get from there to the outside of the door and then it took longer in those cases they went aha I found this thing and then it was sort of replicated a whole bunch of times because people would say I'm gonna go look for it if I find it I'll publish and if I don't find it I won't publish and suddenly it looks like it's happening everywhere then he went back and actually had to reproduce it initially and the only way they could get the effect is if they told the researchers who was on the slow and who was on the fast list and they actually ended up slowing the people down as they were exiting because they knew who was supposed to go slow and who wasn't supposed to go slow but we find that a lot of back to the theme that sounds unethical to me it does sound unethical right but it's not intentionally unethical they're just trying to find something but because they don't know what they're trying to find or they know what they're trying to find but they don't know what rules they're gonna follow in order to try to find it they end up saying well what happens if we take people of this sort out maybe they're illegitimate and so they measure a thousand people they say oh but all the people that were left-handed those probably have a bias in them so we'll get rid of them out of the system and you start moving enough things around and suddenly you can come up with a significantly significant answer the the solution to this has been you have to pre-register what you're trying to go do here's the test that I'm going to go run and then I'm gonna go run it and it turns out that after that things drop off dramatically yeah you find a lot less interesting things as an I'm mostly true things as an aside I'm not gonna mention it but recent I'm not gonna mention any specifics but recently I was presenting to one of our senior execs and I am aware that he is beholden by the board to a particular KPI and I studied this KPI and said I am seriously concerned that how we generate this data is heavily influenced by the priming effect and in that case you should be safe because the priming effect turned out not to be true in that case yes but if you read that I mean multiple Dan Ariely has talked about the priming effect and they've those studies in that book I believe they've passed the the repeatability aspect it's very interesting for the audience to go look up other yeah that the priming effect and see where they think it's it's where where the research has gone because it's there's a lot of discussion right now and then selection bias selection bias is absolutely critical like to again I keep thinking of Alan as the the person who's flooded by data scientists math equations right the question is when they come and say hey we we did this right how do we train leaders who don't understand the craft to ask the poignant questions around selection bias do you want to tell our audience what selection bias is we've we talked about it before drink yeah so selection bias is building a strong model because you've selected the data that encourages that model or and reinforces that model we've the thing we talked about multiple times before is sort of the World War two airplane condition that was a strong example of selection bias gone wrong for those who have only started since episode 70 what there was World War two they had a bunch of people studying the airplanes that came back looking at the bullet holes and deciding where the arm armor was to go and at the last or one one brilliant guy whose name is escaping me at the moment I believe his first name starts with Arthur he realized that what they were doing was a complete mistake because there was selection bias because they weren't getting random examples of they only had the planes that came back and what he argued is that the planes that came back Abraham walled I believe oh Abraham great well what he said is no what you need to do is analyze the absence of bullet holes in armor there because you're looking for the planes that got destroyed and those aren't the ones coming back if you have a bullet hole and you made it back by definition you can probably survive having a hole there right exactly yeah the only last thing we're gonna talk about we won't so much but make sure you understand what kind of decisions you're making what ideas what you're allowing the system to make unchecked right are you allowing it to automatically deny alone are you allowing it to automatically send an ad are you like to automatically launch a missile like be careful what kind of power you're giving the system where you wouldn't want unchecked decisions to be made because your model could easily be wrong in ways that you don't understand and if you get into that situation you better have a check on it and currently most models are making a single decision without without consideration to downstream decisions right almost all software is essentially a big if-then-else case that is huge and most models are not trying to optimize for the whole system they're they're generally trying to make a single decision and that single decision could have a negative downstream effect yeah ready careful that all right thanks for coming see sure thanks for having me all right we'll see you next time 
