Welcome to AVE testing podcast your modern testing podcast. Your hosts Alan and Brent will be here to guide you through topics on testing, leadership, Agile, and anything else that comes to mind. Now on with the show. Hello Brent, I am the human that used to be Alan Page. How are you? I am also the human that used to be Alan. Well I want to know your name. My name According to the session here, the label is Weasel, but I'm Alan, but I like all good sheep. After a couple years of pandemic and stired non grooming, I cut off six inches of my hair and six inches of my beard and I look, you know, normal. No, no, I'm gonna go with that. You just keep on going. So Brent is Satya Nadella is calling Brent right now and it's a question about whether or not Microsoft should do anything with LLMs and Gen AI. And the answer for Satya is maybe, maybe, but it should probably make sure he's doing things to stop people from being dumb, etc, etc. Okay, so once he gets done with Satya, I have a quick phone call I have with Kamala Harris talked to her a little bit about technical advisory for upcoming White House stint. And then after that, we're gonna do the podcast. So we're on the podcast here and I am joined by Brent Jensen. Hey, Brent, say hi. Hello. It's really cool because unless you listen to the podcast, you'll never know what I said. So I hope you told Satya I said hi. We had, you know, we've talked about this before. Well, before we get into the topic, how are things squeaky chair Brent? How's life? What's new? Do anything cool lately? Sounds like a no. No. So drama's coming up this weekend. My eldest who is about to turn, my eldest is about to turn 26. So we're gonna be celebrating his birthday. He's home with us. But what makes it dramatic is at 26. Do you know the magic thing that happens? You're not in your parents insurance anymore. Yeah. Yeah. He is just about to finish his second week at his new job. He was looking for something different and I'm like, dude, you're about to turn 26. Go for something different, but you need something different with insurance. Yep. Yep. America for healthcare is a privilege, not a right. He did go with, he did get something with insurance, but it's not something different. Oh, well, that's insurance. So, you know, at one point, don't listen if you work with you. I thought, you know what, I want to, what's the easiest decision for my retirement? Don't worry. What's the easiest job I can do that as health benefits? And of course the barista came up. That's a lot of weird customers. One in their double half calf frappe whip Oreo mint pumpkin scream green thing. And so, but my retire, the job I'm moving toward my career goal is assistant librarian. Interesting. It's quiet. I don't have to talk a lot. It's a, yeah, don't look at him. That's my thing. Now. So the thing is actually, I'm wondering, is that one of the few jobs that AI won't take away? Maybe not. Maybe not. Because well, you know what? AI has not taken away any jobs because you know what? Well, I shouldn't say that AI today, as we know it is far too stupid to take away a job. It doesn't know what it's doing. And I can't even go to LinkedIn anymore because people keep on telling me AI won't do this. I say, thanks for you. Thank you for the straw man argument. Of course it won't, you stupid freaking idiot. All right. So topics today, speaking of AI, there was a time earlier in the life of this podcast. Are we episode two 10 this week? We are. It's welcome to episode 210 of the AB testing podcast. We bring you valuable and relevant, well thought out, practically scripted information. I'm kidding. We used to make a Kanban, like little lists of things to go through. It was great. We had topics to talk about and they rolled through them. And maybe you liked that and maybe you didn't, but now you get this version of the AB podcast testing podcast where Brent types while I'm talking, which I may or may not go through and, uh, and, and remove later. It's horrible because we both have mechanical keyboards and Brent is redoing an exercise that we've already done, but he's doing it now in order to get me to continue to share just nothing, nothing at all while he does his thing. So what we've done for today's topic on the fly, like sometimes I'll come in and go, Hey, I don't want to talk about this and Brent won't know what it is, but we'll get there and we'll roll with something. And occasionally, occasionally a little teeny nugget of something good comes out and someone says great podcast. And I say, man, drugs are good where you live. But today I asked Brent for a topic and Brent said, oh no. And I looked through my like five for five. I got a topic on her about tomorrow, my blog post, and that's not a topic for today. It's a rare situation where Alan too like, I don't know. It's been a day. Usually, usually I will take, because as the producer of this podcast, Brent just shows up. I spend time thinking about topics ahead of time. I do some Googling, please bring your screen back up. Cause I need to see it and I will be prepared. I go, here's what we're going to talk about. I don't worry about the details. No scripts, no special effects. But what we did today is you may have heard on this podcast or maybe, maybe even on the internet about a thing called chat GPT and my chat for those of you that don't know chat GPT is a tool based on generative AI. There's indexed on a whole basically the internet. We have a special one that's, uh, I don't know. Did you use, you did use a special one that knows more about us and what we do a little bit more, which could be a good thing or a bad thing. And then Brent, if you can scroll up for a second, Brent asked this thing, uh, as advocate, the way that let's go up. Hey, Hey, he starts off with, Hey, I left out Brent is you can tell something about someone's personality by the way they interact with the chat bot. It's like, good morning, dear sir. He says, Hey, give Alan Page and Brent Jensen a good mailbag question that talked about on their podcast. So we have three ideas, but before we start on any of them, let's do this. Okay. And Brent, I'm going to let you pick. We're going to, but we're going to, but no, I'm not going to let you pick. We're going to kind of vote here. Would you stop moving the page? I'm trying to read. Oh, this is the new one. No, wait. Now I got two questions. Oh my God. All of this crap is it just defined why we would care. Imagine giving a presentation and someone keeps on changing the slide randomly on you. So, um, one of it, one of the questions is, Hey, Brent, no says, Hey, no, it doesn't say that either. It says as advocates of modern testing principles, how do you see the role of testers evolving in AI heavy environments? That depends on what that means with AI systems handling more decision-making. Don't want that to happen, including testing. How do you ensure teams are still responsible for quality, especially when AI outputs can be unpredictable or biased? I am happy to talk about this one. This is actually, cause there's a lot of kind of hints at straw men here. And it goes back to my comment earlier. People, they're, they're straw men arguments on LinkedIn. Like they would see us and they would go off on some tangent around how test cases aren't testing. And I would blow my brains out and life would move on. So is there another question we want to look at? Another one. The other question is you've discussed the importance of shifting quality responsibilities across the team and modern testing. However, with the increase in AI driven development automation tools, how do you see the role of traditional testers evolving further? Should they focus on new skills like understanding AI biases or models? Yes. Or is there still value in deep exploratory testing for human centered validation? The interesting thing here is we're going to answer that question. Because no, the question, no, no, the question, because no matter which one we pick, the answer is going to be kind of the same. There's a little bit of new ones. Well, if you're going to give a three second answer, they'd be different. If we give the answer of any depth or substance, it'll be kind of the same answer. So I want to leave this one. Let's go back to the other one. We're going to come back and touch on this one to make sure we, my prediction here. Oh, what if I'm Gen AI and I just have a bunch of words that I'm trying to throw out in an order that makes sense based on context? Are we all living in a simulation? That is 100% the definition of Gen AI. I know. Fuck. Oh, wait. Sorry, kids. That I didn't, I'll forget that at that, but my brain is spinning. Okay. Let's talk about role of testers in evolving AI. And you're going to have to, okay, the question's gone now, because now Brant's asking new questions because Brant is off script as usual. Wait, is there a script now? When did that happen? The script? No, dude, dude, it's in here. You just haven't prompted it out of me yet. Ah, okay. Got it. Holy cow. Maybe, maybe Gen AI. I got tired of your, of your beautiful voice preamble. So I just went ahead and asked. To answer it. Okay. I asked open chat GPT to answer the question. Oh my God. No, because it won't have the, it's probably right. Honestly. And I'm going to read one thing on here because first off is from the second paragraph. Stop scrolling. First off, testers need to stop thinking about testing is finding bugs and more about accelerating teams learning and decision making. Let's talk about how AI is helping teams accelerate the achievement of simple quality. Yeah. No, it's actually, this is not bad in terms of what you would say. I'm sorry. I would, I would add flair. So I think it's a question that gets asked all over the place. And I feel like maybe we've talked about it before, but we can dive in is one to repeat the thing we've said 10 million times. And now I find a thousand other people are saying the same thing. Gen AI AI is not taking your job away. People who are adept at understanding how LLMs and Gen AI work may. And okay. I'm gonna, I'm gonna end up repeating myself and I've been talking for a long time. I'm gonna let you start with this and I'll roll in in a bit. Okay. Because it's interesting because it's not taking your job away yet, but it does absolutely having an impact. There was a report I saw this morning from a professor at University of Berkeley. And he's already observing that a lot of graduate students are finding it difficult in the CS degree, finding it difficult to land new jobs. So yeah, it's not gonna take your, my job away yet, but it's gonna start, it's already starting to not take anyone's jobs away, but not open them either. Speaking of which, and here's a different take on this question that I'm gonna, this is where chat GPT can't find the patterns. There was an article this week, last week at the latest, no surprise at all, talking about the rise in bug rates due to in, I don't remember the details where they got the applications from, but due to people more or less blindly accepting co-pilot suggestions. Yep. Not surprising at all. We knew that it's gonna happen. So if we're putting more bugs in the code, Brent, does that mean we need testers even more now? No, no, we have a different problem. Yeah. Right. Because the, and it's in alignment with actually GPT's fake response for you. Right. If you read this last one, in short, testers in the AI driven world aren't trying to catch everything before it happens. They're accelerating learning. Okay. And, and, and here it italicized and I'm not going to read the rest of it. It's all about, so we can respond faster. But here is the thing that's problematic with AI systems and it's in alignment with what you just said. These AI systems, are they accelerating learning or are they accelerating laziness? Because if they accelerate laziness in ways that are important, then it's going to be problematic. I don't think, I don't think bringing back, you know, dedicated testers are going to be the solution. Right. It's going to be, how do we battle laziness? And in quite honestly, if it comes down to that, we're going to have a big problem, a big problem. Cause laziness is like, um, uh, a key principle of the software development. You know, it's, uh, it's interesting. Sorry. I have a bunch of different threads going on at once, but where I think AI can, so let me go, let me back up a step. There's a lot of testers out there who feel their job is to find, I mean, if you don't want to go back 30 years is to find books. Right. And we know, we know most, I think most testers who have paid attention realize that's not their job. It's a byproduct of doing their job at best. Uh, there is a school out there that says to all testers do is provide information to stakeholders, which I, which we've talked about before. I don't want to go deeply into that. Um, sure, but that's not going to really help here. So again, fast feedback loops. When we talk about, when we've talked about teams not having dedicated testers, it isn't because we don't like testers isn't because we don't think they're valuable, but what's more valuable is getting fast feedback loops on the work produced by the teams or trying to accelerate the team. So if we're trying to accelerate the team and the team has potentially more bugs, more functional correctness bugs, because they're being lazy with, uh, with, uh, code prompts, uh, what can testers do to help? Because that's going to slow you down because now you have bugs and you have rework and that, and your, and your cycles are slower while you get stuff fixed. Potentially. Uh, what does, and I have an answer for this, but I'm curious on yours first. What do, what do testers do in this environment? Do they just report the bugs and perform information, report information on the bugs to the stakeholders and call it good? No, no, it's right. It's, it's this, it's the same pattern, right? If we go, if we go to what's the definition of quality and what's the goal of a test, the definition, uh, as we call out, we don't know the definition of quality. It's based on the customer. But customer is the one that judges it. Um, and the testers job is, is to, to understand and help drive towards business impact, having them go back to sort of a traditional model, uh, finding bugs and all of that. No, that all still needs to fall into the role of, um, the developer in this. Yes. But that, that does not change ever. Correct. But the definition of quality does to some degree, because now we have the system in between, um, essentially making shit up and sometimes it's going to make shit up in a good way. And sometimes it's going to do it in a way we can't expect. Um, and so I think testers need to start training around how to identify the patterns around these problems. And the way, the way I see it is still on the quality coach, the quality coach angle, but now they're going to have to be a quality coach in a space that a lot of these folks may not have learned before. Right. And so that's where they need to be, uh, aggressively learning. Or as, as the, the fake Allen page bot said accelerate that learning, but then tie it to back to quality and then tie it back to the developer in a way that, that adds friction to the, the laziness concern. Let me build on that because you took my slow pitch and you, you get a nice, soft over the wall home run with it. So nice work. Uh, when we talk about a lot of the testers we see on LinkedIn, again, we have folks living in a world we're not in that much anymore. And they are a, in a role that's specialized to doing part of the development role. And that's, you're absolutely right to say that doesn't work. That doesn't help. Doesn't change. We need folks who, and again, it may not be considered a test role anymore is the issue. I think testers are exceptionally good at this quality coach testers are exceptionally good at in general at systems thinking and critical thinking. Although sometimes if I'll sign LinkedIn, maybe question that thought, but I'm going to, I'm going to stick on optimist Allen optimus, optimus Allen optimist, not optimus prime. Wow. I wonder if optimus prime was an optimist, but it doesn't seem like it, but that's a, that's a thread we don't have to go into. So here's where I think they can help. So again, going back to the AI angle, cause things are changing. So if I said they're good at systems thinking to a tester who is good at system thinking is going to be even better when assisted by jet AI, let me feed our entire code base into an LLM and ask and take some time asking the, I'm going to call it chat GPT, the, the, the LLM via whatever interface you want some questions about the code and how it works and, and areas of concern or impact. It's actually pretty good at code reviews, even if it can get some things wrong because it's just, it's doing some copying and pasting. If you will recall, and this is going to come up again at the very least in our end of year show, which is not that far away. Uh, but last year in my prediction episode, I predicted, and maybe I'm a year or two off on this. It hasn't quite made that turn yet that the ability to read code would be more important than the ability to write code. It's probably not going to happen this year, but you can see with what's happening with code pilot and co-pilot and pro and even asking chest GPT to write code, the ability to read it, understand it and critique it is more important than the ability to write it in the first place. Won't be true in every case. There's some things that, that the LLMs won't be able to help you with for now, but the ability to read that, understand, fit it into a system is great. I may have told the story before, but I'm going to tell it again. There is actually, this is, uh, I can mention it here. Uh, I think a lot of folks know I am on the board for a, uh, uh, not the board board, just an advisory board for a testing tool called autify. Autify started off as just another, yet another, uh, machine learning assisted you animation tool. Uh, we had some folks off from another company a while back. There's a bunch of these. They're all pretty good. And I think they're, if I was a developer today, I would 10 times out of 10 use one of these tools over selenium. If I had to, if I had to have UI tests, I will, I will fight you on that and I'll win every time. Now, what a cool thing that autify showed me, and maybe it's not announced yet. Maybe I can't mess it. I'm going to say it anyway is the demo demo, demo where the demo where right now, but they took a design doc, a spec, fed it to an LLM, the LLM gave with an eye LLM. It gave them a list of test cases, which were editable case. They were wrong. And you know, for a model based testing, a lot of times we found if we created test cases based on the spec, which we did, uh, it was because the spec was wrong. So it's and specs are always wrong to some extent. Um, but all editable, so you could fix it. And from there it would could generate the playwright code for those tests. Super cool. I think it works backward and forward. It's the nugget of something cool, but God, why would you spend a bunch of time reviewing a big in test cases? Aren't testing. I agree with that part, but why would you spend a bunch of time, uh, looking at a spec, reading his back, vetting his bed, ask questions about a spec, writing some tests, you know, figuring out what tests are going to write automated or not. Um, and it just seems slow. So yeah. Uh, I think what it does to what tools like this will do, looping it back to the quality coach person and the role that like a lot of, you know, today's testers should be in in the future is figuring out how the team can use and not just cause their AI tools, but help the team use tools that help speed up their feedback loops. If I can write code and get the test for that code super fast and run those and get the results from those tests all in seconds for brand new code, that's pretty good. And as, as a quality coach, and that's how I'm going to help the people use these tools, understand when they should and shouldn't use them. I may even like, if it was me today, 30 years of software programming and, you know, a half a minute of working with LLMs, I would pair program with someone who was taking co-pilot prompts to help get a second set of eyes and code review on those things are blindly accepting because, you know, with, with, uh, uh, sorry for, I forgot to work for a second with pair programming, uh, one person at that 10,000 foot leo or one, that one person is deep into it. If that person deep into it was like, yep, looks good. But if I'm out there going, um, that's not going to work because of a B and C that's kind of cool. And that's going to help solve this problem. So why aren't we, my question to the survey that I don't have a link for this, this story I read about bugs coming. People are blindly accepting their co-pilot, uh, suggestions is why aren't they pair programming to that with that? Huh? Why not? Why not? Brent Brent's too busy asking you the scenario. Um, you know, we're working hard when you hear the keys clicking. Right. I didn't see, I didn't. All right. The. Yeah, that's a good idea. Yeah, I know. The, the thing that I was, I was listening to you for. Okay. And here's where I see, I, I'm trying to see it not as a sort of like, don't do that. Sorry. I was disciplining my cat who was trying to eat my mule near. Yep. Just punched him in the face. Her. So in your story, right? Hey, if you can do the code and you can do the tests and, and read them really quickly and, um, get code out in production in seconds, why wouldn't you do that? And the short answer is you would, you absolutely would. But here's the thing. How does that accelerate learning? To me, I'm like, that is not a, that's not accelerating learning. That's, that's you being a monkey now. It accelerates laziness. I forgot that in my gen AI, uh, generated response, but yeah, we should accelerate learning pair program and gets to that, but that's, um, I'm, I'm, I'm retro answering now. Right. But the pair programming. So here's the thing that I'm seeing it right now is that pair programming, why is that valuable? Right. Well, it's not valuable right now because you and me are old geezers and we know stuff and we know common ways that the GPT could be screwing it up right now. Right. But the thing is that's temporary. I don't know if you've seen that. It was, that was what I was just trying to type out the new version, um, uh, GPT, one of the new models. Um, you know, the, the, the chain of thought model. I do not. Okay. I mean, I know, actually, I know it completely, but it would be good to explain it to our listener. Chain of thought is essentially when you, when you do a chain of thought prompt, what you're doing is you're, you're giving it a clue around how to work the problem, how to break it, how to decompose it into smaller parts. Okay. It does that now. It doesn't automatically. I'll see if I can find, um, why is it not letting me scroll? Where the hell is this roll bar? Okay. It's cause you're using edge and edge sucks. Okay. You see right here. Yeah. Okay. So on the chat GPT, oh one dash mini model. Okay. I asked it a prompt, uh, to create, uh, three pieces of PowerShell code, a code that will reproduce the problem test code that validates it. And then code that fixes it. Okay. Interesting. Interesting. Okay. Um, but right here, what I'm showing Alan right now is a new prompt version that says thought for four seconds. That should, that thought should be an air quotes, but go on. Yeah. And now I expanded it and showed Alan what it's doing. Okay. And what it did is this thinking for four seconds is it generating its own chain of thought prompt. I see that. Yeah. It's, it's solving it. It's taking a problem. Like here's what I tell my team to do all the time. Take the big problem, break it into solvable steps. And Chad GPT is showing that's exactly what it did. But it's, it's, it's not what it did. It basically created that plan and then it executed that plan. Yeah. Okay. And I'm going to tell you in terms of what I said in terms of what I asked it to do, um, it did it really goddamn well. Okay. And here's the thing, cause I've been in this AI business now for 10 years. The first thing you do is you make your AI transparent because everyone is suspicious and they learn about, right. And then eventually people are like, yeah, it's good enough. Like, yeah, that's some bugs, but no one complained or we worked through them when they did complain. And then it's just, it's just this, right? My coding then becomes me writing, what do you think? 50 word instruction. Yeah. Yeah. So what, what's, I'm going to interrupt for a second. I do want to go in and see the answer, but going back to the answer, our original question is it's interesting because what I am, and this is something you've talked about a lot on the podcast. This is where I think we can help accelerate learning on dev teams is one of my big gripes with gen AI is like all the people who they just don't understand how it can help them solve the problem. Like, like you, like the credit to you is you inherently knew this is probably a question that gen AI can help me solve. And it did it, it didn't know where that delighted you, which is great. I think a lot of folks, to be clear, delighted part of me scared the crap out of the other side. All right. Fair enough. Fair enough. But what I see from the internet, you saw the thread two weeks ago, three weeks ago on the people all freaked out how bad LLMs were because they couldn't count the letter number of hours and strawberry, uh, super dumb. Yeah. But one of the key, like the key to knowledge worker success in the future is understanding when, and when not a LLM gen AI can help you solve the problem and then giving it the right prompt to solve that problem for you. I don't want, I don't even want to talk about prompt engineering, but it's like I get praised a lot for my Google Fu, uh, because I can, I, my wife or somebody, she's actually pretty good at it too, but someone will search something in the internet and say, I can't find anything. I can find the right words in duck duck go or Google to, to find what I'm looking for via search. Uh, it's a skill and it kind of, uh, yeah, Brad's showing it this broken too. Um, we're going to fix this in a second. Uh, people don't know what we're talking about, but the ability to understand, oh, this is a problem that LLM can solve, or this is a problem that LLM can't solve, uh, is critical. And then the coming up with the prompt is almost, I think, I think actually, um, all I'm just going to get more forgiving on that. So maybe it's just the first part, maybe figuring out this is a problem that can be solved. This is a problem that can't, so Brent has asked, um, he has done the question in chat video, many, and he's asked how many Rs are in strawberry and it says two, which is incorrect. And it's even confident as you can see the letter R appears twice in strawberry. Now try this, Brent, try this, ask it to write Python code to count the number of Rs in the, in the word strawberry. Okay. Uh, I'll do that. You talk. Okay. And what should happen here again, because again, this is people just don't take the time to understand the LLMs look for, they have looked at such a wide body of text. They don't know what they're saying. They're putting the words together in a way that makes sense based on the gazillions of words they've looked at. Now there's no story books. There's no research papers written about, uh, written about how many Rs are in the word strawberry, but they are really good at writing code. So Brent wrote exactly what asked him to write Python code to count the number of Rs in strawberry. It says thought for four seconds, examining the count. He says, let me see. I'm identifying three Rs in strawberry positions, three, seven, and eight. Contrast it didn't even know. We didn't even say you're wrong. So just write some code for this. And it automatically got the right answer because it knows code and it can fit and it, and it can get the context right. It goes, Oh, the question I get it. And it's just understanding how they work. You can make them behave in the right way. And there was a five or Friday post like two months ago, where there's an actual, someone wrote a nice little tutorial where you had to get an LLM to give some answers. And the goal was to give it the right prompt, understand enough, what was going on. It was a good little, almost a capture the flag on prompt engineering. Again, I hate that word. But anyway, the code doesn't matter. The fact that the write the code makes it understand what it did wrong, which I think is fantastic. It's a little scary that it learns like that air quote learns like that. But so that's, that's that. Okay. We went on a tangent and we export it deeply. Let's pop the stack. I am saying that the ability to understand when and when not to use an LLM is one of the key skills of knowledge workers in the future. Fight me. I do think that's going. So yeah, in terms of accelerating learning, that's what you need to accelerate learning in for sure. Right. Because like I look at this thing, like Alan, Alan neglected to point out that yes, indeed, it wrote Python code that would 100% generate the correct answer. Yeah, of course. Yeah, that was. I've while we do see bugs in in in LLM generated code, not usually on simple things. Let me just double check. Python string class. Oh, see, there's a better way to do it. Now. So the only thing I have seen it do, particularly with with Python, is that it will sometimes invent interfaces that don't exist. Oh, interesting methods. In this case, right string is going to be a common class that is used. And so yeah, it does indeed have a count function. So yeah, it generated it. Every method exists if you have the right libraries installed. Right. No, but some of the problems like Azure Data Explorer or what used to be known as Cousteau. I don't know if you ever had experience. I do remember. Oh my god, the blast from the past. Yeah, no, it's it's alive and well, and it's awesome. You can get this to like, I'll do it now. Right? It is. And what I do is I give commentary while Brent's typing in a chat GPT because this this is the podcast you pay for. It worth every penny of your subscription saying, write Cousteau code to count the number of ours in strawberry. Is it going to know what Cousteau is? Yeah. Was Cousteau, was that ever external? Multiple things released with that. I mean, its formal name is is. Oh, I can't wait for this. Data Explorer. Right. So yeah. Oh my god, I recognize that code. Oh my god. Let's see. Yeah. So what it did. So walking through it first created a variable and you didn't call out, but I completely murdered my spelling of strawberry. No, you just added a backslash at the end. Extra Y and a backslash. And no, and I actually two Rs. Yeah. Okay. All right. But it wrote the code, did the right spelling of strawberry, converted it to lowercase, then figured out the string length of it, then removed all Rs from that string and then counted that string length and then did the delta. Well, that's an interesting way to do it. But if you don't have like a counting function or a way to index it, this is an old. You're not going to, you're not going to loop in. So Cousteau, by the way, if you ever use Cousteau, used to use it at Microsoft, it seems like a million years ago, probably, you know, eight years ago, just a query language for looking at usually analytics data. Okay. But now here is why I brought in Cousteau. Okay. So god, I hope there was a reason. There is. So this function, do you recognize that function? I can't see your pointer. Oh, Stirlin. Yes, I do. Okay. Where does it come from? C. It comes from C. What about this one? That's not a C function. I don't know what it is. That's a Python function. Replace. Yeah. Okay. Okay. So the problem with the Cousteau language is that the developers of it pick and chose things, names for things that already existed. Okay. So when you do, there's certain ways you can ask it to do something in Cousteau. You go and say, okay, create me a thing that does this in Cousteau. It will often invent things that don't exist because they do exist in other languages. And because Gen AI is nothing more than a probabilistic thing, it knows, hey, this, this fake, or this function that I want to use here, I know what comes next. And it has, it has lost the fact that it doesn't work in Cousteau. I get it. So you picked a more obscure language with, with attributes like this, because it's more apt to make errors. Right. Right. Now, as it, Oh, and by the way, I just scroll down. It not only gave me one way of doing it. It gave me two, three, four. I would have honestly, I would have done four. A regex. The last one. Yeah. Yeah. Um, yeah. This one says count if lower word matches regex. Okay. So this fourth one would have been broken. Um, because this is a single Boolean condition and there's only one word. So it would have returned. This one would return a one. Um, but there is a way in Cousteau, they picked the wrong function. There is a way in Cousteau where you could use regex and then you would count the number of groups. That one would not work. Okay. So what's the, what's the main point here? Well, so I like, like you connecting the dots. It's like, yeah, you need to accelerate learning, but your, your counter, it may, you may not need, you may not need to be accelerating learning in what you think you need to be accelerating learning. Yes. Yeah. And it goes back to the other question on, which is based around what AI tools should do use. And the answer is not until you have to, I mean, they're not magic and you're there, and there are some that are going to help you. Of course, Gen AI for first, it's all the air quote AI power tools. It's the new dot net that I'm a little afraid of. I think actually in the top concept of AI, I think you're now world famous technology might need to be updated, which is what automation should be right. Alan. We should automate all the tests that should be automated. Okay. So what AI should we be using? All the AI that we should. Right. And no more, no less. Yeah. Oh God. Have I ever, so I'm okay on, we've got a few minutes here, but that reminds me, I never shared these, but I do have a list of the weasel laws. Okay. Just in case I ever need to refer, am I old and I forget things. So I'm just going to share these as a bonus for our listeners. And I think you've heard all of these. These are all things I say a lot of times that you're, you've said, and I've stolen them. Oh, I would love to have that. Um, you should automate 100% of the tests that should be automated. Weasel law number one, weasel law number two, the answer to any reasonably complex question is it depends. Number three, code coverage is a wonderful tool that a horrible metric. Yep. Number four, the more widespread a term is the less it holds to its original purpose. Case in point agile. Right. Weasel law number five, you can change your manager or you can change your manager or the version we also use, you can change your org or you can change your org. Yes. And weasel law number six, which, um, this is the newest one. They've come in order of having using them. You are not nearly as much of a snowflake as you think you are. Is that a law or is that, is that officially Alan entering into geezer hood? No, but I'm stupid. No, no, it's not that it's like, well, Brent, Jenny, I, well, it goes, it goes right to the software testers. You know, is it Jenny? I, it seems like a really cool advanced technology, but for the kind of testing I do, it's not really going to help. Right. No, that. And like, uh, you are not as much of a snowflake as you think you are. Actually. Yeah, no. Yeah. Your scenario. It reminds me of, of, of the three principles from, uh, how to measure anything, which of which I'm forgetting. He has three key principles. I'm nearly and the book is usually right here. I don't know what I did with it. It's one of the books I keep an arm distance. Mine I do as well. Oh, I know what you're talking. Yeah. Yeah. I know what you're talking about and I have them written down somewhere. God, where is the book? Find yours. This is what we do here. This is what we do. Uh, God, it's, um, it's like you, it's, there's three things around data. You probably already have enough data. You probably, or something that you, that's not it. Those aren't it. Those aren't the right ones. I know. I'm, let me, let me ask the better search engine. You have just a second. I'll see if I can find it for you to this. Now this is compelling podcasting. Yeah. This is really, and I'm gonna, I have a different way of searching for it. I'm gonna see if it works. Um, I got it. I win. No, no, that's it. You have it. I was going to accept, put this in my blog once, uh, the three, you have more data than you think you need less data than you think. And adequate, adequate amount of new data is more accessible than you think. And I remember even in a metrics course, uh, I did a taught at Microsoft a long time ago. We talked about these because yeah, people want to measure everything and, and see if any magic comes out, which anyway, you have more data than you think, which is true. You need less data than you think. Also very true. Adequate amount of new data is like getting, getting the new data you need to answer it like some nuance of a question. That's probably pretty easy too. The, the, and I, I'm realizing I may need to go back and reread this, his book to reenergize it and tie the context to, to current. Yeah. Because I look at this and I'm like, yeah, he's right on. Right. The, the, um, you need less data than you think. Right here, he's inspiring. Um, Hey, spend a little extra time thinking about what's the decision you're trying to make and do you really need to, to do this? And I'll, I'll tell you. So for example, I do a lot of A B testing, um, things for people. That's what we do here. A B testing. And sometimes the scenario that they're trying to validate is so infrequent. Like, Hey, if we do this, it'll stop this bug that happens. And the bug only happens at Wednesday at midnight and only in a random region. Right. It's a rare bug, right? How do you get the, the sample set enough to, to, to do an A B test, get statistical significance to a degree that you can use the rules of data science and bless it. Right. And the answer is, is you let it run for F and ever because of the smaller number of sample size, the more you have to let it run. But if you don't have the time to let it run, when you look at it, you measure it a cup, two, three times you go, okay, is it trending? Do we, do we see any evidence of having, right? You go, all right. You put what I often end up doing is I tell them what it says. However, I also say we can measure sort of the probabilistic angle around how we're seeing these results land. And we can then kind of accelerate, um, what will probably be our decision. If we let this run, it is a risk. It'll be wrong, but I can now measure the distribution and from that distribution, I can run through simulation and go, yeah, this is probably heading in the direction where it will not pass significance or it will pass. Yeah. Well, you've, you've applied critical thinking to data analysis. Right. I mean, no, I mean, I mean, it's, this is knowledge work and I, and it goes back to the, you know, testers, blah, blah, blah. It's, uh, it's, it's all knowledge work and not to minimize what testing development, lawyering, doctoring is we do our job based on the knowledge and context we have. And we focus on continuous improvements. So we need to adapt and do that work better. Uh, AI can help accelerate the learning. We need to do that. End of story. Agreed. All right. Let's, um, let's call that good, man. That wasn't a bad question. We're not going to do this every week. Sometimes we'll think of our own stuff, but in a pinch, not too bad, not too bad. And yeah, we need to make sure. Um, one of the things we need to talk to Jason about is how he gets, how he updates this thing with the transcript or podcasts. I don't know if our transcripts make it in. Let's figure that out. But, uh, no, no, no, we, we just need to, you know, tell them to make that happen. Yeah. Right. Because in a couple of months when we asked, I think we're doing that right now. Cause Jason, listen, Jason make your completely free, completely free to us all effort on your thing. Could you please put a whole bunch of effort into it to make your free thing more valuable to Brent and I. Right. So that next week, next week, we can ask a new question and it'll be a new one that a repeat. And can you sample our voices and just get like, do we have to be here for this? Uh, that would maybe not, maybe not. That's can I just go to chat cheapy to say, Hey, please post a new AB testing podcast episode two, two 11 on this date. And it just shows up and like, why not? Why not? We're not that far away. Please. All right. No, we're not there yet. It's not going to work. It's going to, but I can't wait to see what sort of confidently incorrect answer it gives. This is going to be great. Fred is asking it. Please post episode two, let the AB testing podcast by Sunday, October 6th. Um, it's now searching for that term. It's browed. It's browsing podcast addict. Unfortunately, that episode hasn't been released yet. So it, it gave you an answer that was correct. It's like a political debate when it didn't answer the question you asked. Right. We're done. Brent, Brent, no, Brent's going to be insistent. I want you to generate it and post it. Oh my God. This is the worst podcast, but you know what? It's probably better than AI for now. It does not have the capability to generate or post content. So, uh, oh, great. Now we have a sample script. Scroll down a little bit. We can act out a little bit. Then we gotta go. So wait, go back, go up, go up, go up. We're going to talk. We're just going to do a little bit. We got over our lines here. So Brent, let's kick things off with AI again, but this time let's take a closer look at how it's truly changing the way we approach quality beyond the hype. Right. We've talked about AI plenty, but there's something critical here. The role of data quality and bias in AI driven testing tools. AI is not the magic bullet. It's a magnifier of your data's quality. Then we have a discussion. And then I say exactly. And Jason Arbin has been talking about how biases creep into AI testing systems. He emphasizes that we can't eliminate bias entirely, but can minimize harmful biases in our data and models. This is huge for testers. All right. There's your preview of episode two 11. I'm Alan. I'm Brad. We're out of here. Did we just change our podcast to ABC testing? 
