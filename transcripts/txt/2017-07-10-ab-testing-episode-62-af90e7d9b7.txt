It's good to be seen by you. Our every other week meeting is commencing and we use that time to talk about stuff and record a podcast at the same time. Yes. Coffee. So what episode are we Brent? 62 Alan. Coming on along. Awesome. Welcome to episode 62 of AB testing. If you are a new listener, welcome. That means somebody quit because Brent and I only have three listeners we admit to. Yes. For those of you that have, I was just thinking about this the other day, like we talk about one of the three all the time and maybe there are listeners that don't know what the hell we're talking about. That's true. When we first started the podcast, used to joke about our three listeners and then someone came up to us and said, hey, I'm one of the three. So that's awesome. And the whole one of the three was boring. So and that's where one of the three dot slack dot com came from where our three listeners hang out. Yep. In their many different personas. As they rotate through their listenership. And I want to thank three listeners for all the multiple downloads they do every month. Yes. Hundreds. If you're if you know some professional podcasts have have place where you can go and get premium content. I'll do air quotes for premium. But if you want to join the community, you know, go to one of the three dot slack dot com. DM either me, Alan, or our honorary admin, Percy on Twitter. And we'll hook you up. Hey, Alan, you know, one of the things that that's currently one of my favorite. Items up on the slide channel. It would be I don't know. Okay. Is it has to do with kittens? No puppies that the kitten channel shut down. Oh, yeah. I've been posting pictures to sad puppies on the Internet. I'll tell you why later. What's your favorite thing, Brent? My favorite thing is the spontaneous posting of people's pictures with their new A.B. testing sticker on their laptop. We do have a limited run of one of the three special A.B. testing podcast stickers decorating laptops now around the world. Yes. I probably sent as many off of North America that I have sent in North America. Maybe a few more inside. But they've gone to Germany, Israel, UK, probably some other places I forgot. We're large and worldwide. We are. I want to. Not bad for three listeners. Not bad. Not bad. I'd like to thank anyone, non-native English speaker for struggling through comprehending Brent and I. And sometimes when we get on a rant, I think we are probably one of the more difficult podcasts to listen to at one and a half speed. Yeah. From the feedback we get is people will often go back and replay and replay and replay. When English isn't their first language? Sure. Sure. Last time, if you listened, you may remember that we had a great conversation. I thought it was one of my favorites, but we never actually got to the mailbag that we had planned to. So we're going to do a whole lot of mailbag today. But first, speaking of scoring on some cool swag, this episode of A.B. Spacing is brought to you by the folks at Techwell. And if you know Techwell, you know, they're the folks put on all of the star conferences. Star West is coming up in October in Anaheim. They gave me some script here that I always sort of one of the my consistency is in speaking is I always come back to star and star is a big conference, a good conference for when you want to get a lot of information, assimilate and have a lot of choices. But I have had some great experiences there and it seems to get better and better all the time. This is me saying it, not a script they gave me. Star West is coming up in October. It's October 1st through 6th in Anaheim. You know who else lives in Anaheim? Mickey Mouse. Mickey Mouse. You are correct. Yes. So Brett and I have talked a lot about maybe what could be called agile testing. But our approach is to testing, which align very much with what's happening in the world of agile testing and star not coincidentally given they're paying attention to the world, have 39 sessions on agile testing across the conference. Like test automation strategies for the agile world. Where did my testers go? Which actually sounds intriguing to me. It does. Test management on agile projects, which is also intriguing to me. Shift left testing. Now I get the concept, I'm kind of just for the record, FTR. I am burned out on this phrase shift left. You want to do right shifts? I just think it's like oh I won't. Yeah. I won't use the blue. Let me ask you, would you prefer shift left or lean in? Because lean in is driving me crazy. It's okay. All right. I'm going to move on. But the full title is shift left testing going beyond agile. The point is so much, if you're trying to transition into this world and you just want to get a bunch of different ideas, and where do good ideas come from, Brent? Old ideas coming together, making new little idea babies. Yes. And this is if you are, if you recognize that your world of testing is changing, pretty cool way to get a whole bunch of information, assimilate and make your own plan. Form your own opinions. And there's not a formula. It's going to be, it's context sensitive. Yeah. That's the truth. So you have to pick up these ideas. Nobody will give you a blueprint. If they're telling you they're giving you a blueprint, tell them to shut up. You need to understand the principles. You need to understand the philosophy. You need to connect these new ideas to your context. Yep. And in addition to the variety of testing and QA learning sessions, lots of networking events. There's the expo, there's speaker one-on-ones where you can get free consulting if you want it, or free just to chat with someone about a particular problem you have. And those usually speakers from the conference. And there's a bonus day session on women who test for women in testing to learn from and be inspired by each other. And if you are one of the three listeners and you're interested in checking it out, Star West is offering 10% off your registration with the promo code AB podcast, or, and I'll put this link in the show notes, the blog post that goes with the download. You can just go to well.tc, WAC, S-W-A-B testing, and I'll post that. Or just go sign up, just use the promo code AB podcast, get 10% off. That's multi-reusable. Take your whole team. Wait, if you take 10 people, is that 100% off? No. That's like... Yeah, so if you take 20 people, the PU. Maybe. Take 10 people, get one free at full price. Anyway, thank you, Techwell, for sponsoring this episode. And please check it out because it's a cool conference. I will be speaking at a star sometime in the next year. Wasn't our promo code last time AB testing? It was its AB podcast. Yeah, so for those who have used this in the past, it's the new promo code this time around. The way I do it right now is I go around the internet and I buy things, because that's what I do, I guess. If there's a place for a promo code, I always just put in AB testing to see what happens, because you never know. How many times things have happened? That's worked exactly zero times so far. Okay. So Brent, are we ready to roll on? We are. Let's go. Are we ready to open the mailbag? Yes, I am. Okay. So we are going to do a mailbag clearinghouse. I was going to say some answers will be short, but we're not capable of that. No. It is sad. Our first question is from, oh, and by the way, if you are not a member of one of the three.slack.com, Brent just laughed because, oh my God, Alan can't non-tangence aside. We do have our Slack channel, but if you, like at least one of the three, just like, I don't want to bother with Slack. I have enough distractions throughout the day. You can just go to angerweasel.com, whackabtesting, where our podcast is hosted, and there's a form you can submit that occasionally gets good questions to us. It also includes a bunch of, of course, all the spam search engine optimization plugins have send me mail once in a while, but so far the traffic isn't too horrible. Anyway, there's a form on the website where you can enter a question, and that's where David entered this question. David asks, what is your opinion on model-based testing? I know in Visual Studio 2012 included Spec Explorer, a tool we use for model-based testing. What's sort of going on with that? I will give my opinion first. I think I've used model-based testing more than Brent has. I don't know if that's true. Maybe not, but I do know that the very last chapter I wrote of How We Test Software at Microsoft was actually my favorite chapter, and one of the chapters that is still relevant on model-based testing. Interesting. I find it interesting. We've talked about this before. I find it interesting that in your book, there's still content that you find relevant. Some of it, yeah. Okay. Yeah. Model-based testing. My opinion on model-based testing, it's a great testing approach when used in the proper way. Like most test techniques used in the proper way, it's great. When you have software that behaves like a finite state machine, and you can automate, it's easy to write the oracle that can verify the state, and the automation that executes the transitions, you can make a pretty good model-based test suite that can basically a very, very smart monkey that can traverse your application in ways that maybe you didn't think were acceptable, or possible, or usable. I use Brent, a lot of database background, CRUD operations in a database. I think you're a good example of where you could apply model-based testing. Yeah, there's a lot of uses for it. When I think of this question, I think about ROI. You brought up smart monkey, and that reminded me of a very old deck I did a long time ago around how automation progresses. I classified model-based testing as a dumb monkey. And the main reason why is it just follows paths. Model-based testing is a great tool when done right and done cheaply, and that is often the challenge. And I'll correct you on what you said, model-based testing is a great tool, but it's really just an approach, a technique. What happens is, I want to bring this back to David's question about including Fekk Explorer. Fekk Explorer, a good tool, a little too complex for some people to figure out for doing model-based testing or creating models. Oh, Microsoft, when Microsoft did all the protocol documentation as part of some European lawsuit and they had to go back and one create specs for these things, then prove the specs for correct model-based testing protocols are very state-based. Great example for that. What happened, I've seen a lot of people fail in model-based testing, and I've said this before many times on and off the podcast, is the same way they fail in many other testing approaches. Or they want to know what tool do I use to do model-based testing. They want to start with the tool versus actually understanding what model-based testing is and whether or not it applies to the thing they're trying to test. That is a huge mistake. Well almost always. The Fekk Explorer, so I've used Fekk Explorer on multiple occasions. The real big challenge with Fekk Explorer is the dependency on a solidified spec. The theory is fantastic. Hey, if we can write up the spec in a formal modeling language, then we can use that, pump into a model-based exploring tool and use that to drive load on whatever it is that we're trying to test. Of course the challenge, and I've seen this work on protocols, I've seen it work on services, I've seen it work on UI. The real challenge with it is your ROI only comes when there isn't a lot of dynamicism in how the product actually functions. The other one is it can also be a high cost to build the model in the first place. The requirement of going back and updating a spec, that was, in my situation, it was a challenge because once PMs, once they write the spec, they don't want to go back and update it unless there's going to be some further use of that spec. Let's say it's going to get published as a formal help file or something like that. But a spec can be, you're talking about the Microsoft style, big aspect, where a spec can just be a state diagram. This is how it's going to work. Now if you do that in Spec Explorer, you're not using it to feed your model. You don't need, if you go to GitHub, you'll find many tools to help you with model-based testing. But the main thing is, if you're doing it on a small scale, you can do it yourself. For me, back in the day, the thing that I found most helpful about model-based testing was what I did with my team at that time. This is one of the times I was in a middle manager role, is we had a need for model-based testing and we used it to drive exploratory testing from an automated fashion. We had a principle back in the day, this is when we still had labs, and we're like, okay, the CPUs in the labs better be doing something at night. This helped fill that gap. It was still a bit of a vanity metric. Once you have a successful MBT, yeah, it will find bugs as long as you teach it the paths. But a lot of the times, the bugs it finds are things that you and I would look at and say, okay, great, what's the customer impact? Now with the old-school model where everything was a shipped product, those would be things that would be fixed. In today's world, we'd be like, okay, is it worth fixing? But the real value was, and it's aligned with the discussion we had with Hans several episodes ago, the real value that I found is when you hook up model-based testing, filling up a suite of atomic methods that your model can call into, that suite, those set of modules are then relevigable by your static test case suite is what I called it at the time. Then once you have that, hooking it up to other algorithms, like my favorite one back in those days was a genetic algorithm, which is a simple data science model where you say, all right, here's all these actions. You figure out how to chain them together. I think as for the, and again, we're incapable of short answers, but I think as for some of the fun, the interesting, maybe at least just from an academic point of view, bits of model-based testing come into play where you can apply a bunch of methods from graph theory to models. Shortest path, longest path, most, whatever. And then if you hook up AI techniques, you can have the model, this is where I start calling it a smart monkey. You can have the model not just actively exploring, but actually learning and trying to discover things like an easy one back in the day was, all right, find the shortest path to this bug. Once you find a bug, now find the minimum number of steps necessary to reproduce this bug. The genetic AI stuff back in those days was just fantastic. And today, I wouldn't go into model-based testing unless I found a cheap way to automate building the model. Yeah, honestly, and to close this, we've beat this, but I would add to that or build on that or throw that away and give me my own idea, which is, I think it's a good technique to be used in the right place still, but you should understand what it is for you to dive in. Don't just wave around a model. I'm going to do model-based testing on this whatever that doesn't. The maintenance costs for model-based testing can generally be quite high. Yeah, I think even at the sort of unit integration level, you could, again, you can do lightweight model-based testing. And if you want to exercise a bunch of states to make sure you have coverage, if you're worried about the sorts of things that model-based tests might discover. Yeah. All right, I am going to, I think we have two questions that kind of relate to each other. So I'm going to set up a twofer. Go for it. First of all, Heath asks via the submission form on the website, I've listened to a few of your podcasts and correct me if I'm wrong, but it sounds like you are more or less approaching conversations of testing from the developer point of view than from someone who purely does testing as their only job, whether functional or automation test. What I'd be wrong in assuming you foresee the testing role as down the road being mostly phased out and being handled solely by developers. And my quick answer on that is ish. But I think the main thing is I see that there is a role on a development team for a testing and quality expert who may likely have some coding and some technical skills. But they are the person who makes sure that just like you may have a database or a back end or a performance expert on the team, if your product or feature relies on that, if your product or feature relies on quality, you probably have somebody on the team who's more or less an expert in those things to help coach the team, write some of the tests, do some of the things. And whether you call them a developer or not is sort of beside the point. And then before Brent jumps in and we lose it all, I want to tie that into a question that Percy asked on the Slack channel, which is almost an extension of that question. He says, in regards to accelerating the achievement of shippable quality and statements like this from Josh and four colors on Twitter, where he says, and Josh is a listener of the podcast. Hey, Josh. Sup, Josh? He says, is the role of the tester simply evolving and expanding its tasks? Or is it time to introduce a new role to avoid reduced confusion for all involved in IT who might not see, understand the role of testers going that way? That's actually the question from Percy. But in response to Josh's tweet, which says, modern testers should strive to find valuable work on their teams to help their teams and help the product. This is a good way forward, which was, I think Josh also watched my keynote at the tech conference. It's interesting. I'm, it was an evolution and I'm going to start this and then I have no idea what I'm going to say. So I'm going to let you jump in in a minute. He just hold those thoughts for a second. I am sort of against making up new words in, in software engineering or new goals or new, I believe in the specializing generalist and the generalizing specialist. Damn Percy. That is a very good question because there is a lot of baggage that comes with not just tester, but the, uh, the hundreds of different titles we've given people in testing in Heath's question. He mentioned, uh, whether a functional tester or an automation tester. And the fact that we have functional testers and automation testers and, uh, test analysts and UI testers and the, and these testers and functional test analyst automation architect does make me think that maybe we've just used up the test titles and, and for a role like Brent and I are advocating and for people like Josh and many others see testing moving toward does that role need a new name? Will it always be a tester or will tester always have that sort of baggage and stigmatism with it? I can jump in now. Yeah, please go ahead. All right. Cause I don't have an answer, but we're going to discuss and see what happens. So many years ago, one of the most read. I wrote what is now one of the most read, uh, blog posts on, on my site that was titled test is a four letter word. And the point of that post was to call out that you get a bunch of people into a room that statement test is the, a four letter word is in fact, the only thing you will get agreement on. Yeah. To Percy's question, if the shift to what we're calling modern testing constructs, uh, more specificity around the role, then yeah, I can do it. Right. The, there is a lot of baggage with the word tester. No one actually knows what it means. Yeah. You ask five testers what they do. They'll give you seven different answers. Right. Um, uh, and as I call out in the blog, if, if I were to tell you, Hey, Alan, I'm going to, uh, code this problem this weekend, do you have some sense of what I'm going to do? Really? If I said I'm going to code this, this solution. Oh, you said code this problem. Oh, code the solution. Okay. This is maybe a little bit easier to say. Right. You're going to, you're going to write some code to do, to solve your problem. Right. Now, if I said, I'm going to test this solution this weekend, what am I going to do, uh, any one of a hundred different things, right? It's, it's, it's an ambiguous term. And for, and I do think that that term itself has caused a lot of the codependency, the anxiety, the, uh, problems, because again, no one really knows what we're doing and a big part of it is, is test itself. We haven't, uh, to Josh's point, uh, which is our point. So I'm going to take it back, take credit back. Um, like the biggest thing is, is that you need to have whatever the role is, it needs to make very clear what you expect from it. And in a world of, and I've been thinking about this while you've been talking in a world of specializing generals, does it maybe just, maybe Microsoft got it right. Which is something that's rarely said. Just give everyone the same title and they are the first person company to do this, uh, and stop worrying about it. Were they on the surface? Cause they did, they did it wrong on so many levels beyond that. Brent's nodding, even though he works there. So, so it's review time here. Oh my God. And it's that annual time of the year where, where I'm very conscientious of all the things I, I don't, so yeah, I enjoy have lower blood pressure and. All the joy that comes with that. But anyway, if it is impossible to have a discussion without a tangent and a tangent tangent conception, I wonder if going back to Heath's question. Yeah, I think my hunch is the tester rule is fae the tester title is maybe phased out for a couple of good reasons. I don't think testing is phased out. Testing will still happen. May happen by different people. Uh, more testing is happening, uh, by developers and developers are very good at it better than many testers want to give them credit for, but there is, we'll always need that quality and testing expert as part of the team. And furthermore, since I don't care what they're called, uh, coupled with the task, coupled with the fact that the tester title has baggage, I would like to see the tester title go away. I think it makes it solves some problems. So that test is a four letter word. I completely agree. I don't know if it needs a new name or we just call everyone just like I don't call the perf expert perf guru. I think what we're going to see, and I don't, I don't know. I don't really have a sense of a timeline, but every year I see more momentum towards this, this direction than momentum against it. So I do think, uh, eventually testers are phased out. I'll just be explicit. In addition to that, in from my own experience, for those who are managers or, or trying to sort of route their team, like Percy's a manager or I don't know Heath's role, but in the example here, we'll just presume he's a individual tester, right? For those who want to control their own destiny, what I have discovered is, um, if you target that the role you're going into is a developer role, and it's going to be a specializing generalist, that your specialty will be your testing expertise. And I fully believe that the path for, there are testers today and Ollie Blunt as well, since we're on that path, there are testers today who will just not have jobs in five years. Just, they don't, it's the role is becoming one that needs a lot more expertise. I'm going to go back to my, comparing with the, the, the, a lot of the old value propositions aren't valuable in the future. Sure. I'm going to go back to the perfect expert idea. And you're not going to start off a brand new person. Okay. You're our perfect expert. Please ramp up. No, it takes years of experience to get, to be good at that. I want to rant a little bit again about how often I see posts on Twitter and on forums of testers who want to get into test automation. And then once again, I'll say that is not your path forward. If you are good at accelerating the achievement of shippable quality via providing value to our team, technical contributions, getting in and digging deep as needed, those are the things you want to be good at. Those are the things that will make you the quality and testing expert for your team. That's going to find you a job long, long, long, long time into the future, because that's what kinds of roles will be available to the people who are good testers today. Maybe, maybe. Right. Cause the other thing too, you got to pay attention to is the tooling coming up in the open source community where there's a lot of, there's a lot of companies that have pure developers. They, they, um, don't hire test or pure testers. But they have these testing problems and they're working through how, how do we automate these things such that we don't have to hire these guys? Yeah, perhaps. But I would say that the testers that have been paying attention and doing testing things for several years, they have gone through a lot of the failure points that these teams are again, using the open source tools, all the glue is there, but they don't know, they don't know what the tip, what the traps are, what, what, what things to try and do, what things to try and avoid experience tester can again, accelerate the team by identifying those. I guess, I guess the point I'm saying is, is, and we, we talked about it in here right now, the current shiny role, uh, in software is the data scientist. And, you know, I can easily see a path where 80% of the problems that I work on, on a, on a daily, weekly, monthly basis, uh, readily automatable in the, within the next 10 years. And then if we connect the dots between the testing role and how, how you and I have been talking about leveraging telemetry, when you add that to what I'm predicting for the data science role, like when you, when you have an agile software development team and you have a backend decision-making framework that kind of 80% of the time accurately tells you what to do to improve the quality of your software. That's going to be good enough for a lot of companies. I think so. I think there's agreement, but a difference in approach and point of view on this. I think the coach, let me clarify, let me clarify. I think the coach role is going to be, it certainly is critical right now in the phase we have right now, as, as these companies are going through these transitions, um, people who go into this role, I think they're going to have a lot of meaty work in the next five years for sure, but they can't sit on their laurels. They have to, they have to continually figure out, all right, what do I need to be learning now that's going to be valuable when my current role isn't? Yeah. And I like, you actually, uh, got where I was going, bringing up that coach role. Yep. I don't know if you paid attention, but there are a zillion, I counted, uh, agile coaches out there, people coaching agile. Yes. Agile, there's a lot of tools out there. There's a lot of books, great books. You could learn it on your own yet. There are a zillion agile coaches. You know what else there is out there? There's the, and it's two zillion openings for agile coaches. Really? Yeah. So interesting, maybe a zillion, maybe a zillion and a half. So I could see a place down the road where the quality coach, maybe it'll just be called an agile coach, but, but there is perhaps a path there. Uh, a parallel path. I'll call it a parallel path for coaching and testing as teams begin to get good at that. A lot of teams don't have agile coaches because they've had one before. They figured it out. They're rolling along great. I think it's part of that transition. Many teams may need, they're aligned, but 30 coach, different specialties, absolutely different specialties. I'm just saying that, that even though agile has been around for a while and many teams do get it, there are still a whole lot of agile coaches and teams looking for agile coaches. Uh, that makes me think that five, 10 years down the road when, uh, the quality testing role is something that's coached into a team versus a separate independent team to do it. There may be a parallel there. I need to look that up because as an agile coach, I'm just thinking through like, what are these people looking for it such that they would hire on a full time agile coach, because right now in the roles that I've been doing, that wouldn't make sense. Then to address perzies question, if, if I may, if you go, oh, please, Brent, is it time to do a new role? Maybe, but I do think that that new role is an interim role. Cause I do. Uh, and again, from my experience, what I find is if you, if you push these guys to be the next generation developer, you're going to have a much more successful development process. That's the direction that I would go. It's harder to learn how to be a talented tester than it is to be a talented developer. Sorry, Dev. That's my opinion. I'm sticking with it and I have a concrete experience that proves it. Finding the O of one sort algorithm is not hard in today's world. It's a Google search. Finding what makes customers happy. That's hard. And, um, a Google search isn't going to give you that context today. So I think it's going to close out both of these, my opinion. I'll give you the final word. My opinion is yeah, it's phased out. There, there may be some value in creating an interim role. I have no idea what we would call it. Uh, and given that I like Alan's idea of, uh, you know what, screw this. Let's just call it engineer. And, and stop associating unexpected and an ambiguous value with a group of people. All right. And, and I want to address Heath's question directly where it says, uh, the part says, it sounds like you are more or less approaching conversations of testing from the developer point of view. Uh, not directly. That's where testing, we're talking about what Brent and I are calling modern testing where testing is evolving. Yeah. Where the tester is part of the development team and their responsibility isn't to test everything that Dev team writes. It's to accelerate the achievement of the shippable quality. I will say that, that I don't view me approaching this problem from either a Dev or a tester point of view. I'm approaching this from a business leader point of view saying, if, if my job is to make a better product, make customers happier, I don't care about the current distinction between a developer and tester. I think that's the important part. I need software developed. I need it to, to, uh, solve customer problems to the point where those customers pay me to solve more of their problems with software development. Yeah. And that's the point that we'll use track of in these, uh, in many of these discussions and sometimes philosophy wars around the need for test is the goal isn't to justify your job as a tester. The goal isn't to find bugs. The goal isn't to, uh, test quality into the product. The goal is to get software to your customers that they love and they love enough that you, they want you to make more of it. Yeah. And if you can be in a role that accelerates that process and makes that process more efficient by that's, that's what you do. I don't give a crap what you call it from a pure economic point of view or a pure business point of view. The decisions made there are entirely apathetic to any individual's, uh, loyalty to a discipline. All right. Right. So I like how you always keep say I can have the last word, but then you keep on adding onto what I say. So we're going to move on to the next question. Now, it feels bad. Ha. Okay. Uh, one last question for the day and we probably won't finish it, but so we'll leave some loose ends. We'll finish it. Okay. Uh, from Derek who is one of the three, uh, Hey, Derek, uh, you mentioned in episode 60, very good episode. Go back, listen to it. Uh, that the maturity model a tester should follow is collect inform, recommend, and act. The example given was related to analytics and seeing as something was being used and if actually had metrics to measure, if not, then recommend that get implemented and so on. I have a few questions. And Brent, you may want to editorialize on, on collect and form, recommend act later, but, uh, questions are, I have found that sometimes the role of product manager and tester can sometimes overlap. Yes. I apologize if I misunderstood, but then is using that model. You mentioned an example of this. Where specifically is that line drawn between product manager and tester as a tester? Am I using this model as a baseline to evaluate the product overall and, or when I discover a bug? So I'm going to go first and then, because you're a model, you can talk about it more, uh, and give more examples. But, uh, what I've found in, and I'll relate it back to the last question, this role of modern testing and where teams are going, there are very few hard lines, in fact, uh, my team's no hard lines. Every line is blurred and overlapped. That's kind of how generalizing specialists and specialized and generalists work for my team. Yep. So I don't worry too much. I have testers who definitely and explicitly play a product owner role, product manager role, because there's nobody doing it. So they fill in a little bit on that side to make sure that that chunk of work is getting done. And that's okay. Some do development work. Some do data science work, other work. That, and there are no hard lines. Although I do as a manager, my job is to make sure I'm not stepping in any toes and the right overlap is happening. There are, there are, there's work to be done. There are skills necessary to get that work done. And there are people who have those skills. So, uh, I'm going to paraphrase Derek's question, which it really says, dear Brent, would you please talk some more about the model of collect inform, recommend, and act and in doing so remove some of the ambiguity I've discovered while I've been thinking about it. So when I saw this question, I had intended to do some deep thinking on it, uh, and did you write a, no, and write a blog post on it. And I still intend to do that. And maybe I'll do that over this weekend. I'm off to wazoo to register my son, uh, for college. Wow. And it's interesting. I wish I could do that. My oldest is 13, so I can't quite do that yet. So he has to be there for three days for, for registration. And so, so do the parents. That is, that is purely an exercise done by the university to generate income for the town of Pullman, Washington. I have no doubt. I'm like, why do you need, you know, and they require the parent. I'm like, why do you need three days? I would just send them off and say, just tell them you're an orphan. Yeah. I feel like I'm going to be going there and, and being meetings about the new wazoo timeshare. Um, oh yeah, they probably want you to buy a timeshare for your kid to live in for his, uh, six years there. Right. All right. So first and foremost, uh, I would not say that I have formalized, uh, CIRA as a test maturity model. And just to clarify, KSTM is that CIRA is not the alternative to Siri on the iPhone. CIRA is the collect inform recommend act. Right. What I, if you, if you go back and listen to that episode, what I, it is a data science maturity model that, um, I believe can map to the modern testing model, right now, there is an overlap between PM and tests in this world. One of the things that I actually didn't like about our, our example, when we talk about recommend, Hey, let's recommend adding telemetry. Yeah, whatever. I mean, um, it kind of reminds me of code reviews, uh, where inevitably whenever you do a team code review, you have the one guy who all he's doing is looking for hard coded constants or hard coded strings instead of constants, right? It, what we want to be able to do. So the, the whole collect inform recommend action model is intended to be goal driven. So a you as a tester, you need to look at your techniques and your skill sets, the specialty that makes you valuable to that organization. Think about the business ROI. And then in this maturity model, right? Figure out what you need to do to collect data, connect the dots between the, as we talked about earlier, collecting the dots between your talents, what the business needs and what the maturity model is, what we're saying on the modern testing world, uh, in particular, we're saying just informing people. That's a, that's, that's over. Like move on. But now what we have to do is figure out our techniques with our techniques. How do we then bolster it and say, this is the recommendation. Now I have worked with multiple PMS. Okay. They are not in general good with working through data. What they are good with is coming up with ideas, um, socializing with the customer, um, getting in, receiving feedback with that customer. Um, and I think they're going to continue and sort of persist in that role. In terms of recommendation, I would say that the, that the tester role needs to be a bit more tactical in this. Whereas the PM role is going to be a little bit more strategic. I'll let me try to be a little bit more concrete. Like one of the, one of the act, uh, things that, uh, we're doing here in Azure is we're trying to improve deployment cycles. Okay. And with there's this mantra that we have is safe deployment. Now the more you make a, uh, so what a deployment is, is, is essentially a new code package being, um, rolled out into production. So what makes it safe? Well, safe means, um, bugs have minimize impact, right? So there's a quality aspect to it. But the other act, the other aspect to it is, um, you can make things safe. You know, add three months of big time that'll make things safe. So it's how do we balance the, the, the quality aspect of it while tightening up the cycle time. That is a very concrete problem that a lot of, uh, companies have today that I think aligns well with, with the testers, uh, skill set. If tests can then make a recommendation around, Hey, these are the types of things that we should proactively test. These are the type of monitors that we should put into our ring zero deployment. And these are the ones that we should continue to monitor as it goes out to production with the idea of, um, making a recommendation of either reverting or moving the deployment forward. That is something that I think is very strongly something that the test has talent in doing. So, uh, I'm going to jump backward to the last question in a, in a tangential comment, but as I've heard you mentioned, and I agree with everything on the tester is good for this because testers, the good ones are traditionally systems thinkers, they're good, critical thinkers. They see the whole picture. They're good at gathering data and making decisions or they should be, if you want to be successful in the future. But, uh, I don't know why this didn't pop up in my head earlier, but I had a little revelation, a little light bulb went off. You don't know if you saw it or not, but, uh, maybe we do change. We do come up with a new role, a new name. And cause I heard you say tester tester tester. It's not right. It's not right. But in the sense of accelerating the achievement of shippable quality, what if we called the new role, the software accelerator brand doesn't like it, but I'm going to throw it out there. And I'll tell you that, I mean, I, I like, I like the way you're going. The thing I don't like maybe quality accelerator QA. Who? No, that's not good. The, the, the, we could just repurpose the acronym. Like last year and call it quality assistance. And they're very excited that they assist developers. The issue I see with, with the software accelerator quality accelerator. It's QA. The, well, what you initially said is the idea of, if I saw that title, like I would expect these guys to be like on the engineering systems team. Right. Working on build or things like that. One of the things. But wouldn't that role also be accelerating the achievement of book quality by making builds faster? Yeah. As long as it was well, and actually that, that, that, that, that, um, springboards into what I wanted to say around the recommendation. I apologize to Derek for the tangent, but it's expected. All right. All right. Pop the stack back. What you're going to say about, uh, informing and acting. Well, so on recommendation. Right. Easy recommendations like, Hey, let's add telemetry. Like anyone can do that, but the hard ones are around what is the risk. If we take an approach to tighten cycle time on a deployment, say to two hours, deploy it in the wild, how do we mitigate that risk and how do we measure it? Now. That measurement aspect. That's something that I think is, it would is best suited to be measured in terms of customer impact, um, which then kind of brings it into, uh, the QA realm. But the problem with that is you kind of time trying to simultaneously optimize two variables. We want the highest quality and the safest deployment and striking the balance often is a subjective call. And what I'm saying in this case, I think it's something that QA is well suited to do with their knowledge base. I agree. Now driving it to action. Right. So the first step is recommendation. And this is where you put together manual reports. You visualize the problem and you, you make it very clear to others why your recommendation is viable. The recommendation is a data-driven decision because you've collected before then. And once you've gone through some period of time in terms of, uh, people establishing trust that your recommendation is fantastic, then it's just the action part of it is generally just automated. Now there can be a lot of complexity, right? Um, uh, and I don't want to, the thing is I, so what got me into data science, um, is I never cared about testing. I've always cared about quality. Quality is fascinating to me. And, and I found that data science allows me to deliver that. Now the, the basic testing skillsets, uh, around, uh, around data is probably not going to be sufficient, but any sort of data science 101 course is going to introduce you to 80% of the basic techniques. Of course. And, uh, there's a lot of value in just those techniques. I want to, you said something a minute ago, you didn't pause for dramatic effect on enough, which I think is really important, which is you don't care about testing, you care about quality. Right. And Whitaker did that talk several years ago. I think it was all this testing is getting in the way of quality. And when I think about the really good testers, I know, uh, they're driven by quality much more than testing. Well, and it's just, it's something we talked about. Like we don't care about the activity. We care about the goal. Yeah. And that's, that's an important thing for, I think all three listeners to reflect on and reflect on with your teams is remember that the goal is quality. The goal is to make software that your customers love. It isn't to run a million tests or get 99% code coverage or whatever. You want to make software people love. And that's the goal. It isn't, isn't like, Oh, well I did exploratory testing and I found six bugs that customers never care about. You want to make software people love and use and do it before your competitor does. Yeah. Right. So there is the quality and time cheap, fast, good, um, pick two is what every MBA one-on-one class will teach you. What we need is good and fast. And what agile does is it helps to optimize the expense. Yeah. Like I like how Reese calls the point of a startup. It's to come up with a viable business plan before the money runs out. Yeah. I forget the, the remember the name of one of the blog posts. So maybe just a few months ago, I did a two blog posts within a couple of days of each other, one called the fallacy of now, like, Oh, we got to do it now, no matter what. And one kind of called failure to launch, which is trying to get things perfect before you get them out and losing the market opportunity. And I think, uh, uh, that balance is, uh, very difficult and again, one that can be very data driven and, uh, and a risk can be assessed and measured by people in the QA role. And it's, and it is far from a black and white world, right? A lot of the times as an agile coach, when I go talk to teams are like, but we can't do that. And when you decompose it, it's essentially they can't do that because they haven't learned about continuous integration and their current build system or their current architecture. Doesn't support it. But then I said, okay, great. So you may not be able to do it now, but you can do it and you can start. Right. It's just code. It's not magic. Everything is baby steps. None of this happens in big chunks. Everything is small steps. All right. So, you know what I think we've done. I think we cleaned out the mailbag. Mailbag episode number three. I don't know. Complete. Uh, thank you for the questions. Uh, it's, I always learn a lot just from talking through these things. So, uh, please keep them coming in. And, uh, we have, uh, plenty more topics, non mail bag to talk about, but we'll, we'll try and fit them in a little bit more consistently in the future. Maybe. No, we won't. Okay. Brent. I was trying to be all like, uh, we're improving and reds going, no, we're not. We're done. No, the, the ADHD aspect of our thing is part of our charm. ADHD testing. All right. Okay. I'm still Alan and I'm Brent and we'll see you next time. 
