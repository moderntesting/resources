Good morning, everybody. Hello. I'm Alan. I'm Brent. And guess what? It's a detest. Episode number 35. You know, I've been in this room for a few minutes. I just noticed that Brent has no facial hair on his face. Yeah, I'm going back and forth between trying to decide, do I want to look younger and fatter or less fat and older? Not touching that with a 10 foot pole. Hey, so not only are we here recording at our normal time of 8 a.m. Good morning. But it's a daylight saving time weekend. Yeah, that was fun. And the weird thing is, for the first time in as long as I couldn't remember, it snuck up. I didn't know it happened. I thought my clocks were wrong. I thought there was like an AT&T bug for a while. I got up yesterday morning getting ready to go do some homework. And then I looked first thing I do is I looked at my phone and I said, wait a minute, why does my phone say it's an hour later? First off going into, oh my God, my phone's broken. Then even worse. Oh, wait, I may have just lost an hour. Yeah. So which caused some frustration because my son had to be at baseball practice and deep in the bowels of Seattle early that day. And I just lost an hour in prepping. Oh, well. Oh, well. So it's early. I'm a little tired, but I think I'm OK. But it reminded me of a story I wanted to tell. Actually, a story I don't think I've told before on the podcast. What's that? Whoa. There was a time must have been around right around the turn of the century. It's great we can say that now, right? Right around the turn of the century when I had just joined the Windows CE team and was doing some kernel and driver testing. And but I was also in this test architect role and one of the the role is still pretty new. I didn't know what it was, but it was a really weird spot in the product cycle where it was kind of almost too late to jump in and do like anything major. But there was a bunch of work needed to get done. So the first one of the first things I did was sort of get an overview of what's getting done on the team, what's not getting done, what are the plans, try and get an idea of what was happening with the product. And one of those meetings, I found out that there was this all this work for the Shell and Windows Explorer on this is on Windows CE 4 3 3. One of those versions back there. Probably three. He said this work is all planned, but there's no developers for it. We're not going to do it. I looked at it and I said it was like five features and 20 bugs. I love fixing bugs. It to me, it's a very fine line between that investigation of testing and figuring out, oh, that's all the problem is. I can fix that. I'll go ahead and do it. So that's so for about three months, I was a developer. I was the shell developer for Windows CE. Did you do all five features? I did. And the reason the story is actually relevant, despite your wonders about that, is that one of the features that I wrote was implementing a DST check, Daylight Saving Time Check, on Windows CE, making sure that the clock changed at the right time. When is the clock supposed to change? Ah, it depends on the time zone. I don't want to dive deep into the implementation, but the point is I know because of the effort I put into getting this right and in making it right for everyone, that Daylight Saving Time is a place that's just ripe for finding bugs. Absolutely. One of the things I learned as a developer was when you were, and maybe some developers don't make this connection, but when I'm maybe they're really proud of the cleverness, they got it to finally work, at least on their machine. But for me, I finished that and I thought, this is probably never going to work. So I immediately, this will tie in some other topics today, I immediately began testing it. I tried different time zones and sending timers. And in addition to looking at the time zone and when the time is supposed to change, what day and date, sorry, what date and time, there's also two different paths where that update can happen. When your machine's on, like it happens to be running when that happens. And then when it's off and you boot into the new time zone. Right. So I tested the heck out of that thing. I'm not going to say the code was bug free. It worked in every situation that I could think of to try. And I would love to say that there were no customer, but actually, I can say there were no customer bug reports ever on DST. But I don't think that many people actually use the feature. I don't think CE was used that much at that time. It didn't do an auto auto switch. I'm sure some for D for DST. Yeah. But I don't know. Again, this wasn't like something you went out and bought a device with the Windows shell on it. I mean, the Windows shell was sample code. So basically, I wrote DST detection sample code for embedded developers. But at that point in time, that that I think was Windows Phone 6 5 days. No, it's before then. Before then. Before. Oh, really? This is before Windows Phone 5. This was in. Well, the thing is, I'm willing to bet that that code went forward. Like who was going to rewrite that code? I don't know. I don't know. It might even be the code. I am here. I hope not. No, it's not. Brent's holding up his Windows Phone because Windows Phone is now based on Windows 10. Windows 10 embedded, not Windows CE, which was a whole different operating system. This was a true embedded. Well, I guess it's true embedded also. Very small, a true, small, componentized OS. I remember my friend and I used to play foosball all the time. He was actually the architect and I played foosball. Really smart guy. But we didn't like walking the 100 feet to the table to find out somebody was already there. So you younger listeners may not remember floppy disks, but we had a floppy disk. One point four four Meg. It could boot our OS with a 1394 driver and a camera and a and a website off of a one point four four. With the whole thing off there. So basically ran a foosball table monitor that booted and ran off of a floppy. Was it a camera? How did you monitor? Just a camera and a website. Oh, gotcha. We could actually do HTTP servers showing a video stream. I love it. Try that today. Try and boot that damn thing off of a two gig USB drive. Hey, I have a are we done with the ST? I have a surprise topic. Oh, yeah, we're done with the ST. I don't know if I like surprises. I'm tingly. So I got you a gift. A gift, a gift. It is. I hope it's appropriate for podcast. It's not as relevant as it was when I ordered it, but this will probably shift into. And I know I know only two of the three at the most play video games, but and I've been so busy, I haven't played my Xbox like in two weeks. But Brent and I was it just last episode. We haven't reported to episodes. Two episodes ago, talked about the fact that we were all like playing Fallout like crazy, playing Fallout like crazy. And you can only laugh if you know it's gift means nothing. You haven't played the game, but it's a vault boy agility bobblehead. And it actually bobbleheads. Oh, my God. I went and looked at it and I'm like, ah, which one to get them? That is they didn't have one that was generically just special. Otherwise, I would have gotten that one. But the next appropriate one seemed like agility. Thank you very much. You betcha. That is fantastic. That will go up on my trophy case right next to my little doll of Deadpool. Did you see Deadpool? I have not seen Deadpool yet. I don't know if you ever read the comic. I'm a big dead Deadpool's hilarious. I love Deadpool. What's his face? Ryan Reynolds, the guy that you feel like the part was written for him. There could be no other actor in the world that could play Deadpool. That's my sense from ceiling seeing seeing the trailers. Right. I think Ryan did. I'm with the rest of the world and saying he did a horrible job in Green Lantern, but because because Green Lantern is DC and all the DC movies suck. I was a big I was I have a heavy Marvel bias from my childhood. I do as well. My my favorite character was Thor, and I think Marvel has totally screwed him up. Yeah, I don't like what they've done with the Thor movies. It's kind of weird, right? And they the they're saying that the next iteration will be Thor as a woman because anyone can pick up the hammer. Yeah. And there's a comic on that right, too. Yeah, I'm just like, really? So this is the A.B. testing podcast. We should get there eventually. Yeah. So I sent Brent some mail this like three weeks ago. And the gist of it was 10 days ago. Was it just 10? My world is just weirdly paced. It feels like it was so long ago. So much has happened since then and it made so much progress on that. It's weird. But anyway, the mail basically said, hey, Brent, you know, I can read the mail. Oh, why don't you read the mail? And for context, those of you may be listening to your first testing podcast, maybe your last week or you may be shutting it off right now. But I am the quality guy on an engineering team. I'm the guy that has to make sure that probably put it. I'm the guy responsible for or making sure that quality work happens within the dev team. We're thinking about horizontals and we don't let anything follow through the cracks. There's the context. But I was running into a little bit of a problem. And as I as I do when I have questions or things I want to get some more ideas on, I will reach out to those in my network. Brent is in my network and I sent him the following email. Getting to zero bugs is the title of the mail. Yes. Brent, curious on your advice here and how it compares with what I'm doing. We're down to fewer than 100 bugs across our product and continuing when the wheels fell off. Bug counts and bug age are drifting up. Excuses are I want to fix this, but I have more important things to do. We're going to get a bunch more bugs when we're going to internal dog food. I don't want to stop writing features just to fix bugs. And those are a little exaggerated, but I wanted to I wanted to tickle something in, in Brent's rhetoric. Yes. Challenges at most teams are overbooked on features and used up their quote unquote buffer time to add more features. To me, the challenge isn't caring about bugs, but helping the team balance features, bugs, automation, performance, the illities, et cetera, when all are important to ship. Balance all of these is interesting. How have your teams done this in the past? And that was the main point. The bugs thing we will get under control, but it's the paraphrase version of that is and don't you dare read your answer because I'm not OK, good. Having a low bug threshold or having a low number of bugs, it's important because it gives us the ability to better ability to respond to customer requests without working on top of all the bugs that are, et cetera. But we also have to have good performance and good reliability and illities across the board. We also have features that we think quote, think we need and other stuff. There's all kinds of how do you. So the question really is how do you balance all that stuff? Before we go into discussing that, because I want to try to turn this into sort of a brainstorming discussion. Just me and by the way, me reading back, Brent sent me a nice list of things that most of that were already on my radar. Should be feel good. Plus some new ideas. But but yes, from now, let's go back into brainstorming. Yeah. So I have received two other similar type problems recently. And the reason why I think it's similar is because I think it all comes down to changing behavior or changing incentives. OK, first on my particular team, and this is very common. I do a lot of agile coaching, help people shift over. And there's this concept from the extreme programming called collective ownership. It's very important when you're trying to shift over to agile, but it's really hard to get individuals to buy in. The second one, let's go back into those. We'll talk about collective ownership a little bit. But let me let me cover all three. I just want to make sure I know the right place to properly interrupt and discuss. And then you can let me know. We can go wherever you want to go. I'm going to sit here quietly and you let me talk. Let me know when I can speak again. And then the last one is we've had the discussion of generalists versus specialists. As we've talked in the past, I kind of teach people to I'm beyond teaching people going through the scrum model, it's not ideal in my point of view. But there's a problem there. What happens if you follow either the scrum model or even the Kanban model to the nth degree, it kind of encourages you to be an assembly line robotic generalist. And one of the current problems people have is how do I differentiate myself from everybody else who's also a robotic generalist? So there's this idea that, sure, I'm bought into specialists are evil. But over pivoting on generalists is not what I personally want because I can't differentiate when it comes to review cycle. I'm done with the perma on all these things. Where would you like to go next? So collective ownership. Yes. And that time you started answering a little bit in the in the massive generalist. The generalist generalists predominantly on the team, which we've talked and I've mentioned this a million times. I think imagining you have an engineering team where every single engineer owns everything from soup to nuts is a little is a pipe dream. It just isn't one. You're not going to have that many generalists who can do all that. And two, stuff's going to fall through the cracks. There's nobody's really good at anything. Everybody's pretty good to OK at everything. So this is where I really believe the generalizing specialist or the specializing generalist is critical. One, that's where differentiation happens. That's where you can shine. Yep. But two, and I want to talk both sides of this. That's where you can start to plug holes and not let things fall through the cracks. Huge believer in it again. Anybody listen to our podcast knows we believe in the generalizing specialist specialized in generalist. Yes. The challenge is we give you the example of our team. The other end of that. I'm a big believer in that. We have a lot of it's less, but we still have a specialization on our team where we want to move. We're moving towards more collective ownership. There are some places where we have examples of that. But in many places, there's too much expertise in the technology of the feature area for other like for the larger team to come in and be effective in that area quickly. What do you mean? So feature, feature team A has some specific bit of technology or functionality they're doing, even though it's in the same language, same, you know, same part of the website team B, we can't there's no collective ownership between what team A does and team B or little because the guys on team B, they don't really know how the stuff in team A works at all. They can't come in there and be effective sharing code, fixing bugs. So as a team, they're in charge of distinctly separate verticals within the product. Yes. Right now we have feature teams that work on distinct feature areas of the product. We're getting less of that. In fact, one of our more one of our better managers, hopefully they don't listen and think I'm stack ranking them, but one of our I think one of our better managers actually quote, air quotes owns a set of disparate features. He's the guy sort of the kitchen sink guy because he has no problem with his team flowing between things. And he's sort of I think he's sort of figured that out, although he has sort of specialists on his team. It's more of the example of what I want to see across a larger team where there's more flow out to me. If I had my druthers, I could be in charge. I just I'd have managers who had people working for them that they coached and mentored, etc. But as far as what features they worked on, be a lot more fluid as was needed by the project by the project in times, etc. Well, we're I believe we're in a transition where we can get somewhere closer to that. But right now, as far as collective ownership goes, it's difficult because we have too much expertise, expertise, specialists in certain vertical feature areas. The just to add a little extra color to what you just said, because the every time I go in and I do another coaching session, bring another team online, I walk them through this and they nod their heads and buy in. And it's because it takes a few months for them to execute, to realize, number one, that assigning allocation resources to projects from an ROI point of view actually makes the team better and makes everyone else better, because much like what you just described, it would be super fluid. You'd have perhaps a shared backlog. People come in. Anyone can add an item to the backlog. It automatically or very low friction gets stacked ranked appropriately. The top ones get doled out to whoever is available to do the work. OK, management very quickly buys into this model and is primarily because it's awesome. OK, and employees, after they get over the, hey, maybe it's not so bad not just being a specialist. I think it over this. They go, wow, you know what? Every two weeks I get an endorphin rush because I'm producing a new thing and I can immediately see if it was useful or not. Creates a sense of purpose that was more than what the prior review model does. The prior review model is, hey, I created a bunch of features, let's say in the case of something like Windows or Office, where it used to be three years, I created a bunch of features. It's review time now. The features I just built aren't going to go out for another two years. So how do I get reviewed? I get reviewed based off of a bunch of people's opinions. Right. No, no extrinsic sense of purpose or value. It's all I did better than Alan, therefore, I'm awesome. And I'm going to let you finish. But I don't think that I don't think the updates in Microsoft's review model necessarily will have helped that in every org, have changed that. There are large orgs at Microsoft, large orgs that I was pointing in an object in the room to reference with the org where it still features. It's not quality, it's features. Yet I hear about some orgs where execs say features are second place to quality and they back it up. So it really depends on leadership and what they value. And in fact, any time you're on a team talking about these opinions of others, it's really based on what your manager and your team around you, what they value. Right. More than anything. If they value features, if they over quality in the in reviews or whatever, they're going to any sort of feedback, they're going to base that on those values. It's tangents. It's an interesting system. Right. Because certain one offs like that, I can easily take a team, a team that has that sort of value system. I can once I know that that's the key, the root value that's causing it, then I can go attack that and get rid of that. Right. And for me, I think it's critical that the team knows if the team knows what you value, they're going to maybe you're going to talk about this. No, I want to. But let me. So if if the team has a shared set of values, like we care about quality, we care about collaboration, we care about velocity, for example, that doesn't be what you care about and you as a leader in that group, whether you're a manager or an exec or just a technical leader, I see if you as a leader celebrate and point out on people, do those things, they do show great examples of collaboration. They show great examples of of value and quality or getting some shiny new feature in the product. If you when you bring those up publicly to the group, other people will want to get that. Those accolades to them will do the same things. Yes. And the opposite will happen to you. If you say we must add these features, getting these features done and into our product is above all the most important thing. That's exactly what will happen. Despite you will always get more of what you measure. Correct. Yeah. Nobody has ever heard that before. No. Yeah. The one. But just to close off what I would mean by the generalizing versus specializing as as a new level of maturity gets into an agile team and the management gets it in the ICs get it the ICs then notice a general tendency towards generalization. Right. There's a general tendency that managers go, hey, I do the stack rank. Here's the ROI. People should just pick the top ROI thing regardless of whether or not it's something that is interesting. So then overuse of this type of system will reduce people's love of the system because it it's removing their autonomy, their ability to to master something that's of interest to them. Anyway, in a paragraph or two, like that's possible for you. What would you say if what I say to you, hey, what's the high level advice you have? What should I do? What action should I take to help the team balance quality, ill it is any sort of quality stuff along with advancing features? How do you make that balance as an engineering team? The very first thing that I would I would say is you've got to understand the incentives of the people involved, as we said, over and over and over and over and over and over again, it is always a people problem. The thank you, Jerry Weinberg. Yes. The situation you you talked about as you were talking about the the value systems of the team, right? I've encountered multiple teams have gone in and they were like, hey, this is our value system. Well, what do I do as a coach when I can readily see that the value system is actually the cause? If they have a value system of velocity, velocity is a great thing. But that as as part of your value system could actually I've seen it also over and over and over again, where if you have your value system be velocity, then it negatively impacts adaptability. The first thing I would do is say it honestly doesn't matter. Number one, make what you do transparent. The more you make it transparent and show how you are measuring towards your value system, then it will be a matter of time before someone smart or a bunch of people smart within your team go, hey, wait a minute. These things that we see on a day to day basis that is transparent is not aligning towards our value system. And then it starts to create a conflict between what's right, the specific data that we're showing on a day to day basis or the value system. Get adaptability part of the value system as a first cousin. OK, and then recognize that it is a it's a people problem first and foremost. And what I put in Alan's mail is also what I think about first is everybody on the team wants the Daniel Pink motivations, but that's dead. What is it? Mastery, autonomy, autonomy and purpose. Correct. Right. Huge. The E.B. testing podcast are huge fans of Dan Pink. Absolutely. So how do you how do you change the incentive structure such that it is aligning to the actual value system that is important, whether or not it's the one that's published, right, there's also we can throw in all kinds of quotes. We got we got Weinberg, we got Pink. So let's throw in Amor as well and write the what is it, the fourth order. How do we create a method by which we understand that which we don't know, we don't know? Right. And transparency is what does that. We need a suitable mechanism for discovering what we don't know. We don't know. Philip Armour. So do you remember about 25 minutes ago when I asked Brent for that paragraph answer to my question? I was looking at the time it was four minutes ago. Yeah, Brent can't tell the time it's DST at the bug. So this actually reminds me, I last you know what? I've lost all sense of time a weekish ago, maybe a little more. I spent a little bit of time on a Skype call with one of the three. There's shout out to Percy. Percy, whoo. Thanks for listening to Purrs. Purrs. It's not Purrs though, it's Purrs. I know. I was being all cool on the deets and things. Now it's called the offended. He's a cool guy. Sounds like Purrs. Z. Right. Anyway, I don't know why I go off on these tangents that I have to edit. Right. Thanks, Brent. Or maybe I won't. So anyway, you want to know more about this, what it means for an engineering team and devs to own test. And part of what we're talking about was in my answer to him. I'm not going to recap our whole call. It was fun for me just to sort of brain dump what we do as a way for as sometimes when you talk through something, you end up explaining it and discovering things about like, oh, maybe I want to change this and do something differently and blah, blah, blah. So I'll give you some of the highlights and talk about this balance, which has been fun for me. So as you know, we don't have we don't have a testing. We have a couple of vendors that help out with some of the matrix of ideas. But as far as verification and throwing things over the wall, you throw it over the wall to yourself. You bounce it on the ground in front of you, whatever the right metaphor is. So my team, I expanded my team, I doubled my team size. I didn't tell you this. I now have two people on my team. Nice. Way to go. And we own everything from the moment code is checked in until it gets deployed to production. I say own as ownership. It's not a quality thing, but we're responsible for making sure all that goes smoothly, so build pipeline, test pipeline, making sure things get the tools are in place to increase efficiency, developer productivity, et cetera, et cetera, all happens. So you own the flow of the assembly line. Yeah, yeah. We keep we keep the wheels turning. That's a good way to put it. We keep the wheels turning. So how do we scale that to quality? And again, if I may rant every, you know, I'm so annoyed with the testing world of self-proclaimed system thinkers who can't believe that there's ever a possible way for a team without testers to produce a quality product. Every time they see a bug, whether it's a major bug or, you know, a font is clipped, they they point out the fact this is what happens when you don't have testing and they're just stupid. I'll tell you right now, you're stupid. The problem is in a nutshell, it's also what happens when you do. Exactly. Exactly. You know what? The testing still happens. Stop. I think it's self preservation or often or tweet bait or both. But it just drives me crazy. I've I've there are testers. I follow on Twitter who I honestly don't know why I follow because they just push my buttons with their their stuck in 1980 and it drives me crazy. I don't know why. Because they're like in the net because I understand that there are testers out there and test teams out there and engineering teams out there that are stuck in 1980 and they need consultants that live in 1980 along with them. And that's fine. I don't want that to be my world. But anyway, back to my story. A couple of things we've done is we have the highlights. We have three environments. We have a C.I. environment, a pre-production environment and a production environment. We have a couple of contract vendors and I refuse to let them touch anything in C.I. C.I. is where developers play. They can go look and make sure their stuff is working, etc. And we'll look at stuff across a large matrix when it gets to pre-production. So that helps scale things as well, too. They know that initial verification all there is making sure it works. They own that completely. The developers write unit tests. They write integration tests. They perform their own bug bashes or bug, you know, focused exploratory sessions with the long story for another podcast. They do that work. And then so that's good. And then there are things around. Do you remember in the old days, like you were in exchange, you were in office. I was never office used to have like these drivers like the accessibility driver across office, it was some tester, almost always. They put in charge of gathering stuff up across the teams and blah, blah, blah. Every team is has a unique culture. One of the things that come out of office that I noticed is office believes very strongly in just hordes of teams. Oh, they do, too. And we've talked about our loathing of teams before. So anyway, I looked at performance, load, accessibility, security, internationalization. I looked at my team one, two, I said, crap, we know that stuff. So I took one of my peer engineering managers, very senior level dev managers, and I took five of them. And guess what? They are my dotted line test leads in charge of accessibility, internationalization, et cetera, et cetera. Nice. So they're in charge of driving that stuff all across the team. They're drivers. They're collectors, drivers. OK. Dashboard fillers. Do they know that they're dotted line test leads? Absolutely. I tell them that all the time I meet with them regularly and I make sure that they're gathering the right data for me, preparing what they need. So I really treat them like they, you know, they get a half hour or so of my time a week to make sure they're getting the right stuff together in a clear, cohesive way that we can consolidate, et cetera, et cetera, and make sure deadlines are in place that we're they make sure they know the schedule, all the stuff that a test lead would normally do. But OK. And the thing is, it's weird. In some ways, they're good at it because they're more senior and able to work better across the team. But in some ways, because they've never done it before, they're just like the most junior test they've ever dealt with. So it's it's it's the only way to make it happen. Yep. But it's it's an interesting process to watch. I've seen that before, too, in helping developers across two teams now write tests. All the things we learned years and years ago about writing automated tests, the dumb things to do as well as things work pretty well. It doesn't matter whether you're a entry level or a super senior mega architect level. When you start writing tests, automated tests, you kind of suck at it. In general, most people do. And they need to help to go. How often have you heard it? Just just had a curiosity. How often have you heard someone on your dead team ask the following question, couldn't we just set up a test recorder? Actually, I've never heard that. Oh, good. Thank God. I work with a team of pretty smart people. Yeah, they just it's really experience based versus critical thinking based. OK. We just had a unit test bug last week where we tried to promote our dev build to pre production, which we only do twice a week. And it failed because someone had hard coded the unit test, the brand unit test, hard coded the path to the dev environment. And I thought about calling them out in mail and then I and but again, I don't like celebrating or pointing out huge mistakes. I don't know if that's a I don't know if that's a that's a harass worthy fail, right? It was clearly a mistake. Again, but if his unit test was testing code under the assumption that it was in the development environment, then maybe it shouldn't go forward when it goes into. We run unit tests again when we go to pre-prod. But anyway, I sort of my philosophy is I really try and celebrate when people are doing things well, for example, we had one team like about balancing these things, they self proclaim that they had a little bug backlog build up. So they self proclaim themselves in bug jail. And that meant is that like for this iteration, for this week, we're not going to work on features. We're going to work on getting our bugs fixed only. Except for what there's one critical thing had to come on. So one developer on that, everybody else focused on debt. They did that on their own. I was happy to watch that. Not that sort of technique works better when you have collective ownership. You can scale that across. Yeah. One thing I mentioned is I talked to a few people at similar companies with a similar size to our team. And one thing they do is they dedicate one person per team, managerial team, whether it's a featured team or not, per iteration, usually one or two weeks to just focus on debt. So one person is offline from feature work. That's focusing on debt. Yeah, I hate that model. I'm not against it. I'm going to. It's it's way better than ignoring the debt. I would rather just have everyone fix it as they go along. Yeah. And the one other thing that I'm going to start up in the next few weeks here is because of my one, two people and all the work we have and I also believe that I mentioned this to my boss that we are one more morbid alert, we are one bus crash away from not being able to keep the engine running. My team keeps the engine running. There's a lot of specialty built up and expertise built up and knowledge built up that isn't shared across the team. So you are you have you have a specialist bottleneck? I do. So one thing I want to do very soon that I'm planning to start very soon is start rotating the dev team through my team, so I'll spend I'll spend a week working for me, ramping up on knowledge, virtualizing that knowledge of how things work. I think what happens in the past when I've done similar things is people recognize efficiencies and they create some tools or automation or process around them to make them more efficient. We did something similar in my team recently. We call it the DRI, the designated responsible individual. And that's actually what I'm going to use is we have a DRI who mainly focuses on live site and we add the backup who kind of shadows them, make sure they're ready for the next week. And the backup DRI is also going to dotted line to me and work on infrastructure so that DRI is going to double up. I had one of my ICs pull together a DRI V team. I'm not a V team. Well, the way I do V teams, I'm OK with because I start them with an end. The very first. Yeah, the this is the goal. Once this goal is achieved, this meeting is gone. Right. This goodness group to spans. And because we don't have a bug database, but we still have bugs, all the bugs come across as tickets on our task board. And I went specifically looked for sticky notes that I wanted to find sticky notes that were black because I wanted it a bug to what color pen do you write on them with? You can get. All right, whatever. Go on. I had to get pins that were right on there. Like I kind of knew there'd be an answer. Yeah. All right. Go on. Black sticky notes are hard to find, but I can find blood red ones. So that's what we use. And people when we're when it's the only thing that causes an interrupt. Once we got the team to the point of a bug, once it lands on the board, we have to turn it around in 48 hours. And so when the team sees just hordes and hordes of hordes of bugs coming across, we have weekly retrospectives that is completely now run by the team. I've actually kicked out all management from the retrospective meetings. So they it's highly transparent. They see the cause and effect. When they go into retrospective, they go, holy crap, we had 20 bug tickets this time and we only executed on two of the plan tickets. What happens is very much different than what I see in, say, teams much like you describe. People go, we have to figure out what's causing these bugs and root that out because that is what's slowing us down from executing against the plan versus I don't want to work on the bugs. I want to do more features. So there's a story that I'm going to tell. Yeah. We'll go on to the final thing. Yeah. It's a parable fable. It's not one of those guy walking out and he's walking near a river. He's somebody floating by. So he calls his friend and his friend runs out and tries to go save him. Then there's two more people floating by in the river and more people from the village run out and try and save them. Then people are floating down the river and everybody from the village is running out, grabbing them, saving them and bringing them to the shore. And my friend starts this guy starts walking up the river upstream. Somebody goes, where are you going? We need help. He goes, I'm going up to see you throwing all these people on the river. Yep. So you know what it's time for? Everybody knows what everybody knows what it's time for. Mailbag. Hey, everybody. It's time for the mailbag. Yes. Very exciting. Very exciting. Denny Fott on Twitter has done something that I've only this is the only only. He's now one of the three, but he is the only one of the three I know who has done this particular thing that is binge listen to AB testing. Yes. I saw that. I'm like, wow. I can't imagine a deeper, more painful experience than binge listening, AB testing. So huge props to Danny for I don't know if there is alcohol or drugs involved and I'm not going to judge. But Danny had a question. We have hugely improved our use of drugs. Now, yes, we have. No, I actually periodically go back and listen to episode one and two. And I just go, wow. It's actually all the production quality of the production staff of AB testing. Absolutely. Absolutely. So Danny asks how along the same lines a developer owns testing is ties into our last topic is he asks, how well do you expect developers to master functional test design skills? And it's actually there's a 11. There's a depth to that question. There's yeah. Because at a level of should developers be able to do functional test design? Absa freaking Lutely. Should they master it? Now let's talk about Abso freaking Lutely. How well should they master it? That's actually where the MBU as well as they need to to have a sustainable business. Yeah. And but the thing is for actually all of the three, I think, are pretty experienced testers. How many of the three rhetorical question because you can't answer because this is so you don't have a microphone, nor are you connected to my laptop. But if I were to ask you, have you mastered functional test design? And I think I'm pretty good at functional test design. I don't know if I can self proclaim myself a master or not. I always think of new things. And I don't know. What does it mean to be a master at all? I think a master is just the ability to teach others. They confidently teach others because most masters on any topic, they will give you some sort of Confucius quote with the more I learn, the more I have left to learn. Right. Exactly. But I think it takes time. Is a brand new developer or a tester going to walk in the door knowing mastering functional test design? No. But if you're responsible for functional quality, whether you're a developer or a tester, over time you approach or you get closer and closer to mastery. I don't think there's anything there is. In fact, there will be testers with job self preservation in mind who think that, no, coders, people who write code cannot master functional test design, which is B.S. because they can just as well as anybody who's become a tester where that testing mind where the testing mind kicks in is not in functional test design mastery. It's in those weird corner cases. Those thinking of the system is finding, you know what? I think if I am in this time zone and I switch to this time zone right at this time, I bet it'll fail. Let me set up something to simulate that boom. Cool bug that no one will ever find. But bad example. But I think where the test mind kicks in, the people that are really good at testing in investigation, exploring, etc. It's not their specialty, their value to the product doesn't come in their mastery of functional test design. That's a learnable skill that any any software engineer can do. Even where I think their mastery comes from is their ability to instinctively see risks in the system and exploit them. Sure. Even that is trainable. Yeah. It's actually one of the classes that I'm taking this semester. The professor has a passion for risk management. And as we go through our projects, he's asking us to focus on risks. And what I've discovered as I've gone through this thing is he's basically backing me into sort of TDD through this back end process of thinking, considering and thinking about the risks. And I'm really like, huh, interesting. I have to talk to the professor about this and say, hey, you might want to formalize this because there's this whole community of developers out there who think it's magic when it's it really isn't. It really isn't. But I think of any of the things that are considered functional test design. And I always give a shout out to Lee Copeland, who I think has the best sort of overview, readable overview and understanding of what test design is. There's no rocket science or math critical Confucius skills in there. It's just, oh, I need to know this. This is learnable. Now I know this. And I've taught these things through code review, through feedback, like, hey, make sure you have a test that handles when you have negative input. Oh, yeah, good idea. And guess what? Then they learn that and they do that for every time they have a function that takes an integer that can be signed. These are all learnable traits. Completely agree. I'm going to take a stab at directly answering Danny's question. Wait, this is the first. We're going to directly answer a question on A-B testing. We'll see. So he asked, how well do you expect developers to master functional test design skills? When I think about his question, I think about two things. Number one, one is a coverage question. And another one is a value add question. And in other words, I don't want developers to go in and just create test cases, the test case bloat we used to have when we had a bunch of testers. Right. We have we're testing calculator. Oh, let's do a test case one plus one, one plus two, one plus three. No, we don't need that. What I do expect them to be able to master is the positive and the negative cases. And in additionally, be able to master their test case design to objectively defend the test cases they do not have. In other words, to say, hey, yes, that's a test case. Absolutely. I could pull that in. The value proposition isn't there. I would use a reactive design, a written on a reactive design, a reactive motivator. I will let customers tell me I was re-listening to our redfin. And our example in the redfin was this buy dye character set in the middle of Georgia on February 29th. You know what? I'm going to let the the life site telemetry tell me that that's important. Somewhere right now, someone's screaming, you can't let the customers do the testing. It's not fair. You can. And I'm at a point where if if you are an engineer and you are you caring about the business, I'm at a point where I firmly believe you are morally obligated to do so. So a lot of a customer is the only person who understands the quality proposition of your output. That is absolutely correct. I believe you. One interesting thing. And thanks, Danny, for the question. But I want to add one little follow up to that before we close here for the day. It was what I believe in. The whole system I'm building is, as far as quality on my team, is being able to react. I want to I want to have an engineering system and a feedback system and and quality in place that we can react quickly to customer complaints, requests, ideas, etc. And obviously we won't just listen to every single feature that comes in and go, oh, my God, we got to do that. We'll look at things in collective and prioritize and do the right thing. But I want I don't want to put myself in a position or put our team in a position where we can't react to that. I was in a meeting last week where unrelated to my product, but we're talking about the quality and this big release that went out and there was a manager in the room that said, so we'll have to let the customers can find those bugs. This is testing and production. And it was a pretty senior guy. So I didn't I think it's a it's an abuse term now, I guess. Testing and production. Yeah, it is a little bit because I knew and I didn't call them out. But I talked to some of the other people in the room afterwards, after he left. But to me, yes, it's it's no, it's just it's just bugs and production, really, because they ship monthly at best. There's no way for them to react. Sometimes it's three months. For me, test you do testing and production. When you can when you get Delta between hearing about or actually not even hearing about it, discovering an error ideally automatically through monitoring to mitigating or fixing that error has to be the smallest time possible, not in the matter of hours, maybe days if you're slow, but not months, no more than a week. And so I that really kind of stirred something up. I mean, you reminded me of that when you're talking about your answer. There is is yes, the customers and I am on all four months using monitoring and diagnostics figure out how they're using the product. But you got to be able to react to that. You can't just go, yep, that's a bug. We'll put it in the backlog and we'll prioritize it to fix some of the future day. One of the things that crazy one of the things I would say is an overall theme for what we were talking about today is the real value out of a systems thinker is to really understand the balance between black and white. Almost always you need both black and both white. You do. Should we go all reactive? No, that's stupid. You will not have a business after a week. Should we go all proactive? No, that's stupid because you're now handing all of your business to your competitor because you're going to go too slow. So how do you balance these things is an important role for anyone who who is actually a systems thinker inside their organization. That's actually really important because I and we're often tangent those to try and close on here. But it's a technique I use time and time again is to look at the extremes. It's a system thinking is about balance. Yeah. And one way to kind of figure out where that balance is is think about the extremes and figure what that is and what that means. So what does it mean if we have 100 testers for every developer? We don't have to go through that. OK, let's let's let's enumerate what happens there. Thought exercise for listeners. Now what if we have one tester for 100 developers? What's different for your context, for your product? And maybe the answer is in between. Maybe it's closer to one stream or not. But that's for you to figure out. The other thing, too, though, is see, you know, I don't think that's it. But to blindly say that one extreme is wrong is if you say that and call yourself a system thinker, you're just an idiot because you don't even know what either means. Yeah. But your extremes, the ones that go is a bad experience. I'm just like, one is wrong and the other one is great. So in this particular case, it's not one like the balance would be on one of the extremes, in my humble opinion. Anyway, but you need to think about the extremes to figure out how to balance. That's the point. And how would you judge between a selection in between? Right. The interesting thing about thinking about these extremes that I find valuable is I go, OK, which one do I like? Why do I like it or not? Can I turn that into an objective measure that I can then use to guide where I should actually be? Like using the extremes, you can immediately have a visceral reaction. I go, oh, my God, one to 100 testers. Let me give a different example for the same people who because they tend to be the same people who think testing and production is idiotic and stupid and irresponsible. Like, can you think of a product where that would be OK and one where it wouldn't be OK? Testing and production. Yeah. Or can you think of let's take 100 percent automation. Is there 100 percent test automation? Is there a situation where that would be the right answer versus the wrong answer? What if it was just an API that wasn't even exposed to customers? I like your prior pick one air traffic control software. That one probably shouldn't be tested in production. There we go. All right, Brent, we have used up more than all of our time. Yep. So let's say goodbye until next time. But you know what, actually, this one can be a little longer because it took a little longer break between the last one and this one and no, I'm not going to be like split it up into two. You get one shot. What episode was this? I didn't announce it at the beginning. 35. I did. At least one of us. OK, we'll see you soon for episode 36. I am Alan. I am Brent. I thought I fool you. Yeah, well done. We'll see you guys next time. Bye. Ah. 
