I'm Alan. I'm Brent. And we are here for episode 50. 50. Five zero. Sequenta. The golden anniversary. Brent, I got you a present. Did you? No. Yeah. By the way, before we go on, thinking of presents, like... Why would we go on? You couldn't have even gotten me coffee. Like what the hell? How do you work in this building with a broken coffee machine? I use my little itty bitty legs and I go up the stairs where they have two coffee machines. That's a good solution. Yeah. Except for the whole going upstairs. I'll tell you what. The B and AB testing is not for brains. It is also not for burning unnecessary calories. So... So, did you ever think, once upon a time, years ago, Brent and I would give each other crap in these test architect group meetings, which really kind of sort of touch wine fests and... Which was really the start of this. It was like Michael Hunter or somebody said, you guys should record your conversations. Said, yeah, we'll do a podcast. Those were amusing. And then we started one and then we... And we still have one. So, let's get going. One comment we had after the last podcast, which was episode 49. That was essentially Debbie TF on the TLAs. Yeah, Debbie TF on the TLAs. TLAs, those three letter acronyms, those abbreviations that every corporation flies around. One, I don't know if you changed jobs. It's been a couple of years now. But whenever you go to a new job, people start talking and they all understand each other. And you go, what the heck are they talking about? It took me a good three months on the current job to get up to speed on the TLAs. Yeah, and sometimes... Now I'm generating them. It gets a little ridiculous. So, let's see if we can remember all the ones we talked about. So, we mentioned SRE. Yes, Service Reliability and Engineering Team. Typically, Site Reliability Engineer, which is what the term came from, the role was invented at Google, where they call it a Site Reliability Engineer. And actually, I can really recommend Google's book on Site Reliability Engineering. That's the name of the title. I'll have to ask... I say the name of the title. That's the name of the book, the title of the book. It's a good book. I have to ask my SRE team what he calls the S. Yeah. Pretty sure it was Service Reliability. That's the problem with TLAs, is they can morph, right? You know, the thing is, they can morph. Like WTF is... But in this case... It could mean what the fudge or where's the fire. Windows Test Framework. Yes, exactly. If I would have written it or if James Woodaker wrote in that book would have been called How We Do Site Reliability Engineering at Google, but you know. Yeah, I have no clue. You have to speed on what James is doing, Lillian. James is a professional talker to peopleer. Yes, I know that. That's pretty much it. I had lunch with him last year. Once a year he writes... He starts doing PR for a new thing of how to be awesome. What ifs. What ifs. He's being James. Yes. Another acronym we threw out. What's another one we... There were DevOps, there were TSGs. Okay, DevOps is... DevOps is pretty... It's not really a TLA, it's just an abbreviation. I thought DevOps is pretty well known. You can search that one. TSGs, troubleshooting guides. I actually listened to them episode and put it on the Slack channel. DRI. DRI, designated responsible individual. Which is a dumb acronym, but it's caught on at Microsoft. It's the guy that answers the phone when the site goes down. Yeah, it's interesting though. Someone says DRI and no one calls it the DRI. Anyway. That's not interesting. It is to me. Usually when you pronounce an acronym people say crap. Oh, GA. Our product will go into GA sometime next year and that's general availability. We used to call that RTM, but it's... We don't really release to manufacturing anymore. They called it RTW, released to web for a while, but now GA, the term is like, it's available. You can use it. All right then. Any others? Did we get them all? Yeah, I'm sure people will tell us if we have them. I know. I'm sure we'll fire a few out here. So we had... Is this a mail bed question? Essentially. So on the Slack channel, so the next topic... What the f***? Mail bed! Now you can go on. All right. Man, you are just making your editing job so much harder this episode. You know what? You would not believe how hard it is. Have you automated the scanning of the use of F words? No, but me dropping a few F bombs is nothing like editing out your coffee slurping and sniffling and coughing and microphone fondling. You're not warmed up yet. Brent is now eyeing his microphone stand seductively. Whoa, Mikey. Hey baby. All right. Can we go into the topic? I've been waiting. All right. Salty Gunner and H.E. Gold. Henry, I believe. Yeah. Yeah. One of the topics that resonated with them on episode 49 is essentially your rant on your PMs asking you, hey, when are we going to ship? When are we going to ship? When are we going to ship? And the sense that only the front end shipping is the only thing that matters. And it occurred to me, and I wrote it in the Slack channel, I'm like, oh, crap, I was too busy joining Alan in the man PMs or stupid party. And realize that, no, there's actually, there is a solution. It's not simple because it's a system problem, but there's a solution. And I said, okay, well, we can certainly attempt to talk to that on episode 50. Well, talk about your solution and I'll talk about mine when I feel like interrupting you. Okay. All right. So first and foremost, why are the PMs all hot and bothered about shipping the experience? Is that a rhetorical question? Like me to answer, go ahead and answer. He was expecting the answer rhetorical. I was expecting rhetorical. There's in general, it's because it's visible. It's that, and I'll go ahead and go into my part of the answer here is there's a nice, I like quadrants and diagrams. There's a quadrant of visible and invisible and features and bugs or, or we'll call it bugs for now, but you have visible features. That's, you can see it. It gives me new value. It's really cool. You also have visible bugs. Oh, this doesn't work right. Then on the other side of the quadrant, you have invisible features, which are architecture and things underneath and how you do your protocol handling, et cetera. And then this whole pile in the corner of, uh, and actually, actually the services are kind of there too. They're not visible features. They're super important, but they're not visible. And then in that bottom corner where you have invisible, uh, bugs is just where your tech debt lives. Right. So, uh, but this is all an engineering perspective. What about from a customer perspective? Customers don't care whether you have two services or 12 services or 150 services, they want their pretty little UI to work. More importantly, they want their problem solved. Like, and if the pretty UI doesn't solve their problems, then they're not going to use the pretty UI. You are correct. Right. The, um, for once the quality from a customer point of view, it's, it's as, as all of our listeners are well aware is, is multifaceted. Um, but first and foremost, customers want their problems solved. Right. We were talking about teams, right? What's some of the, what's the number one hypothesis for, for a customer solution for teams? Rephrase your question in the form of a question I understand. What problems does team solve for customers? Uh, team collaboration. Okay. Or team digital collaboration. Right. Your product is an experiential product. Correct. Right. It is not like a SQL server or any actual server product where, where the architecture itself is the solution to the problem. The architecture in your case is an implementation towards a problem solution. Sure. Right. Um, in a nutshell, why PMs are all hot and bothered about this is because it's the experience that settles the product. No, I agree. Right. Now, the other thing that was talked about a lot in the Slack channel is, Hey, uh, they're cramming in a bunch of features. Uh, the architecture can't stand that up. Right. We have six months. I'm now making up stuff, but we have, we have six months of work to put into our back in order to be able to handle the scale for, um, the front end. Right. And it's when I saw that, that, that longterm thinking around what the plans were for the backend in the hypothetical example on, on the Slack channel, I realized, Oh crap, this is a be def big design up front. What appears to be super common is still, uh, to design your architectures with a big design up front with the architectural layering model. And what doesn't appear to be common yet is sort of how to adapt the system. Uh, the system of developing product so that you're delivering vertical slices, uh, in an emergent design fashion. You are correct in the general sense. There are other dynamics on our team that for job preservation, I won't go into, but in general, that's why these things, that's why these views come this way. Yes. Yeah. So it takes a lot of effort. I like, uh, the, there's two real good sources, uh, combined together that really, uh, help people understand the direction. It's a system problem. So, so there's no real magical do this and, and that stuff happens. Um, but if you, if you look at Eric Reese's book, something we've talked about a lot here, we have as well as, um, pretty much anything from the pop and dikes. Oh yeah. Um, and if you can connect the dots between these two topics are these, these two, um, subject matter experts, then and connected to the context of your business, you should be able to move forward. I can go into more detailed examples, but you read Eric Reese's book couple times. Do you remember the concept of a concierge MVP? Yes, I do. It's called a concierge MVP because the only thing that is solid is the front end. They, they bolt on the back end on a as needed basis, but a lot of the, a lot of the stuff is literally the guy behind the curtain. So there, there will be people in some, I failed to remember the concrete example from, from Reese's book. But essentially if you go to teams and there's something that's supposed to, there's experience acts like it sends an email, but that's actually not hooked up. Then there would be a physical person behind the scenes that sends the email. Okay. Now, why is that important? It's important because when you ship these services, at least in Reese's concept, you're trying to come to a decision on, are we going in a valuable direction? Okay. So having, uh, um, shipping an MVP to a couple hundred people and having one guy for a couple of weeks forward emails manually is cheaper in the grand scheme of things. It's a way of seeing if they want to send email, if they're going to use it. Right. It's a cheaper way to evaluate that. Right. Um, and it's the same thing with a lot of the data database architectures are expensive. No, no, I get that. And that makes a ton of sense. You want to, uh, the part I like about that a lot where we've honestly failed in a few cases is, which is a little, when you're doing this sort of a V one thing, it can happen a little bit more, but we want to, I totally believe that you just don't get value from your engineering effort until the customers have validated and tried out what you're doing. And so if you need to hack together some things on the backend or have a guy behind the curtain, given that experience, see if it works, it's great. Yes. There where we've, uh, there's a term I've used for that. Probably can't, I can use it on the air, but when you are building a product and it's all for yourself and you keep on adding features and features, but they're really things that you want and you've never validated them. You know what I call that? What technical, we'll call it self-satisfaction because you really, you're just, and, uh, seriously, if you're not using the customer, if you're not involving a partner, you're just playing with yourself, right? It's, it's not the most appropriate metaphor, but it's the one that comes directly to mind when you're just focusing inward on trying to make a product and thinking just in your own mind that it's the right one for customers without getting any validation from them. Yeah. It reminds me of the data phase that Steve Rowe, uh, invented on, on the episode of a few episodes back, which is the, the data affirmation phase, which is I only trust the data as long as it agrees with me. Ah, yeah. Right. Um, wouldn't you also call that confirmation bias? It's absolutely confirmation bias. Um, I just noticed that we, that's one thing. That's why to do it. That's what, that's what Reese's thing is saying. Hey, there's an ROI here, do what's cheap to figure out whether or not there's an, there's a viable investment. Now the pop and dyke, um, principle that I want to pull in. I like that the pop and dyke principle. Um, is so Mary in particular lists out seven different principles around what makes something agile. And, um, but her principles aren't, that's not quite right. It's, it's sort of principled rules of how to go about agile. And one of the ones that I'm most fond of that I, that resonates with me on this topic of emergent design is something known as deferring commitment. And what that specifically means is from a, from a technical standpoint, you defer any major decisions until the last possible responsible moment. Yep. Absolutely. Right. And, um, an example, um, when I first introduced this to my first agile team when I was in bank, um, I had a whole bunch of developers coming to me saying, Hey, Brent, I was thinking about this design and I would like to add this thing because somebody might want to one day fill in the blank. That just then shivers up my spine. I actually, I actually, um, when I, when it had occurred with enough of my staff, I pulled together a team meeting and I said, all right guys, from here on out anytime someone comes to me with, with anything that contains the words, someone might want to just realize that the answer is no, think, think about what you're trying to, to actually ask for. And then here's more than likely my, my response. Okay. Um, I told them, um, pointing them out to the defer commitment and I said, what this means is we're not going to implement anything under the premise that someone might want to one day. We, um, as I, as, as you know, anyway, um, part of how I do agile before I can declare done, we do a process called customer validation. So we actually go talk to the customer and validate that we, that the reason why we built it is, um, it's solving the problem that we intended it to solve. And we do that by not theorizing by actually talking to a customer or looking at instrumentation, but I said, what I'm going to do is I'm going to say, Hey, you might be right, but you are not to implement it. What you are to do instead is re-arc the design of your, uh, what you're doing right now, such that if you are proven to be correct, we can shift to that design and moments. Sure. Right. So it's about making the architecture adaptable. Had a very similar conversation yesterday. We had someone come to our team and say, Hey, we need to do this feature by, uh, we own a little bit of the product features around, uh, how we deploy rings. And without going into details, someone came from one of the team said, Hey, we really need to build this feature. Cause customers are going to want it. And immediately my team's well-trained. I mean, even the most junior guy on my team says, what customers are asking for this and they said, well, no customers are asking for it yet, but they're going to want it. How do you know they're going to want it? Well, because we think they need it. These are, I mean, actual quotes and it's so well-trained. He says, okay, let's do this. Let's, uh, go ahead and not do anything that will block this from happening, but let's just hold this for two weeks. And we'll see if any questions come in about this, it's all ready to go. We can turn it on, but there's definitely more important work to do right now. So let's just not do this now until we know that it's really needed. Right. So it was perfect example. It's like, and in my case, it's like you have this mountain of stuff. I know you need to do, I know why are you even asking about how are you often the weeds of, I, we think customers are going to need this because I have work. I know that is much more important. We don't have time right now for, for you thinking about what customers want. There is, um, wait, you don't want to, you don't want to completely shut down. Exploratory ideas. No, no, no. Which is not what you're saying. You're saying it remains in the backlog. It will remain in the backlog. And, um, what, uh, actually I'm glad you brought up backlog. So backlog grooming is, is a key part of being able to improve this, this front end problem. Um, by, uh, by having it. So for example, there is this system called W S J F weighted shortage job first, you can look at W S J F on the internet and figure out the equation. Okay. Um, but it's very equation driven that then the key aspects of it is it cleverly combines time, criticality, business value, and the cost of doing the work so that you have, you can then order your backlogs by business ROI. Um, and dynamically the, um, in this particular case, when you go through a backlog grooming, right? Uh, the scenario you just brought up, um, we would revisit it in that couple of weeks and plug in a new business value number if we have proof of some degree that, that this is something that's viable. One of the advantages again, to echo what we talked about previously and Eric Reese is now that we have customers and a few hundred thousand of them, we, it's much easier for us to make those decisions and see, is this really something customers are asking for versus something I think they're going to want. And of course, uh, someone will always come back with, well, if your Henry Ford would have worked that way, you just would have got faster horses. Henry Ford's quote is, uh, if we gave people what they wanted, what they wanted or no, no, if we asked people what they wanted, they would have said faster horses. Right. Right. Right. But, um, I get, so that's, that's used as defense for this. Well, we need to, we need to innovate. I'm, I'm a lot like Steve Jobs. So I want to, that's what I say. Yeah. Yeah. So customer data trumps intuition every time. Well, and the thing is, is that you don't have to make it in us versus them. Like the, my answer to that would be, Hey, look, we have a system in which we are going through and we're building things in terms of a validated ROI for the business and we're working on most ROI for the business first. So you know what? Sorry, your item. Great idea, dude. Even if it's a stupid idea, it's always best to say great idea. However, the situation is this is intuition driven. These other ones that are data driven are higher priority. And one day maybe we'll work through and we'll only be working off of intuition driven ones and yours will be top of the stack, dude. Um, but do you lie all the time to your employees? Uh, that's not lying. Yeah. One day we're only going to do intuition based features. I said maybe. Um, it has never occurred in, in any period of time that I've seen in my career, um, the, but the other thing too is, but you know what? One of the things we really need to do as a team is make it low friction to get this data on these, these fantastic ideas. Like this guy's trying to come up with an argument. He's got an intuition. He's kind of using the old school waterfall way of getting this on the backlog and you're basically saying, yeah, that, that ship don't sail anymore. You can't get to where we're going from where you're at. We can beat this dead horse a lot, but it was just, this one wasn't, it was, it was a great idea here and I'm holding up my tunnel vision, uh, glasses. Uh, one challenge we talk about systems thinking a lot here and this particular problem, um, particular solution, particular feature, looking for a home, uh, there were, uh, scale and distribution and test ramifications that were a huge part of making this change that just weren't thought of. So there was, there was more to it. But the, the, the issue is, is that this guy is trying to get it booked in the end. Yeah. And what the, where I would have pushed back on that is say, no, let's figure out how to do, um, uh, a spike in a prototype. Let's figure out how we do a, a low impact AP test of, of whether or not customers will actually value your thing. So this is where I would have, I would have take you have a hundreds of thousands of users now, how do you, how do you AB tests with just a few thousand and do something concierge to prove out this feature. Sure. And then the next thing is how do you make AB testing a feature for your product? And how can you tell via some sort of monitoring or what's your metric? What's your KPI to determine the business value of doing this? Is this, how is this providing value and how are you measuring that value? Yeah. The, um, I actually have this discussion with, with my own team quite often. Now, um, we spend a lot of time building pilots and prototypes. Okay. Cause, um, much for the same reason, like I, one of my most used speeches is the pivot or persevere speech, the, we will build prototypes and I have every now and again, I have a couple of people on my team going, this, the performance of this is going to blow and the scale of this is going to blow. We really should spend the next three months and I'm like, stop. We don't know anyone's going to use this. So let's get, I get that, that you want to build something that you're proud of. And I want you to build something that you're proud of, but let me ask you, if I spend those four months and we release this and we found, found that no one uses it, have you built something that you're proud of? It just makes my heart sink. I'd be, I just feel like I would, that that's depressing. Yes. I'm like, no, let's, let's, um, get something out there. Let's be transparent in terms of what it is. Let's find a set of early adopters that are, that aren't allergic to, you know, bugs or perf issues, and let's prove that this is something that, that solves a problem before we go and solidify it. Right. And we will, right? That's, that's the other thing. And you also have to be careful that you don't have too many prototypes in flight, um, that you can't scale to solidify the ones that prove out to be positive, right? That, that ends up just creating this hordes of technical debt. Indeed. Hey, I want to go on to another subject. That's okay. Yes. You all right with that? Yes. Let me just pause here a second. All right. So while you're doing that, what I'll say in a nutshell is concierge MVP realize that the experiential stuff is important. Uh, on backlog grooming, one thing I didn't mention is, uh, I have a rule on my team. Whenever we go through a backlog grooming, no more than one third of the total cost of the team can be on pure architectural. Okay. Cause pure architectural isn't what improves the brand. Correct. All right. Okay. Hey, do you remember? No. Yeah, you're almost as old as I am. Do you remember, uh, a long time ago, I gave you some questions off of a test certification quiz. I do not going to do that today. Uh-huh. But instead I have some questions off of a sample CSM certified scrum master test for you. Okay. Looking forward to it. It's a 10 question quiz. I'm going to skip the ones that have long answers just for brevity. Okay. But, uh, we'll kind of go through this. Um, are you ready, Brett? I probably play along at home. Feel free. Write your answers. These fill in the blank, multiple choice, multiple choice. Oh, that's fantastic. He's a multiple choice. And we'll go over your answers and then we'll go at the end. I think we get, uh, like, uh, some explanation. Okay. Okay. Some are pretty easy. Um, involving the team in planning and estimating and providing early feedback on delivery velocity is best used to mitigate what kind of risk schedule flaws, requirements, creep, employee turnover, or poor productivity. All right. Do that one more time. Involving the team in planning and estimating and providing early feedback on delivery velocity is best used to mitigate what kind of risk schedule flaw requirement, creep, employee turnover, or poor productivity schedule. Question two, which of the following is the best approach for estimation? Expert opinion, analogy, dis-aggregation, a combination of all of the above. Dis-aggregation. That's the word on here. Expert opinion, analogy, dis-aggregation, a combination of all of the above. I have no clue what dis-aggregation means in this context. So I'll just go with, uh, expert. I mean, the right answer is probably combination of all the above, but I'll go with expert opinion. All right. Uh, this is a horribly designed webpage. Um, this question is too long. I'm skipping it. When forming an agile project team, it is best to use, I'm going to, I'm going to give the answers out of order, uh, best to use top management officials, highly specialized developers, generalized specialists, or all of the above. I'll go with generalized specialists. What was the first answer? The top generalized specialist was the first one. So I know his top executives or some crap. Yeah. Top management officials. Yeah. On a scrum. Yeah. Uh, this one's fairly easy. Scrum would have you fire those people. What is the unit of measurement that is used to measure the size of a user story for agile projects? Function points, story points, work breakdown points, velocity points. Story points. Yeah, that one was pretty easy. I should have just skipped it. Okay. Uh, how is agile planning different from the traditional approach to planning? Agile planning is done only once agile planning is non iterative. Agile planning places emphasis on the plan. Agile planning places emphasis on planning and is iterative. It's the last one. Yeah, but it's not on the plan. It's on planning. Yeah. That one's long. Oh, so far, this isn't as funny as the ISQT thing. Wait, this is the one I wanted to lead up to. Okay. The process of testing delivered or quote, done, done stories is known as unit testing, integration, testing, exploratory testing, or release testing. Given the non sequitur between the answers and the question, I'll read the question again, the process of testing delivered or done, done stories is known as unit integration, exploratory, or release testing. I will say in that list, it's probably release testing. And dear God, I hope the correct answer isn't unit testing. Now I'm going to skip the rest of it and look at the answers and discuss. All right. You've reached the end of the quiz. I'm done. Ooh, I skipped a bunch. We got 40% right, but I skipped a bunch. Okay. I think we did well. Um, the first one we got right involving the team and planning and estimating, blah, blah, blah, is best used to mitigate schedule flaws. Well done. And for which of the following is the best approach for estimation? It was an answer, the answer all of the above this aggregation. I need to look up. Does it, I see there's a lot of words there. There's a source that comes from a Mike Cohen's agile estimating and planning book. Okay. This aggregation is probably the process of taking a large story and breaking it into smaller pieces. So, um, at which point in time, I agree. Yeah. So when forming an agile project team, it is best to have general, generalized specialists as any listener of the podcast, any of the three, all of the three know between those options. But what I didn't know is, uh, there's source for that specialized generalists who would even be better. There's source for that is a book that I haven't read by, uh, Michelle sliger and Stacey abroad, Rick called the software project managers bridge to agility. It's in that book. Nope. I will take a look at it. Story points, of course are correct. That's also from Mike Cohen's agile estimating and planning book. Um, and the planning one also same book, same answer. Very good. Uh, we skipped that one. Now here's the one that this came up on Twitter and I thought, what the F, cause I've read this book that they cite the process of testing delivered or done, done stories is apparently known as exploratory testing. And so then this is, this is where the tangent happens. They cited, uh, James Shores book, the art of agile development. Which I have read. Okay. And I went back and looked at it again. Not well known in the ET world as a. No, no, no. He, that book is really good. Cause it talks about using expert testers for exploratory testing to help figure out, to help with things and where they can plug in. And, and the authors of this test made this huge freaking leap that, oh yeah, done, done is the, is the act of exploratory testing, which is this. Massive leap in kind of the wrong direction. So it's just a weird, again, the rest of the test wasn't too bad. It wasn't as crazy as the, as that test certification, but this question, no, it's, and that's just weird. It's just weird, right? So luckily James Shore. Um, so what happened? Uh, someone on Twitter posted this and said, this books, this book doesn't get exploratory testing implying that. And I replied and James Shore was on the tweet and I replied back and said, I don't think it actually says this. James Shore says there's no way in hell it says this. Yeah. That book is, I don't know if you've read the art of agile, his book. No, it's I like it a lot. I think it's really, really well done. It's a nice holistic systems approach to the art of agile development. The art of agile development. That seemed like a book that would be up my alley. Yeah. I should take a look at it. Um, easy to read. I like it, but explore it. Tori. No. So the, so first and foremost, like you just talked through like on, on Shores book, he's like, uh, no, I should not be cited as the proof that this answer is correct. Right. But the other thing from a scrum side, right? It does the text there say, Hey, you should hire expert testers. No, not here. Uh, James shore talks about the, how the value of expert testers, uh, okay. In because agile development, anybody other than a dev on a scrum team is sort of anti scrum. It's one of the reasons why I don't like scrum. Um, but, um, exploratory testing in this context, like telling a bunch of devs to go do exploratory testing. Um, sure. It's useful. But how, how do you declare done done from that befuddles me? So, but there's a logical foul scene here. I forget which one it is, but Prince having tea drinking problems. Yes. It started off with having tea, set a coffee, stupid building. Could walk up stairs. So earlier we're talking about Eric Reese's, uh, lean startup book. And it would be the equivalent of me reading that book, seeing something about, I have to think of a good concept. I can screw up from there. Um, hummingbirds see what the hell. So I read about the concierge MVP and, and from that I make, oh, services don't matter. We just hire people on the, what we're going to do is we're going to hire a team of people on the backend to take care of everything on the backend. We're only writing a UI. Right. Like this massive, no, that's not what I meant. We are going to create the UI and connected straight to mechanical Turk. Oh crap. Yeah, that'll work. That's a good idea. It's like, yeah, you obviously glossed through here. And, and again, I, it's a sample test and I give the test writers a break, but I feel bad you're taking the CSM tests. You get to that question, you go, what the hell? Yeah. I mean, the thing is, is that most of these as, as, uh, so you know, I'm certified and so a lot of these certification tests are essentially how well can you regurgitate the class that we taught you? So it'd be interesting to find the slide deck, um, for this test. That presents the material in the first place. Cause I'm sure there's a strong chance that the ET that they've listed here is not the same sort of ET that we think of in the test community. No. And again, having read the book, I know it's just like, it's mentioned a lot. It's, it's brought up in very good way. It's not like it's over abused or abused wrongly. Uh, it's just a weird leap there. And it almost sounds like they had different, because it's a sample test. It could have had different people running questions. I'm not sure what happened, but some cray cray went on for sure. For sure. I will. And we didn't answer any other ones, but I really wanted to lead to that one. So I, um, uh, I found the test, I found the testing question, which was not set it on the internet, but I know how to use search engines. So I found the right test and I wanted to kind of get to that question. That was my, that was my goal for the day. Not as much. Well done golf club. I'm going to, I'm going to ask you one more question. All right. Then we're going to get out of here. Which of the following is not one of the five core risk areas common to all projects? Really? I should have you list the five core common risk areas, but which one is not specification breakdown, scope creep, strategic alienation, and intrinsic schedule flaw. All right. Let's not one of the five core risk areas. So let's do one at a time. What's the first one specification breakdown. That is a risk area scope creep for sure. Risk area strategic alienation. Have no frigging clue what that is. Intrinsic schedule flaw. That's a risk area as well. The way I interpret. You're correct. The reason I brought this up, like if you are looking, if you're like, have a nerd band, like, you know, a lot, you know, a nerd band, like angry weasel. No, no, a nerd band. Like, like you, it's like bad synth rock kind of stuff. These are your band names. Like Flakossegal. Ladies and gentlemen, specification breakdown. God, strategic alienation. Rock on. Everybody wears scope creep. This is called lover love radar booger knocker. What do you would that's the long ass blurb there explain what strategic alienation is? No, it's just a made up word. These, this is the disadvantage of multiple choice. What tests are there are, you don't have to know the answer. You just have to have critical thinking. Right. But in my case, I'm picking the thing that I I'm like, yes, yes. No clue. Yes. Okay. Let's go with no clue. And they picked like a, I don't even know what that means. I can't fathom what that means. So alienation means rejection. Yeah. But I'm doing it strategically. So I'm going to strategically reject like features. Like I'm using my strategy to reject this work. Reject. I don't know. Maybe customer pruning like, Hey, you can't prune the customers. No, but you can. It's not nice. You can target. You can say, Hey, we're not going to try to solve all problems for all people. We're going to solve. Um, some of the problems for some of the people. There's a phrase I use often nowadays at my current work. It is way better to make 50% of the people, a hundred percent happy than a hundred percent of the people, 50% happy. Absolutely. That makes sense. Yeah. And that's not a risk. And if that's what strategic alienation is, it's definitely not a risk. It's a go forth and alienate strategy things. Cool. Hey, a question for you. Yeah. Have you ever taken electronics class? I think the safest answer for me to say is no. All right. Cause I feel like mocking is coming. Yeah. Brent just laid his phone on top of the microphone wire. So that buzzing you just heard is thank is thanks. Thanks to Brent. Oh, gotcha. You'd be great in a band. You should join strategic alienation and be like their sound guy. Right on. All right. Okay. We are done. We did 50. I think this is probably our, I think we do one more this year. Yeah. And we have to do, um, we will do our best of 2016 show. Right on. Well, we'll talk about our favorite podcast blogs and celebrity presidents. Yeah. Something like that. Okay. I am. Bri not Brit Allen. Yeah. And I'm Brent. 
