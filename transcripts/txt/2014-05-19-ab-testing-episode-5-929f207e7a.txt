Hey, I'm Alan. I'm Brent. And we're back here for another episode of A B testing where Alan and Brent just talk about random crap and see what comes up. Semi-random crap. As usual, we have a lean coffee board written up and now we begin the process of kind of ignoring it. No, we're going to slowly work our way through it and get through part of it. That's the plan. Yes, we are. We are on a pretty good schedule. We had a three week break between podcasts because I went to star East for a week. So I thought we'd start by I'll give you a little recap, tell you some things that happened there if you missed the explosion on my blog. And I'll see what Brent has to say about all this. Yes, the so I'd love to start off on the on the the blog that Alan did. The only problem I see with Alan's blog was that I didn't write it. Oh, Brent. There's a story behind this blog post. This wasn't like some plot I had or some like, oh, I want to be controversial. What happened was I was signed up at star to give a lightning talk, which are five minute talks. And they do. Lee Copeland does lightning strikes the keynotes and everybody gives a five minute talk, not everybody. Eleven of us. I went there purposely. If you listen to the last podcast, you know, I didn't have a topic plan. I didn't have one figured out until we got to the day of the morning of the lightning talk. I was out for a jog and I was thinking about everything I had heard so far. And a lot of people were struggling with their automation. They're writing a lot of GUI automation and end to end automation. And they're spending a lot of time investigating failed tests, thinking about this. And then I was thinking about what I'm doing at work with, you know, getting more focused on data. And I was jogging along and I just stopped. I thought, well, it's not a heart attack, but I have an idea. I'm going to do my lightning talk on stop writing automation with two main premises. One is that for the things that automation is really good for short confirmatory tests, or as I have been lectured now several times, I should call those checks. I'm going to call them short tests because I've, you know, it's, it's my vocabulary. Anyway, those short tests, it's so much more efficient and so much more applicable to have the developers, the programmers of that code, write those tests. So what's left those long tests that we can't maintain. And to be clear, there are a lot of long tests. You can even do some sort of end to end scenarios automated at the object level versus the GUI or the lower, the better API testing. Those things can be done quite easily through automation. Long story short, cause it was only a five minute lightning talk and I'm taking longer than that to explain it. Dev should own the short tests and a lot of the, a lot of the results testers typically look for in their long tests can be obtained through customer data and instrumentation. The only thing I would, I would add to that, and I intend to, to write a follow up blog post hopefully this weekend, similar to that. Cause we also in, in the services world, there is a role for things that feel like test automation. It is those who, who have heard about tip tests. In addition to that, I completely agree with what, what Alan is saying regarding the traditional way we think of test cases and test automation, right? Those, those regression checks not only should be done by dev, they actually dramatically improve devs throughput by dev only. And they improve code design. In every case I've seen this people write, when they write testing their own code, they write better code in the first place. I actually, so, so my team is undergoing a, a reorg and I was just on a, on a phone with a dev architect in a different group. And, and there is a, a rumor going around that, that we may follow the combined engineering model, which will mean. I hate that word. I think it's stupid. Yeah. Great. I don't care about, I don't, the point is, and we're going to, no, you, you don't like it because it, it, you, you want to get rid of this, this, we're all just engineers is kind of what I think you'll say to them. Yeah. I think with special, I believe in God, it is a generalizing specialists. And I think that you want to build a good team with a lot of specialties and I will always be a test specialist and I don't care what you call me. It doesn't matter, but don't put me on a team where you call it combined engineering. That it's like you're trying too hard. Just anyway, I don't know if that. So I, I don't know if they'll frame it as combined engineering. It just pushes a button with me. I really think titles are going away at Microsoft at least at some point, but anyway, go on. Sorry. I agree. The, anyway, the, I don't think I've mentioned this. Maybe I have. So I spent the majority of my career as a middle manager in test. Uh, here at Microsoft, we call that a test manager role. Also called the loser. Go on. Yes. Uh, or, or, or meeting a tender. Um, I then shifted into a dev role where I did not have any testers at all. And one of the things that that forced me to do is of course, in order to, to ship quality software, I had to have my guys owning their own testing and we didn't want to do the full on test process where you test passes and, and all of that sort of thing. Uh, so we, we kind of did it, adually and figured out, well, what, how exact much of testing we needed to do. So we, uh, we, we learned very quickly, I've been a picker in terms of words to you. You learned the exact amount of testing you needed to do. We, we were constantly learning that. Okay. But it was iterative. So what I was about to say is that like, for example, first time, uh, we have a principle of we shipped straight to prod. So there is maybe hours latency between check-in and it's, it's live what the team discovered very quickly is that we had insufficient testing because the, we constantly, once we checked in, we constantly had to revert and thankfully we had the signals in place to know when we had to revert. Uh, but that reversion process caused friction caused annoyance on the team and what ended up happening is each of the guys learned very quickly. What checks they needed to put in place. It also caused learning, right? That's the big point. And that's, you know, I heard so many people, I gave this talk and then, um, took out the blog post in a second. I gave this talk. It's like, well, that won't work at our, we have a mission critical it app people use for expenses. And I go, you know what? That's a good spot for it. And the guy at the bank says, this is a bank. We can't do it here. And I just, it was your aside, you know, he's talking, he's doing like, um, if your application is maybe you have some comments on this, but I conceded when your whole application is helping traders make trades, um, in real time, I think you want to be a little bit more careful, but just generally being a bank, you can say, well, we have to have accounts. Right? Yes, you do. But if you are a bank and you are not AB testing that credit card offer on your homepage, you are screwing up. Yeah. And if you're not clear, uh, what AB testing kind of means in this context, that's right. We have two AB testing. We have Alan Brent, and then we have controlled experiments. Right. Now, one of the, one of the, the, the processes in AB testing typically is it's not an all or nothing. So when we say we shipped a prod, we don't light it up for the entire world. We, we, we mitigate risk by lighting it up to a small number of people, the number of people that if we do screw over, uh, we have the ability to reach out and, and personally engage and repair that issue. Like if you light up to the whole world and you negatively impact millions of people, your team cannot scale. Yeah. And that's the really, that's the really important thing that, um, I forgot, you know, I sort of in my head is taking for granted, but I forgot to mention is that when you roll out these things to and get data from real customers, you, when you're rolling out changes, you're doing it to a very small audience versus like 1% or less first. And then then the concentric circles growing with monitors in place to let you know the second things, whether it's perf, whether it's, you know, disk space, if something's CPU, if something's going, not as expected, you can immediately stop and roll back or roll forward depending on the product. Yes. So back to the, how, how it worked on my last team. So my, my guys got really irritated with this and I made it very clear. I will not be looking for testers. So you guys need to figure it out. And I offered, you know, more actionable suggestions to that, but I, I, I primarily empowered them to work through it. And what I discovered is on their own, nice on their own, uh, they, what they did is they realized first and foremost, in order to do this, they need to get the right set of checks in place to, to manage what they now have learned is things that can cause the service to go down. And then what they did is they realized that their code wasn't built for testability concerns. So they ended up refactoring their code. And as they refactored their code in reducing coupling between objects and, and making objects much more cohesive, turns out we found out that it was a lot easier to add new features because things, the, the overall complexity of the system had dramatically reduced. That's a really critical thing. Not, you know, it's not just more efficient, touching this before, but that's more efficient for developers to write their, their, their short test for those things. But it has such an impact on code quality. They were, they realized right away, oh crap, I can't test this code. There's no way. And they, and they, the code is designed so much better. It's, it's, I'm really excited. It's a little fantastic. The, the current model that we do where, where there's this, this awkward segmentation between dev and a handoff over to, to test. What this does is it, it optimizes for the maximization of costs to produce crap quality. So, so in addition to, to the code quality going down, because the code quality is now more isolated, the need for additional tests or checks also goes down. Absolutely. You'd no longer have to create this, this huge bank of end to end automation because the system is so put together and so complex that you really have no freaking clue which of these test cases could fail and win. Correct. So back to the original subject quickly, we'll get, one more thing and then remind me that we were talking about something else. Automation. So the, the other thing I would say, you mentioned, Hey, this is the type of automation we should have. The one thing that I don't, I wonder where you will put it in. Cause in my way of thinking, uh, we will need synthetic simulation. We talked about this in the past and I do think this is, this is code that sounds and feels like automation from the old school way, but it differs in that I don't give a f*** if this automation contains validation code. All I wanted to do is drive synthetic simulation against the product. Yeah. And for, um, I don't have talked about synthetics before in the past, but that's kind of what we did in testing for a long, long time. It's, it's a bunch of it's code to or data to pretend you're the user. Yes. So I was getting ready for this. Uh, I decided what I was going to do for my, uh, star keynote, uh, start the lightning talk and Brent is touching my coffee. Brent don't touch my coffee and I was getting it together in my head and I think I, it was the same day. I did a, uh, uh, interview for sticky minds. I'll, I'll plug it when it's ever was published. And I'm thinking about it all day and I'm trying to get it in my head. I think about writing some notes out and I go, you know what? I am just going to stream of consciousness, blog, my lightning talk. And I ended up making some slides and putting the pictures in the blog post. And it was literally, I wrote that post in 15, maybe 20 minutes, threw it up there, did the lightning talk. And then I doubled the, the I have my old record for the most hits on my blog in a day. And I doubled that in four of the next five days. As people tell me, some people said, Oh yeah, we do this all the time. But most people said, you are a flaming idiot. You have, one person told me posts like this are annoying. You should let the testing profession have the credit it deserves. I'm not taking anything away from testing. This is not just for Microsoft, how we do it at Microsoft and almost every team, but how a lot of big companies, especially with services do things. It's not, it's not, I'm not like describing the future or some hypothetical thing. There are companies already doing this stuff. When I get similar guidance of not giving tests, the credit that is due, I make it very clear. Look, this stuff in the, in, in the world that we exited 10 years ago, by the way, this stuff in the world that we exit, it was hugely valuable. But we didn't exit it because in the star expo, there were like a dozen vendors selling GUI automation tools. I would say modern testing techniques changed around 10 years ago and testing companies, testing institutions, maybe that's a better word. They haven't really caught suit, including here. It takes time. I found a GTAC talk. Maybe I can't remember who pointed this to me. A GTAC talk from 2007. That's seven years ago for those of you counting. And the statement in the talk was monitoring, monitoring services is indistinguishable from test automation. Brent made a face. So it's a little bit of a stretch. And I probably script the paraphrasing, but he's talking about these exact same things. There's definitely a mapping is very similar. The, the, when you first start off, it's, it's almost verbatim test cases against prod, right? It's, it's testing in production. But over time, as you get the right data exhaust, then you, you end up building towards leveraging more of the exhaust and actual product or customer traffic. Then the signals generated from these synthetics. So one of the processes that I see very common is once that flows, people then start building muscle around, okay, which of these tests can we shut off? Because typically what happens is these, these monitors get put in play and when they fail, they wake somebody up and that generally perceived as annoying. Got it. Got it. All right. We've, we've beat on that a little bit. So we skipped over the first one. Well it's because that's your fault. Yeah. I, I, I, and I would do it again. All right. Just star in general. It's a conference is growing. I can't remember how many stars I've been to at this point. It was a good time. It's, it's a, it's a weird crowd. Probably half the crowd has been testing for less than a year. And most of the speakers are consultants trying to drum up business. I sort of inferred that second part, but most of the other consultants and nothing against consultants. And this is a great place to drum up business. But they're drumming up business from people that haven't been testing very long. Take that for what it is. And as I mentioned before, a bunch of vendors selling, uh, trying to sell GUI automation tools, but there were some cool vendors or two sauce labs. They are really like them. Smart bear was there. Smart bear does some really cool stuff. If you, you know, smart bear software, the first, my first exposure to smart bear, this is like their flagship legacy old product is code collaborator, which is a fantastic code review tool. Really good for capturing comments and lie and keeping them and, and in a variety of different formats and sort of takes all the best things from all the different forms of code review and puts them into one tool. That wasn't even a paid plug, but I was a big fan of that. And now they make a bunch of other testing tools as well. And, and Scott Barber, I'm off on a random topic, but anyway, big fan of smart bear and I'll shut up there. So Scott was there at then Scott Barber was there. Yeah. I'm, I'm a, I'm a fan of Scott. Um, and I didn't mention this last time. Like I'm a huge fan of, of leak Copeland. I think he's done a great job in terms of sort of curating the history of, of test and what's value in that. I am curious. I haven't, I haven't followed him recently. Is he staying in sync with sort of modern testing techniques from your point of view? You know, it's honestly, as much as we rant about it, I'm not sure what modern test techniques are, if I could describe them, but I can say dropping test automation would be an example. I think if you don't think test is changing drastically, I think, you know, there's a lot of people out there stuck in 1990s testing and they're really good at 1990s testing. And guess what? There's still software being made like it's 1990 and all that's great. I think, uh, at least for the star conference is somewhere in between. You're starting to see the audience is a lot more technical. So this is a period generalization, but I would say I've seen star grow. It's sort of lagging, you know, some of the industry leaders, but they are growing. There were a lot of talks on forgive me father test automation and some more technical topics. Alan Richardson, Missouri gave a tutorial on selenium so that people get testers writing code, a lot of testers and a lot of, you know, S debt type testers know how to code. Whereas even five years ago, in fact, it was probably five, six years ago, somewhere back there, I gave a talk and as part of the talk, I showed some code and people got up and walked out. It's like, ah, I won the code. I go to a programming conference. Interesting. So what you're seeing, so in old school Microsoft vernacular, those types of testers we used to call STEs and are you, are you seeing there's a shift. There's a shift from the tester to the tester who can code whatever you call them. I, you know, from the moment it was introduced, I didn't like the S debt title, but a tester who can code. So I wonder, do you guys have access to the title of the attendees? Cause I'm wondering if actually if what you, what we're beginning to see is actually people who are actually developers who are trying to enhance their skillset. I don't know if we have that Lee may have access, but the thing is titles mean nothing. They don't mean nothing. They mean something. It may be fuzzy. No, they mean nothing. No, they mean nothing. Nothing. Nothing. If you look, you're, you know, I have a four year old Alan. I could do this all day. Nothing. Something. All right. So anyway, all I'll say that I can from my observations that I talked to, I didn't manage to talk to every single person at star. I talked to 20 or 30 and most of those were not every single person at star. No, there's about 700. Don't be a dick. Anyway, a lot more recoding. I talked to really interesting. I talked to, I'm going to leave all anonymize this. I talked to a few people who work at a company and they're telling me is you're going to love this story. I didn't, didn't prepare Brent on this one. They were telling me that about their command and control managers. And I was giving a talk about software engineers are knowledge workers and they can't be managed in the same way. And they had very much these command and controls. How do you work within that? And I have some, I have some good ideas because we have a very not command and control org living within a larger command and control org in my head. After talking about, about their managers, I say, man, these guys sound just like some of the, the old school managers I've worked with who have a way they want to do things and want to micromanage, et cetera. Then they dropped the bomb that their managers were former Microsoft managers. Yep. And, and to be fair, I, I've been talking a lot. I think we might've talked about this before, but people, um, I think people, it's really easy to rely on your past to bring you forward. And you're thinking, I spent 15 years, you know, managing people this way and I got promoted and so they hired me because of my background. So I should do things the exact same way. Yeah. So it's interesting because about 10 years ago, I was frustrated with that aspect that we were losing some, some, some of our management talent to other companies. But what now occurs to me is it actually might be a competitive advantage for Microsoft. I didn't like the way you use talent in that sentence. Fair enough. The, the, it's interesting because I have seen that a lot of our old, 10 years ago, we were way more command and control than we are today. And a lot of the old talent is now in my view, corrupting our competitors. And I'm like, okay. It was very interesting. Um, in the, I'm going to skip that one because I think it's leads right into number four. Okay. I, on Friday of star East week, uh, there's a smaller summit. Um, hundred people ish, I gave a keynote there and sort of serendipitously what happened that week at work is I was thinking a lot about our team culture and what made my current team a great place to work. So I, I sketched out some of our principles and our, and the values. I'll talk about what those mean in a second. And I was, it was going to bounce those off our team. And they said, this is exactly what we're doing and how we want them written down. So this is great. The one thing I want to stress is that when you're trying to, I tweeted this yesterday and, and I'll repeat it again, paraphrase, is that, I think a lot of software teams undervalue how important it is to have a good team culture. And in order to make great software, he had a great team. And I think too often going back to command and control, we think we don't need a great team. You know, people are just, you know, they're like cogs. We were a place and we'll just, as long as they keep turning the crank, the right thing will happen at the end. And it doesn't really work that way. Some of the ways that, unless you view the right thing that happened at the end is everyone's working 30 hour days and producing crap. Exactly. Exactly. 30 hour days are good. So one of the things that I've found, and I've read this a lot like in Pat Legione's work and that I found is being ultra transparent in, as a leadership team, what you care about is probably the biggest step in my experience for helping, helping build that strong team culture. Nobody likes, you know, I talked about surprises at review time and nobody wants the mushroom concept. I can't remember what it means anymore, but people don't want to be in the dark on things. And if you let them know everything, like here's how we function, how we expect you to work and over communicate that over communicate that because everybody on the same page and it does amazing things for team. There's a risk with over communication. And if you over communicate, then, then everyone understands that these are the words that we want everyone to follow. But if you don't follow it up with action from the leadership, then they will bit flip. Then the next step is you over communicate and then you praise and reward people who, who show examples of following those principles or values. Yes. And that you don't have to like, you know, John followed principle number two very well. He gets a hundred dollars and a gift certificate to target. Thank you. Thanks to, uh, to Wendy, she did a fantastic job working with others. You know, we really value that on the team. Um, so it's a great example for others. And that's just that public praise is massive because, Oh, I want to be praised too. I'm going to do those things. Well, there's other ones, right? Not everyone's not anyone, not everyone's motivated by public praise, but for example, for me, I will do those types of things because I want to accelerate towards the goal. And I would try to measure my acceleration towards those goals. You're reading my mind on this, uh, this talk I gave. No, it's really good. Um, I spoke a lot about the knowledge worker. It's been a lot of writing about this. You know, Dan pink wrote about it first in a whole new mind and then a little bit more in drive and in drive, but perfectly logical point. When you think about it, knowledge workers are motivated by progress, pay them enough money. So money's off the table. And then if you give them the tools and frameworks they need and they can, they can see progress happening, they get excited and want to make more. And that sounds very much like what you said. So to follow up earlier, you were just talking about the, the smart bear project, sure. Uh, along the lines of, as well as you talked about command and control. And one of the things that I found is right. Command and control is a decision making framework. It's, they wait for data to bubble up to them. And then they add their experience and expertise and bubble data down in this new world. The easiest way you can defeat command and control, even if you are deeply entrenched in command and control is increase the rate of knowledge at the lowest level. So, so creating a good shared knowledge management strategy, whereas the guys at the bottom have all this knowledge and all this data, and they're very quickly making decisions. You will blow the stack. This guy at the top will not be able to scale. I also talked about decentralization and the work of Steve Denning, who says that in leaders guide to radical management says that for knowledge workers, give them a framework to work in and then get out of their way, let them self-organize into teams, let them figure things out. But the key insight that I would say here, and I'll let you finish your train of thought is knowledge management as an important technology is something that is more and more becoming evident as critical in this new world order. The, I think a lot of people are like, what? I wrote a doc, it's up on SharePoint someplace. Go, go, go search. No, that doesn't help creating, creating something like like we have with Yammer, something that allows people in this, this, this, I forget the name of the code bear, smart bear, smart bear. Yeah. No, the, but the project where a co collaborator co collaborator where they're capturing shared knowledge in the context of the code, those types of things accelerate the people who will be actually taking action and negates the need for every decision to flow up. So now you have Phillip armors, five orders of ignorance. I'm not going to go through them all, but we need to provide the top levels when you know about the order. So you know how knowledge acquisition works, but the level below that is you need a suitable means to discover what you don't know. You don't know. Yes. And Alan has a phrase he uses here at Microsoft and I have been finding that, that I am repeating this phrase over and over again in my own conversations. And I've also found that I continue to feel rather dirty every time I use the phrase because it's one of the few things I've noticed from Alan that is terribly smart. And that is, and that is if only Microsoft knew what Microsoft knew, there are numerous times I sit in my office and I'm now sitting at my desk and I know I am solving a problem right now that someone, someone somewhere else in this company has already solved, mastered and optimized it. But the reason why I am solving this problem right now is I have no clue how to find that guy. Well, yeah. Um, I have a, um, I have a thinking about it. Yeah. Um, I have a, um, I have a thank week paper I never finished and I probably never finished it about it's called if only Microsoft knew what Microsoft knew. And the idea is that I think at this stage and maybe it's getting better. Um, it's hard to tell cause things are so incremental with such a large company, but right now I would say Microsoft size is sort of slowing them down. And I think Microsoft size, if there was a suitable means for people to discover what they didn't know, it takes a while to get there that we could use our size as a serious competitive advantage. If we had so much mindshare here, yet if we had a really efficient means for capturing and sharing knowledge, we would, we would destroy our competition. If we just had that, but because it's an indirect asset, who knows when that's going to get funded? Yeah. I'm just going to book drop all over the place. Peter Singar wrote a book called the fifth discipline about the learning organization about, um, and that's one of the, the bits of one of the principles for our team is that for my team and our principles, I wrote, we are a learning organization and we strive for continuous improvement. We realize that failure is an option and that making mistakes is part of learning. That's so we five principles. And one of them is, you know, we're going to try things out and if they don't work, we're going to say awesome job trying that out. Try something else. I have it when usually I give this type of speech, I simplify it. And I said, in order to learn, you must fail. You don't learn from succeeding. No. Um, anyway, I had a good time giving that talk. It made me think a lot about sort of our own team and reflect on that a lot. So I'm really, there's some good stuff going on to, uh, recap, figure out what you care about on culture on your team and make sure everyone on the team knows. Yeah. So I, do you think you could publish your principles? You know, I think I could, I showed them at the conference. I could probably publish them. Do you want me to go through them quickly? Yeah, sure. So the principles are the organizations, teams, and disciplines don't define the limits of what we do. Our team members will feel free to work across the quality team boundaries, take on dev work as needed and represent their own work as a project owner when necessary. So we're not worried about walls. In other words, tear down the walls. We don't have, you know, we don't let walls between, oh, that's not my job. It's your job. Get in the way. Something needs to get done. You get it done. Yes. We have a culture of innovation and we embrace positive deviance. You familiar with that term? So positive deviance is when you have individuals or groups whose uncommon behaviors and strategies enable them to find better solutions. So we're looking for people to try to try different. So risk takers, innovators, that sort of thing. Yeah. Okay. Everyone, and this is actually the thing I'm most proud of on our team is everyone on our team has meaningful work that aligns with their passions and expertise. We've been able to poach and cherry pick a little bit on this team. So we've been able to kind of fit pieces together and people are super excited about what they do. That's critical. We don't want people, again, goes back to Daniel Pink's work is that if you like your work and you make progress, you're just going to do more of it. Yeah. And then we're open and transparent about what is going well and where we need to improve. And this principle is multi-directional. It's true across the team from individual contributors to leadership and from leadership back to individual contributors. Yep. We share everything. And I mentioned we're a learning organization and we strive for continuous improvement, failure is an option, making mistakes as part of learning. And then the last one, unlike the others, but very important in how we approach, this is all the how we approach our job. The best place to evaluate customer experiences is in production. We will push our components to real users as frequently as possible. It's a little bit different, but it's still part of the how. So we... Yeah. I don't like the best place. I mean, I completely agree with it, but it implies that there's other places that you could... You want me to change this to the only place? The only valuable place. If you evaluate. Yeah. And the other thing, just to tack onto that, that we're really transparent about is any company has a, some sort of review system, whether it's the, the evil stack rank or something else. And we want people to know, you know, I say no surprises at review time. We want our team to know exactly how they're being rewarded, what we care about. And it's very simple list. It's, um, we value output. Our team gets a lot of work done. We also, that not blind output. We also value business and customer impact that the output directly impacts our business and customers. So it's really pot. You can, you, you, you've seen, you know, examples of people who are really busy doing a lot of stuff, but it's not relevant stuff. And that's not, that's not valuable. Back at again, re-emphasizing innovation and experiments. We experiment with new ideas, tools, customer experiments, et cetera. We're, we're, we're, we're experimenters. We're trying new things out. And then, an obvious one, collaboration, our team works together and supports and helps each other to be successful. So when we're looking at, when we're having rewards discussions, you know, we're having discussions about Brett, what, what Brent did, how his work impacted the customer, what new ideas he brought to the team in order to get a lot of ideas, good ideas. You get a lot of ideas and throw the bad ones away. So we need lots of people bringing in ideas. I need you to work well with others and help others better, especially for our more senior people. We expect that you're doing significant work to help make those around you better. Yeah. I had a discussion, uh, recently with, with one of my mentees and it was interesting cause he was trying to put together a Brownback and he's like, he put together one and he talked through it. And he noticed that people weren't really reacting to it. And it was interesting, but I'll, I'll cut the story very short. But at the end of that event, one of the things I made it very clear is that there is an important distinction between teaching and learning. Yeah. Right. Just because you're standing up and teaching people doesn't mean that they're learning and make sure that the goal that you have is the right one. If your goal is to teach, great, you did it, but it non-trivial chance, zero impact. The all you gotta do is say, no, teaching is a means. How do I get these guys to learn? Right. But he, he had approached it as that more of a teaching exercise. How could I tell them what I know? Is that closer? How can I get what I know stuck between their ears? Yeah. You got to think about learning objectives and what, what you want them to get out of it and focus on those. It's a, it's a, it's a difficult art. Yeah. I think it's a lot, a lot of room for learning and making mistakes here. I've realized looking at the clock in our list that we're not going to get very much farther. So I'm going to share one little bit of, uh, mail I received and just keep your reaction on it. And then, um, I hear that you have a mail bag reaction. We'll get to that. We'll close. Yeah. So where's the, uh, the, the new mail bag theme? Oh, wait. Okay. Excellent. This is rapidly becoming my favorite part of the discussion. All right. So the first one is I saw a mail, uh, recently with someone talking about planning and just some, some work the team had to do. And then the last two paragraphs I thought were really interesting where the manager in question felt the need to, uh, take two paragraphs to explain we are not waterfall. We are not waterfall and explain and start starting to justify nothing above was really too waterfallish, but I thought it was interesting that the manager felt the need to spell out to the team that we are not waterfall. So there's nothing above it that was really waterfallish. Not really. I think there's, why do you think he, he felt, there's gotta be other motivation there. Yeah. But whenever I see, and in general, when I see people defend whatever they're doing is not waterfall, like waterfall, like, you know, waterfall is the evil death and we're not that you need to look at what you're doing and think, and really think deeper about, you know, what are you doing? It, it, I mean, it does come across the way you've tell the story. It comes across as a, as a defensive rationalization. I think someone may have like talked to him offline and that showed its way in there, but I think it also falls into that trap that people think that waterfall and agile are a dichotomy and they're not opposites on the end of the scale. Brent's about to go into a rat hole here. No. So the problem is, so I agree with what you're saying. The problem is, is that at least in the context that you're referring to both waterfall and agile are dramatically overloaded. The people you, absolutely people using these terms, uh, know what it means in terms of the culture that they're used to, but they don't know what it means in terms of like the, the, the data that you have and due to the investigations in the research you have, like I can't tell you the number of people I've talked to that says, I hate agile. And it's primarily because they see it as totally slow and inefficient and, and it really, when you look at it, it's what, what has happened is they have taken the waterfall process and applied two week sprints to it. So they're merely iterative. They're very merely iterative, but the, the, the heavyweight big design upfront model that, that often typifies, uh, I'll double quote it the waterfall process that that still, uh, part of their new agile team. So you spend 13 weeks building, building, no 13 weeks. So people will do one month long, um, agile milestones, spend a week and a half building doing all of their design work upfront, a week doing coding and then a week running around like headless chickens. Little sprinter fall. Yeah. And they, it's basically they're trying to shorten the cycle, the iterative cycle and they're not getting it right. It's a big part of, of, of agile that, that really makes things efficient because you don't try to boil the ocean. You try to go find the coldest part of the ocean, make it warm that up, and then figure out how to keep it warm so then you can go find the next coldest part of the ocean. I guess the important thing that often missed is the value of constant learning and adapting that comes with that. You want to learn as much, do you want to be a learning organization? Like a good agile organization is a good learning organization, not that other organizations can't be learning, but sort of built into the process. I was on a Twitter discussion recently and it was a bunch of people, uh, venting about how, how the learning loop is important to agile. And, uh, one of the, one of the stories that I shared on that is the, if you go to the dictionary and look up what agile means, it's, it says to move quickly. And so people interpret this as, oh, well, we're agile, so we got to go as fast as possible. And, and I said, no, okay. This is false. That the definition in terms of how we need to use it is not correct. You go look at, uh, what was the champion, uh, four 40 runner in the last summer Olympics, the, the, I forget his name. Yeah, go on. We'll call him that guy. So that guy is really fast, but they don't call him agile. They call him fast, right? Agile is the ability to move quickly in a new direction. Ninjas are agile. I throw something at a ninja. He gets out of the way pretty damn quickly. It's always a great podcast. We can bring ninjas into the mix. Yeah. I was trying to quickly think, is there a way to do pirates? Uh, but no, I can't. That's all right. Yeah. Arr there. Um, so agile is about changing direction quickly. And if it's about changing direction quickly, then you need to know the direction you were heading. You need to know the new direction. Both of these are often guided by goals. And in order to establish the new direction, you need the ability to learn quickly. Yes. You know, when I saw that mail, it's like little red flag in my head. And I thought there's more, one, there's more going on here, but two, you don't need to be defensive. I just, you know, I think you want to make software based on the context of your team and your customer and, and all kinds of other things in the most efficient way possible that often that's, very agile or as I heard it called in a meeting a few weeks ago, super agile. This guy, I've still, it reminds me of slow drill. I still want to buy a term and I use it. Um, this guy, he would have done a better job instead of arguing why he's not waterfall, uh, arguing, or maybe not even arguing, but instead of saying what we aren't, he should have said what we are. Yes. Uh, notice the principles I gave for our, for our team. None of them says what we don't do. It's all about what we do. Right. Jensen's number one rule of politics. He who is defensive loses always. You cannot win any game, any competition by playing defense only. And when you start up a team who is with a framing that is defensive in nature, that will permeate throughout the team culture. I, my advice to you would be to go talk to that guy and get him to do a follow up and say, by the way, I've gotten some, some guidance. I want to be also clear. This is what we are. Yeah, that's good. That's good. So, um, we got a closer, but Brent, you had a little personal touch on the mail bag. Yeah. So I have a mail bag story as well. Um, so last time we, we spent some, some time talking about, uh, my son and he, he, he told me, Hey dad, I have a mail bag item for you. I want you to let, uh, Mr. Page know that yes, it is true. It is because of him that I'm following this podcast. That's excellent news. That's great. That half of our listeners, um, at least are following just me only. Yes. So I, I, uh, two, two, two thoughts. Very cool. Number one. Yes. I have a confirmed listener and number two stupid teenagers. All right. We're going to get kicked out of our rooms. We're going to take off. So I'm Alan. I'm Brent and we'll see you next time. Thank you. Bye. 
