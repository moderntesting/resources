Hello, everyone. Happy. You all right, Brett? Yeah, not enough coffee for you to already start hitting on me. Well, you know how things go when you're having fun. I'm Alan, and that's Brent, as you can tell. We're here for another episode of AB Testing. Number 90. Okay, so... Hey, so what is the AB Podcast all about, Alan? I... It's... I don't know. And to your financial report of political views, AB Testing is a podcast by Alan and Brent, and we talk about making great software. Sometimes it's about software testing, sometimes it's about data science and other fancy buzzwords. Software engineering, agile, leadership, most importantly. Lately, this thing we've been talking about... Shift left. ...nonstop, shift into my pants. Modern testing. And man, I got to tell you, it's weird. We've been talking about... We talked about the principles for a couple episodes. Then we deep dove into the principles for a while. And then we took questions that came up during those discussions for a while. And it's been a long time since we had a podcast where the topic wasn't specifically modern testing, even though we always... Will come up. For sure. But it's a little weird, as I was driving here this morning, there's a weird thing that happens if you don't live in the Northwest, in the tech center of Washington. There's a vortex where... Maybe it's a vortex, I'm not sure, but you can tell how close you are to Microsoft Campus by how bad the driving is. And there's a correlation, and maybe you can do the data science on this, it's expensive cars driving poorly. Oh my God, I must be near a Microsoft building. Are you sure it's Microsoft? I'm pretty damn sure. Because I think it's the tech center. It's not like there's a lot of other tech companies buried around here. They are in the Northwest, but around the Microsoft Campus, it's Microsoft. Oh, so yeah, if you come closer to the East side, yeah, it's definitely... Yeah, as I get closer to the Microsoft headquarters... There's Google on the East side as well. But they're up in Kirkland, they're not here in this epicenter. And they may drive poorly there too, but I can tell as I... I'm fairly certain they do. All right, well, I want you to go do the... And then I would also... Do the proofy proofy data science stuff on that. Yeah, I will get right on that. All right, so that's my hypothesis. My hypothesis is that if I see an expensive car driving very, very badly, I'm near a Microsoft building. I think it's also going to be heavily correlated to your latitude and longitude at that moment in time. Because I think if you worked at or near Amazon and you had to drive there on a day-to-day basis, you would find the same things. Perhaps, perhaps. Although that is also just because driving in Seattle blows. Near someplace that had... Well, at least if you work there, you have a reasonable public transportation route to get there. Yeah. Okay, so anyway... Which also blows. For our next number, we'd like to play the soothing sounds of boss gags. Okay, go ahead. Okay, we have... It's weird. We have sort of an agenda on the board, but we're not going to do it right away. Well, maybe not in that order. Actually, in terms of soothing sounds, like really buzz? I was trying to pick something that I didn't like in the 70s. Oh, okay. Fair enough. It was sarcastic. Yes, it was. I'm like, why would you pick that? There's so many better... Okay. I immediately started thinking of Miles Davis. I like Miles Davis. That's because he's awesome. Well, really, he did all these relaxing, cooking, all these albums with this great band and John Coltrane. Those are the best. People always go to the mainstream. They go, oh, I'm into Miles Davis because they have Kinda Blue. I go, fine. Check out the better stuff. Kinda Blue's good, to be clear. But relax and cook in all, which I just said before, but that whole series of stuff. Remember there, at that time, putting out an album, they'd put out like three a year, four years sometimes. That's impressive. So these guys in different bands, all the time. I saw it. They were great players. They could improvise. Record companies were making most of the money from that. Those recordings are so good. Anyway... How about Leon Redbone? Never was much into Leon. I introduced him to my... I think there's really only a couple of songs. I introduced him to my son the other day and he looked at me like, what is wrong with you? Oh, my daughter and I had a weird moment that we don't speak of anymore, but we actually found out that sometimes our musical tastes overlap. I was listening to something and she goes, wait, I like that song. I said, all right, let's just move on. To be clear, she listens to a lot of stuff I don't like. She's into K-pop. I'll just leave it there. What did I do wrong? I was just like, oh, I have that on my playlist. I could annoy Alan right now. Oh, God. Some of those songs is extremely fun to keep on your... What is it? K-pop band 21, I think, or something? I don't know. They have the song, I Am The Best. That sounds like a Pokemon song. Pokemon song. Don't most of them... Yeah, shucks. Okay. But any time my middle-aged son, or not my middle-aged, but my teenage son, middle son... Fred's so old he has a middle-aged son. No, I kind of combine middle and teenage inappropriately. Wait, what did you do in a pro... Never mind. Any time he's misbehaving, I could throw that one up, or pretty much anything from Taylor Swift immediately... Don't be harsh and untate. I'm not. She's fantastic. I find her music extremely useful. Say what you will, that woman can write a pop song. So on that front, it's actually interesting because my son, the same son, he's now at a point where he's starting to express that he likes dad's music more than dad. So he'll find some obtuse band from the 80s or the 90s, and he'll go, yeah, this freaking rocks. I'm mostly into what used to be called alternative rock. Our local radio station, The End, is mostly what I would listen to, although... Are they still around? They're still around. I haven't listened to anything on the radio in so long. Anyway... The radio station that I grew up with, which was KROQ in LA, they're now streaming on Radio.com, and that's pretty much all I listen to now. He'll listen to some songs that used to have a lot of play, and I'm like, who the hell are you listening to? He's like, dad, oh, this is... What is it? Social... Distortion? Yes. And I'm like, oh, yeah, it is. He's way into Foo Fighters, he's way into Green Day, he's over the top into Red Hot Chili Peppers. Are those considered alternative? I never paid attention. Alternative rock. All right, whatever. I was done with Red Hot Chili Peppers about 10 years ago, and the fact that it's come back as he's full on addicted to them, I know he's a crap out of me. Give it away, give it away, give it away. Now. Give it away now. All right, there was... Oh, quickly, Weasel is heading to changing the subject, is that okay? Just interrupt me. Yeah, you know, it's funny, because I didn't pick up on the change, and I'm like... I interpret... Speaking of LA, I'll do a segue. Speaking of LA, I'm heading to Anaheim next week. I interpreted your statement as Weezer, which by the way, the Africa song is fantastic. Oh, did you see... But everybody's seen, I mean, lots of play about Weezer covering Africa by Toto very, very well. Did I see the Toto hash pipe cover? Did you know that Toto covered hash pipe? No, but let me finish my freaking story. Did you see it gets better? Imagine what is better than Weezer covering Toto's Africa. What could be better? Imagine when Weird Al joins them on stage to play an accordion solo during the song. I have not seen that. That is worth checking out. It's not just Weezer playing Africa. It's Weezer playing Africa with Weird Al, not coming out to be Weird Al, not coming out to do a parody, just playing a rockin' accordion solo. Did he take over the keyboard solo part of that song? It's a moment that I'm glad was captured on film. Usually I hate it when I go to a concert and there's like 50,000 people, I guess I don't go to concerts that big. I go to concerts that are in very small places usually. But when everybody's holding their camera up the whole damn show, their phone up the whole damn show, it's like, just watch, man. Use your eyes. All right. It's very easy to find on the internets. Brand has already bookmarked that. That's my homepage now. So anyway, I'll be at Star West coming up here. Kids and I are going down early, going to the Disneyland thing. I'm going to send them back. Hopefully they make it home. Then I'll do my tutorial. I come back. I'm back for a week. I go down to Portland for PNSQC. Really looking forward to that. I kind of need to finish my presentation for that. So go ahead and ask your question. Star West? Star West. Is this because you had that prior commit? Yes. Okay. Yeah, I'm just doing a tutorial, nothing else. And then I'm flying back afterwards. And then PNSQC, I am working on my presentation. So for those of you that have seen my modern testing thing, which would be just those of you at Test Bash, hello, everyone. Welcome to the cast. Yes. I kind of ish some of the same stuff, but so much has changed. So at Test Bash, if you remember, the one listener, one of the three that was there, if you remember. You're talking about Brighton or Philly? Brighton. Brighton. Philly was like a year before. Everything was a twinkle in our eyes. And I was working on Microsoft Teams. Oh my God. Is there a story that you can tell me off the air? I don't think it's inappropriate to say on air. So I'll just do it this way. Teams, lowercase teams, some are being forced to be on capital T teams instead of Skype. Honestly, I don't really care except for, you know, forcing people to do exactly the same sort of crap they've been doing on a totally new interface. But it's the best. Defined by who? I can give you a name. I know the name that you would probably name. And I just, so yesterday I loaded up Skype. I needed to talk to one of my PM's. Skype had the little green indicating, as you know, hey, my PM is there and he's available. So I started typing, wrote a long ass thing because I knew he was in his office, not in the meeting. And I think it's absolutely rude when people go, are you there? And then you respond yes. And then you write a 15 minute diatribe and their attention is captured the entire time. So here's a little tidbit I do on these things. If I'm going to ping you with some long ass question, I'm going to type it up in notepad first. Then I'm going to say, hey, Alan, are you there? And when you respond, I'm just going to copy and paste because I don't want to waste your time. Okay. So I pinged, I wrote this long question, sent it. Then I got a response. Oh, this guy is exclusively on teams and cannot see your message. And I'm like, what the hell is this? Could you not have told me that before I spent five minutes typing this goddamn essay? Can you just copy and paste it from notepad again to someplace else? Turns out actually in the middle of me typing, it turns out this same guy was actually walking to my office. So literally like two minutes after I hit send, he was at my door. And I'm like, not even two minutes, like the 10 seconds. But I'm just like, this is dumb. Why don't they? What I love Alan is our Slack usage. Slack is like teams. It's another product that lets you collaborate and communicate across your team. It's just so natural. It's just such a natural part of our communication. But I think Microsoft grew up embedded in email so much, it's hard for people to get out of there and find another place to do it. And what's weird about Skype is I have to do a Skype call later, which sucks because I had to install Skype. Sorry. There is a web interface. There was something, oh, I was telling a story about test, my presentation coming up. There was a story, if you remember, back on our regular schedule program, I'm modern testing talk. If you remember at Brighton, I talked about modern testing, kind of how we got there, how my journey through software made these things make sense. I thought your deck was like Adventures of a Modern ... And then we had just sort of finalized the principles. I knew they'd be tweaked a little bit, and I think they maybe haven't been tweaked since then. They've actually lasted like seven months. But I didn't go into them in depth at all. I just showed them one by one, kind of walked through them and said, stay tuned, listen to the podcast, learn more about them a lot. I think what I want to do in Portland, what I'm planning to do in Portland, is actually have a slide each on the principles and kind of talk about what they mean. So it'll be the one minute version of the last eight episodes at that part. So I'll take 10 minutes and talk through each of the principles and why I think they're important. And then it leads up to the potentially controversial number seven, which is interesting. I'll bring it up there. I love bringing it up in test crowds because it's like, but what's cool about number seven, and I don't have it in front of me, but number seven is the one where the tail end says, it may reduce or eliminate the need for a dedicated testing specialist. And if a team is just starting on the modern testing journey, or they're not even there yet, they're making software in a very predictable, traditional model, that principle is like, these are, it's usually the reaction. If I can quote the air quotes around that. But once you start moving down the modern testing journey, you look at that and you go, oh yeah, I could see how that makes sense. That's been my experience in both my team and talking to people that have looked at these things. I think it's only controversial until it's not. And the way it becomes not controversial is that you start to realize, you begin to realize that, okay, I can see, yeah, I could see that. So you and I talked offline in terms of one, we'll probably weave this into podcast episodes. But one of the things that I think is needed is sort of some tactical scenario discussions around problems and how the scenarios weave together. Because I think the other thing that we need to talk about on MT and come up with sort of a cogent way of doing it is these principles do not operate alone. And it's very similar when I go and talk about Agile and I go and talk about Lean, or even Kanban. And now the sort of operational principles. One of the common things about Agile, as you know, is it's principle oriented. If it were rule oriented, then you start, it's just a matter of time before you have an inability to adapt, which is the point of Agile. And that's probably where, just thinking quickly, where a lot of the fragile approaches fail is they become too prescriptive and rule-based. They turn Agile into something it's not. And another part of it is also simplification. People at the very beginning, they will look at, what's an example, Lean I think has seven principles. And until you practice, it's hard to keep all of the principles sort of in your head and see how they sort of interact with each other. The issue with the principles is that if you do one to the extreme and ignore the others, it can absolutely take you into the weeds. And it's another principle that balances that. It's a check and balance system. It wasn't intentional in the original, but as they evolved, and actually as we discussed them, I'm sure I mentioned this a couple times during our previous discussions, but I was pleasantly surprised on how well the principles did interact with each other and support each other. People need to practice the principles. This is why I think the scenarios are critical, so that they see how you connect the dots between them. And I do think that as you connect the dots between them, I don't think, this is not a belief. I've lived this. It becomes fairly easy for people to see how it works. I think the issue is it's still kind of too new and they don't see how the dots connect. We're not being prescriptive. Of course not. Yeah. So I think, yeah, we have definitely plenty of future podcast fodder there. Cool. I have a couple things I wanted to... One, sometimes Brent and I have conversations over Slack. Not over Skype, that's nice. Over Slack. And it ends up being interesting enough. Well, one, I'm too lazy to type it. And then two, I think, we can just talk about that on the podcast, because that'd be a fun one to share with the three. And remembering back to our roots, the original... The reason we have a podcast, it's not because we like each other, because that's not it. No. He used to have these test architect group meetings and Brent would crash. He wasn't like a good tester or anything, but he'd want to hang out with the smart people. Yeah, that's it. But we would have these discussions and then I think it was Michael Hunter who said, you guys got to record this into a podcast, because we just... We're happy giving each other crap, respectfully. I respectfully think you're full of crap, Brent. I don't know about respectfully, but yeah, everything else in that sentence was accurate. I'm mad at Twitter. Okay, tell me about it. I saw you change your name on Twitter. My protest name is Twitter hates me. Twitter doesn't like me? What is it? Would they go on? It's something like that. All right. Really, are you really surprised? No. Can I finish my story? It's an interesting story. I don't know if it's a good story, but I'm going to tell it anyway. Twitter-ter. Oh, because you can't have Twitter as part of your username. So I put a hyphen in it. Nice. Doesn't like me. Yes, yes. I tried and also, Twitter also has a crummy password policy where your passwords can't have spaces, but I'll save that for another rant. Oh, nope, I didn't. Anyway, as I've mentioned, maybe on here before, but there is a... My current Twitter handle is Alan Page. You're welcome to follow me or block me. Many times I've gone to the Angry Weasel Twitter handle. Twitter.com, like Angry Weasel, hoping that it would just show up as a 404 at some point, but no it doesn't. It's there from a person who tweeted once nine years ago. That's it. One tweet nine years ago. So I read some stuff on the internet about how to... What do you do? How can you reclaim used accounts? Some people have had some success just reporting it as an impersonation and explaining it as not an impersonation. I can prove this is associated with me and had success in getting unused Twitter handles from Twitter. So I did that. There's a crapload of Angry Weasel. There's only one spelled correctly. Loop back into the conversation here. So yes, I can change the spelling of Angry Weasel and get another handle, but I wanted the regular spelling one. So anyway, Twitter told me to F off. We don't do that. And so that happened. So then I thought, I wonder if modern testing is available. And modern testing is also one tweet from 10 years ago and then, or maybe an account but no tweets. I think that one was one tweet from nine years ago as well. And then I also checked for AB testing. No, AB testing podcast. One of those. And it was created and it's never tweeted. So modern testing joined 2013 and you look at their, no, this dude is all about us. No, I appreciate spelling. They have the modern testing account. There's no I in testing. That sounds like a, oh, that sounds like a quote for a t-shirt. There's no I in testing. Oh wait, there is. Seven. But not this one. So anyway, that's my Twitter rant that, and Brandt doesn't care about reading, so he doesn't notice that this letter is missing. So my wish for our, none of the three who work at Twitter is that we could reclaim accounts that haven't been used in a decade. Yeah. What did you means to similar, right? You have to, you have to pay for your domain, right? You have to pay to renew it. Oh, right. And since there's no, yeah, they cost money. I wonder how long it'll take before Twitter has to do something on that front. So it's not, they don't have to do anything, but at some point in time, you would imagine that, that certain decoration for redundant handles, uh, becomes so fricking obtuse, it's becomes harder to use. Yup. Okay. Let's do, let's go, we'll go up that way. So I have, uh, in my hand for those of you not on the video feed, Oh, you know what I want to do by the way, in this all tangent episode, uh, we should just for fun do an episode. Why is your leg twitching like that, man? You're freaking me out. Listen, ADD guy, move forward. I'm trying, but man, like the roof's shaking. What was I gonna say? We should periscope it. We should periscope one of these. So like most of this stuff will get edited out that you don't even hear it, but we should just live feed a podcast recording for the three. Sure. Not a tangent episode. Oh, and, and, uh, I just wanted to point out, this is the first podcast recording I remember in a while where branch has not been wearing his AB testing shirt. I thought about putting it on today. So you can go to cafepress.com, whack AB testing and pick up your own AB testing shirt. But in the meantime, I have some questions for you that were thrown to me and I think I have answers for them, but I'm going to check them. These aren't mailbag. I'm actually supposed to answer these at some point, but I'm just going to work through you and see what you think of my answers. Okay. By me listening to your answers. This is a good one. I actually think that that model is one thing that makes our partnership work well. You're very good at writing concise language and I am not. I'm going to edit that as just says you are very good and I am not. A couple of questions about AI, ML. That's artificial intelligence and machine learning. For those of you not into the, the name game. Do you think AI and ML will replace the, replace the traditional tester? As is a loaded question. Cause I don't even know what the traditional tester is. I do. But does the question asker mean traditional? I do think, I do think automation will replace the traditional tester and automation techniques will include AI, ML. Yep. Yep. Pretty straightforward. There's a couple of companies right now using, it's interesting. There are companies like Testem and Mabel who use ML to improve automation and actually make, I've always hated record playback, but they make some test generation tools that look so deep at all the different ways that items can be selected that even if the, on a webpage, that even if the item name or class or location changes, it can still figure out what to do. Which again, which is solving a problem that could be solved just by having an approach that makes like adding data IDs to those things. So just think of how the, how the market's moving. I don't know how much you're paying attention to it, but I actually had a one-on-one with one of my employees just this week. And I actually think we're going to start to see the fall of the data scientist over this next decade. And primarily, so I was talking to an employee and this employee, there was a bit of AI I wrote. He picked it up. Generalized it. He doesn't under, he doesn't understand data science really, but he does understand how to take an algorithm and turn it into code and pass it inputs and evaluate outputs. And what I think is going to happen is number one, with computing the cloud, it kind of raises, it kind of changes the definition of brute force in parallel. So there are a lot of techniques that can be done brute force. So when you have the cloud, you can do them really fast. So old algorithms will come into play that just couldn't scale because you didn't have enough hardware resources in your lab or didn't have beefy enough machines. That kind of all just goes away. It can scale as much as you're willing to pay for. We're going to have AI techniques in APIs readily accessible to developers. So you drifted off my original question a little bit talking about data science in general. No, but this, as these come up and as people solidify specific contexts, there is an old tool. So AI has been in the industry, in the test industry for a long time. We've, there's a tool that I know Alan's familiar with that was, it's probably been dead for 10 years now. And Genesis, right. And it used genetic AI to hunt through and find test cases. And it was very effective at finding bugs. What it sucked at is finding bugs that were important. Yeah. So what I've discovered so far, I spent a chunk of time. In fact, I just wrote my, some of the three notes I've shared from a few of these on the Slack channel, but I do a weekly newsletter for my team. And I wrote up one this week on ML and AI. And I just got a lot of screws on my mind because I was looking at these questions. And what I'm realizing is while there's a, he, there's definitely a buzzword status push for AI and ML in testing conferences are filled with talks about AI and ML tools, getting massive funding. They use AI and ML, but they're not doing things particularly interesting. Like I mentioned the tools that I think it's really great. If you don't have access or the ability, like when we talk about software development, it's not an us and them, it's just a we. That's the way I put it. And these tools are designed for, I need to write a bunch of web, a UI automation for a web page. And I can't get the developers to change their code and add accessible IDs and they're not writing automation. We've got to get something put together here. Those tools are excellent in that situation. I just don't want to be in the situation where they excel. Or I'm that last line of defense tester trying to write automation at the very end. These tool, these, these AI solutions, if you're in that situation, if you have to write a bunch of a web automation and you can't work with the dev team or, or you're unable to or they're unwilling to work with you and you just have to get this done, these tools will accelerate your ability to do that, but they're not solving the root problem. That's what bugs me about them. Not yet. I think, but also in looking through what's out there in ML and AI, and there are some tools that do some test generation based on examining the page, based on, based on learning about the product. But I still feel like everything that I've seen out there is scratching the surface of what's possible for improving quality using AI and ML. For example, oh, I just looked at this thing called sourced. It's sourced with a D in curly brackets. It's a static analysis tool, but like the things you've internally at Microsoft uses an abstract syntax tree to break things down and figure out how things work together. But it has models and does a bunch of ML analysis to try and figure out where bugs may be in the code based on a bunch of heuristics, which is, which is again, a start. If we could look at, if we could have ML and you might have this at your fancy data science stuff that looks at customer usage patterns collected and then uses those to figure out where risk areas and or what problems they need to be looked at by the internal team using ML to help figure out pinpoint where things should be done. Better test selection, which we've both done, are all great things. I guess the point is, without me listing examples, there is a big push for AI and ML and testing. There's a lot of it out there, but I feel like it's just barely getting started of the capabilities of what we could use those things for. In that, I would say, and therein lies an opportunity. The Alan, I'm sure you're tired of this question, but where do ideas come from? Ideas come from other ideas, Brent. So we've talked at the story like where air conditioning came from, right? Yeah. Did that turn on? Did air conditioning get it? Did it land on, ooh, what we need to build is air conditioning overnight? No, of course not. I will tell you that I know. So for example, we have AI here at Microsoft. So to the prior question, this customer usage data? Yes. Do I have it? I have lots of it, tons of it. What we still have work to do is connecting different data streams together and then coming up along with an approach to context. So we here in Azure, we have the ability to, when we see, so we can use AI to sort of cluster common failures together. We've had a similar thing in Windows for a long time where we get like call stack telemetry. This is actually, no, we're clustering similar signals based off of multiple different failure classes. For example, hey, the network blipped, meaning it had a momentary loss of packets. We now have the ability to sort of group a bunch of different signals together, then do what's known as discrimination analysis and go, okay, which set of signals really is important for this cluster? And then go and pinpoint a root cause. Now what we're doing right there is essentially, if you think about it, what we're doing there is using AI to find system integration bugs. Now because there's a lack of testers inside Microsoft, Dev's really interested in these type of things because it helps them to scale. No one inside Microsoft, to my knowledge, and I think it's also because there's a lack of testers in here, is trying to really apply this towards a test context. Although I'm on the compute team, like the one thing I would be interested in doing and I haven't done is go and talk to some of the UI folks and see, you know, what, if anything, they're doing in this. Because a lot of the times on the compute team, you can view it as sort of a system integration bug almost all the time. On the UI side, do you have another question? Yeah, we've actually, interestingly, we've actually already answered the questions that are coming up here. So we've actually talked through the other questions in talking about this, but one, just to get your thoughts, I had a little bit of rambling, but what are some other ways you see AI and ML supporting testers? And this is weird for me because when I see a tester, I think, I don't want to do that. Supporting quality. You want to frame it as pre-production quality? I mean, there's a whole bunch of existing techniques. It's essentially where do we move the needle? Where I found AI is really effective is in scaling calendar time. And so if I had known we were going to talk about this, I would have done a particular research, but there's something that I'll share with others. I did this a couple of months ago, and this is probably a deflection of your question. I don't focus, as I think I just, I am not focused day to day on how to use AI, ML to solve the test problem. I'm focused on customer quality, which hopefully if you're a long enough time listener to this podcast, you realize we view as a separate topic. But if you go to scholar.google.com and you do a search for test testers, software testing, as well as AI and machine learning, you'll see that there are a bunch of published articles already and that they're beginning to increase. So what's happening right now is academia is definitely doing research in a bunch of different places, and it's not going to be, it's still going to be a few years before any of these things hit something that becomes really useful for the masses or really productized. But to me, this is absolutely on its way. Test prioritization, that might be one place where it would be an obvious relationship between testing and the user data. So if you have test cases measured under code coverage, if you had, if you did an experiment where some customers, you know, interacted with an instrumented build, then you can start bridging the gap between what are common sequences that customers go through and associate that to the test cases that can validate that. Going to a world of how we do continuous deployment, right? Sure. You could run all of your test cases in five minutes by leveraging a million dollars worth of cloud compute resources, right? Probably is not effective. But you could also mitigate your risk by using historical data on both sides, your test cases and your customer data to sort of prioritize what these cases are important. And then, like a very simple AI technique would be to use AI to decide the stopping condition. When is the risk minimized enough that you can just ship it now? Yeah. Okay, cool. All right. A little more long-winded than I wanted, but I'll take it. Oh, sorry. Hey, we are... One thing, this is definitely last but definitely not least, one thing we haven't brought up here is, but worth bringing up now is, we lost one of our heroes last month when Jerry Weinberg passed away. I was looking through his list of books here and I realized, I didn't realize that I had all of them. I recommend Secrets of Consulting a lot. It's a book about leadership, which is a good book about leadership. I recommended a lot to people. I forget who. Someone on Twitter asked me the infamous, how do you measure quality question? And we started off with a little bit of, well, what is quality and da-da-da-da-da. And I recommended that he take a look at the quality software management series by Weinberg. I think I might have recommended it started with book two, but maybe book one. Anyway, because the book is 20 years old, maybe 25 years old, I can't remember, he asked if it was still relevant. I said, yeah, I think it is. And I hadn't looked at it in a while. I have it on my shelf at home. And he got back with it a couple days. He had already ordered it and got it. He goes, oh my God, this is so good. This is so relevant. It's helping so much. Yeah, it's really timeless work. I reread Are Your Lights On probably every other year or so. Just a good book to remind me how to frame problems and to think about solving the real problem, not the problem that appears on the surface, but taking a step back and solving the real problem. Well, I'll miss Jerry, but his written works live on, which is great. He just gave a lot to the software community. I wish I had never met him personally, have you? No, I never did either. But based on his writings, I don't know if I'm going out on the limb, but I wonder if really Jerry was continuously the ultimate modern tester. Could be, could be. His approach on things, it was timeless because he had done it with a set of principles that don't die. And the guy could tell a story too. Yeah, you know, RIP Jerry, you've had a lot of impact to us for certain. And I'm sure he's well aware of. He's changed probably millions of lives. Yep. Okay, man. So I'll miss you Jerry. Well, if we had our drinks here, we'd raise a drink to you, but I'll do that tonight when I get home. And in the meantime, we have finished another episode of A.B. testing. So thanks for listening and we will talk to you next time. 
