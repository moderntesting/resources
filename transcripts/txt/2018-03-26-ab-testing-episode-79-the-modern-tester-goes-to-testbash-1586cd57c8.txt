Hello listeners. Howdy. I'm Alan. I'm Brent. Back for another episode of A.B. Testing. We have an episode number, don't we, Brent? We do. We're number 79. Is that prime? You should know these things. It might be. All right. A.B. Testing is a podcast about things like agile testing, software testing, software quality, data science, organizational change, coaching, and very much recently, a thing Brent and I have been calling modern testing. Yes. How's that for a summary there? I didn't forget too much. No, you did a well. I have to write one out. Fantastic job. As Brent suggested last time. I should write one down. So Brent, what you doing? I'm trying to fix my microsomes stand without interrupting the podcast. OK. So I, as you know, as the listeners would know from the last podcast, in between then and now, I have, through the magic of air travel, gone to England and back. I knew that too. I was at Test Bash in Brighton at a wonderful time. At 9 AM? Was it 9 AM? Yes. What do you mean at 9 AM? What do you mean? No, I was. What are you talking about? I was. We're doing a podcast. I know. All right. It's 8 AM here. It is. Ish. No, it seems the, I could tell when you were presenting or rather when you were done because Twitter was a flurry. There was a flurry of Twitter. There was. A flurry of Twitter. Tweet. Tweet. So I must have been on, I was on at 2 o'clock there, which is early morning here. Not too bad. Like 7 o'clock here. Yeah, something like that. So anyway, I gave a presentation about modern testing. One of lots of good feedback, lots of really good feedback. But I'm trying to tell the story of modern testing. And something weird happened in the space time continuum. Vernon. Richard. Yep. Vernon asked how many people listened to the podcast. And in the way I counted there was one, two, three, one, two, three, three, three. There were only three listeners, but there were quite a few of the three there. Fantastic. That was pretty cool. But they told me that, yeah, this is what I aim to do was take all the stuff we've been babbling about for God knows how many episodes and summarize it. 78 at that time. Which is a challenge. A big, big challenge. But I think I did OK. I was surprised how many head nods there were. Usually I didn't scare enough people. And that's just a tribute to the Test Bash audience. I think Test Bash is our peeps. Yeah, they are. It's the official conference of the AB testing podcast. Yeah, and Tope. Official favorite. Tope we get off her butts and do our own. Speaking of Test Bash. Yeah. I saw Patrick Prill, Test Pappy, one of the three. Nice. Hung out with him. We had some food one night, had some beers here and there. And he sent this a Ministry of Test Stein. Oh, thank you, Patrick. He brought like a dozen or 18 of them or something. Some huge number from Germany over to Brighton. And was a little disappointed that security didn't give him a harder time about him. He wanted to take a picture of what they looked like in the X-ray machine, but they wouldn't let him. Apparently you can't take pictures of the X-ray machine. Why not? Rules. OK. Rules. That is cool. I'm holding in my hand a brilliant Stein. It's got a good weight to it. It says Ministry of Testing on it. Love it. It's cool. Very excited. So anyway, I gave the talk. There were lots of tweets. You had some questions about some. Had a few people join the Slack channel afterwards. And by the way, by the way, welcome to any new listeners that I met or talked to or you just saw me at Test Bash. Because it is a fantastic conference. And I'm happy if you are listening. And you may never listen again. But hopefully, maybe. I don't know. Whatever. Anyway, thank you. I always appreciate having more people in the three. So thanks for listening. You can stop now. You don't have to. You can. You can stop at any time. At least finish the episode maybe. Sure. Sure. If we ever finish the episode. So what do you want to do next? You had a bunch of questions about the talk and some of the feedback you saw. We had a few questions come in that are sort of mailbag worthy. How do you want to approach the next question? So let's talk about Test Bash. So this time around, I know you probably attended some of the other talks. Yes, in fact, I went to it. I did the full Test Bash experience. I did two workshops on the first day. I spent a day and a half at Unity doing some work there, meeting with a lot of people. And then I did a full day of two workshops on Thursday. I got up on Friday and did the Test Bash run along the water. A little cold, not too bad. Full day of conference on Friday. And Friday is the way they do Test Bash. It's great. What they do, instead of getting a bunch of mediocre talks and breaking them into tracks, they get a bunch of good talks. Mine, meh. And put them into a single track. Everybody in one room, everybody watching the whole conference. Oh, OK. Excellent, all the way through. And then the next day, there was an unconference, which is an open space event, which I love those sorts of things. I went to an event to draw a comic from Constance. I forget Constance's last name, but she's the artful tester on Twitter. And she just kind of, I liked her style. She pretty much said, look, there's no extra skill involved. It's just practice. Now draw yourself a damn comic about your Test Bash experience. That was actually really cool. Well, OK. Because I drew, oh, I named the modern tester, the sick figure. I saw. Monty the modern tester. So I drew a comic of, I tweeted it, you might have seen it. I did. Monty goes to Test Bash. That was fun. Monty looked a lot like you. Now I was in, I even replied to your tweet. I'm like, ooh. Because we've talked many times on this episode about the squirrel. And I'm like, ooh, that's a great name for the squirrel. It just occurred to me that the quote that the artful tester shared with you is almost similar to what we're saying. There's no special skill to testing. Now Delvers, sit down and start testing. Well, you got to start somewhere. But the thing is, it's the practice. And what Constance said was, if you just keep on doing this and practicing, you'll get better and better. And you don't have to be great. And she brought up the fact that XKCD, not particularly well drawn, but it's also written by a genius who has funny things to say. But if you look even at the really early comics of Dilbert and compare it to today's, it's a huge difference because Scott Adams draws every day. So I actually, and I've failed so far, but I do want to draw some more Monty comics. I think I'm going to try and draw a Monty comic a week and see if I get any better over time. Or you know what? You for sure will. They're not well. I mean, they're not horrible. I can tell the story. But I think the practice will be fun. So thank you, Constance, for that. A couple great talks. I met a lot of great people. In fact, all the talks were great. So if I single any out, I may make it seem like I didn't like the other ones, but I liked them all. Let me ask you. Go ahead, ask me. And then I'll go ahead and give a few of the ones that I really liked. Well, it'll be aligned, I think. So my favorite question to ask you whenever you go to these conferences is essentially take all of the talks that you listen to, aggregate them up into three distinct themes. Those distinct themes would be what? Oh, man, that's tough. I was surprised. This is going to be the long-round answer for one. I'm giving a talk on web testing tools at Star coming up in end of April, beginning of May. And for that talk, I put together a small little workshop on Postman. You familiar with Postman? No. Postman is wow. I know the song. Stop it. Postman came up a lot. Postman's a tool you can use for REST API testing. It's really, really good. It came up in a bunch of different talks. So first of all, I went to a workshop on exploratory testing web apps, and it was using Postman for almost all of it, which was cool. Got a couple ideas. From Amber Race, also from Seattle. And then it came up in one of the talks I had. Danny Dainton, if you ever read his blog or seen him on Twitter, I didn't realize it was his first conference talk. He was in the military for a long time and just told this story how he was basically an F up as a kid. Yeah, maybe that's a little harsh. But no, not really know what he's doing. Parents made him figure out what he was going to do. He joined the army, really learned how to learn there. I think that's as long time A, B testing listeners. Well, no. Once you learn how to learn and develop that passion for learning, your ability to do anything is unbound. As long as you don't lose your drive to get shit done. Yeah, exactly. It can't just be theoretical research. Anyway, he is doing really well working at New Voice Media. I might have got the order wrong, where Rob Lampert, who writes a lot of great blog articles, works. And using Postman a lot, talked about that and his experiences there. But he gave a really good talk. He made me get my slides out and step up my game a little bit. Did he? Wow. And he just did the thing I usually do, which is mostly pictures on slides and tell stories. But for modern testing, I had to put a few more words on slides than I normally do. And I had to make sure I had kind of my, that I had to make sure I had my stories in order. So this was sort of the first real official presentation of the principles, which yes, right. I get why you would have to add additional words. Yeah, yeah. My notes on, I shared my slides with everyone. And my notes for that slide say, most words, if you look, I don't think I deleted this part, but it says most words on a slide since my stupid presentation at Star West 2003. I was learning. So as far as upside, Danny's presentation was about 10,000% better than my first ever presentation. OK. I started at the very low bar. I also really enjoyed Rosie Hamilton's talk on logic and testing. She worked for a long time in games at EA. And I forget where she is now, but I'll remember later. And she'll tweet at me and yell at me if she listens to the podcast. If she doesn't listen, she'll never know. Don't tell her. But it was a really cool progression using principles of logic and testing. And it was very insightful. I thought it was very well done. Sorry. I got into my answer. And I need to go back to your answer. Community was a big theme. My other workshop was on building community, a lot of head nodding, a lot of feedback. Community came up at a few other talks. Ministry of testing is a community. And I explained to them, and something we talked about on the podcast, I've explained to the audience that while my title is Director of Test or Director of Quality, or I don't know what it is, Director of something, I consider myself a community leader. My role is a community leader at Unity. And there are people on my team who are transitioning their people away from them. And the role they're considering for themselves is, we're calling it temporarily, role, not title, is QA community leader. But I realize that it's really just a quality coach. So we may push that direction. But anyway, we've talked about it before. Community came up a lot. So community, postman, oddly, very specific thing, came up a lot. I'll have to think about what maybe the third thing is. But I was surprised and pleasantly surprised how often community came up. Somewhere between now and the end of the podcast, I will go, aha, and tell you the third big theme that has gelled in my mind. Next time I go to a conference, I'll prepare and coagulate that ahead of time. So that's the longest I've ever talked to not answer a question at all. Man, maybe not. Maybe you answered 67% of the question. Exactly. What about takeaways? You said there were a lot of head nods, which is kind of what I would have expected at Test Bash. Again, I was talking to Marcus, one of the three, about this. And if I were to end this talk at a different conference, a conference that was geared more towards brand new testers, didn't care. So here's the difference. There were new testers at Test Bash, fairly new testers. But the difference is, I think, people go to a conference like Test Bash because they want to go to Test Bash and learn. There are some bigger conferences that people go to because their company sends them. I think the other thing too is with Test Bash is if you feel yourself a member of the community, going to Test Bash is also a communing thing. Yes, there is a communing aspect. Richard Bradshaw, who's the new boss boss, he asked me, so you were at Test Bash Philadelphia two years ago. How's the vibe? So largely the same. It's really cool. I like it that people come to commune. And most Test Bashes, they're doing five this year, but they need, although they're technically run by Ministry of Testing, as Rich explained it to me, that you have someone in the city of the venue to kind of be the point person, to do a lot of the work, to have stuff shipped to them, et cetera, et cetera. So that's kind of cool. I think, not in the short term, but I think you and I should think about whether there's a Test Bash Seattle at some time and how we get involved in that. I'm going to plant that seed. Yeah. I'm not against that. OK. Seattle would be a good fantastic venue for this. This passion for learning we talk about all the time. I think 99% of Test Bash attendees have that. Often some of the more mainstream larger conferences, they're good for getting a whole bunch of different kinds of information. But I think the people that get sent there maybe don't have that, I hate to use the buzz word, growth mindset. That passion for learning, that hunger for learning, that I see across the people I meet at Test Bash. Yeah. I think it makes a difference. The other thing too that I've been thinking about is I've been getting a large number of reports around essentially what we're saying is it's very attractive to younger testers. And I think number one, I think the idea of life hacking, growth hacking, growth mindset, all of these current buzz words resonates with that crowd. And I think there's another thing too is humans are wired with this, a good portion of humans are wired with this idea that if it ain't broke, don't fix it. And I think there's a lot of folks who have built a strong career off of the traditional testing methods and are kind of hiding behind that old theme. Hey, why is it broken now? And it's mostly because they're not paying attention to how the world is changing. Yes, agreed completely. I've gone through and I've reviewed your slides. There's a couple of things that I think are really important. Modern testing, your first slide, you talk about this is not a future of testing thing. Yes, because I want people to know, and they get it here. I'm going to interrupt for one second. Because that was really important. As I'm putting this together, I don't want people to think this is theoretical. I spend the first half the presentation kind of talking about why modern testing, before I talk about what modern testing is. But I've seen people, and I mentioned in the talk before, I've given future testing talks, not my best talks. Because they're speculative and they're not riding the wave of a trend. And I think what we're seeing in modern testing is it's something that it's actually happening. And the fact we get head nods at a conference like Test Bash, I would get, the point I was going to make about the other conferences, there are some conferences where I would get very few head nods and a lot of looks of disbelief and disgust. So yes, I'll let you finish now. No, I was going to piggyback off of that anyway. What modern testing is about is there is a clear change coming. And what modern testing is about is how do we help people stay in the driver's seat for their career as this change occurs versus skidding out and falling off a cliff someplace. You did a surfing metaphor. I went with the driving one. OK, whatever. It is absolutely clear in my mind that this shift is occurring. Gladwell, we haven't yet hit the worldwide tipping point yet. Not yet. But when it hits, this change occurs, will occur globally very rapidly. I think so. I also worry about it, tension time. I talked about in the presentation, we've talked on the podcast before, actually there's nothing I talked about in the presentation that we haven't talked about on the podcast before. So that's a lot of drinking. Companies often will like, oh look, you can make software with no testers. OK, we don't have any testers. Like whoa, you need to transition. You need to have a plan. You can't just do it. You can't just swing the pendulum. Yeah, I saw my tourniquet quote on Twitter. I did give you credit. I worry that some company with, OK, we're going to fire all of our testers except for three. You three are the test coaches for the org. Go. But, uh, ugh. Well, hopefully those three are part of the three, and they'll be prepped. They are. Yeah, maybe. One of the things, you know, I experienced both a team in the story I tell is I was on, in the short version, is I was on that stupid Android app, some Windows Phone team, where we just said, OK, no more testers. You're the quality and data team, whatever they called them in Windows. And I was like, whatever. And so I watched what happened there and how to survive, how to transition that team to doing better engineering. And then I joined teams where I was the only quality guy on an established dev team, and my role as quality coach there and what that was like. And from those experiences, then I joined Unity, had no idea what I was going to do. And to be clear, when I joined Unity, I didn't have an agenda. My agenda was to not get fired. I said, please don't discover that I'm a fraud. I have imposter syndrome very, very badly. Please don't discover I'm a fraud. I'm just going to try and figure stuff out and see if I can provide any value at all before I have to go look for another job. But what I discovered very quickly is, OK, this is, something's gelled. My hindsight, my retrospective of, oh, yeah, all that work I did for building communities, it's like, I'm a community leader. This actually helps. Some of the things Brent and I have been talking about with modern testing, these actually work really well here. And I actually like that I can do it in a very guided and purposeful, I can do a very guided and purposeful transition to many of the things we've been talking about. So I've been very, things have sort of appeared before my eyes. So I, you know what I mean? Like the agenda wasn't there, but the strategy became obvious. I'm trying to test in my mind, because there's this old phrase, if the only tool you know how to use is a hammer, then every problem looks like a nail. But I don't think that's the story you're doing here. I think it's more of a, hey, my strengths and my experience are a great fit here. I know exactly what to do to add business value and what differentiates me from the other. I guess that's not really what I'm saying. What I'm saying more is that I think the idea of doing a transition into modern testing and having some coaching through the whole process of don't do it all at once, it is a tourniquet. The transition to modern testing is a tourniquet. You have to treat stuff while you're removing it. You can't just rip it off like a mandate. For a team doing even agile testing, which is, as we've discussed, very close, we're definitely an evolution, a variation. I think you can't just turn the mandate off, go, okay, now you're doing this, it won't work. No. So that's the point I'm trying to make is I'm lucky that I'm in a position where I can do it gradually and purposeful and make it happen successfully. I think the biggest problems when people take the rip the band-aid approach, a lot of folks will take the rip. I will rip the band-aid every now and again. But the thing is that you have to do is that the leader who's ripping the band-aid has to be prepared to react in near real time. And just to be clear, for people who haven't seen the quote, and I apologize, the wonderful quote from Brent on this is, sometimes you have to rip off the band-aid, but be careful because sometimes it's a tourniquet. Yep. This was a quote that came to me from years ago. I did a presentation around how to transition into what Alan and I now call a unified engineering role. And if you go too fast on that, you're actually gonna cause more harm. Shifting back to manual testing and test bash. No, shifting back, we never got there yet. What have we been talking about for the last 30 minutes? Test bash, but not manual testing. Not manual testing, I meant modern testing. Yeah, interesting. The MT acronym, modern testing, ministry of test, manual test. I hope all three of you feel a little bit sorry for me right now. Okay, Brent, please go on. Every episode. Every. Oh, no, no, no. Not every episode. There was one episode. There was the one where I wasn't there. The one where you weren't there, right? Oh, and to be clear, I did, no surprises. I did not bring my recording equipment. There was no podcast recorded without you. So at least you have that. This is truly 79. Oh, fantastic. Where were we? I'm not expecting much here, but after, I assume people came and talked to you. First off, from the crowd, so it sounds like Vernon introduced you? Yep, Vernon was the MC for the day. Okay, yeah. And it sounds like modern testing may not have been a new concept to a lot of the folks. But not entirely. So one thing I missed, and then I do wanna dive into some of the questions we had, is one part of the conference I didn't mention is in addition to the unconference on the last day, during Friday conference, during the breaks, there was an unexpo. And when the unexpo is, it's a cool idea, is during each of the two or three breaks, there were a bunch of poster papers. Okay. And you stand by, people come and look inside your poster, they ask questions about it, et cetera. So in order to kickstart it, Richard asked each of the presenters if they would do one of these unexpo topics for the morning session to get people an idea of what was going on. This is on Friday. So on Thursday night. So it was like PR for the talk? Didn't have to be about the talks. Could be about anything. Okay. And Leila Lea, it goes by Swani, who was getting into testing from a communications background. A lot of, has that passion for learning, has everything you look for, just passionately looking for a job. She had a hire me paper. Oh. It was really cool. I have a feeling if she's not hired now, she'll be hired very soon. Okay. So I thought about a couple different things, but what I did was I just listed out the modern testing principles. Okay, now I understand what that picture was. That paper was, I stood by that. People walked by and they went, oh, uh-huh, uh-huh. They go, tell me about this one. We talked about, what does that mean? A lot of the same vetting we've done before. But I feel good about the vetting. There's a little more to go. There's a couple things that are like, don't sit quite right. But it sat pretty well. So that's where that came from. And then that was cool to have that precursor, because I don't get to the modern principles in the talk till the very end. So it's good to have a few people familiar with them before I gave the talk. It made a nice little flow for the whole week for me. Yeah, I was briefly thinking, so like the expo portion of conferences, generally love them, because that's when you walk around, go from booth to booth and get free stuff. So an unexpo, I guess, is where, and all the people come give the people at the booth stuff. Love this idea. Yeah, no. Just a way to get people talking about new ideas. Where do new ideas come from? Where do good ideas come from, Brent? From other ideas getting together. Yeah, new and good ideas come from the same place. Good. Other ideas getting together. Sure. Making sweet, new baby ideas. Before my talk, Vernon said, hey, so if you could finish it, that whatever time it was, leave time for about at least five minutes for questions, that'd be great. I can do that, I can do that. And then I thought of a few extra stories I wanted to tell during my talk, because the squirrel came in and told me I should tell the extra stories. I think this story will be relevant, I'm gonna tell it. And then I was wrapping up, and I didn't rush the end. One thing I hate watching talks is someone who will go, okay, we're running out of time, so I'm gonna skip this slide, I say screw you. So I have a pretty good idea of, pretty good sense of time on my talks and how much, where I wanna be. So I didn't go over time by any means, but I finished with about two minutes left to go, instead of the five minutes. So Vernon said we could take one question, and as you know, we probably could have taken two, but I'm incapable of answering a question quickly. So someone asked me, so does this mean if I don't write code, I don't have a career as a modern tester? And then I had an answer for that. I'm gonna ask you first. Yep. And then also over Twitter, I had this question. Alan, started listening to your A-B testing podcast just this past week with the two episodes on the modern tester. Welcome, welcome to the three. I found these ideas challenging for me, and they will be for most of the testers on my team who do not write test code, but do their best to add business value to the product we test. Wondering about your comments on teaching devs to write tests. In your experience, what does this look like, and what do devs not understand about testing? So there's two things I wanna cover there. So I wanna get to the devs thing in a second. Yep. But the first bit. So I'm gonna ask you first, and then I'll tell you what answer I gave, which is what is the role? So I'm a very good tester. I'm the kind of tester that goes to test bash. And, but I don't write test code, whatever that is. I don't write code. What's my future as a modern tester? I would say my answer to that question is it's grim. I'm going to put some more context in you and un-grimify it for you. Well, so let me just, so first off, my view, I guess I could turn about and ask you a different question. And that is, is there such a thing as a modern tester? That is actually a great question. I know. Because you can be, you're an engineer that follows modern testing principles. Right. But your role isn't a modern tester. There really can't be such a thing as a modern tester at the same time as a modern tester being a specializing generalist. I get that. So I think a modern tester, if it does exist, is a transitional phase. Right. If you think about what we're saying in the principles is that we're saying modern or testing over time disappears as a specialization. And I do think that manual test jobs. So don't look at it as manual. Go back, let me just throw a little bit of context. You may get an aha. Go back to my presentation on becoming a technical tester. Yep. Which was none of it was about being able to write code. I think, I think that you can be a valuable member of the team and apply each of the modern testing principles, you don't have to be a coder. And I will teach people at STAR that you don't have to be a coder to do pretty good API testing for REST APIs. If you're willing to put in the effort to learn how tools work. Using, learning how to use Docker to set up environments to accelerate how fast the team can do work. I don't disagree with these aspects. I don't disagree with these aspects. I don't necessarily think you end up being a coder the way the traditional test dev arguments have always landed. But I do think you become a much more hands on engineer in this world. Like manual testing in the world? Yeah, do I think that goes away? Yeah, I do. Because the customer does the majority of the manual test. The majority, the majority. But the point I'll bring up, the point I'll stand behind is, well first of all say, there's a large chunk of manual testers who the feature is grim. Yes. Absolutely. There are a set of testers who do not write code that can be very successful in a organization following modern testing principles. I fully believe that. I don't disagree with that. I think they can accelerate the achievable quality by finding the right tools, by being a force for continuous improvement. You can push, if you look at those principles, none of them are about writing some kick ass tools. Certainly it's a means to an end for several of those, but there are other things you can do to drive quality. Yeah, again, the challenge though I think with this is maintaining your role as a specializing generalist. Yeah, and I also think if you pursue that technical tester and maybe, okay I'm gonna be a technical tester, but I'm not gonna write code, because I don't like that. That's fine. I think you will actually stumble across some coding, like you'll like, oh, a SQL query here to get at the data. Not that hard to do, you may go, this is actually kind of cool. I might do a little bit more of this. You may write a little bit of JavaScript to automate a task or a shell script to automate some Docker setup. My hunch is that if you are striving to be technical and via the tool route for your team, that you will stumble across a little bit of scripting and eventually you'll do a little bit of what you could consider coding, but you would never have called coding today. Yeah, and I think a good portion of, I think in the long term when the transition finally closes globally, I think actually we'll find a lot of those people who are still around have shifted into probably a job closer to what we describe as a PM role today. Could be. It's more of a customer focus. It's more of a, I'll give you some advice I gave PMs as an example when we were going through this transition because one of the things that ended up happening when Microsoft was going through the unified engineering transition, TESS was all aboard of sweet. We're no longer part of the stupid safety net. TESS realized very quickly that actually this move helped move quality upstream because it forced Dev to do their own testing. On paper. On paper, right. But what ended up happening is they shifted at the same time to PMs acting as the POs and doing the acceptance testing, acceptance testing from an agile point of view, not an old traditional point of view of the new feature. And I had to stop a bunch of PMs from actually doing the actual testing of the feature. I'm like, no. When you do acceptance tests, you're interviewing the developer, you're not even loading the feature up. You're going, okay, this was my requirement. How did you cover, explain to me how you satisfied this requirement. What tests did you do? And the PMs that listened to me, they actually helped to reinforce the behavior. The PMs that didn't started recreating the safety net that Dev was accustomed to. No safety nets. That can be one of our mantras. We need like some one-card, one-liners, like no safety nets. Yeah. I like that. The other half of the question I want to answer, because it's a quick answer, wondering about your comments on teaching Devs to write tests. In your experience, what does this look like and what do Devs not understand about testing? Actually, there's nothing. They don't understand. Well, there's lots they don't understand about testing until they try it. And just like when we first started testing, we tried things and we learned from them and we tried new things. You can accelerate that trying by coaching, by pairing with them is the best example, giving them new ideas. I talked a lot about how I taught a team to test by just doing code reviews all the time, how to write automated tests. There are better ways than that, but that's one way that works. In general, test every developer, I have never had a challenge like, here's a testing concept that developers just can't learn. Never, ever, ever had that. No. The only challenge that I've had around teaching Devs to test, walking through the tool, walking through learnings we've had around data-driven testing or things like that, these are all algorithmic concepts. They get those pretty quickly. The only challenge that I've ever encountered is addressing the, okay, now, why am I doing this versus you or those other guys? Why am I wasting my valuable coding time doing testing? That's it. I haven't had that. It's a cultural... Oh, let me give you one challenge. One of my teams I work with are very good at writing code or delivering a feature where the feature is, okay, it's supposed to do this, it does this, and it does that very well. But they're getting better, but there was a point where it was always like, well, what if we do something slightly differently every time it would break? That probably is one of the more common learning curves I've had to get some developers over is just thinking a little bit about what are the ways in which this may not work and planning for those, that defensive programming. Yep. Coachable. They get better at it. Completely agree. One of the models that I do, so in retrospective, actually, so my engineering model, which is a variant of Kanban, my engineering model helps us solve this as well. So number one, we don't maintain a bug database, and every time there's a bug that's shipped in the wild, that's the thing that interrupts the whole team. And the team gets kind of irritated when it's very clear that we're constantly being interrupted because Alan keeps shipping bugs. I suck. Shouldn't let me write code. No, and by making it, the team pays the consequence for the weakness of one member, creates sort of a social pressure on that one member to stop doing it because they get tired of having to explain to the team yet again why they're having to come and save the day. I'm pretty sure it's your fault. The second one that I found that works rather well is in retrospectives, I don't hold the code owner responsible. I hold the code reviewers responsible. And that sort of helps to support, we talked about spider and starfish, that kind of helps to support the starfish principle. Interestingly and coincidentally enough is I send a part of my community leader, I send a weekly newsletter to my org, my community, I should say, because many of them don't report to me anymore. And yesterday the article is about retrospectives and how damn valuable they are and how more teams should be doing them. And a bunch of some tips and things, again, I plant seeds, some will stick, some won't. That's true. All right, shall we get to this? Can I get one more question for you? Yeah, yeah. Actually one thing though, the other thing that we've talked about, no, hold on. Because one of the things is what does it look like to teach devs how to test? The other thing too, so we've talked about a little bit, there's some set of testers in the traditional mindset that we basically are warning, yeah, you should probably make a shift because the thing that you're used to is not gonna be as valued and significantly not as valued. And we talk about how testing can be technical without coding, but let's also talk about, there are technical testers in the world who can code and their role, in my view, does not, is also not protected. Those folks are writing new tools or writing new automation and I actually, from my observation, what I found is that those guys become the devs of the future. Those guys shift over to starting doing product development with a strong, solid point of view of how it needs to be test and these guys rock. Yeah, and I think, and one point to make there is there isn't a single, if you're a tester today, this is what you do in the future. No. And that comes back to modern testing principles, our principles that an engineering team follows. We should look at them again in that view to make sure that's clear. It isn't about how you become a modern tester. It's about how a team uses modern testing principles to accelerate the achievable quality and to get value to their customers frequently. Yeah, again, if there is such a thing as a modern tester, that is a transitional role only because eventually it merges into, now this is a set of principles and there is a modern engineer. So the way I'm describing it in, and this rang pretty well as well, is I drew a diagram of sort of the Agile team which has the test specialist on the team. I said, really, if you look at it, everyone is a software generalist with specialization. And some of them may be, you want someone on your team generally, maybe if everybody's good at performance, you don't need a performance expert. Same thing if everybody's pretty good at testing, you don't need a testing expert, but you're gonna have some expertise on the team. But really, Brent's making a face. You saw the slides. I did. I'm going. In that area of specialties, a team following modern testing principles, I'm gonna rephrase a question that we saw in the Slack, one of the three.slack.com. If you wanna join, you can send me a DM to at Alan Page with your email address. So in a team that's following modern testing principles, who will perform tests such as security, performance, and all non-functional tests? In a traditional testing company, it's the QA team is in charge of it. But when you have that homogenous engineering team, unified engineering, team following modern testing principles, whatever you wanna call it, I know the answer to this, but where does that testing happen? Well, I'm curious as to what your answer is because to me it's very clear. Yeah, it's absolutely, just like you are a testing specialist, you have performance specialists. The team does it. The team does performance testing. The team does security testing. I actually, you can outsource it? I think I disagree with that, yeah. So the NFR tests. So number one, I think, I agree with what you're saying in terms of within a unified engineering team, it may start off with a testing specialist that's in-house. But remember, their job is to teach the others how to succeed. It's if their job isn't the big portion of coaching, their job is to make that testing specialization not as special. So over time as they succeed, the other members pick up that skillset. Yes. Now- I'm in agreement so far, keep going. On the performance, security, scalability, any of the illities, right? Those are deep topics. And I do think deep topics, warrant a specialist. There are certainly breathwise things that should be introduced. But one of the things that we also, and I experience this with teams that tried to go too far on the, in my view, the unified engineering model. What you found is that a single developer had to know too much in order to just do their regular day job. They had to have done the deep security testing, the deep performance testing, to make sure that they're, and it was just too much to move forward. So it stalled the system. I do think some combination, and I wouldn't be surprised if the, if the quote unquote QA team still persisted, but they only owned the illities. I would say, so there's a, I get where you're coming from, and I would modify what you said to say, rather than saying it needs specialization, it may need specialization. And it's gonna be context sensitive. What comes to mind a little bit, there's a couple, two models that come to mind. One is the Spotify squads and tribes and guilds model, where you could have specialty that spans orgs in a way that covers those things. But also if you look at the quality coach rule, so I will soon have people on my team who are responsible for, they will be the QA community leader or quality coach for maybe 75 developers. You could, I could actually see a world where maybe they have a few direct reports who have those specializations. Maybe someone in performance or security, or security is actually, we have a separate org worried about that. Performance, reliability, who could work deeply with those teams around that, be their reliability engineering coach, if they didn't have that specialty. So I think what I'll say now is there isn't a one model fits all. I think in many teams, that specialization can come from the team. I think it depends on the scale of what you're delivering. I think at the Microsoft scale, I think you probably need that specialization. But in a smaller, depending on what you're delivering, you may not need a specialist to help with that. You can get good enough performance, via the performance skills of the engineers on the team and those that have some sort of specialty in performance. I agree with performance. Reliability, you can go up to a certain point, right? At some point in time though, the reliability problems are due to the architecture underlying the solution. To solve that, I would traditionally put that on the architect shoulders. What I'm looking at is there is... Security. Let me finish. So basically, I agree with you. I think given the business impact of security and privacy, specialists for those, and I think most companies of reasonable size have special security and privacy, that's warranted. I think on the reliability side, there is a community building around chaos engineering, where the Simeon Army and the Chaos Monkeys, et cetera, came from Netflix. It's beyond Netflix now, this idea of chaos engineering, but that can be a specialist. You may not need a peer specialist for that. You need an interest in that. You need a guild, whichever the Spotify model is, you need a community around chaos engineering, for example, to build reliability. Yeah, although my point of view today is that a chaos engineering team is just another engineering team. Sure. A unified engineering team. Yeah, I'm calling it community, but yeah. Okay, that's the questions. Do we cover, probably not. Probably didn't cover everything, but I'm afraid our time has come to an end, Brent. Do you have any final words? Of course you did, go ahead. No, next episode is 80. I like whole clean numbers, 80. You're so frickin' weird. I'm still Alan. And I'm Brent. Talk to you next time on A-B Testing. 
