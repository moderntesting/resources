for any testers listening. People who are change agents are always going to have a job, right? But they might just have to be open to what they do exactly. Welcome to A.B. Testing Podcast, your modern testing podcast. Your hosts, Alan and Brent, will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now, on with the show. Hey, everybody. Howdy. Guess what? It's not just me and Brent again. Someone came with us. It's me. Hey, me. Welcome to the podcast. At least I pre-warned my guests. Oh, my God. Oh, that I'm obnoxious? Is that what you're pre-warned about? What are you pre-warned? I don't even want to know what's going on here. Yeah, that's all right. Go for it. I can tell you what's really funny is because I've realized that I start every podcast saying, Hey, everyone. And then, like, we're coming back from break and I have a meeting with my whole team. I go, Hey, everyone. And there are a few of the three in my org. And they go, that sounds familiar. I go, I got to change my opening line. Oh, Joe, amazing me. I just got through half of the introduction to your latest webinar, which I'm sure we'll plug shortly. Oh, yeah. And the host said literally exactly the same thing. Yeah, Joel, you know, he just said he says, Alan, say hi. And then awkward silence. And then he says again, Alan, say hi. And then Alan comes in and goes, Oh, hello, everyone. And then Joel goes off and says, that sounds awfully like your podcast. This happens a lot. So you actually observed my postmodern talk. I am 30 seconds into it. Okay. So let me, this is segueable into why we have this mystery man, Henry, on our podcast. And Henry, I'll let Henry introduce, Henry, Henry, you're new. Hey, just like Burton, Henry in the talk. So postmodern, of course, there's no modern testing isn't modern. It's not about testing. So there's no comparison to be made between modern and postmodern testing, as there is with modern and postmodern art. It's not a rejection of the principles that took us to modern art to become something even more different for human entertainment. Modern testing, of course, was simply our reaction or our way to describe the thing that wasn't the traditional testing, the air quote, traditional, the test last, big test teams, testing quality in testers, owning all of the testing, that kind of stuff. In this talk, I wanted to talk about like the fact that Brent and I did not invent modern testing. We just talked about things we were seeing that were already going on. And what's happened in the years since we began talk about modern testing. So, you know, I remember, I don't remember exactly the date, but many, many years ago, I started talking about the roles that I had had on quality coach. And it was sort of a, you know, almost a placeholder role of like, what do I call it when I'm helping a team or somebody on my team is helping a team do better testing, but doing very little to no testing themselves. And what's kind of blown me away is this rise of the quality coach and all of the people who have quality coach in their profiles on LinkedIn and Twitter and all the people, you know, doing quality coaching, you know, Ann Marie Chard has a blog about it, Kim Engel, both which probably have on the show someday. But we put out a call a couple episodes ago. Hey, if you're a quality coach, you should come be on the, because I don't do it anymore. And Brent's never done it. Come beyond the podcast and tell us more about what quality coaches do and can do. And Henry picked up the phone, literally a landline he's had since 1943 and plugged it into the wall. No, he, he pinged me on Slack, like a, like a human being real, real, you know, for the 21st century, people don't make phone calls anymore. But anyway, uh, Henry's going to talk about with us about quality coaching today, including a talk he's given recently on project he's done at Microsoft. And, uh, I'm just going to shut up and let Henry introduce himself, give a little context and talk a little bit about test. I don't know, Henry talk for a while. I will. Thanks. First off, welcome, Henry. Welcome. I'm happy to have you here. Thank you. Yeah. Thanks. Yeah. So, um, so my name is Henry Golding and, uh, I did indeed answer that call, although I don't call myself a quality coach, but I think what I do aligns with what you were talking about. I know this is, I know there are no tangents on this show, so I'll try and keep it on topic. Um, but, uh, so yeah, I consider myself a developer. I've never considered myself a tester, although I do listen to your podcast. Um, and, uh, I, I, I'm a game developer, a game play developer specifically. I started out, um, and I was working at a company, um, and it was clear that that company was about to go bankrupt and I needed a new job quite fast. And I thought, well, here's this Estet thing that I haven't heard of. Um, that's a six month contract at rare, which is a Microsoft subsidiary game studio in the UK. So I thought, well, how bad could that be? I'll do that for six months. Um, and so I did it and I found that it was actually, um, quite for me, quite a nice combination of, um, helping an audience that I care about, ie developers, uh, do better at the work they were doing. Um, and I ended up doing it for quite a while. Um, and I can, I can talk more about, um, the journey that we went on there, which is kind of interesting cause we, you know, we went from, like, I was hired to integrate a like over the network automation driver system and write tests in C sharp for things. Um, and I fairly quickly decided that that was like a dumb thing to do. Um, and was trying to, well, I fairly quickly decided it was a dumb thing to do to write tests for other people's code. Um, so I was trying to kind of figure out how can we help the developers like write tests. Um, and the talk that I alluded to is a talk that I did at GDC, uh, the game developers conference, um, last year now, cause it's 2022, um, in which I talk in a lot more detail about how all that happened. Um, but in the talk, I talk about a mistake that I made, when I tried to, um, like I wrote a really nice automation framework that was ever so decoupled from the game and it talked to the game over the network and I hooked it up with spec flow on the front end. So you could write like Gherkin style tests. And I was like, this is great. Everyone will write tests. And no one did, uh, except one manual tester who it turned out was actually a programmer. And so we hired him as a programmer. Um, so then we had another big project come up, uh, see if thieves, uh, you may have heard of it and it was very important to us that we were able to release it on a regular basis. So we were aiming for continuous delivery. Um, and we, we had a goal which we met of shipping it every week. Um, so we started out shipping a black screen, um, to the Xbox preview program, um, and then kind of went from there. Um, and so on that I, I was at this point running the team that was like helping out with the sort of test framework side of things. Uh, and we, we basically worked with the team to put a test framework that was embedded in the game. Long story short, I learned some lessons, wrote framework that worked for developers. They didn't have to come out of their environment to write tests. They just wrote tests in the environment they were in. Actually, I was listening to your episode where you talk about, um, test cafe that kind of struck struck me that there were some similarities in there, in terms of embedding the test code into the product, um, for ease of authoring. Um, and then yeah, that's, that's probably a long ramble. You guys may have thoughts already. I, I, I do have one thing. I'm sure Brent wants to jump on this too, but I jumped in first. So this idea of shipping, you know, shipping every week and the first thing you ship is a black screen. I am so frustrated by how bastardized the term MVP has been used as I love that. It's fantastic. It's, it gives you value. It lets you learn something from your customers. And Brent, I know loves this as well. So I'm stealing all of his thunder. No, no, yeah, we also have a certification process to go through. So one of the, one of the goals of shipping every week was that like no one else was doing that. And so we were like, well, we better start doing it. So they get used to like receiving so many certification requests from us. And then they eventually decided that they give us an exception. Actually, yeah. Uh, right. You keep hitting the wall until the wall breaks and the river flows again, right? Um, yeah, there's, there's a lot of things on, on that. So did you struggle to get certified to ship a black screen? Right. I'm looking at, see what it wasn't to paying customers, you know, it was just to the preview program. But still, but still that's, that's the example. People try and try and put too much into what they think needs to be their first release. And what you want to do is, is ship, deliver customer value often. And when you start with a black screen, it doesn't crash at least you have nothing to do, but improve on that. But you're actually practicing the art of getting that through all the hoops and ladders that needs to go through to get done. So you've, and I imagine that got easier and easier until the certification folks said this, you know, until they even made it even easier. Yeah. But as a studio, we were very into continuous delivery and that mantra of if it's painful, do it more often. So that kind of drove a lot of thinking. So what happened the first time you shipped something through that mantra and you clearly shipped a bug? You know, I'm just, I'm going to be honest to say, I don't remember. It's quite a while ago. Yeah. And then actually my focus on that project was more on the, like, how do we, well, see, you, you may or may not know that the games industry tends to lag behind the rest of the software industry in terms of adoption of technical practices, but very bleeding edge in some respects, but kind of culturally as a bit of a lag. And so, you know, it's very rare to find a gameplay programmer who has written tests or knows how to write tests or, you know, doesn't expect a manual person, a manual processor to test their work for them. So that was kind of like the main thrust of my work on that project. And we kind of ended up with this formula, which I've since applied on, on Minecraft, where I work now, which is kind of, I guess, you know, I assume that people want to do good quality work. You know, no one gets out of bed in the morning saying, can't wait to write bugs and crash everything. You know, and, but so there's some reason about the system that they're in that is inhibiting that. So by giving them tools that fit in their workflow and are easy to use frictionless, as well as a bunch of education, you know, lots of training, coaching, pairing, and mod programming, actually. Well, I didn't know about mod programming at the time. It was more recently adopted. And then kind of the support, kind of the organizational support to spend time doing that stuff. So that's kind of like the sort of three prong attack that has seen some success. Yeah, I think the thing, one thing from my past to underscore there is just make it so easy to use that it's, it's just dumb not to. And I've done that before with, hey, Frank, do you remember the test harness on Windows CE called Tux? Yes. So I ended up, as I always end up owning, no matter what company or team I go to, I end up owning all the tools. And so Tux was on my team, but we just wrote, in fact, I think I wrote it. Maybe someone fixed it for me because if I did, it would have sucked. But a little wrappers, like I can't remember what the command line was, but it would create the entire skeleton, you know, based on the code that was in that and put it in the right directory and basically do everything, but actually code the test for you. So all the, all that stuff was taken care of that made it easier for people to write tests. And then I'm thinking analysis tools. People would be too lazy. They wouldn't run them locally and they couldn't. So we just make it not only make it easy to run, but make them part of the build. Same thing like getting code coverage data, just make it so easy to use. They'd be much more likely to do it on their own desktop. Reducing friction certainly is an important aspect to like here. That's the right word. I am thinking, I'm thinking back to, to my Perseus days. Perseus is a fantastic little test harness last probably maintained in, I mean, maybe recently, I don't know, but invented by someone we all know, the man named Brad Jensen. Yeah. Somewhere. Oh, you can't see it right there is the patent for it right there. Oh, by the way, I'm speaking of paths, by the way, I was going to ask based on what you're doing. Are you doing any, are you doing any remote, remote testing of performance graphics in your testing? Is that for me or Brad? I think that's either one of you. I will confidently say no. Okay. Because if you are, I have a patent on that shit. Nice. Which is me highlighting the uselessness of patents. But anyway, go on to a more meaningful conversation. Yeah. No, I was thinking through. So reducing the friction is, is absolutely important, but it's, you, you, you need to, to make it easy for them to construct it. You need to make it easy for them to run it. You need to make it easy for them to get the results back. That's all important, but insufficient to get them to use it from my experience. Right. And I think, so my story with Perseus, which I've blogged about it led a different direction. It's probably because I was 20 years away from even hearing the term quality culture, let alone implementing it. But in my story, it increased intellectual laziness. It just became a place where testers added more and more and more tests and, and devs actually stopped testing because they're like, Oh yeah, the whole Perseus infrastructure is fantastic. I can just barf code, then submit it to the system, go get a coffee, come back and then do stuff. In other words, you admitting you built a big fat, ugly monster. I did. And, and yeah, well, I wrote that blog. You probably read it years ago. It's problematic. However, what I'm wondering, cause the thing I did learn when I went to the Bing team that fixed it is number one, CICD is fantastic. You get things fast out to the public so that you can get the feedback. Of course, if you're not wiring up feedback, I just have to, you know, head slap and ask WTF, right? The, so reduce small batches, get it out fast and have a good system where you can get the feedback. Number two, get rid of the safety net. Cause getting rid of the safety net then puts the, puts sufficient accountability while reducing friction in the risk of, cause the other thing is developers are scared, right? They, they, I've learned that the main reason why they get tensed and blame testers as, you know, as we've talked on the podcast before, like in the traditional world, the testers, number one service they provide is scapegoating. The, the reason why devs get all hot to trot on that is because they're afraid. They're absolutely afraid, but if you do smaller batches and get it out there, the risk goes down way, way lower. So I'm interested, Henry, tying back to you. Are you finding agreement on that front? Like are there still, so your title is architect and previously software engineer. Have you found a way to sort of shift that accountability or did you find shifting the accountability even important? Do you find developers, did you, did you come up with a magic equation that, that made developers love writing test cases? It's a good question. It's a good question. I mean, one thing that was on my mind is when we were talking a second ago is, so when I was working at rare, we had a tech director there who was, who was very behind the program. And he had this mantra of it should be easier to write a test than not to write a test. And I was like, Oh, this is great. You know, I sure. Okay. But like surely always it will be not easier not to write the test, right? Cause you don't have to do it. It wasn't until like a few years later after we had kind of done it, that I realized that that is a, it is a real thing. Right? Like when, when the culture has changed such that like, if you don't write a test, then you're going to introduce bugs and break the bill and the lights are going to go red and the lungs will go off and people will be like, Hey, why'd you break the bill? And you'd be like, Oh, I'm embarrassed. You know, but there's like cultural pressure and it's actually easier to sort of like follow what everyone else does. But getting there is, is a real challenge, of course. So like on, on CFTs, we, we were starting from scratch. So we had, we had considerable advantages in being able to like, you know, build the team out of people who were on board and that kind of thing. For Minecraft is a much bigger team and we didn't have that advantage. It's already, you know, a decade old. It's among the list. And, you know, we had a lot of quality problems as you would expect, right? Like it was pretty much impossible to change anything without breaking something. And we have a lot of backwards, like back impact issues and partner content that we mustn't break. So there's considerable challenges and considerable pain for, I don't know if I'll say developers, but considerable pain for the organization in shipping bugs to players and heavy reliance on manual tests, as is common in our industry. As to exactly how it happened, I, I'm not entirely sure. We applied the same kind of formula. Well, I guess some, some brief history about how I ended up there. So one way that Alan and I know each other is that Alan hired me onto the team's team. And that's how I left before he even started one day. It was gone by the time I got there. He knew I was coming as that's why he had to clear that out. Yeah, I ran away. Sorry. Yeah, definitely. Which is, which is fair and reasonable. I understand. I know. Anyway, so I ended up on the team's team because I was like, you know, I had done this and I kind of, my approach is to try and work myself out of a job, right? I think if you're a quality coach, your goal should be not to have a job because like if someone, if you, why would a team need a quality coach on a continuous basis, right? That would mean you're not doing your job. Yeah. And I, and just to interrupt, I think, and that's something I've done a lot in my career. I've done it at Unity twice where I've worked myself out of a job. Unfortunately, a lot of people in the industry hold on to their traditional test role because it ensures they have a job if they do it right, because they can put themselves in a position to be that dependency. But as I think through it, I think all the quality cultures, I'm sure there are exceptions, but all of the quality coaches that I can think of work as consultants. They do jump into a team, do the work, and then work themselves out of a job and move on. But just wanted to add that. But anyway, go on, Henry. Yeah. And so that had happened on CFPs. I was like, well, you know, the culture is there, the processes are there. They don't really need me. Like the developers are earning this process, right? So I think like you're, as a quality coach, you're successful if, you know, you're not needed and the team is now earning that process. And if you go away, then they'll keep that process because they see value in it. They understand it. And so I joined the team as a programmer, back as a gameplay programmer. And then I didn't enjoy the lack of autonomy. I got used to the autonomy that came from kind of running a team and trying to have a goal to solve. And I didn't really appreciate going back to like being told, develop this feature, monkey. So then I was like, you know, I know I'll go to Alan's team and learn how the real pros do it. Through twists and turns, I ended up working on the thing that didn't really work for me. I ended up kind of working on like open source compliance and build stuff. I was like, what am I doing? I knew people on the Minecraft team. I joined the Minecraft team as a gameplay developer. And while I as a gameplay developer, I was trying to write tests and they had a test framework. And I was like, okay, you know, no one is more highly motivated than me to write tests for my work here. Right. But I couldn't make it work. The friction was too high. There were terrible systems that when they had like created these systems on the build and infrastructure side that treated tests as hostile entities and quarantined them and put them through a gauntlet. And I wrote a test and I didn't get feedback for two weeks that actually had never passed on iOS. And there was no way for me to find that out before I submitted it. So I was like, well, this obviously isn't working. And yet, like you say, they were kind of like, we have quality problems. We want tests. We want developers to write tests. But they were like, we just don't understand why developers aren't writing tests. They have the frameworks. So I was like, well, hey, you have the frameworks, but they don't work. Will you let me take a look at that? And they said yes. And so then I started a team that did the same thing that I did at REN. Because that's kind of the long roundabout way of how I ended up doing that work there to try and answer Brent's question about how do you get the developers on board. And then I don't know, I guess I feel like you have to be very pragmatic about it. Like I've seen, you know, colleagues come in and try and make the same kind of cultural change. And sometimes they'll come in and they'll be like, you know, I have this idea of how you're going to do it. You know, like you're going to write C sharp tests. Oh, and you know, oh, you won't write those. I'm going to hire a set to write them for you. And that was a giant failure. You know, or like, you know what, like high level tests, total waste of time. Only unit tests are going to work for you. What's that? Your architecture doesn't support them. Well, you should change your architecture entirely. So like, I mean, you know, then they'll spend like years fighting with the senior engineering leadership kind of trying to say, you know, as a prerequisite to writing any tests, you have to completely rearchitecture your system, which of course doesn't go down very well. And so but that's kind of common, I think, especially with people like from outside the industry coming in anyway. So I guess what I'm getting to is that I feel like the answer is maybe just having empathy for the developer, kind of like I was saying, right, like they want to do a good job, you know, but there's no one who's like, sitting there trying to break stuff. Right. And I'll quote from my GDC talk, I think I said, in the context of developers, the ones who need to write the tests, but when given the right tool and training, we're actually really good at it. Yes. And that is wait, wait, no, no, no, no. I mean, that's heresy. Because so so Alan will best teach you this, like there's this thing called the developer mindset. And now now Brent's using the sarcasm thing. How cool. Because there are people in the industry that aren't on this call that believe that developers are not capable of doing testing because they lack the mindset to do it. And I have I have course have learned as Henry has, and maybe Brian has that that's a bunch of B.S. I have. I've run a dev team without testers. Good for you. Yeah, it's yeah, they can write tests, but often it's just it's often permission or they've been a lot of developers have been told for so long that they that they're unable to write tests because they have the wrong mindset that often all you have to do is tell them it's okay. And they may not be perfect at first, you'll get better, which, you know, hopefully they figure that on their own. But in the coaching aspect, a lot of it is empathy. A lot of coaching is empathy. And while you were talking, I'm thinking of all the people I've interacted with throughout the years, many of them at Microsoft who were frustrated because they had a great idea to do something. And just like you described, I said, I wrote a framework for you, why don't you use it? Something's wrong with you. You you obviously don't care about quality. No, I don't know how to use it. It's not easy. It's not intuitive. I don't have the coaching. I need to do it. Please the build it and they will come mentality, I think is especially prevalent from testers writing tools. I think it looks like standard agility applies as well. Right? Like so you when you when you roll out a test framework, then you want to put something out there and you want to see what happens who uses it, why they're not using it, talk to them, try something else and just iterate until you get to the point where it works for them. And I showed this in my talk, but, you know, we kind of tracked basically for reasons coverage is hard to collect for us. Not that I would recommend coverage as a metric, obviously, but I like to think it's informative. Coverage is a wonderful tool and a horrible metric. Indeed, indeed. And a wonderful tool for people who are trying to like, get a development team to adopt testing. And so, but I just fell back to like number of tests. We actually we introduced a framework that was I feel like there's probably a proper name for it, like a proper testing name for it. I doubt that we've invented it. But it's basically like imagine if your architecture was such that you couldn't write unit tests without extensive changes, but you wanted to write unit tests. So you kind of, you know, you spin up the game and all the dependencies, and then you like take the little piece that you care about and you kind of like write what looks like a unit test, even though there's all sorts of stuff going on in the background. So that I've been calling it like a middle ground test or whatever. But that was the bridge they needed, because it gave them something that worked in their workflow. It's like, okay, now I don't have to wait for the specific example would be I want to test a component. But in order to instantiate a component, I need an actor. To instantiate an actor, I need a level. To instantiate a level, I need the whole game. And so it's like this incredible this chain of dependencies is very hard to break. So what we do is we spin up a level. And we say, okay, you're going to write a single one shot function that looks like a unit test. You can spawn an actor in it if you want. But you're going to like take the component and poke a couple of functions on it. And just that kind of unblocked then that sort of latent desire to do unit like testing. And actually what we now see is that our actual unit tests are starting to grow at a faster rate than those tests. So initially, there wasn't much adoption. And then we suddenly get this uptick, which was like we've been, you know, I think it's about sort of two or three months or so where we were kind of iterating very closely with the team figuring out like what would work for them. And then there's this moment where they suddenly have what they need. And then it just explodes. And then that kind of gets them addicted to testing, test-insected. And then they start realizing, oh, you know what? If I did a little bit of refactoring, I could make this an actual unit test. And so then they start writing, they start doing that and they're writing unit tests. Lastly, these make theory of what we see. I feel like I'm kind of off topic. I'm not sure. No, no, there is no such thing as off topic on the A B testing podcast. Don't be Yeah, I'm currently trying to find a name for your thing. I'm thinking, I'm thinking pseudo mock maybe. Yeah. Ultimately, I don't really care what the name is, you know, like, as long as we can, you know, use it as a tool to move people in the right direction. Hanks, Hanks special environment. Oh, you know, it's, it's, it's, of course, Brent's special sauce. Uh, yeah, let's move it on to the next question. Let's just do with that. Winding back. So what you did is you, you treated test culture shift. And I, here's something that I think is very interesting, right? As, as you were talking, every time you need, need to make a major change, I've come to learn that you need to look at three separate things, the people, the process and the technology. Far too often people just think of it as a technology solution. We just need the right tool and then we're done. Right. And, and Hey, I built the tool, but why is no one using it? It's because you're requiring people to use it and you haven't invented the correct friction free process to make it easy for them to do it. Now, what I, what I hear from Henry's story is what they did is they viewed quality coaching or, or the change of the culture as a product. And they, they sort of a process and BP, right? They viewed it as a product and they viewed it as a sign that there was an error in either their design, um, or in their process, if they weren't getting adopted. I think that's fantastic. I, I, I don't know that I've ever mentioned it. I may just be projecting in, in this conversation, but whenever I roll out something new, I try, what I'm looking for is how do I get people to love this? Not how do I get people to use it? How do I get people to love it? And if I only get 50% the way there, guess what? I'm getting people to use it. Well, I used to say it rare, but, it's like 98% of people problem, uh, this kind of thing. And then I realized pretty recently that I was actually wrong. It's actually a hundred percent of your problem. That's a, that's a Weinberg quote, um, you know, paraphrased in, in software, eventually it's always a people problem. And as a tangent, just because something I've been sharing with my team recently is, you know, I think one thing we've done pretty poorly during the pandemic, we as an industry is learning how to work asynchronously and have, and get work done without the need for a meeting to discuss stuff. But then I also realized that I'm noticing this a lot in managers and leads in my organization who often have to solve people problems and not technology problems. So I think there's a people problem and those people problems as an aside, I have, I am noticing have been getting tougher and tougher over the last almost two years now. Stupid pandemic. Yeah. I mean, I'll, I'll, this is, this is a tangent, but, um, that's what we're famous for. Well, indeed. Yeah. I'm so glad to be here and contribute to the, the proud tradition of tangents on the show. Uh, but yet more programming is something that we have found super effective, um, for all of those pandemic woes and it's not asynchronous. It's very synchronous. Um, but you can do it remotely quite effectively. Um, and I, yeah, I'm, I'm encouraging it as much as I can, because I think especially for early in career folks, it's just really hard in the, in the work from home environment, because you don't get that coaching that you would get if you were sitting next to someone, you know, you could just turn around and ask. Um, whereas when you, when you marble ensemble on a problem together, you know, that all comes in and it's, it's supercharged. So, you know, like we started doing it. We just started experimenting, uh, with an hour here and there. Um, and we're at the point where my team is, is it has decided that they are going to, um, I think like one, one week a month, they, they do an investments week, um, where they mob as a whole team on something that helps us, you know, get better. So like the last thing they did was they put a piece of the test framework under test under, under unit tests, which I was also going to say another big mistake. I think a lot of, um, people who are in that kind of quality coach or test framework writing role make is not testing their work. Right. So like every single framework that I've seen for the most part, you know, I'll be like, oh, where are the tests? And they're like, oh, well, we don't have tests. Where are the test framework? I'm like, what are you doing? You know, like it's so when I, when I took that S that role, I was like, Oh, I guess I didn't know anything about testing. Actually that's how I came across the podcast. Cause I was like, well, I guess I better read this book. How they, how they test software at Microsoft. Cause that's relevant. Uh, that's time you'll never get back. Well, that's a really good book. That's how I learned how to be an S that and write good bug reports. It actually was what I was hired for. Right. Um, to, to be an S debt, even though I didn't ever really do it. Um, cause I thought it was done. That was a great, that's great. Uh, yeah. So what advice do you have for someone who wants to become an S debt? Um, become like you're a programmer program things. If you're interested in test stuff, like help people write tests, write tests for your own work. Um, you know, learn to teach yourself. You could just said don't, but, um, I was actually gonna ask, uh, the question I was going to ask for you said that and changed it was what advice would you have for someone who wants to do more test coaching or become a test coach? Um, I guess I would be curious why, um, like what, what's the motivation because you know, it's, it's kind of a weird role, right? Like we were saying, you have to be willing to work yourself out of a job. Um, so for me, the games entry is, is well known for, for crunch, um, where we work in St hours. And, you know, so a big part of my motivation is I don't want people to have to experience that. Right. And so, so I kind of empathize a lot with like my customer as the, as the developers on our team. Um, so that's a lot of like my personal motivation, but I guess if like, for example, I, depending, I don't know what parts people tend to have, but like if you were a tester, um, and you're like, Oh, I want to become a quality coach. Um, I don't know. I think increasingly it's going to be tough to do it without being a programmer, you know, like all the, all the good stuff is in the programming side, right? Uh, if you, if you can't dig into the code, uh, and dig into the technical practices of how that code is created, then I don't know how you can be effective longterm. Um, so yeah, I know what I did when I took the Sestet role was I was like, well, I know nothing about this. I had a really bad experience with TDD at, um, college. I had this class where they made us try it and I was like, this was dumb and terrible. I had a bad experience. I didn't like the person that I was pairing with. Uh, so I didn't do it again, but though I made an effort to teach myself, um, and it's hard, right? That's why I don't recommend people to just like, just say, Hey team, you should start doing TDD, right? Because it's hard. It gets worse for it gets better. Uh, it's hard. It takes, it took me like sort of three to six months, I would say, to kind of rocket that from like, just doing it every day. Um, but I think you have to kind of teach yourself those technical practices. So if you're curious yourself about those technical practices and how you can write better code, how you can write code with zero bugs, um, then that probably makes you a good quality coach. Let me throw out an idea here for discussion. Just thought of this and someone's going to get mad at me and yell at me about this, which is the best sort of thing to bring up. I wonder if, uh, there really is that big of a difference between a good agile coach, which there are a lot more of and a good quality coach. And if the reason we have quality coaches is because a lot of teams implemented agile, which typically textbook wise would have everybody writing tests and whole team quality is in a lot of agile teams, the way they, the way they adopted air quote agile was to have a air quote agile tester on the team who did all the testing for the team. And I wonder if like those folks eventually, as they, if they again want to work themselves out of a job, they end up teaching the rest of the team to actually own their testing. And that's where quality coach could come from. I just kind of wondering like, what is the difference between like, cause agile is delivering, you know, frequent quality to your customers. What's really the big difference between an agile coach and a quality coach is just a little bit of expertise on testing. No, no. Cause I'm going to add a new coach. Oh God. Because I think, I think, so I was thinking about, about what Henry just said, and I'm, I'm going to invent a new term. It's something that we've used on the cast before. And I think, I think we may be conflating quality coach with code correctness coach. Right. And, and when I, when I think about if you're trying to coach to Henry's point, too, if you're, if you're coaching quality in a more of a traditional means, like, okay, create more test cases and things like that. I think it's going to be hard to coach a team on that. If, if you do not have empathy for their problems, and I think it's going to be hard, much harder to, to have that empathy. If you haven't experienced yourself, I think the more challenge the code correctness part of that, I'm like, I want to let you finish, but just as a twist on here, like something I know, Brent, you've heard me share before, but just in Henry, maybe on the podcast, I've said this, but when my old, but now isn't a quality coach role, I had a boss ask me, how's the team doing at testing? And I said, they do all the testing they know how to do, which meant that it was good. They, they had full ownership of testing job done. Not really, because there was a lot of test approaches and ways to think about the, in which the way in which the code may fail, they hadn't thought about, but I did, I mean, code correctness, they always knew how to do that. That's kind of at the beginning where it stopped. So what I did as a quality coach took them, took them from code correctness to quality. I think there's probably, you know, I think there's probably no real difference between a good agile coach and a good quality coach. I feel like there's probably lots of, there's lots of pitfalls, I think, coming from a like traditional testing background. You know, I have many colleagues who are in that space. I also see a particular, I don't know how to call it, like a, like a trample pitfall where people, people burn out, you know, they get really frustrated because people aren't doing what they told them to, goddamn it. You know, they're like, I came in and I told you that TDD was the thing you should do and you didn't do it. And I think that's, that's kind of, that's not going to be effective. But you're not going to be effective and you're going to burn out and be really mad and upset. So I think it's, it is much more like an agile coach in that you're there to help the team see what they could do better. You know, so if someone on the team is like, oh, you know, I just wish there was a way we could test this and be like, oh, how, how, how might you go about that? You know, maybe we could try a bar and kind of give them like, like you were saying, Alan permission. Yeah. So I think it's quite similar. Yeah. And then, and just to throw out there, I've done a lot of pairing in my past as well, super helpful. So mobbing definitely, especially if you have some folks with some test experience in that mob can be a great way to coach and teach and, you know, osmosis phi is made up that way. There's, there's a new term for you. Brent osmosis phi that testing knowledge into them. But anyway, I promised to let Brent finished, but now that we've told him he's wrong and telling me I'm wrong. Go ahead and finish. Brett. I don't think you've, I, I going to assume your arguments underneath my own. If I take it literally like the point of agile, right? The point of agile is essentially to successful or iteratively to adapt. Adaptively. Yeah. Adaptively. Thank you. Adaptively improve the value stream over the cost. Right. So it's an ROI function that you're optimizing. How do we get the most value in the least amount of time? And then put it in that light, right? Then a quality coaches is probably a subset of an agile coach. A quality coach is potentially focused on the numerator of that equation. How do we increase the value? Whereas an agile coach is focusing on value over time. I would say a good agile coach can do both, but many. No, no, I think, I think an agile coach's responsibility is both. Absolutely. I think today, probably 90% of them don't do both. The good ones do both. No, I think 90% of them just do scrum or fall. I'm going to barf and record it on the podcast. Right. He's turn off your camera first. Nope. Yeah. I think, I think that's right on that. The, the one thing though, like going back to your question, right? It's all right. There's two kinds of camps that we've, we've kind of been addressing and a lot of the podcasts has been our target audience as people was traditionally people doing the old, the old world way of testing. And how do we, how do we help them move into the new world and do so in a delighted fashion? Because well, that one and I went into the new world and like, oh my God, why we're jealous of all the new kids starting in the industry at this point in time, because we now look back on it and go all the bullshit we did was just wrong. That's how I've concluded it now, but it does create new problems. A lot of the old tricks, like if we want to do go and recruit a test expert, how do we do that? Do we do that? How do we get code with the right balance of preventative protection, AKA test suites and things like that, with, with leveraging monitoring and leveraging reactivity, or reactive strategies to get, if we did something wrong, get a, a fix out quickly. Right. I, I kind of more of a, I'm kind of a more of a mix on using monitoring and preventative efforts. I feel like, you know, if you, you as the team get to decide that, you know, like the thinking about, you know, what, what, what does it all put one? And I agree about having credibility, by the way. I think a reason, something that helped me a lot at, at Rare was I was talking to gameplay programmers and they had not been used to talking to another gameplay programmer, right? They were used to talking to people coming in from a central test team and being like, oh, and they don't quite talk the same language. And you don't quite have the same credibility to get them to, to trust you that you might know what you're doing. But then like, if you can talk to developer, like that, you'll say they'll have, they'll want to test something, right? There's a latent desire. You know, they know that they, that they need to test, that this needs to get tested. And they might be in a mindset of like, well, up until now, I have been able to just throw this over the wall to someone. But once you get rid of that, then they become curious and they're like, well, how, you know, how would I test this? And then as a coach, you can kind of help them answer that question. Right? So like, they'll be like, you know, I want to test this. What I could do is I could, you know, simulate the controller being pressed forward and then you press A to jump and then you do this, you know, and it's like, okay, well, but what are you actually trying to test, and it's like, okay, well, what are you actually trying to test, right? So you can, and you just keep asking them that until I tell you, you know, I'm trying to, I'm trying to test that if I, you know, that when, when, when I call hit by pickaxe on this block, the block disappears or whatever, you know, it's like, okay, we'll write that test. So to your point, right, I think that what you're talking about is kind of an implementation detail, right? That the team gets to decide because, you know, I'm into the whole team quality thing and it being everyone's responsibility, which is kind of like a lean thing anyway, right? It's like, we want to look at the whole system and say like, you know, if, if the bottleneck in the system is release or, you know, bugs coming back or whatever, then why is anyone working on anything else, right? Like we should be fixing the bottleneck as a top priority. But very often we, you know, that games entry is renowned for, you know, what to scrum fall or match a hell or whatever. And so you end up with like these, you know, these little assembly lines going and they distribute all the pieces and bring them back in. I was reminded of a really great talk by Henrik Nyberg called Confessions of a Change Agent. It's on YouTube. That sounds familiar to me. Yeah, it's really, it's really good at talking about like, you know, because a quality coach is a change agent, right? Like you're the kind of person who is a change agent. You want to make things better. You're not satisfied with the status quo. You're the kind of person who is frustrated by some process and you write a tool on your lunch break to automate it, you know? Yes. Right. So like, if I was looking for to hire people, which I might well be actually, I would be looking for people like that, who have that mentality, if they just want to improve things. Uh, yeah, actually I was in a conversation. I forgot about the term change agent. I've been called that many times in my career. It's, it's, I am allergic to the word can't. I can't stand it. When someone tells me I can't do something. You just said the word can't twice. Are you breaking out in hives? Uh, yes, it's a podcast. Uh, yes, absolutely. Look at Brent. He's blowing up like a balloon. I bet he can't stop. Um, yeah, so much of my career has been, uh, we have this big problem and then I start investigating and people like, Oh, we can't change that. And it's this sort of prevailing. That's the way it's always been though, Brent. That's right. And I just look at them and like, yeah, it's not going to be that way. Give me three weeks. Uh, those things really, particularly when it, when the word can't is in the way of something that is just really harmful. You can just clearly see that this is a bottleneck and, and, or a big wall in the way, the whole team knows it. They all have sticking with the Minecraft paradigm. They all have pickaxes, but they're like, Oh yeah, you know, you need a, you need a diamond axe to, to get through this one. Then go make a goddamn diamond axe. It's, it's, it's, um, change agents are really important because they don't, it's a, it's an attitude, right? It's not, it's not a, it's not a role. It's not something that you can hire into. It's an attitude. It's, it's no, this is wrong. And I, it needs to be repaired. They, they, they don't like to hear it's not my problem. Wait, change agents like to hear that. No, they don't like to hear that. Oh, they're not the kind of people who would say that. Um, I would say as well, like, you know, for any testers listening, like people who are change agents are always going to have a job, right? They might just have to be open to what they do exactly. Yeah. And that's, that's, you know, as a tester, I was a change agent. And I think some of this comes from our, you know, I, I, most personality types have a little bit of horoscope to them, but both Brent and I are INTPs. And I don't know what you are. If you've ever done that, Henry, same. Yeah. Well, that makes sense. Then, well, there is a pattern there because one of the things about INTPs that I love for my little green book is we abhor inefficiency. So those things just, things that are inefficient bug us and we want to fix them. Yeah. There was a podcast, there was an episode and it was, this was eight years ago. This is when I was at rare. So I don't know when I would have been, it was before you guys had invented modern testing or, or we didn't invent it. We just labeled the thing we saw. But anyway, go on. The label to it. Um, but you, you said something very similar. You said, um, waste burns. Well, maybe it was when we met and we were chatting. I can't remember if it was on the podcast, but I know it really, really resonated for me. I was like, yeah, that's, that's how I feel. It's just like, oh, this is so inefficient. Yeah. I burned. Especially waste of humans. Time really pisses me off. Oh, that one wasting a human's time. I am much more mad than if you're just like, you know, some other kind of waste. I actually, my order is now calendar time, human time, uh, then, um, you know, business issues like calendar time. If you think about like CICD, right? The longer we take for calendar, the longer we're going to get the feedback that we need to, in order to improve. Right. This is something that maybe one last thing to talk about in our, our, our weaving path through here. So I am big on making sure cycle times are very, very tiny on the services side of our, or we're pretty successful there. The time from coding up a change to having that customers use is as fast as you'd expect in a company that makes services. We also have a big, uh, at Unity, a big monolithic product, as most people know, and I am tracking and I probably should share too many details in the exact times, but I can now see, um, median P 75 times, et cetera, from the time a check-in is made until it shows up in a customer available build. And it's a nice little start up my, my value stream for that. So, and we'll work on reducing that, but the fact, I'll let you know there, they aren't numbers that I'm happy with. I'll say that it reminds me of how much work I have to do. Yeah. Well, and I think, you know, it's like, but as a change agent, you're like, well, obviously we can make this better. Right. Whereas I had a conversation with someone about a, a certain game title, not one that I've worked on, but it was someone on the services team. Um, and they, they said to me, oh yeah, the client is a lost cause. And I was like, I don't accept that. That's as good as it's going to get. It really can't get much better than that. I said, that's a challenge. Challenge accepted. Yeah. Uh, I've, I've been in those shoes. Uh, I may have even said it, but I'm, I was thinking, I'm thinking now of the, uh, the zoom client. Works better. If there are no people working on the client, if there are no humans being impacted by, by the client being a lost cause then, okay, maybe, but like, well, there are, well, there are dev teams toiling and, uh, you know, in these terrible environments, then I think, you know, if it's a lost cause and it's a critical component that, that cannot be permitted to be a lost cause. Right. Yeah. That's that's, that's problematic and people are just accepting problematic and that has to change. Yeah. And the thing is like, if you are, if you take that view of it's the last cause and you're kind of saying, then we're going to continue testing it the way we've always tested it. But increasingly that's untenable for games. Like we, we, many games are already at the point where it costs more to test them than them is economical. Like we actually get rare, a big, um, uh, a big factor in why we committed so heavily to canoes delivery was on the previous title, which was fully manually tested. We had wanted to release some DLC. Um, and we had not been able to, we had to cancel it because it would have cost more to test than we could have earned. That's good. In the context of INTPs and waste burns and talk about, you know, teams as an example, not because I want to riff on teams today, but I think it is an example of something I've seen, not just a Microsoft, but other companies in the industry do is to me. And this is probably, it's probably a paradigm for this. Maybe even in the mythical man month, or maybe in right now since flow book, but the team's client got progressively worse as they added more people to it. And I've struck that away from teams and it's not like I rip it on them, but often you'll a software project will be going well. So they'll fund it more and put more people on it. But I think it's probably mythical man month because eventually it gets to the point where there's so many people working on it and complexity that it ends up grinding to a halt as far as innovation and quality goes. I think a lot of times higher quality products are worked on by a bare minimum team delivering iteratively and adaptively versus a whole bunch of people trying to get a big bang for the buck faster. It occurred to me as you were talking through like adding more resources. Sometimes call people. Yeah. Well, sometimes it's machines, sometimes because something else, right? But adding more resources, right? If you think about the switch statement, adding more resources is the default action plan. Sometimes people get in such a bad state that the sky is falling. It has to be fixed by yesterday and everything you come up with is going to extend it much further or be outside of your means. And so, and a lot of the times when you're drowning, it's really difficult to think through very calmly and go, okay, how do I swim? Right? If you haven't learned how to swim, it's like you need, someone has to have their head above the water and guiding the panic to the shore. Now, if the whole organization is in the panic stage, then it often ends up being the case that adding more resources, of course, as you call out potentially thoughtlessly, because the more resources you add, whether it be human or otherwise, is going to add additional complexity and runs the risk of actually adding more problems, not solving them. Yeah. And there's a lesson. Go ahead, Henry. I like the model from the Repennin and Sternum paper, like the one that's called, no one ever gets credit for fixing problems. Oh, I remember that one. It's like 20 years old by now. Yeah. But I like it as a model of like, you know, you have a certain amount of resource and you can choose the slider you have is how much am I investing in improvements, improving the system versus how much am I investing in delivering work? And the problem is that you can push the slider over and invest it all in producing work, but then your system degrades and over time, things become terrible, but you get that short term boost. Whereas the other way is harder to stomach because you get a short term decrease when you stop producing, when you invest more in improving the system. But then over time you become better. I think that's like the kind of number one, you know, you go to people and they're like, Oh, I just can't do this. We don't have time. You know, management won't let me, you know, it kind of comes from all that kind of thing. Always management. So what I like the kind of teams that I've led, they've kind of been, well, certainly the Minecraft one, it was kind of a skunkworks thing, you know, and I think you can be quite effective by taking just a couple of people, like two or three people off to the side and saying, Hey, you people help other people get better. It doesn't solve the whole problem, but it can be effective. Well, a few people whose role is force multiplier or change agent is good to have throughout the org. But I wanted to build off real quickly before we end here, the thing that Brent said earlier, and I think maybe I've mentioned this on the podcast, but I, I, something I reiterate to my team, especially as we're looking at our growth for 2022 is I will, and my team knows this now to their initial frustration, but now understanding is that if your team has a whole bunch of work and can't keep up, I'm not going to give you head count to help solve that problem. You need to solve that problem first. You need to figure out how to prioritize and get the work done that you can and choose what you can't do. And once you show me as a lead, you know how to run a team that can actually deliver and not be underwater. And then I'll add head count to your team so you can grow and take on more stuff. And it's a hard lesson for people. They're just, I mean, a lot of times managers will do the opposite. Oh, your team's not keeping up. Okay. We'll give you some more heads. And that was very much, you know, I add that experience at the old place. So actually, I learned this a long time ago, your Reese, and it was through a mentor. And, and I've applied it to multiple things, new, new resources, whether, again, whether it be compute, people, money, whatever, new resource, way more successful to allocate those to your strengths, not to your weaknesses, right? Much, much very similar to what Alan's saying, like, no, no, no. I have, I have five additional heads I can assign to a project, right? I'm going to assign it to the, to the projects that are showing strength, not the ones showing weakness. Those type of resources are not there to add a safety net. Root out your problem, get back on track on an ROI, show your, establish a strength, whatever that may be in the context, then the veritable candy will start flowing again. And I guess it's kind of, it's sort of related, but it makes me think of when you're trying to change a culture, you're trying to be a change agent, then you're going to find your allies, you know, you're going to find your bright spots and help support them more. You're not going to find the people who like disagree with you the most and try and help them, right? You're going to help the strong people first. We should probably close for the day, but I want to thank you, Henry, for joining us on the podcast and sharing your stories and helping me give Brent a bad time. Could it a little bit better there as your feedback, but it's okay. Brent, Brent relieves today relatively unscathed. So any final words from you, Henry, before we go? No, thanks for having me. Thanks for joining. And as always, you can find out more about us and on moderntesting.org is where you can go. Also, if you want to join our Slack group at one of the three, I think that's kind of it. So Brent, anything to say? No, thank you. Thank you for joining Henry. And I assume, Alan, you're going to post a link to his latest talk. I will try and remember. All right, everybody. I'm not Henry. Nor am I. All right. See you next time. 
