This is the AB Testing 343 podcast, a podcast where we ask one of the three listeners of the AB Testing podcast, three questions about almost anything. ABT 343 is a fun slice of what's going on in the world of modern testing. Let's get started. Hello, and welcome back to ABT 343. I'm joined. This is not a repeat. If you've already heard the awesome episode with Connor Fitzgerald, it was so awesome. We had him back again. Hi, Alan. And hello to the other two listeners. Perfect. Welcome back. Thanks for joining us again. We talked for a while last time and I think we both had more to talk about. So I'm glad to have you back. Delighted to be back. The last podcast was my first time being interviewed for a podcast and I'm on the Slack channel. A lot of people were asking, could we do a second round, which I was quite surprised with, with Delighted with a course. So it's great to be back for round two of the modern testing podcast with yourself. All right. I have some more questions for you, three more to be exact with potential bonus questions to throw in there, but we'll start with three. Are you ready to go? Sure. All right. When we talked last time, we talked about how you use modern testing in your current role. Four of the other two listeners, could you recommend one key change they should do if they want to start to use modern testing in their current role? Yeah, for me, it's critical you stop being the gatekeeper. That's a very challenging thing to change, especially if you've been testing for a long time as a traditional tester. But as long as you are the gatekeeper, quality, quality, nothing will ever change. And that's directly rated to principle three. Principle three being we are forced with continuous improvement, helping the team adapt and optimize in order to succeed rather than providing a safety net to catch failures. So if you're always the safety net, the second part of the principle, then you'll never get the benefits of the first part of the principle. And I guess from chatting to you in the first interview, we spoke about the fact that through some painful lessons, I started to learn that being a gatekeeper doesn't work and the team based testing was the solution. And there's a couple of other things I can talk about here, maybe we can dip in an algorithm if you want, but it's kind of like the pain or the negativity of being a gatekeeper, the positive side of changing and then how do you do change. So we can start to touch in on those points if you'd like. Yeah, I want to do that because I think a lot of people, there are testers out there not to listen to the podcast, unfortunately, but I run across them on Twitter or especially LinkedIn, where they're happy for some reason to be the gatekeeper. And Brent and I talked about a weird codependency there, but can you talk more about that pain of being a gatekeeper and what you've seen happen there? The turning point for me was after being on a very painful project that failed. And then during a period of reflection reading the book, lessons learned in software testing, and some of the things that made me realize was firstly, when you're a safety net, it means others aren't learning about testing or gaining valuable product knowledge. And on a personal level, I don't know that people realize that maybe they don't, but it is a huge burden for testers. It can be isolating being the gatekeeper, and it can be quite stressful for them in the workplace. And if you are that safety net, then you in turn are, or if you're the gatekeeper, you're more likely to be a bottleneck potentially because the activity of testing is not shared, then it's most likely that other activities aren't shared either, such as writing automation. And there's loads of data to show that high performing teams along with developers working on the automation. I believe that was an accelerate, I think they got the data from the state of DevOps reports. And I guess why those teams perform better is because the developers are much more aware of the product and the various challenges when they have to go and automate it. And it probably helps with things like testability. And a related topic to that, what I found in the past is when you're the gatekeeper, the decision to release software or not can be quite confrontational, rather than with team based testing is collaborative. So when you're a gatekeeper, the tester can sometimes be blocking the release rather than when it's more collaborative and it's a discussion. And I've never had a problem in the last number of years when we use mobbing and bug bashing and things like that, the team just say this software is good enough now or not to be released. And there's no problem. And one other thing I think is around bias, a quote from you on Alan Fitter recently was, if testers say we are the only ones who care about quality, it excludes everyone else. And I know the opinion that it's highly risky to not include others in testing. As testers were test specialists, we bring a lot to the party, but we have biases the same as everyone else. So by working with others, we're getting a wider diversity in our testing and thus reducing the risk of our own biases. So that's my side on the pain or negative side of being a gatekeeper. Yeah, I had a lot of flashbacks. I remember being a gatekeeper. I remember working for test managers who blocked or wanted to block the release. I worked with the dev manager once who said the primary role of the test team was to sign off on release. Wow. Wow. That's great. Yeah. So yeah. And of course, what Brent and I say is a gatekeeper is a bottleneck. Yeah. And as you know, in modern testing, we don't like bottlenecks. We borrow heavily from lean. Yeah, indeed. Yeah, like lean. And I was even watching your videos and Coderdojo today, and it brought me back to a book. I read a couple of years ago to gold, which is a very nice bottle. Yes. Nice book of open next. We were just having a discussion. The meeting I was in before we chatted was a dive into unity details, but a retrospective with another team outside of my org. And I messaged someone in Slack during the meeting and said, you ever read the Phoenix project? This team is Brent and not Brent Jensen, Brent, the bottle, the human bottleneck from the Phoenix project. Let's move on. Uh, this is a fun one. Some people, and we've talked about the controversial principles and some people consider at least one, usually two of the modern testing principles to be to have some level of controversy, uh, especially principle number seven, which is very carefully worded, but it says that you may not need dedicated testing specialists. What do you think, uh, do you have any opinions on controversy in the principles? Yeah, like, so I guess one that does cause a little bit of controversy is that the customers don't even can, uh, determine quality. But I think the one that probably causes more controversy is probably seven. Um, as you were saying, which is, you know, uh, we expand testing abilities and know how across the team understand this may reduce or eliminate the need for dedicating tests as specialists. So to me, principle seven is actually something to aspire to, which was interesting for me when I was listening to the podcasts and listening to those in depth conversation you did to me, I did wasn't controversial. And if you eliminate the need for dedicating tests as testing specialists, you have actually been successful in a role. Essentially you've helped that team ensure their self is sufficient at the activity of testing. And if you achieve that, you will be in demand. Your current company won't want to lose you and other companies will want you. And I think it's worth noting that often it may be the word reduced rather than eliminated in that teams, even when they're doing team-based testing, they'll often look to you as the dedicated, uh, specialist to help tweak and fine tune them, help out with the retros, uh, help facilitate those bulk bashes, whatever it may be. Um, and even if you get to the point that you eliminated, it may still be that you're doing a great job. But I guess on the subject of being controversial, soft test Dublin last year, there's a speaker, uh, John Paul, uh, Farik, I hope I got his name right. And, uh, he works at Rabobank and he's a big fan of the modern testing and he did the keynote and his keynote was actually on modern testing and his talk was to have testers or not to have testers. And it's probably one of the best, uh, talks I heard last year at a conference and it really made me reflect afterwards that for traditional testers, probably principle seven probably strikes fear rather than controversy for them in that the fear of change and feared that their role is changing. And I guess that can be very hard for some people. When Brent and I started talking about modern testing and modern testing principles, it wasn't to create fear. It was because of the fear. Yes. We wanted to provide some tools and wording and discussion to help people navigate how software development was changing. I'm going to shove in a side question here. I think this is related that fear of change, especially among traditional testers. We saw this, there's always been change in testing when all of a sudden 15 years ago, Microsoft and a bunch of other companies decided all the testers needed to know how to code. There was this fear that people thought we could automate everything, but I don't think anybody's ever really thought we could automate everything. And you've heard my tautology statement on this. We should automate 100% of the tests that should be automated. Very sensible. The test design, the test design challenge is figuring out where those hundred percent are. Yeah. So here's the side question with that setup. Last night I was in a, I was tired and I opened up Twitter and happened to open it up right up to someone have an argument that I'm going to paraphrase it. I think they're already mad at me on Twitter. I've been afraid to look, but without calling them out, all I said was something to the effect of I opened Twitter and saw another argument about how all testing can't be automated. I'm not mad about the argument. I'm just wonder how I got to 1997 and why Twitter still works. I think that statement, the first half of that statement comes from some fear of, if we don't have testers, we'll try and automate all the testing and that will, and of course, we can't do that. We can automate all the tests and we can automate all the tests that should be automated. Where does the rest of that testing come from in your world? How does testing happen if there are no dedicated testers? So the organization at the moment, there isn't a tester in each team. So some teams have a dedicated tester and others don't, but the team still do the activities of testing. So they bake in the quality as they go. So they're testing together when they're doing their mobbing sessions through the code reviews. And they've taken responsibility for automation that was written actually by a dedicated test specialist in the past. So they've taken over a lot of those activities. They, from time to time, do look outside and would ask to have conversations with a dedicated test being specialist, or for the greater part, their self-contained team. And from my part, from my observation of them, the quality of the software they produce seems to be very good. You knew the answer I was going for and you already answered it, I think, in the first question is, by the time you don't need a dedicated testing specialist, it isn't because you've automated all the tests. It means it's because the team does enough testing on their own, of all types, that they don't need that specialist to do it for them. And that's the part that gets lost. The point we want to keep on driving home is why not having testers doesn't mean you're not doing testing. Yeah, definitely. And I guess you seem, from listening to you over time, I think we think similarly on this point is all which is related in that testers, I've worked with some excellent people who do automation testing, or probably with no other test tests, who are excellent at their role. But for the greater part, testers are skill set, are better placed outside automation for the greater part. And that being working with developers, for example, to write the test, so that developers write the test, or at least help guide them in writing the code, is a much better use of the resource of a tester. So working picking out what are the valuable tests, because you were talking about the tests that need to be done. So testers will be very good at picking up personas and user journeys, finding those critical paths often, ones maybe that are tied to revenues, and saying these are the most critical things we should automate. And then working with a developer to write really reliable, good quality automation that you can trust. And similarly, for your alerting and other things working with developers or working with people from an SRE or DevOps background to get these things in place is much better use of a testing specialist. You have been, as I've announced on the podcast before, I'm doing far fewer conference presentations. I did one last year, I'll probably do one this year. But you, on the other hand, have been doing a reasonable amount of speaking over the last few years. And I'm curious how modern testing and what you've learned and what you've practiced has influenced what you speak about at conferences. Yeah, sure. So maybe give you a little savor what I've been speaking about. And we can see it was anti interesting in there. So I guess I'm relatively new to speaking at conferences. I've spoken at about five today. And I've spoken on three broad categories. The first is how we can learn from other industries and disciplines, learning from aviation, healthcare, economics, the world of business, for example, marketing. The second broad category is Kanban. I never talk Kanban from a testers perspective. But ultimately, it's a story of two teams using Kanban for the first time in the challenges they faced and the benefits it brought them. And the third one is one Robert Meaney and I delivered many listeners will probably know, Rob, and we delivered a talk called Experimenting our way to team based testing, which was a reflection on three years together in Pablo working together. And I've also delivered number talks at the ministry testing Cork meetup, and Rob and I kind of folk found it or got that off the ground back in 2017. And we've done a mix of solo and joint talks with Rob there. So we've done things like exploratory testing, heuristics and a few more technical ones as well, like how to test microservices and an introduction to automation. But I guess what's interesting for you is, I spoke at Test Bash Brighton, the year after you after you delivered the modern testing principles talk. And you'll be glad to know included two modern testing principles in my talk, which was principles two and six. When I was speaking about how we can use models and about how we can use data. How was I have a couple questions about that. And actually, I'm probably even more interested in the Kanban talks, we're going to come back there. But to start off with, about a little bit more context around maybe what you talked about with principles two and six. And then if there was any feedback around that, that would be interesting to share for the listeners. Yeah, I guess it was I think a little bit of feedback because some people at the conference weren't word modern testing principles. So they were interested to talk a little bit more about it. So I talked about principle two, because I talked about when I was studying business, part time a couple years ago, one of the subjects I loved was marketing. I'm actually a really, really good at their various models. They have a model for everything. And anybody can just look at their model and quickly analyze what it means. So what I said is that something that we can learn as testers that I'm a big fan of modeling, whether that being architectural diagrams are using something like a mind map. Anything that kind of visually represents what a system looks like. And then related to that as well was that when I was doing the same business course, I was also studying e commerce. And I was fascinated with the use data to figure out how people work through the systems in that what point in a transaction do people leave the landing page? What point do they leave the cart? Do they leave the payments page? And they use data to figure out where they left and they use the term like these personas and the terms user journeys. And I found it fascinating at the time that I was trying to write at that point in time, I was trying to automate all the things. And I started to realize that if I kind of encompass some of the principles from e commerce and focus on personas and user journeys and the most important things to automate, that there's a lot to be got there. So what I was trying to do in summary there was trying to articulate the importance of point six is the use of data and use that to actually try to help you figure out what you should automate. And also things like figure out should you work on a feature or should you retire a feature and all those good things. It's an interesting I wrote down my questions this time, so I wouldn't forget. One thing that's worked out, I think not intentionally, but also not surprisingly, is the interplay and the interaction between the principles. And you're 100% correct in principle number two is about understanding the system and trying to figure out the system and how it works so we can understand where the if we don't understand the system, we can't find the bottlenecks, we can't optimize it. So two goes into three there. But with data with customer data, analytic data, that gives us an even better, more accurate view of the system and how it works. Yes. So that's it's I like thinking about it that way. It reminds me also those believe it or not of the question I was going to ask earlier, a lot of people, even people who don't know about or care about the modern testing principles, do agree that testing is a learning activity. And does it doesn't mean that testers are the only people that learn because as you and I know, anyone can do testing. And somebody's gonna yell at me for saying that. But you know what I mean. Yes. And but that activity is done well, is a inquisitive learning discovery. Trying to learn the system is part of that. Yes. It's understanding what it is we're testing, I think, I don't think anybody, any of the names in the industry would disagree. That's important. The thing I want to bring up in, in regards to the previous question, which fits in well here, is that pairing with other people, developers, product managers and talking about what are we making? Who is it for? What can go wrong? And having those discussions also helps us understand the system or understand where the risk places the areas of risk may be in the system in order to get a better grasp on the system and have a better idea of what we need to do to alleviate that risk. Wheels within wheels, it all fits together rather nicely. It does indeed. Let me go on a little bit before we close. I want to talk because I'm a big fan of Kanban and I could go on a preachy thing on why I like it. But I kind of want to hear a couple of your big takeaways and from your talk, like what were some of the main points? Like what were your big takeaways from your, from what you learned using Kanban and that, that maybe everyone should know about? Well, there is, yeah, it was quite, I got a lot more feedback after this talk at Test Bash Manchester than I think I did at the other talk at Brighton. And some of the things I focused on was just some of the things that people liked and then told about was moving across the Kanban board from right to left, rather than left to right. And have a focus on finishing things rather than starting new things. That was one thing that people really liked. And another main thing was that we used an acceptance queue. And initially, when I started, one of the teams that became a bottleneck. And that was a really helpful thing going back to the gatekeeper that we realized the acceptance queue was the bottleneck. There was one of me and there were several developers. So we started to change things around. I started work with the developers earlier, which meant there was more testing done in the earlier lanes. But on top of that, we changed the rules that anyone could test anything in the acceptance queue, as long as they had not developed it. And that made a big difference. And that's really what helped us shift to team-based testing. And I know that some people see an acceptance queue as a bottleneck in its own right. But the acceptance queue wasn't there to be a bottleneck. It was there to be... It had a purpose. And the purpose really for us was that we mocked up a lot of data in the early testing on the developer's systems. And the idea for us for the acceptance queue was to test it on a real environment and make sure everything was okay before we pushed that button and got it out to production. So yeah, there were two key things in it, moving from right to left, and then how we tackled the acceptance queue. And the interesting thing was when I went to work with the second team then, they initially didn't want an acceptance queue because in the past, they had seen it as a bottleneck. Well, because of my experience in the first team, they took my advice essentially and worked with me and we put in the acceptance queue. And I really think it helped create that culture of team-based testing in that second team as well. Two rules of Kanban are the main two rules are limit your work in progress and visualize your work. Yeah. Usually stated in the opposite direction. So Kanban, your last statement there is what Kanban is for. It shows you where your bottlenecks are. Yeah. And there's two big reasons I like Kanban. One is it's an easy way to immediately find or very quickly find what are the bottlenecks in your process. And you can figure out what to do to mitigate those so they aren't a bottleneck anymore. I see a lot of teams use Kanban, but not really enforce work in progress. But in your case, it's a great example because you have, because of that acceptance queue and the fact that anyone can test that people are in order to be able to do more work, they can go help out doing the testing for a piece of work to free up a spot in the queue for the next thing, which is great. Yeah. It's driving all of the right behavior. The other thing I like, and I'll talk about a little bit and then we can close out here, is I think the conversations around Kanban, I like a lot more than what I call the scrum zombie dance. Yeah. Where you have scrum and people, yesterday I worked on the thing, today I'm doing the thing, I'm not blocked on anything. And the next day they come to their scrum stand up, they sit down and they say, yesterday I did the thing, today I'm doing the thing, I'm not blocked on anything. And what that sounds like is they don't care, they're not blocked. So when I do scrum standups, the question is easy. What do you need to do to move this ticket to the right? Then I get the details I actually need. I know why they're still working on that thing. I know what we can do as a team to help them if needed. It's just, to me, it's a much more powerful conversation. And of course, if you do scrum well and you have a good scrum master, you can get past that scrum zombie syndrome. But I've had a lot of luck just with that question in Kanban meetings. It works well for me. Your mileage may vary. Yeah. I fully agree with you. That's one of the great things about it. Because I went from waterfall environments to scrum and a couple of projects and companies. And then when I joined my most recent role, I really wanted to use Kanban. And all the things you're saying, it works like visualizing the work, which is the first thing you're going to do to get it implemented. But also as well, how it changes the standup. Because it's another improvement to move away from scrum what I did yesterday, what I'm going to do today. And instead talk to the board and talk about getting things unblocked and moving things to done. And that's not a powerful change. When you're moving the board from right to left, it's powerful. But changing the standup to talk to the board and not talk about individual's activities is a big shift for me. Yeah, it highlights what everybody's doing. Anyway, our Kanban love fest cannot go on forever. Ping us on slack and we'll go on. Any final words, other things you want to mention? I will be covering my Kanban talk. I will be covering of the Kanban other subjects soon, but at Agile in Ireland this year, which is a big Agile conference in Dublin, I'm going to talk about the learnings from the aviation industry. So if anybody is in the Dublin area and enjoys all things Agile, look me up and maybe you might like to attend a conference. Awesome. I'll try and dig up a link to that, a website to that and put them in the notes. And I do like to see that I think we're seeing a trend of a lot of things around modern testing, Agile starting to see more testers who are, it should just be anecdotal or recency bias, but I'm seeing folks in the modern testing community out there speaking at non-testing conferences about quality things. I think that's really cool. Yeah, it's really cool. Yeah, my good buddy, Rob, Mimi is similar to me. He's very eager to speak at non-testing conferences, but they get our thoughts around testing out there into other disciplines like DevOps conferences and Agile conferences, if we can get out there into those kind of conferences as well and start to share our thoughts on testing and the interactions you get with people are brilliant. There's a conference in Cork every year called RebelCon, and I spoke about that last year. The connections you get with all the different disciplines at these kind of conferences is wonderful. It kind of shares your own knowledge and thoughts on testing and probably generally how we develop and develop software as a whole. I totally agree. Excellent. Thank you for doing yet another ABT343. I think our listeners, we just love to have you back every week, but we'll take a break from you for a while. We'll have you back soon. But like I said last time, I love hearing stories like this, and I hope our listeners love them as much as I do. Thanks again for being on the show. Thank you, Alan. All right. We'll see you. 
