Hey everybody. Hello. Hey, I'm Alan. I'm Brent. And guess what? We're back again for episode lucky number 13 of AB testing. So how you doing Brent? I'm doing swell. Good. What's new with you? Well, so we're running a little, we started late today. It was very exciting. Did you have a hard time getting out of bed this morning? No. So I ran in, got to the car, I was going to be right on top behind the school bus on 40th. Yeah, and apparently you can't just like zip around them. You can't. They put that little sign out that says don't go. And I was in the wrong lane. I couldn't even do a UE. That happened. Oh wow. Yeah, that happens. Um, even in my neighborhood, if I leave a little late, I may as well leave a lot late because I'm going to be driving behind a school bus for a while. Oh no, I, I, it's exactly true. If I can't get out of my house before seven, then stay there until eight. I'm usually in by five 36. Wow. I get in it. Yeah. I think I get in early because there's nobody here when I get here at eight usually. There's a couple of people here. There's one guy in my building that I compete with to see who, who gets there first. That's usually me. That's a really dorky competition. Yeah. And it's not an example. I would suggest others follow. All right. Hey, uh, what's new with me? Well, I'm glad you asked. Uh, last week, was it just last week? Yeah. I took a couple of days and went to the Google test automation conference up in Kirkland G tack as the cool kids call it. A couple hundred people there. It was fun. Met a lot of people reconnected with a lot of people. It was a good time, but a couple of things I want to talk about. I'm not going to go through a laundry list of the talks and what they were about. Just out of curiosity though. Have you gone to these events before? I assume this is my second G tech. I go to all the G techs that I can drive to and use in Seattle and I'm, I can't remember 2000. I can't remember was the size of the audience about the same as last year. Yeah, it is about, well, I didn't go last year about the same. The Seattle, when I went last time. Yeah. They keep it. They keep the attendance at a cap. I don't know what it is. Okay. All right. So anyway, a bunch of people giving talks about stuff they're doing a lot on tools, obviously for Android and web automation. Selenium is definitely, you know, becoming not becoming it's, it is the de facto for, for web automation. Selenium is sweet. Yes. Cool thing like Sel Android for Selenium on Android. Some cool little ideas and guy from American Express just talked about the different freely available tools he uses for testing. It was just kind of a cool, nice little overview, but a couple themes came up. I wanted to talk about, well, one theme. And then one observation. So a guy from Facebook was there. Okay. And, and Brett and I have talked about combined engineering and if, can you have one engineering team? I just wrote a couple blog posts about it. He gets up there and he up to give his talk, not up there, but up give his talk. And he rehashes, he restates the fact that, you know, Facebook has no testers, you know, and we get by. And then I thought it was interesting that his title, his role is he's the lead of a product reliability team. Okay. They don't have testers. They have product reliability. If I, and if I heard right, they also have engineering productivity teams. Okay. A lot of the things we expect those test minded generalizing specials to do, they don't have testers, but they have, you know, it just, it, it rehashes to me, it reiterates to me that the testing activity continues whether you have a test discipline or test team or not. The, the, do you, do you have data that confirms that? Like, so those in podcast land, I don't have data. I have, hold on, hold on. Those in podcast land holding can't see Alan's face, but the, what your body language just communicated was, yeah, dude, it's still a testing. You're calling it something else. Yes. And no, I think maybe a little bit I'm saying that, but I think more importantly is it doesn't fricking matter. I think, I think, I mean, you call it by a different name, but we've discussed this numerous times. And I think on the podcast that, those roles, productivity, engineering, reliability are really sort of non-functional requirements required specialists. And so I don't know. I mean, I would say we were to disambiguate the term test. Yeah. Those are the type of responsibilities that we funded in our organizations prior, but the key pivot is things around component tests and functional tests, right? That's going to the, the dev org. And I haven't heard yet you say that this guy owned those, either of those. And I'm actually, to be honest, and I hope he's not listening because I don't remember what he talked about. I guess that was the interesting point it took away. Generally at most dev companies, developers are going to own that unit and functional testing. And they definitely do it Facebook. But for some of the big picture things that people worry that when a lot of people go up in arms and they say, Oh, they don't have a test team or some company X doesn't have a test team, they go, well X and Y and Z aren't going to happen. And my point is that those things do happen. Yeah. It's a couple of podcasts ago, you brought up the one scenario, the one Twitter where a guy was like, if you don't have a test team, you suck in essence. Right. And, and it's just, it's just not true. Facebook definitely has a continuous deployment model. They definitely have flighting rings. They use exposure control to minimize risk. Right. Yeah. Anyway, what was your takeaway from, I mean, did you think they were doing something? As I mentioned before, and I now have to say it again, because you don't freaking listen to me. No, you said that testing activity. I don't remember what he, what his talk was about. I just thought that statement was interesting. So I'm going to move on before you asked me again and I get angry, like the angry weasel, um, people that read my, the, the about angry weasel part of my blog will know the whole story behind what angry weasel means. Nothing to do with me has to do with the tooth of the weasel and a German automotive issue. So yeah. Anyway, moving on. Thanks for sharing. There was an interesting theme. Probably I'm not going to say half. It was more than half. Three quarters of the talks mentioned a two word phrase, some more often. And it became like the joke, like, Oh, they said it. Everybody drink. It's like the G tech drinking game. Um, which was the term flaky tests. Brent, have you ever heard of this word before? Flaky tests? Yes. Over and over and over and over. What the hell is wrong with us? The flaky tests are so prominent. Uh, there's an interesting vanity metric people put up on the slides. Uh, how many automated tests they run? I don't care how many automated tests you run. I guess it does give you, if you're trying to show scale, but even then it's such a, you know, tests aren't created equal. We know this idea of flaky tests came up blatant vanity metric. And, uh, guess what? It's come up at Microsoft about a half a zillion, no quad zillion, I'm quite drill zillion. I don't know a number, a big number, Googleplex. Yeah. Maybe it's come up with Google times. Yeah. Uh, with, I run this test. Sometimes it's flaky. Sometimes it's not. Let me give you a statement and I can argue both sides of this. Uh, someone said at the conference, a flaky test is worse than no test. What do you think about that? Yeah. I, so when we talk about flaky tests, it reminds me of a white paper comment, uh, Bill Gates did years ago where he talks, talks about the automation paradox. Do you remember the automation paradox? Uh, vaguely. Yeah. Automation paradox. Was it called the automation paradox? I remember the comment. I don't remember the paper, but anyway, go on. Yeah. The, the, it might've been test paradox. I don't know. In essence, the paradox is not a podcast. On long-term memory and recall. No, but it is one on ADHD for sure. Um, the, the test paradox in essence is if you can make a world's perfect test that validates the code, then you have to actually create the test that's better than the code. So there's a, then so that there's a implication that perhaps you should be using the code inside the test and not the code inside the product. That's what makes it a paradox. I hope that was sort of a philosophical point and not, uh, this was back in, in, in the day Bill Gates still ran the show. All right. And we had what a quadrillion dollars in payroll going to test. And, uh, I remember one small percent going to engineering. It's probably 15 years ago, the test architect group, we had a presentation for Bill Gates and talking about what's going on. And he walked into the room saying, I spend a lot of money on tests. I have no idea what you guys do. We test. Yeah, exactly. The remainder off the track. Yeah. So I think that was a really long answer. Actually, a lot of words that didn't actually answer the question. So let me answer the question. Are flaky tests better than or worse than no test. And it depends because not only are not all tests created equal, not all flaky tests are created equal. And by definition, flaky is a test that sometimes passes, sometimes fails, and you have to spend your time looking at it to figure out whether it's a product bug or a test bug. So if you, if you evaluate it across, uh, payroll cost, yeah, a flaky test is going to cost you more than no test in the short term. In the long term, you don't know it having no test, right? No tests on something that no one uses probably not going to cost you much. But there seems to be an unwritten rule in test automation is I've written an automated test. I've created a test that does something, you know, I could not have made a test. And now that I've written it, because it's automated, it now must run forever and ever and never and never to be retired. It must run on all platforms available because it's automated. It's free. I would say, I mean, I don't think I would agree with particularly with where we are in the modern world with flaky tests, uh, better or worse than no test. Um, but if you're going to have tests, they need as best as possible. They need to not be flaky. The point of putting that test into play is you're trying to create an automated system that you can trust. And that's where I've gotten to is trustworthy tests. You want tests that you can trust the result of. Uh, I forget who it was, but I almost screamed from my back row little observatory when another speaker said, uh, he was asked about, you know, he's talking about flaky tests and they had a bunch of flaky tests and they do things like rerun them until they pass and so things like that, um, you know, investigate them. And he mentioned the fact that we've run through in the past at Microsoft where testers read a bunch of automation and their new job became to investigate failure failed tests all day. But someone asked him and it wasn't even me. Uh, he said, so do you investigate? Do you believe you have flaky tests in your tests that are passing? Do you have tests that are passing that maybe should be failing? Of course. And his answer was, and I'm going to recreate, you know, that scene in the movie where you yell something out, but it doesn't really happen. You're still sitting there. That happened with me. He said, I haven't seen that. So when asked, have you seen tests that are passing? It should be failing. He goes, I haven't seen that. And the little voice in my head stood up on my chair and screamed and yelled, I'm going to back up here. Have you looked? Of course, which the answer was no. And we fight and I punch him and tear his head off and throw it out of the hallway. You are angry today. No, you know, it's the things that happen in your head. They're safe there. You bottle them up and squish them in. Then that anger never gets out into the world. Oh, I've been repressing feelings for decades. Oh, and I get that. That's a whole different story. The other thing, like I want to tie this to the modern world, I guess. This reminds me of a presentation we saw from a colleague of ours, Michael. Oh, hey, Michael, speaking at Sasquog next month, sasqag.org on the third Thursday of November, whatever day that is for Seattle listeners. Come out and listen to what Michael has to say. Michael recently left Microsoft. He's hanging out trying to figure out what to do next. He's the kicker on there. If you go to sasquag.org and check it out. Check out the after first talk. I can't I'll paraphrase it here, but he figured out that he didn't like testing after being a tester for 15 years. So he's going to talk about that kind of how he came to that realization and kind of where he's going from here. But I don't know any more details. I wonder if are we talking about the same Michael? Michael H. C. Oh, hey, peer peer disconnect here. But still, if your local go to Sasquog on Thursday. Tell me about Michael C. So Michael Michael H. Yeah. He was a mentee of mine until very recently, although I guess I guess he might still be. But yeah, I didn't know he was doing the Sasquog tech. I should go show up. Right. But the Michael C. No. So one of the things that he did is he ran tests or test results through a Bayesian network in essence. And one of the things that he was able to find by by applying data science to the tests is that you're able to determine which tests when they report pass, you can trust that they're actually passing and which tests when they report fail, you can trust that they're actually failing. And some tests are very good negative indicators and some are very good positive indicators. And I found that was fascinating because I've been a test manager for a good portion of my career. And flaky tests. Yes, when you're in that role, it's just an irritant because you can see it stealing your payroll day over day. Yeah. So but if I had a signal that could tell me, hey, this sweet, when it passes, I can trust it. But this this other suite, I can only trust it when it fails, then it would help me reduce the cost of flaky. I think that's a start. I'm gonna tie that to something that came out of the conference in my takeaway, and I'm not going to go deeply into my diatribe on test selection. I'll save it for I'll save it for another podcast or a blog. But what was interesting that came out, there was some email after the conference among the attendees. And I think someone from Google mentioned that, well, we use code coverage to figure out which tests, which tests hit which lines of code. So that's great. That's a good start. And people latched on that, like, oh, my gosh, I know how to do test selection. Now I can do this. And let me tell you, it is just the start. And it was so interesting to me that, yes, you can use that. Yes, you can use engines based on, you know, trying to evaluate the trustworthiness of the test. And building that engine where Michael, Michael's work started and needs to keep on going is there are dozens of factors that can go into what makes up a trustworthy test or makes up a valuable test even better. Yeah, whether a test passes or fails, and I can trust the pass and fail result is good. That's great. But I want to know whether that test is going to be valuable for me at that moment in time, which coverage provides part of the speed the test runs, if it's in there, if I can get the same value of a test that runs in one second from a test that runs in 10 minutes, that one second test is a lot more valuable. And I can go down a list and I need to enumerate these, not speaking, so I forget some important ones. But the idea is, what I've done in the past is build sort of a heuristic engine that takes a dozen or 20 or 30 factors of what makes up a valuable test. You know, how long has it been since that test has been run is when I use a lot. Going back to the comment about running every test on every build forever and ever, that's great. But if that test has passed every single time for the last 20 years, and I don't run it today, is that okay? Some testers actually freak out. No, because the day you stop running it is the day that there'll be a bug there, it'll find. So I would say there's less value in a test that has passed every single time. But the moment I don't run it for a day or for a build, it gets a little bit more value. So there's value in how long has it been since this test has run. So has this test found a bug before? Has this test found multiple bugs? I can find multiple ways to wait bugs and then pick, say I have 100 million kazillion tests and they take 30, you know, because I'm web scale, I can run, you know, I can run them in 24 hours. Well, I don't have 24 hours. I want to run tests in an hour. And the data farm junkies will go, well, more machines! But no, there's probably less than an hour of those tests that are actually valuable at that moment in time and run an algorithm to figure out which are the most hour long worth of valuable tests. There was a lot of science that was occurring in the last 10 years. Science occurred? That's awesome. Yeah, no, there was a lot of studies around like, in terms of some of the variables you're talking about, like you didn't mention co-coverage. I mentioned using the cover. Yeah, I did. I mentioned using... You said coverage. That could have been feature coverage, which is how I interpreted it. But that's fine. Use the tools. Use the tools, Luke. Yeah, like a lot of hypotheses around what's valuable. Like my experience is, okay, I didn't find a lot of value in example. Hey, this test found a bug before versus this one didn't. Because it turns out that the bug that it found was just a stupid dev error. And they fixed it. And now it's unlikely to do that. The one reason why we had to do all these tests, because there was so much uncertainty and so little ability to get tests correlated to the code that they're covering and the customer scenario that they're covering. There was also a big push over quantity over quality. A lot of automated tests was better than having some really good well crafted ones. And a lot of times flaky tests, whether they're false positives or false negatives, happened because someone was just cranking out some code that did crappy junk and they weren't thinking about writing a trustworthy test. I remember my second to last TM gig. One of the PMs came to me and just just like I don't think we have enough tests. How many is enough? And I basically said, okay, let's talk about this because I want to be very crisp on this discussion. What if I were to tell you that today we have a thousand tests for this area and that I can take one IC and create another thousand, double those tests in one week. She's like, that'd be fantastic. Let's get that going. All right, a batch smell to make that happen. Yeah. And I said, hold on. What if I tell you that my thousand tests currently cover the product 5% and that new thousand tests will get me to six total? She's like, what's cover the product mean? And I said, co-coverage. So yeah, you're very much right because just like Bill Gates didn't know what test did. If Bill Gates doesn't know, it shouldn't expect a standard PM to know. Right. This goes back into the whole, we want to count things. Unfortunately, some actually, I don't want to, some people want to count things and use that as a measure of progress. I want to measure things and counts may be important to that, but counts by themselves are vanity metrics. I agree. Interesting. I, my big takeaway at the end was I think, you know, I've thought a lot about this and have, I think pretty good ideas about test selection. And I've kind of, I know I'm comfortable retiring tests. I'm comfortable not running tests. I'm comfortable trying to come up with, you know, I'm comfortable that I have good algorithms for figuring out which are the most important tests to run. But it was interesting that I didn't feel like anyone else was really not only were they there or close or whether they cared. They were happy just to run lots of tests and ignore failures or, or have logic in the test to rerun the test if it failed or silly things like that. So your, your story reminds me of another colleague of ours, Harry, who, um, when I worked with him was over, when I was a part of the Bing team and Harry had come from a Google and was working there as well. And he had taught the Bing team this concept called heuristic based testing, which is something he had learned from Google and a heuristic based test is one that's good enough. It has a benefit that it is super fast, right? You don't, um, as an example, if you're trying to test a map function instead of, uh, testing the directions, uh, accurately and, and going through, is this right? Is this right? And coming up with your own algorithm, we go, can I find a better algorithm that it should have picked? What you do is you load, load up another shipping map software and you compare side by side, which ends up being a ton easier, uh, than trying to map through the algorithm. Well, isn't that, you know, I'm very familiar with Harry's work on, you know, using models for this. It was, um, this was more than this was relatively, but what's interesting is, um, and I'm a big fan of Harry. Uh, what we used to do, one of the mistakes testers make a long time ago is, and I've done this myself early in my career is I ended up re-implementing the functionality of the program as the Oracle. So I did this, I was writing, um, some graphic. I want to make sure some grip. Could I use the function get pixel in windows to test set pixel? You know, could I, could I, could I set a pixel to gray and then get a pixel and see if it was gray. If it was a test past and that's where test paradox comes in. And what's interesting is, yeah, cause I, I went through this big thing where, okay, actually I can query the graphic drive and get the actual color there rather than go through the API, that, which is actually what set get pixel does. Um, and what Harry's solution was rather than rewrite that Oracle yourself, use the competition as an Oracle. Yeah. Nothing wrong with that, but it's, it's, it's a hell of a lot easier. It is a hell of a lot of easier, a ton faster, but it does create flaky tests because every so often you'll end up with a scenario where actually what your product is doing is better than the competitor. Yeah. And what I've been on products that I won't name where we had a competitor and our goal was to be bug for bug compatible with our competitor. We had, okay. All right. So anyway, I think, uh, probably more on G tech. We're going to move on. Um, let's talk a little about, uh, so you want to talk about the culture of meeting. No, I talk about meeting. So two, okay. Now, uh, where was I going to start? I had a thread with some peers last night and yesterday afternoon. And I said, we're talking about, I asked some questions and I said, don't really know. And then somebody said, we should hash this out in a room because email is tough. And I'm all about getting out of email. And I did something that I rarely do. I rarely do. I'm, because trying to get in sync with this team and just a lot of unknowns. And I, you will rarely hear this phrase from me out loud or an email. I said, how about we set up a weekly recurring meeting to hash some of these things out? If I were next to you at that time, I would have stared at you like, what did you just say? So, um, a lot of people think meetings are bad. You're, you're, you're starting to grow up. It's meeting meetings can be bad. You'll be suggesting V teams next. I'm going to suggest punching you in the throat. All right. So what problem did you solve with this suggestion? Let me, let me finish. Let me finish. So, um, uh, the reason is, the reason I, it doesn't, here is coming from me is I think many recurring meetings are, end up turning into the, let's sit around the room where everyone does email and someone will basically, I've actually been in an org where people got in the room and, um, everybody got their laptops out there working along and they didn't know what the meeting was about. So someone said, did you read the email I sent? And they said, no. So he read the email out loud to the room. It's like the opinion is like, Oh my God, this is surreal. It should be in like, like an office space kind of movie. It's crazy. I think a good example of recurring meetings at work are daily standups. Yep. They have a civic agenda. You, stay on track. It's like these things happen. You're done. Uh, I think brainstorming that not really for a recurring meeting, but brainstorming meetings work well for meetings. Uh, status meetings. Nuh-uh. That Brad's shaking his head too. So now I'm like, no. So I think when you need to get alignment, um, and this meeting will probably work a lot like a standup meeting. It's weekly. It's weird, but where it's, I will gather agenda items off here. Here's what we want to hash out. Make sure we're on the same page on, um, and then try and end as quickly as possible. Cancel and we don't need it. So it's more of a synchronization meeting. It really is. It is not, not a status meeting at all. I refuse to even attend someone else's status meeting. Uh, unless I shouldn't say refuse, I avoid cause some cases I will go if I want to get information, but generally my experiences status meetings are a big waste of time. They do not make Alan happy. I know this from, uh, even if it's a call in status meeting, I does not make Alan happy. Here's the point. And you know, I don't want to come across as old meetings are bad. Don't want to go to a meeting. What I want to come across is if I'm going to show up at a meeting and everyone's on their laptop and you're telling me a bunch of stuff that I, I could learn just as well. If you sent them to me via email, uh, I, uh, I don't care. The, the fact that everyone's on their laptop is a, is a, is a death note sign. So let me tell it, let me put another spin on this. So recurring me, I mean, I was meeting, it's going to be great. Um, and, but you know, I, I will schedule these things when the necessary. So let me flip this around. Um, I've been trying to meet with, um, a manager and other groups to talk about some stuff and calendar, his calendar, my calendar is pretty flexible. I have some meetings on there, but they're all, almost all of them are movable. Uh, calendar is totally packed for two weeks. So I said a meeting for two weeks out, find a slot, come to his office, be great day of the meeting. Two weeks later, uh, his admin emails me. He's not like a, it's not a exec or anything, but his admin emails me and says, uh, so-and-so is really busy. Need to push this out two more weeks. And to me, I don't know this person or their org, but when you have, even if you're a manager, especially if you're a manager, I know managers do have more meetings, but when you are that, when you're packed in meetings all day long and for two weeks at a time and your next opening, it's a, it's kind of a red flag for me. I don't know if you have an experience similar. I'm just kind of curious if I'm the only one that, that that sort of sparks the, the bristly, spidey meeting sense. Oh, no, I absolutely do. And again, um, for those on, on podcast land, one difference between Alan and I is I am a manager and that is my life, right? How many of those meetings you think you are, you're there because you're moving the product forward versus you're there because you're expected to be there. Uh, I don't go to the ones that I'm expected to be there. I go to the ones where either I actually take a, an open space sort of principle, uh, where either I will learn something or I believe I can contribute. But even then, uh, because of my unique role right now in the organization, there's a lot of high demand for my attention. I actually think it's because I'm wearing too many hats and that I should conscientiously begin a process of delegating some of these hats to people who I'm trying to grow into leadership. And that's what I'm going to look into next. And it's thankful that most of my team doesn't listen to this podcast. Well, this is another reason why I think flattening orgs is a good idea. I think if you're going to have managers that need to be, and they're actually good meetings, but managing managers are going to be in meetings most of the day. Let's have fewer managers. Let's have more people getting work done. The not that manager work isn't work, but you know what I mean? No. So the, the challenge will be once in order to make that succeed, we have to flatten and get rid of command and control. Yeah. Right. The reason why I mean, shameless plug in my last post, talk exactly about that. You need to have flatter orgs help in a lot of ways, but you have to have the right managers leading those orgs, or they're going to fail. If I have to be in every decision meeting with that, that my team of 30 don't have a decision meeting, right? Then it's never going to scale. Right. Well, that's kind of what happens. The reason the rationale for all of these sort of high level status meetings is there, of course, the decision meetings. One of the things, and we have a mailbag item and we're almost out of time, but one of the things that reminds me of is a conversation between a friend of yours and mine, Mr. James Whitaker. Everyone listening to this podcast knows who he is, so I won't obfuscate it, but he wrote a blog post about a year ago, basically saying on the culture of meetings and just banned them all. And I actually- What? James being an extremist? Weird. I know wacky. It's totally out of his character. We love you, James, but we know you're too cool to listen to our podcast. So he's not one of three, for sure. And it was interesting because I pushed back on that and I said, you're going too far, man. And I walked him through a meeting. If it adds ROI to the system, it needs to be done. And I walked him through the lean principle around how you decide. So lean, the number one principle of lean is get rid of waste out of the system. And if we have a meeting that is waste, yeah, get rid of it. And I walked him through. He was like, well, how do you know? And I said, well, it's very easy. If you would do more of it if you could, then it's not waste. That's value you're adding to the system. If you wouldn't do more of it if you could, but would rather instead would do less, or it could do less, then it's blatant waste. But if you can't do less, then it's actually just the cost of doing business. Now, when I bring this up, we talk about stand up, right? Most times you could do more stand up, but you wouldn't, right? And you could also do less, but you wouldn't, right? Stand up is basically a cost for execution, right? It adds some amount of value. You don't want to get rid of it, but again, you can overdo it. So you don't need to have three standups a day. Whereas status meetings, I would argue for the most part, they're waste. Yes, yeah. Agreed. So what's interesting is you can have, it's not all meetings are labeled so clearly, but I like this model because you may walk out of a meeting and go, wow, there was some value there, but I don't feel like it was a great use of my time. And you can ask the question, could we do less of it? And you go, and here's a message that I hit the default meeting length seems to be an hour long. It's like, could we do this less of this? Yeah, let's try and do this meeting in a half hour now. One of the games, because I'm all about data driven now. So one of the games I do to keep myself sane when I'm in these meetings is I count the number of open laptops and I try to estimate how much that meeting costs Microsoft. That's a dangerous game. Oh, it's fun though. And if you're running a meeting and I'm gonna close on this, but generally, if I'm running a meeting, I try and engage people. Some people just are there to listen and veg out sometimes. Like if it's a, some meetings, some people have like something that comes up, they need to attend to over their laptop. And I'm not going to demand laptops be closed, but I try and make sure the meeting makes them want to close them or makes them need to close them. I went to a seminar by Eric Reese. Oh, yeah. It was pretty cool. And one of the things that he started off before he started talking, he's like, if you want to have your laptop open or your mobile phone, go ahead. Because I interpret that as feedback. Very good. Yeah. And my laptop was open the whole time at GTAC because I was, I was tweeting things and they even gave me, I got, I got a special Seattle GTAC hoodie because I was one of their top tweeters about GTAC. Okay. So yeah. So I had lots of up for that, like till I was listening and then taking care of some work stuff at the same time. So do you want to, is it time? It is a while. It's time. Hey kids, you know, it's time for mail. All right. Two things from the mail bag, one short one, one long one. The short one is, uh, someone asked in a while back and I'm sorry, I didn't find the comment, but someone asked, Hey, what software do you use to record your podcast? And I think as far as I know, 90% of Lisa, the, the amateur podcast or what's below amateur, I don't know, uh, use, uh, uh, freeware software, um, or, uh, called audacity. It's great. It works for, um, it scales really well. You can record whole albums on this software. It's, it's really great. And is that what we're using? That's what we're using. Yeah. Called audacity to search a UDA, C I T Y. So that's, that's taken care of. The other one is, uh, last time we talked about, uh, feedback, your evaluation and feedback. And Brent gave me this, uh, exercise he uses when he says, um, you know, how would you rate this, uh, what I did on a scale of one to 10 and what would make it a 10. And then Neil stud wrote in and he says, thanks for a great episode guys. You're welcome. Yes. Go Neil, et cetera, et cetera, et cetera. Good information. You read the comments on angry weasel.com, whack AB testing. But he says my only concern with Brent's suggestion, what would they need to do to make it a 10 is that when scaled up amongst many reviewers, it could result in a laundry list of people's tiny flaws, which could be, would be depressing to receive, though I would trust the manager to filter and collate the list down. But how do you deal with that potential, uh, outcome? So I have never actually encountered that, the feedback mechanism. Um, so Neil does point out something that's, that could be very interesting because usually when I do this feedback, uh, paradigm, I'm doing it face to face with the person and they're not thinking about the, well, other than Alan, uh, they're not thinking about the, the little minutia, um, aggregated up. Yeah. That absolutely could be an issue. And, and I would rely on the manager to do exactly as Neil suggests to, to collate that and to discover the patterns out of those individual things. The, um, the other issue is it's very rare to encounter someone giving, giving, from my experience, giving a number less than four. If you remember the, the process, they come up with their own number and then they have to, um, communicate what could have been done differently in some sort of time range, uh, to move that number to a 10. That first number isn't so much important except for that it, it, it communicates where they're coming from. And a lot of the times a four really means, yeah, this guy really irritates me, but we're talking and I don't want to come across as this guy really irritates me. A five is almost always average and a seven is, you know, just slightly above average. And people have their different interpretations on that as well. You know, we get these, uh, surveys that say, you know, did you, you know, blah, blah, blah, dissatisfied, very dissatisfied, dissatisfied, neither satisfied or dissatisfied satisfied, very satisfied, typical five point scale. And there's some people, um, who will never put very satisfied because it always could have been better. Yeah. So, but, but some people will go, it was, it was pretty good. I'll say very satisfied. So a lot of variation. And so that number one, that one to 10 scale, uh, without any, since it's purely subjective, there's tons of variation. So yeah, it's about the discussion and the points, not about the score. The points afterwards, the second bit, it's about taking the feedback and deriving it into something that's actionable. If you're, if you guys remember the phrase, it's basically what could I have done differently in order to score that 10? Whereas, um, so one of the positive thing is that that kind of filters out the subjective, right? Hey, Alan needs to stop being so angry, right? Well, how much non angry does he need to be? It, it about three beers worth. It translates the feedback into something a bit more actionable. The other thing that I would suggest though, as well, is if, if, if you use this technique or, or let's say we operationalize it in some pure feedback form someday, um, just because the person gave you their opinion of what you could have done doesn't mean it's valuable, right? It's, everybody has their opinion and it's priceless that they gave you that feedback, but it's still up to you, uh, the person, the feedback receiver to decide what they want to do with that feedback. And you may decide, you know what? Yeah. If, if I were to satisfy that guy's criteria, I'm going to have to give up something that I truly value. So I'm just going to have to live with, um, that consequence and it's all good. Of course. It's always all good. Right. Guess what? Are we out of time? We are. That is the, what I was having you guess. Nice guess. Congratulations. Thanks for the mailbag items. Keep them coming. We love those things. Yeah. We'll talk about anything, right? Yeah. As you can tell. All right. Thanks everybody. I'm Alan. I'm Brent. We'll see you next time. Bye. 
