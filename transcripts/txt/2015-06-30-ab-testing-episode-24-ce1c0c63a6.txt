Good morning everybody. Hello. Hey, we are back for the Jack Bauer version of A-B testing. Jack Bauer, 24? We're gonna do this for 24 hours? Episode 24. Oh, I get it. We're gonna do it for slightly less than one hour because we're not as hip and cool and as awesome as Jack Bauer. No, but I'm fairly certain we could do a 24 hour session. Not that I want to. No, the days of me staying up for 24 hours are all done. Why does Jack Bauer never sneak a catnap in there or a bathroom break? He's got 24 hours, man. He's pumped on adrenaline the entire day. He's a rock star. But you're up all night. Reminds me of a situation just last night. This doesn't work very well for the podcast, but let me show you a picture. Oh my God, don't show me that picture. No. Oh wait, there's a picture. So this is my younger son. He's fallen asleep at the computer desk sitting in the chair. I have never seen him do this before. And last night we signed up. We signed him up for the Khan Academy introduction to a JavaScript program. All right, so I was gonna ask you. It's cranking out some code. Yeah, it's like, oh my God, I totally remember those days. Yeah, just code till you can't sleep. All right, dad walked out. Do we mean today? Do we set our world record for the quickest time to hit a tangent in a podcast? We've done it before the podcast has started, so I don't know. All right, we do our best. Hey, so do we introduce ourselves? I'm Brent. I'm not, because I'm Alan. And we're here for episode 24 as you figured out. And shall we get started with our list? Yeah, so have you done anything interesting? Those last couple weeks? No, I sat around. Hey, I just, not just just, but just, just got back from Canada. I left on Sunday up to Vancouver, BC. Sat in a long line at the border, got to our hotel, took the whole family up. How was the border crossing though? It's not too bad. It was about 40 minutes going across, but it was the end of the weekend. Maybe 20 minutes coming back, not too bad. I haven't been to Canada in like 10 years. I like Vancouver. It's nice. We had Monday off. What did we do? We sat by the pool and did nothing, I think. I took a little bit of time to tweak some presentations because on Tuesday, I gave a three and a half hour tutorial covering a variety of topics around building your testing toolbox. And then went out and had dinner with the fam. And then Wednesday was exhaust Alan day and a keynote at 8.30 in the morning. Had to get there at 7.30 for a little sound check. That the keynote went really well. I had a good time. I think the points really resonated and it hit a lot of the concerns or the gripes people had. Or it could just be that Canadian audiences are just very, very polite. I'm not sure. They're known for being polite. Yeah, but it was a good time. I really enjoyed that talk. And then had a little bit of a rest while I quickly tweaked my slides from my inside the mind of the 21st century customer talk, which was really just a deep dive into a couple of the things that I talked about in the keynote in the afternoon. And weirdly and oddly and probably disturbingly, there were some people who went to all three of those talks. Like at some point I get sick of myself. I'm not sure why you would, I have people come up to me and say, hey, I'm sorry, I didn't, I really enjoyed your tutorial and keynote. I'm sorry, I didn't go to your other session. I said, if you would have went, everyone was there that I saw already twice. I was a little creeped out about, so that's cool. And then if that wasn't enough. I guess it's an important lesson to mix up your content. I suppose, I suppose. Yeah, something new to the people. Yeah, hindsight's 20-20. I had a little bit more time and then I was part of the Lightning Strikes the Keynotes where I gave a Lightning talk, five minutes, which I put together. Lightning talks started at 4.15. I have my slides completed at four o'clock because I started them at 3.48. Made it quickly, made a top 10 list of- You didn't publish your Lightning talk. No, because they were silly. I did, people tried to be, I had no offense to them. Some people were very serious in their, trying to get a real message across in five minutes. After having given a tutorial, a keynote, and a track session all within 24 hours, I decided that I would do something a little lighthearted, did a top 10 list of things I heard at Star and just had some fun. Had some good things in there too. What was number one? Number one, is test dead. Okay, what was number two? How far down we had to go to get to something that we haven't heard before? A lot of things we had heard before, but reiterated some of the talks I had seen, some of the conversations I had seen, plus a few fun ones thrown in there. Not important, don't wanna dwell on it. So went out for dinner again with the family and that was Wednesday night. And we got up Thursday morning, had yet another awesome brunch at the hotel and drove back and dropped me off at work, around two-ish, and tried to get caught up on the day job, which I've, I just started this job and I think. Wait, let's not go into that one. I'm not done with Star Canada. Well, we can bounce back. We can play a little ping pong. I spent the morning and I went through your slide decks. Brandt, my post editor. Sure. Well, we had the... Let me just tell you, for those of you that are not watching the video feed, Brandt has notes written down that he's taken while reviewing my slide decks post-presentation. For now, he will grill me on some questions on why I used a non-serifed font on slide 33 of my keynote. No, I thought serif was an excellent choice for slide 33. All right, excellent. First and foremost, I'll say your slides were gorgeous. I make beautiful slides. Yeah, my absolute favorite were the ones where you were, you didn't have the text in your notes, but based on how you and I have talked about on AB, it was very clear the message, and the one where you had the picture of the Avengers and the champions of quality, followed by the last defense, followed by the janitors. I'm like, I could talk to these slides. I have no idea. Right, the whole line there is like, we are the champions of quality. We're the last line of defense. Do you know who else is the last line of defense? These guys with shovels and buckets. I didn't understand, you had one slide that had Bieber on it, and I'm like, I don't understand the Bieber reference here. Well, let me tell you, Justin Bieber is Canadian. Oh, that wasn't the point. The point is, is that whole slide, I had two slides just trying to emphasize, not just how, we've talked here about how much has changed in the way we develop software since I wrote that book. It's been dramatic, but that's happened for, it hasn't happened because of fate. The world has changed massively in those last seven years. The iPhone came out in 2007. Netflix announced streaming in 2007. Israel, I started writing the book in January 2007, Justin Bieber was not around. It was a world without Bieber. It was also a world where Blackberry was relevant, and I made this crack, and on that slide also is the Blackberry, and I said Blackberry was relevant, and it got a lot of laughs, and then I felt bad because I found out later there was someone there giving a talk from Blackberry. Ha ha ha ha ha ha. I'm such a shithead. Hey, you know what? The other thing too, like on test is dead. It reminds me, there's one of my mentees that I've been helping with in the last couple of months has had an issue where they are in sort of a real data science team, not one of these things that you see in Windows. Not one of the ones with air quotes. Right. Right, okay. But they're doing mostly data engineering work right now, and that's not their talent, and it looks like they might be on route to not receiving a positive review message. Well, let's just do it without. And so I've been working through, and the question around is test dead? This is what triggered this thought, and then looking at your slides and knowing sort of what your message was. It's one of those things where test is not dead, testing is not dead, but I firmly believe testers are dead. Well, somebody whose role is entirely that, and there will be people, I'll leave out names because they'll get all huffy, but there will be people who will vehemently disagree with us, but I think they're wrong. It's a probabilistic discussion, right? Cobalt is not dead. True, true. But it is. Well, did you catch, one of the things I'm really, I'm gonna blog, the next thing I'm gonna blog about is, it's one section of my slides where, I'll see if I can walk through it. There are people, and for a long time, I believe this too, software was made by this sort of triad of people. They were mostly, they were developers and testers and some sort of PM, whether that's project manager, program manager, whatever. Those three groups make software. And they are exclusive, like you do this, you do this, you do this. What else would you need? But more and more we realize, especially as there are no titles, it makes it easier to see, or not no titles, a single title, is that there are a dozen, or two dozen, or 50 things, 50 skills you need to make great software. So that's actually where I have the pie charts of showing dev stuff, PM stuff, test stuff for the remainder. And that's not really not true. What's really true is this pie chart that shows 20 different things of things that go into making software. And part of those are tester skills. But if you're gonna say, if you need to have a software tester, you're also gonna have a, you're gonna have specialists all over the place and as you know we believe in specializing generalists, generalizing specialists. I think tester skills absolutely will be needed forever and ever and ever and ever. But that may not be what you're called or what you would do all the time. No, the thing is when someone comes up to me and they ask, is test dead? Generally I've noticed that what they're really doing is they're saying, hey, is my career screwed? And unlike what you do in our approach when these type of things, what you said in your slides, I agree, because you said testing is not dead. But the thing I want these people to understand, if listening to Ellen's message leads you to have hope that you don't have to do a career change or consider it, you're wrong. Well, when I talked about the pyramid, which was also in my last, this is something I brought up in our- You understand my point there? Yeah, I do, I do, I do. And I'm going somewhere with this, just ticking along the way to get there as usual. When I interviewed for my current job, which we'll talk about a little bit, maybe later, I came up with this idea of this pyramid of kind of where quality comes from. And then it's kind of resonated with me. And so I drew a picture of it and put it in my keynote. People came up to me and said, that picture resonates, can I use that? And it's not perfect yet, I want to work on it. But there's these levels of culture and functional quality and moving up to the top where you have non-functional requirements and then customer connection, the abilities explored to our testing, things are a little bit more customer connected at the top. And I talked about the role testers will play in, actually the role testers will play in that pyramid. And at the bottom, it's functional testing, it's advisement and code review and making sure helping developers, implementers, I should say, implementers, write better tests. I agree that when people here is tested, they worry about their career. And I think if you're a great critical thinker and a great systems thinker, you have long-term employment in software. There are testers I know who just write functional tests all day. Those people perhaps, if they don't want to be an implementer, a developer, they want that to be their job, they perhaps have something to worry about. Because I don't believe that will exist. I don't think it's perhaps at... I'm being nice. I know, I know. The thing is even, and I agree with the world that you're weaving, but even in that world, there's not going to be room for all the people that were at Star Canada. It's the cream of the crop are gonna be the ones that hold those pure testing positions. If you love functional testing, then you better be the world's best functional tester. And I literally mean the world's best functional tester if you want to keep that job. In my head, I pictured a title on LinkedIn, Functional Test Coaching Consultant. Yeah. But who are you consulting? Right, that's the challenge. I don't know. So anyway... It's not like we don't have enough testing consultants today. Right, I know. And enough... Wait, we don't have enough good ones. All right, so anyway, I thought the message was received pretty well. People got that, oh look, test is changing. In fact, even when I was done, and I picked on Lee during the keynote, Lee Copeland, and Joe Ville, it was fun. Lee's a good guy. I know, he's fun. I made a comment about when I said Microsoft didn't have any testers, so I started talking about no testers, and I'm saying this at a test conference, and I said, Lee, you know why I'm saying this? Seven or eight years ago, when this book came out, I really wanted you to give me a keynote, but no, you waited until I wrote a blog post saying that the book was out of date, and then you said I should give a keynote and talk about it. So to get back to you, I'm gonna come back to your test conference and tell you that we don't even have any testers. What do you think about that, Copeland? What did you say? He laughed. What else is he... But even afterwards, when I gave the message, and people, everyone I talked to afterwards, they got it. They got that test is changing. And Lee came up afterwards and said, and acknowledged the Whitaker is test ed talk from four or five years ago and said, but what's really important there is that testing as we know it, dead. It's changing. And people get that. That's cool. So I don't know what that means, or I don't know enough about the jobs all these people do. I think about half the people there are sort of in that test development role. Half of them are in that exploratory test, functional test role. I think I don't have a statistically significant sample. The test discipline, whether or not it's dead, it is much better to default than it is dead. That horse that we're beating is not only dead, it's decomposing and smelling up the room. I know we have a lot of people who actively listen to us. No, we don't. We used to, but then we started talking like this. They still listen because it's a teaser. They hope that we'll get to something interesting any second now. Eventually they're gonna say something really cool. Come on, come on. And then that outro music starts and they go, man, maybe next time. So there's a bunch of things that I wrote notes on. I'm not gonna torture you on all of them. Oh, thank God. I had written down, you had one slide around let's play a game. And then I'm like, what the hell is this? War games. I got one through the other slides and I'm like, oh, okay, this is fun. And I got every single one wrong. As did, I did it in the room. Actually I did that, say I've done that three times now. I got to get rid of those slides. I stole those slides from our experimentation platform team, who the leader of when we had one. We still have one. Just our buddy doesn't lead it anymore. Anyway, I stole those slides. I used them in a keynote at Star East when I talked about years ago, talking about customer data. Back in the days when it wasn't telemetry like you and I used today, it was all about Squibbon Watson and some other things. Send a smiley, send a frown, et cetera. And in a room, that keynote was about four or 500 people and I got, I stumped all but three with those three questions. So. How many time do you get them to consider? I give them 10 seconds, so. Sure, that's enough time. That's plenty of time. And again, not looking at the slides, the whole idea is showing them an A-B test and letting them take a guess at which one they think is better. And then we show them what the data showed. And the whole idea is between these three A-B test samples, we can show them that as much, as great of intuition as you think you have, you are not the customer using customer data. Much better way to see what's a better choice. And it's much faster. There was one where you said that one result was 64% better. Yeah. Right, that 64% better. It doesn't take much work to equate it to actual dollar signs. Sure. And one thing to keep in mind that before going too deep, and you know this, is we're really lucky at Microsoft and because of our size is that we can get a significant sample size in order to make these choices very quickly. Whereas some other companies may not. Yeah, there was one comment you made around the sample size. And one thing I think a lot of people don't realize is that depending on the data set, you don't need that large of a sample. The point is to be careful. And that, what you're talking about is taking from a post I found from someone on the internet who had done, he had done AA testing, because he was lazy. He used the A-B testing tool to just test the one version of his email and was able to get statistically significant according to the calculators online, differences between the two emails. But when he looked at it a little bit closer it was his sample sizes just weren't big enough. When you have a small sample size, then you- Your confidence level's low. You have to publish it along with your confidence intervals. Absolutely. And that's what's missing a lot actually the tools they use, I forget whether we used Google's tools or someone else's, but those confidence levels are critical. And they never, we're used to a world where they don't publish those confidence levels along with likelihoods. We see that in elections every year, right? Well, we see, no, if you pay attention, almost every poll I see says, hey, there's a 52% chance that so-and-so is gonna win, plus or minus 17%. I'm like, okay. All right, what else did you wanna ask me about before we go on to something more interesting? So I'm done with this one. So you've now got your new job? I do and so- You sent me a mail saying that you were all jacked up. Yeah, there's- It was a kick-ass job? Yeah, it's going well. And I only feel bad that I had my first full week on the job and then I took three and a half days to go to Canada to this conference, planned in advance, and lots going on, very busy, nice high-pressure stuff. As I mentioned before in the blog cast, the blog cast podcast, this is the first time I've ever worked for a manager I didn't know before. And he's funny, he's funnier than crap, but he's pretty intense. So I gotta figure him out. It's gonna be an interesting time. Okay, I can help you with that. That's all I can say. We'll help, next time we go to lunch, we'll help you psychoanalyze your boss. Oh, I love, yeah, I'm psychoanalyzing him already, but he's pretty good, he's pretty perceptive. Lots of stories that I probably shouldn't share here yet until I get to know my team better. I got a spare strength book in my office you can take and get him to take. All right. If you want. Yeah, I think going to, strengths would be interesting with him. I could probably predict him, but as far as insights go, he's the reddest manager I've ever worked for. That's good and bad. And I'm not even sure if there's another color in his spectrum. Oh, interesting. He could have a little blue down there somewhere. Well, you hired you. Yeah, I don't know. Yeah, I don't know. So lots going on there, probably a lot I shouldn't talk about yet with an unannounced product, but I'm getting some good high pressure work, good visibility. Well, why don't you talk about the type of work you're doing? As I've mentioned before, for people who missed the last podcast, I'm on a team of about 70 developers. I'm the only, I'm the quality guy. Doesn't mean I do all the testing. It means I, for one, kind of look at the testing that's going on and help and audit and coach and correct. And right now our developers have what I'll call a reasonable set of tests, but not a very good one. I'm working on helping them improve that. In fact, I have a meeting later today to go over that. One of the things we're going to fix is right now they have a set of, and this is just mindset things. Because I look at quality pretty holistically all the way up like the pyramid I talked about. And they have this set of tests that run at Check-In for CI and then to make sure we promote one, a build from our sort of, we have two versions of our website. One is not two versions, but we have sort of a staging one and then the production one. Things kind of bake in staging and then once a day we promote those two, our regular one that more of the audience is. It's unreleased, so production isn't actually production. Hackers out there, you can't find this site. Absolutely, absolutely not. Anyway, they call this whole thing the quality pipeline. I think that's overstating things a little bit. Let's find the new word for this over time. Right now, we have a review today and a lot of the metrics that were prepared for this are around test case counts per area, but you and I know are really. Oh. The leaves. The low hanging fruit is ready to be plucked from the tree. I look forward to the day where I, there's like a good couple of years where I hear that statistic. Well, and it's all over, and again, the reason is of course, the educated listeners of this podcast know that all test cases are not created equal, area sizes are different. I'm gonna ask you, actually, I have some ideas for this, but we're gonna move away from test case counts because it's not a good way to measure anything. It may as well count Arm Harris as a measure of quality. So, what would you count instead? If you wanted to, and again, maybe count nothing, but what's a good way for you would use, and I'll give you my answer in a second, to show status of sort of this pipeline of test quality and product quality going through, at least as measured by unit and developer tests. You're adding constraints to the problem, but I'm gonna ignore those for the second. Of course you are. The first thing that I would do facing that, and it's actually very similar to what I'm doing here, is establish what's the goal we're trying to achieve. Right, and the term that I've been using here is operational velocity. I know what that means. So, you're a system guy. What we're trying to do is get production-ready code in the hands of the customer as fast as possible. Yeah, and you've heard me mention before that for a long time I've considered my job to make that build, measure, learn loop faster, and that's what I think you're saying. It is. All right, so go on. But one of the challenges. I just wanted to pat myself on the back that I actually acknowledge to our listeners that we actually agree on something before you continue. Yes. I think I saw each of your slide decks had a reference to that. I know. I referenced that in my third presentation. Sorry, I duplicate one slide. I think that was the only one I duplicated in every presentation. Okay. And I think it's critical. Now, the situation in your case, because if we wanna do production-quality code, the first thing that I get people to realize is there are multiple cases where it is faster to achieve that goal of production-quality code in the hands of the customer by reducing what you test in a preventative fashion and enhancing your telemetry and optimizing for being able to react fast. So think of it even smaller than sort of a QFE class bug. The things that we want to prevent are obviously catastrophe-level bugs, but most people, you'll discover that that's actually a smaller subset. I am now of the opinion, if you don't know, let it go through. But instrument the crap out of that and be fully prepared that if the results of it are falling on the wrong side, your team has to be able to react to it and fix it within 24 hours. 24 is a long time. Okay, so most of the teams I've been talking to, they go, 24, we can't do it shorter than seven days. So I agree with you. It depends on your content. All right. But that KPI, and what you're talking about, your pyramid, right? So the first thing that I would do is I would say, okay, now, how do I define production quality code in the hands of the customer? I need someone to play the role of the customer. Like in your situation, it's a V1. I don't know if you can do this, but the first thing that I would do is I would engage with MSIT and find a way to flight this within the company itself. That will happen shortly. Bolster up, then create a KPI that says, number one, what is our cycle time? How long does it take, so when somebody has an idea, to get it to the point of production quality code, including if we have a testing pipeline, or if we have to react to bugs to get it there, that timer doesn't stop until we go, yes, it's now production quality, it's making the customer happy. We wanna know what that cycle time is. We then will also wanna know the throughput of getting new releases out. Because we don't wanna create a, we don't wanna over-entrench around this preventative model and realize that the consequence is that we're only creating a new release every two months. Because that makes the build, measure, learn loop too slow. Way too slow. So it's really a first thing is really around establishing those two or three key KPIs. There's one that I'm forgetting right now, but what they're called are tension metrics. Where you, for example, if we had a thing of saying, hey, we need to get deployments out as fast as possible, right? Well, more than likely, given most engineering teams, they'll achieve that, they can get a new deployment out every 15 minutes. It's gonna be crap-tastic. We have to have something that counter-bounces that says, nope, we're gonna measure the time it takes to make it non-crap-tastic. And the magical third one, I wrote a deck on this, I can send it later. But by creating these tension metrics such that the only way to make all of these things go good is to change your behavior and measure where you should optimize. Pre-production, post-production. Winding back to, so let's say we don't have a post-production model, right? Again, I can't emphasize enough. If you don't have a production model, it is my opinion that that is the first thing you need to fix. It is way more valuable than building up another unit test suite. Can you describe a little bit more what you mean by post-production model? You need to have your efforts validated by as close to real-world usage as you can. Okay. Right, this is along the lines of what we were talking about from your deck before. Sure. You gotta get rid of this champions of quality. No, the customer is the champion of the quality. And we're clueless. So the model I used to say in a prior team that was very effective is we don't, it was a team motto. We don't know what we're doing. Right, it's intent, we know how to find out but its intent was to block any sort of assumption. Assumptions, particularly when you're in the data space, are critically dangerous. So you have to suppress intuition on conclusions. You can have an intuition around hypothesis to generate and those are extremely valuable but you have to suppress intuition on conclusions. Did I get it in the weeds? What was your point? Yeah, you're, you're. I am weed-tastic. I'm gonna get out my weed eater in a second. You asked me a question. So we were talking about post-production model what that looked like and that was the customers. You have to get to the point where the, you have a signal generated from the customer. Here, my team, I'm calling it the outside in perception of quality. A story. We measure, in my current team, we measure downtime events. We have a contract. So my team is Azure Compute. We're a VM in the cloud service. And when anyone signs up, we have a contract around our service level agreement. How much we will accidentally knock your VM offline. And all of our competitors had the same thing. Now, we had a big incident the other day where a customer was claiming that they were out for nine days and we measured it and we could only find 20 minutes. Now, one of the, the, the junior level guys that I was interacting with was just like, we're right and wanted to go fight the customer. And I'm like, hold on. Like, this is why you're not on the call. And number two, I want you to step back and think about it. Let's assume that the customer is right. What possibly could be going on such that they are perceiving that they're out, that's different from what we're measuring, right? When I talk about customer quality, it's, we're trying to snap to creating a signal that aligns with the actual customer's perception of what's occurring. That's all in the matters. And so we came up with an algorithm. We haven't executed it yet. We're still looking for where the data is. But I firmly believe as an example, like Azure Compute is multiple different services. We own the VM infrastructure, but the VM infrastructure, it's just the VM infrastructure and the networking infrastructure and the storage infrastructure, you need all of these to have a successful cloud computing experience. And they're all different teams. The my junior guy that I was talking about is on the compute side. And I'm like, here's a hypothesis. If the network was out during that time, is it possible that the customer would perceive that his system is down, but we would still have the data that says that the VM was up. He's like, well, yeah, actually it would. Okay, what if AED was down, the Azure Active Directory? Network working. But the thing that manages identity, person can't log in. What about that? Would they view it as potentially their VM was down? Well, actually, yes. Okay. Guys, you were talking in my head, I'm brainstorming through ideas. Well, there's a bunch of different ways networking could be down or logging could be down or the IP address could have changed and they're accessing it, all kinds of stuff could go on. Right. The path that I'm on right now, I engage with our portal team. And so the customer only has a few ways that they can interact with our service. They can go through our portal, or once they've created a VM, they can just do a direct login. Okay. Starting with the portal team. What I want to look at, the only, there's a common behavior. If the customer views that the VM is down, what we've noticed is that they go do what's a CIR. They log into the portal, they find the VM, they look at something, we don't know exactly what, but we have a whole bunch of smart guesses. And then the only tool that we give the customer, if they can't connect to their VM, is to go to the portal and click the reboot VM button. Now, the thing I want to do is go, all right, let's look at all the customers that hit that button in the last two months. Now I want to look at the sequence of 10 things they did in that session before they hit that reboot button. And I want to aggregate those actions, regardless of the order, I want to aggregate those things and I want to know what are the most common things that they do just before hitting the reboot button. With the hopes that that's going to give me a clue of what are they looking at to come to the conclusion that their VM is offline. What do they see? And with the idea that that's going to give me a new signal to operationalize through the data set and make my measurement of the subjective point of view of the customer significantly more accurate. I think I actually did finish, I've forgotten it again, but I think I managed to ask you. Yeah, I think so. So pre-production. Yeah, my answer, look, to cut to the chase here on my side is yes, I think the goal is to increase throughput, how often can we get quality code to our production site. And part of that, it is a balance between, I do think there is some learning and some growth to do on getting developers to write quality code and get that up to the site. So even our, I believe even our sort of staging server can be higher quality all the time. So I do want to put some emphasis on having good tests in there, pre-prod in there, but you're right. You do need to balance that with what, having real people use it, whether they're exploratory testers right now or customers eventually being able to get that data. And I want to loop this back to, I don't know if you caught this in my track session when I talked about 21st century customer and it was really, the short story is, you need a balance of qualitative and quantitative data. And at the very end, one of the things I sort of drove home is that in the old days, we kind of, we had the ship criteria checklist. We go through, these are all things ready to ship. Best thing is getting your product ready to ship. But what's really important today is not that just your product is ready, but that your product is ready to be used and in your allies get that data and your team is ready to get that data. So it's more than just the product itself, high quality. You have to have the ability to deploy and analyze and update quickly to your customers. There was another quote that you said somewhere. I don't remember exactly how you said it, but I think one of the key differences between the, honestly between the world we left and the world today, it is significantly more important that you're building the correct product than to make sure that your product's correct. Yeah. And correct in this case is. Correct for the customers. And a quote I use all the time is it's, in fact I tweeted this, but I said it in my talk down on a slide, but it is perfectly possible to use good engineering. I could use XP, maybe not, but bad example. I could use some pretty good engineering practices to make really good functionally correct software that nobody wants to use. Yes. And this is one of the reasons why I try to get people to not overbalance about preventative. But the thing is, it really doesn't matter what you start with. As long as you are measuring those KPIs at the end, and you have a learning loop that you can measure where to do next. You need to start with a baseline and then determine, hey, you know what? Looks like we're overdoing it on the preventative side, and we need to push towards the optimizing morph reaction. The last thing I'll say on this particular topic, I doubt it. And let's show something else. It is critical that your team has the idea and the muscle that every build is RTM-able. There's a team that I walked through about six months ago on how to scale up Agile. And I said, look, if you wanna make your life way less complicated, what you need to do is decouple engineering from the business decision from shipping. And you make it very clear. The business guys, it's 100% our call. We could ship any build we want to at any time. Engineering gets no vote. What ends up happening when that happens, engineering's like, holy crap, we gotta make this RTM-able all the time. Yeah, there's a story I can't tell you for our listeners. Sorry, I'll catch you up again, I promise later, but remind me to tell you after we're done. Shh, it's a secret, secret stuff. Hey, I'm gonna skip your thing. Okay. Because the clock says I have to. So. Fair enough. Carl, what is that sound? Do you hear that? Mailbag! So one of the things that we have today is Alan has forgotten the mic stands. And it's interesting because he did sort of a little singing lounge thing with the mailbag that time. That was actually cool. I think next time you do it, you should take off the. And I should stand up and do a little crooner pose. Yeah, can you do a mailbag like Elvis? No. Well, practice that, next time. I can do mailbag only like Alan. All right, fair enough. Hey, so what's the mailbag, Alan? I wanted to quickly give a shout out to Nicholas Straw on LinkedIn, who gave Brent and I a shout out for introducing him to Conway's Law. As mentioned in episode something, I don't know, a few episodes ago. But anyway, thanks Nicholas for listening and for mentioning us on LinkedIn, very cool. And then. And I love the fact that he mentions that he's one of the three. He is. There are at least five people that are one of the three. The one of the three brand is slowly building. Yeah, you know what, we should get, oh, you know what, we need to make T-shirts. Like AB testing on one side and the back that says one of the three. Yeah, absolutely. All right. And go into production very soon, as soon as I remember. So the other one is, I'll tie this into a quick story from my past through Star Canada into the mailbag item. So once upon a time when I was in engineering excellence, I used to travel the country in the world teaching courses to people, Microsoft people. And I visited just a hilarious, a really fun group of testers at Microsoft's former office now in Long Island. Okay. I had gone out to speak at a conference in New York and said, hey, well, I'm there in New York City. I said, well, I'm there, I'll take the train out to Long Island and I'll teach these guys a course. So I didn't. I didn't even know we had a sub in Long Island. We did, that's where forefront was. Oh, okay. And anyway, I had a great time, fun guys. And then one of the guys, Mark, was at Star Canada. No! But another guy I met out there, Nathan, who left Microsoft a while back posted, and you'll dig this. On the board, I just wrote, we should talk about monolithic test systems, but he said, another great episode. He's also one of the three. I left Microsoft four years ago. I remember that the Windows org, I was gonna say redactive, but what the heck? Windows org was pushing this tool called WTT across the entire division. In the context of this podcast central org theme, how does WTT fit into the discussion? Is it still being pushed as the way automation needs to be run in the org, or has it finally been dismantled? Are other orgs continuing this longtime trend of monolithic division-wide automation systems? Actually, a very good question. And do you want me to go on and answer? Do you wanna jump in first? 21, do you wanna take this? It's actually interesting. It's actually interesting. Otherwise, we wouldn't cover it here on the riveting A-B testing podcast. Yeah, I know, I use, I overuse interesting. It is an interesting adjective to me. I have come to refer to that particular tool as part of the Windows Test Framework, otherwise known as WTF. WTF. And I am very happy to say that the idea of really bad, stupid, monolithic test systems that don't solve the problem that they're created for and create an additional burden on the testers who have to use it seems to be gone. Oh, man, going away. There are people that will hold onto those things. But monolithic test system is still around, right? And it is growing, but it is now in a different context. The things I've seen coming out of the VS guys, the things I've seen coming in from open source. What I find is what's happening is people are choosing systems now instead of the systems being chosen for them. And as these systems are actually gaining customer following, the required amount of flexibility, the required amount of simplification, the required amount of awesomeness for this type of thing is actually causing convergence versus a forced convergence. So I would say monolithic stuff is still alive and well. It's just the command and control aspect. It's alive and well in a positive fashion. I'm gonna add one more twist. All that's true. And we actively went out, my last group, which was part of the operating systems group, which is largely made up of Windows, we actively fought and avoided using WTT for many of those reasons. Did I ever tell you this story? I was on a team, but so when WTT was first being constructed, very small team, okay? The architect did something very clever and I liked his political strategy. What he would do is he would go around to executives and he would socialize what they're going to do. And the executives would be bought in. Oh, this is fantastic. Great, I need you to contribute resources so I can build it. Okay? Now, so I reported to an executive at that time. He came to me. I went and talked to the architect. I have a used car salesman filter. It doesn't work on me. Okay, that's great that it's shiny and polishy and it'll clean Windows as well. But these are the things I care about. Tell me how it will do about that. Now, at that time, I owned a kernel mode driver and one of the biggest things, in other words, I was on a system where if my devs did the wrong thing, it would cause a blue screen. And WTT at that time wasn't resilient to that. I'm like, okay, so I crashed the machine. Where's the log stored? Oh, it's on the machine. Yeah, but I was on the storage team. I'm like, I just took out storage. So are you saying that I just lost the steps that caused this blue screen? He's like, yeah, I think so. Okay, how do I work around that? Oh, well, you have to wait a year. Okay, now my VP, he was getting pressure from his chain of command on doing this. And I said, okay, here's the deal. He wanted me to go into it in some fashion. I said, here's the deal. I will pay for a vendor and I will send that vendor over to WTT. Okay, and a vendor cost me 6,000 a week. He was expensive, but he did all of the database design for that system, okay? And my condition, if I do this, I don't have to use it. Okay. All right, so what I was saying before you told your story is I've forgotten. Oh, I think everything you said about it earlier is true. I think people are moving towards other tools and stuff's not being force fed. One other twist that I've noticed is because in a lot of these monolithic test tools are now sort of owned, or at least the running of them is owned by the implementers, the developers, as they own more and more functional testing, they're not standing up for some of the things we accepted perhaps too long as testers. They're saying, I'm not gonna use this. It kind of sucks. This is way too heavy weight for what we need to do. And again, if all you're trying to do is prevent every bug and find every bug before, and we talked about that balance earlier, they're really geared towards let's run hundreds of millions of tests that take weeks and weeks, and customers will still find bugs because- We don't have the time for that. I know, I know, I know. One of the most critical things, and I didn't see it called out on your deck, but in terms of the 21st century customer, I call this out every time I do a similar deck. There's a business imperative today that didn't exist 10 years ago. And that is for all of these things, test tools, your team's product, my team's product, the switching cost is so cheap. Because the customers are just different. Customers are different. I don't have to take your crap. There's 10 more versions of you out on the web, and I can go with one of them. And it is so easy for me to just try all 11 of them. I just saw a tweet from somebody I know those last couple days, it says, hey Uber, your app's buggy, so I'm switching to Lyft. Nothing about the quality of the service. The fact that the app's sucked, they're gonna use somebody else's taxi service. Yeah, and it, like that. Yeah, it's like, oh. Everybody's like, hey, try the free trial. It takes you three seconds to create an account, log in, try the free trial, make a decision, and go to the next one. Exactly. That on-prem mentality where once we're locked in, it's gone. Customers are fickle. Yeah. All right, Brent, the clock tells me that it's time for you to shut the hell up. I'm shutting up. Except for this part. Except for the part where I say, I'm not, I am, I am, who am I, I'm Allen. And I am Brent. Thanks once again for listening to episode 24, and we'll be back with you soon. From the offices of a floor 24 in Lincoln Center next time, right? Maybe. Okay. Maybe. All right. All right, thank you. Later everybody. Bye. 
