Hey Alan. Good morning. Good morning to you. So we are back again for another episode of A.B. testing. Yes, number 80. Holy cow, nice even round number. And a good little milestone on the way to 100. It is, I like it. Which we will not hit this year, I don't think. Unless like a bunch of stuff happens and we go crazy. We took a little time off from podcasting. We skipped last time two weeks ago. Yes. Life happened. Life indeed happened. I am sorry for that Alan. It was the last second thing. I am, I am happy to give our listeners a break from us rambling for 45 minutes. But you'll be happy to know, it's usually we're rambling for an hour and I edit out down to 45, 50 minutes. Yeah, hour-ish round. Sometimes I add stuff and just put some things on repeat. They don't notice. They just heard random droning from a couple old guys talking about crap ideas. I bet you we could actually do that. So, A-B testing is, and I need to write this down, some day a podcast about, from Alan and Brent, that's what the A-B is for, but we talk about testing, software quality, agile, leadership, change management, agile, agile testing, and most recently, this thing we've been talking about for a year now on the podcast, or maybe actually in depth for a year, but for a lot longer, is modern testing. I got a tweet today. That's great. I know, it was very exciting. No, I got a tweet today. Apparently, years ago, I had commented on someone's blog post. This was like 2013, and he was talking about, I think test needs to go in a certain direction. And I'm like, yeah, the only thing I can see on your thing was that you're missing something along the lines of using telemetry data to understand the customer. So he circled back and was like, hey, this sounds a lot like MT. And I'm like, yeah, you know what? Sorry for the delay. I guess we kind of figured out what that something bit was all about. Okay, okay. Hey, speaking of Twitter. Yeah. I woke up this morning and looked at Twitter. Not the first thing, I'm not Donald Trump, but I was looking at Twitter, kind of catching up on stuff because I had some notifications on my phone, and Andrew Morton tweeted, I met Andrew at Test Bash. By the way, welcome Ministry of Testing listeners if you're new, because this is the month where if you are looking at your Ministry of Testing calendar for 2018, you'll see a nice little orange thing. Let your eyes wander to the orange rectangle. And April is sponsored by AB testing. Yes. So if you came here because of that calendar to check us out, that's fantastic. We love you. Thank you very much. Anyway, I met Andrew at Test Bash. Andrew is the new dev boss at Test Bash, but we played some card games together. Played Flux. I don't know what Flux is. Oh. Or what a dev boss is. Oh, so he does the development and Ministry of Testing. Everyone's a boss. Okay. Richard is the boss boss. That's a title? Yeah. Boss boss? Yeah, boss boss. He's a dev boss. Anyway, not the point. Not the point. Still not the point. But now, tangent time, Flux. Flux, I play, it is fun. It's a card game where you try and accomplish the goal using rules, but the rules and the goal change throughout the game, depending on what cards you play. I think that summed that up pretty well. I think I summed that up pretty well. Yeah, we should play On the Air sometimes. That'll be a fun podcast. Can you play it two player? I think, I don't see why you couldn't. But anyway, if I recall, and I think I do recall. Are you and Andrew now playing over Twitter? No, no, he won. He won. And I don't even hold it against him because he was cool. But the point I'm actually eventually going to get to and maybe get to faster, depending on how much of this I edit out, is that he was on the latest episode of Desert Island Discs. Cool. Which is the Ministry of Testing podcast or sponsored podcast from Neil Studd on Desert Island Discs. If you remember, I did episode five. Yes. In the 2017, Andrew is Test Chef on Twitter. If you want to follow him just straight away. But the cool thing is, is Andrew gave a nice plug for not only, he talked about my Test Bash talk on modern testing, but he gave a nice plug for A-B testing. Sweet. He started, he's one of the three, and they even talked about one of the three how he needed to recount. And I just recounted again today, and there's still only three listeners. So, done, done, Neil. Neil said, we need to recount? Yeah, he said, surely there's more than three. I said, nope. And Neil also know that every time we count it's three, just let everyone know I recounted him. Thank you, all three listeners. We do appreciate it. I don't understand. I'm a data scientist. Like, it's three guys. Yeah, take it. Data science is magic. So if he says three, it's three. It's three. There's no other believable answer. Andrew's been listening since episode 30. Wow. But he did recommend the whole, he said, just go back to like 70, just watch the evolution of modern testing, and they really dive into the last one. It was a really nice, probably 60, he said 70, but 60. Some podcast hosts would go, just go listen to them all. I'm not gonna tell you that. I think you can start, you can do 60 forward is really good. And you can definitely go farther back. Which episode do you happen to recall which one I give you the ISTQB questions? Keep talking, I'll find it. Keep listening, you'll find the answer to that if you wanna go back. It's a good episode. A lot of people like it. I think that's actually one of them that I have actually managed to title. Yeah, but anyway, the point is, is a really nice plug for A-B testing from both Neil and Andrew. We much appreciate it. We love to have new members of the three, so thank you. One other thing I did this week is if you want to be part of the background discussion and get early by two days releases of the A-B testing podcast, you can join our Slack team. And to do that, you used to have to email me, but weirdly, more and more of the three kept on emailing me and that got out of hand. So now you can just go to angryweasel.com, whack A-B testing, and click on a link on the right side and you'll get invited automatically. I can't not tell you how happy I am that you figured that out. And try and tell me, try and tell me how happy you are. Oh, show me via an interpretive dance. And we're back! Yeah, that was a little bit more than I was looking for. That was pretty happy. Oh, wow. Wow, oh, wow. What was it, IS? No, what is the acronym? You can't imagine what it's like to be in here, ISTQB. Do you need me to spell that for you? Why is it QB? Quality Board, International Software and Testing Quality Board, I don't know what it stands for. I wonder if anyone does. I wonder if there's a name for an acronym that no one knows what it stands for. Okay, so back to A-B testing. Hey, everybody. I hope you've enjoyed the show so far because it's off to a roaring start here where Brent tries to scroll and find titles on his phone. It's okay. It's okay. You can just send it to me later. We can do this. 26. Episode 26 is where I ask Brent about giving some questions from the ISTQB. There's some gems in there. We definitely got better as we went along. We definitely hit a roll, definitely starting with 60, but probably before that, I think we actually had some stuff under control. And then there was the episode I did, oh, without you, which was fun. And don't look up the number. Stop looking at your phone. Stop, stop, stop. I'm shutting it down. Stop, stop, stop, stop, stop. Oh, and then speaking of doing an episode without you, may or may not happen, I'm going to star East week after next. Congrats. I'll be back in time for the podcast. Don't worry. I'm not. All right. If I can fit it in, I'm only going for two days. Three days? Whatever. I think I've gone three days. I'm gonna try and bring a little, see if I can bring some recording equipment, and maybe not do a podcast, but it might be fun to interview some testers about modern testing. It'll be interesting there because a lot of testers are more on the newer side. I haven't figured it out. I may just sit in a corner and look at cat pictures. I'm not sure. That's interesting. Actually, we need to circle back with Joel as well, because I would have thought that we would have seen the survey results for this year by now. Yeah. Oh, did we? I thought we did. Maybe we didn't. But your thoughts are- A note for after the show, go see if those state of survey testing results came out, because now that you mention it, I don't recall if they did come out. I'm fairly certain we would have done an episode on it. Your interview idea is what reminded me of that. Yeah, I think we should talk about what sort of questions. I think that could be, I think that's a very interesting concept. Okay. Shall we pretend to do a podcast? Hey, we're just about ready to get started with the podcast. Yeah. So I think it's, I love that you can, like most podcast listeners have that fast forward 30 second thing, which you can hit about five, six, 12 times on any A.B. testing podcast, if you still want to get right to us actually talking about anything relevant to your job. I don't think that that's statistically provable. Okay, fine. And because- I think there's probably a 70% chance that no matter what you do with that thing, you're going to get to some random tangent. Because you're a data scientist, if it's data, I believe you. Okay, so we have a question or a couple of questions to answer today. Are these questions from- We do not have questions. You lost the poll. Do it. Do it. Do we please have a, wait, how many people voted that I should do the Nickelodeon copyrighted mailbag intro? Don't look it up. I was just going to tease you about- I'm gonna say 20 versus one, you. I guess there's one thing we've learned in the last 18 months of American politics is that lying is totally acceptable. And if you lie enough, it becomes the truth. Right. Not sure what I was talking about. This is not a political podcast. By unpopular demand. Mailbag! Yes! Okay. Yes, we do not have questions. We have mailbag items today. All right. You want to start with Patrick. So one thing, one bombshell we dropped and worth some elaboration is, if I remember correctly on the last episode, Brent asked me, is modern testing a role? I said, is there such a thing as a modern tester? And I said no. And it was a revelation, a slight revelation for me too, but I just needed that moment of reflection. Think no, it says modern testing is a way. Right. It's a, yeah, it's a air quote way. But I want to- I'm actually glad Patrick, hey TestPappy, thanks. And then actually just to piggyback, and then I made a comment, like if it does exist, right, I don't have our principles numbers memorized, but if it does exist, one of our last principles necessitates that it disappears. And I thought about it some more since then, and I'm glad you mentioned that, because I think there is, a modern tester is the role that leads a team towards modern testing. But then by definition, yeah, it is transitory. Right. I don't want to dive into the next question, like thanks, Pepe, on to the next one. Because they do overlap a little bit. They do. But if you look at the, and I don't have this in front of me, but there are three principles called the ways of DevOps. You looked at these? No. Brent will look at his phone right now, and look them up. No, no. But I don't have them in front of me, but they overlap, interestingly, quite a bit with modern testing principles. It doesn't. Shouldn't surprise. It doesn't surprise me. Like a lot of what modern testing is, in my view, is sort of a, it's a repackaging of a lot of existing principles. Absolutely. Susinctly, and then with a test skin. One of the things I ask, well, a quality spin. What I view these principles are about, and how do we focus in, and help the test community do their part to stop being a cost to the business, and shift to being a value add to the business. Yes. But with a heavy focus on their existing strengths, which are very quality focused. So many things to talk about. So let me go through the thoughts and order in my head, which is a difficult thing. My last team at Microsoft. I, teams. As I've mentioned before, drink. I was brought in as the quality guy. The guy to influence quality, basically the modern tester. I think that was near literally your title, wasn't it? The quality guy. Yeah. Across a fairly large dev team. Once my manager put more and more burden on me to own the quality, or to be at least be the scapegoat for quality in some ways, responsible, everyone I put it. Which I didn't totally hate, because I did feel like my role was to mature the quality culture, to improve quality. But what I ended up realizing, sort of taking ownership of, or a green thick ownership of, was basically everything from the developer desktop to deployment. Everything from the dev teams could take care of designing features. But once they started writing code, all the way until deployment was handed off to our site reliability engineers. All that was under my team one way or the other. We were writing productivity tools for developers. We were writing gates for check-ins. We handled deployment. We handled bug tracking. We handled reports. We handled all of the compliance issues, everything in there. And then, and even for my last several months, doing release management. Reviewing every change going in and making sure the right changes were going in. Or the right work was going into the changes going in. Yeah, I remember all this, but hearing you summit, I'm just, vomiting a little bit in my mouth. I'm just like. I wanna come back to that. I wanna know where the vomit comes from, other than your gut. Because to me it seems like you're your last manager. And maybe it's innocuous, but the way you just described it, like over time, every time you take one of these things, you're sort of rebuilding up the, I like how you put it, I use it a lot. Resolidifying your role as the team scapegoat. Right, if it doesn't ship fast enough, whose fault is it? Alan, if the quality's low, whose fault is it? Alan. And to be clear, my last manager was a piece of crap. But honestly, my last job was perfect for me at Microsoft because I got to do a lot more, I got to get a lot nicer view of end to end, that whole pipeline, which I wanna come back to. I think it's very important. Everything from developer desktop to deployment. I got a good view of how all that works, what common pitfalls are, I learned a lot. But also I had a bad enough experience with team culture and management that it put me in a job that I'm a thousand times more happier in. Full, full, I got experience, but also the right kick in the butt to get out. The universe prepared me to go have a great time at Unity. So I'm, I'm, There you go. I'm happy with the way things turned out. So I have a similar experience between my current job and my last job. And, and as you know, your current position, you will always find something to, to begin to really hate. But one of the things that I currently fall back to is I go, yes, but you know what? It's still not my last job. Yeah. Sometimes you need to have, you know, a rough experience to set that bar really low so you, you can appreciate your current situation. It's all relative. One of the great learnings I got from that is I think the modern tester looks at everything from developer desktop all the way to deployment. That's their playground to find bottlenecks and to find places for improvement, to find places where the quality can be injected. Accelerating the achievement of simple quality requires that you look at everything from developer desktop all the way to deployment and figure out where, where the acceleration part or help can come in. Right. So let me tell you something that I'm working on that's relevant to this, right? Now I'm going to do it from a, a statistical or a machine learning point of view. Remember, remember listeners, that means it will be true. That, Okay, go on. Yeah. The, but the problem is, is relevant period. So here at, at Microsoft we, much like any other place that has a service, we have, we have a DevOps type of thing. And we have these, I'll just call them things called outages. And an outage is, Hey, suddenly we stopped providing service that we have promised to customers. Right. You can imagine, you know, a big outage would be a whole data center going down due to like a power situation. Okay. One of the things that I am deeply involved in is the, the measurement of success and the process by which we get all of the developer leaders in a room and work through, how do we make this stop happening? Okay. It's a very expensive meeting. It happens every week. It's an hour and a half. And now, now I have a little VOD coming up in my throat. Yeah. Well, and so good. Rightfully so. Very expensive meeting hour and a half. You've got partners and VPs in this room. And what I've observed is a lot of the times it's, it's regressing down to, very simple suggestions. So this meeting, we have a lot of high power. And what I've now concluded is we really don't know, there's a process that those above us have pushed on, pushed even down onto the VPs. But no one's really evaluating, is this process having an effect? Are we getting the value from the investment? Right. So one of the problems is like outages, a big part of the outage problem is human error. Someone's trying to do something. They did a typo. Instead of doing what they intended to do, they had an unexpected consequence and something else. And now they've created another outage. This actually happens. Yeah, Phil, I've experienced many of those. But all of these are sort of one-offs. Right? And so, what I'm trying to work through is do we even need this meeting? Right? It's a very expensive meeting. And are these suggestions actually adding value? So what I'm now gonna try to do, and it's a difficult data science problem because it, data science as you know, works better when you have multiple data points. These are all one-offs. But that type of analysis and that type of quality check, one of the things that I'm actually looking to do is say, hey, maybe we don't need to optimize this process, or rather what we need to do is optimize this process by removing it, right? And coming up with something else. My current theme, every year it seems like I have a theme in the team. You should have a theme song too. You should work on that. All right. But my theme last year, for example, was Cosmos is dead. So for the uninitialized, Cosmos is the massive data lake used for storing all kinds of crap at Microsoft. Yes. And it's slow and has low reliability. And another fine team in Israel has created for us a much better solution. Now Cosmos, so we do have a... Is it called Cosmos 2 Business Edition? No, we do have a... We do, and this is one thing I also don't understand, but we do have an official branded product called Cosmos DB. This is not the thing I'm referring to. The theme this year is around, essentially, how do we automate away the human? Now don't take that out of context, you context-driven people. It's in some cases, automating away the human is the right thing to do. What cases is it not? To the CDT people? I don't wanna make... Oh, you're talking about the... Automation is replacing the job of the human tester. Testing is a sapient process requiring thinking and creativity. Yes, agreed. Get over it, move on. Fine. Fine. Anyway, sorry for the tangent, but I have to play devil's advocate voice. What I am trying... Voice of the prick. What I am trying to do in this particular case is create a system that learns from the human, but then automates the decision-making process because as we scale up, and I did a presentation to Alan a few months ago, and one of the things that I was able to show is Azure is massively growing. Don't believe you. Fine. Financial reports coming out very soon. Read that, believe that. Just full disclosure, my team's data feeds into the financial disclosure. So if it's wrong, you're fired. Yeah, but I'm a data scientist. So it must be true. Right. Gotcha. Nevermind. Anyway, so the point I'm trying to... Yeah, but what is the point you're trying to make? So just analyzing that process and accelerating isn't... Like it says accelerating the achievement of shippable quality. Just remember, it's not just about... Just because that's the first word doesn't mean it's the most important. Right? That there's also of, okay, how do we achieve the goal of shippable quality? And then in my view, the MT principles or the modern testing principles should feed into that. Sometimes the right thing to do is say, this process is stupid, get rid of it. Yeah, absolutely. It's the systems thinking aspect. Talk about bringing our strengths to the table to help the team transition to a modern testing way. Part of that is just looking at the system as a whole. A lot of people can work for years successfully as a developer by only seeing their part of the system. It's up to us as people driving modern testing to view that whole system and... Measure it against the end goal. I can't tell you, back when I wore the title test manager, one of the things that I realized all the time. And I then started measuring, and it really did start to sicken me. Where, right, dev would find this clever thing that shortened the dev schedule. And they would be praised and held up as heroes. But that same thing that they did added three weeks to the test schedule. Yeah. And so there was this sort of component level optimization instead of optimizing the whole system. I'm like, look, your stupid decision actually changed the schedule by a whole, extended the schedule by a week. This was a bad call, not a good call. Why are we rewarding this guy? Right. Lean talks about optimize the whole. The whole process, not a subcomponent. Because it is absolutely not true. I think we're saying the same thing. Yeah. If the processes are independent, yeah, you can go component by component. And optimize that individually. But if a decision from an upstream process could negatively impact the one downstream, then you need to go after the whole system. That's still a theory of constraints. Yes. I want to make sure we have time to talk about the next question. Go ahead. I think to wrap up, I think it's something we'll discuss more. I think if questions come up around this transitional process and modern testing being a way versus a role. Because I see the modern tester really as the person who facilitates the transition into a team doing the modern testing principle. I think the modern tester is transitional. I think his primary role is to be a leader and an example that others follow to get the rest of the team aboard. Put a pin in the leader part. And then this next mailbag question, I only need to do it once per episode, was actually a direct message on Twitter and from a month ago, so I'm so sorry. But we're getting to it. From Just Voskul. I think it's probably Just. Just, Just Voskul. Yeah, because the J's are wise. Scandinavia. Yeah, he sent a DM. Which I shared with Brent. Yep, and I will say. I probably broke a European privacy law. TLDR, we're not gonna go through the whole thing on this. It's very smart mail. It's not a mail, it's a tweet. DM. DM, sure. So a couple points we wanted to cover in this. Let me start with, I got the last question. So one of the things that he had an issue with was us saying we want you to focus on quality and not code correctness. Yeah, what do you mean by that, Brent? And then he asks if we are responsible for the quality culture, it's quality and then he concludes it's quality in the widest form, including code quality maintainability. Maybe I'm missing the point there, can we elaborate? Yeah, we can elaborate. We can. If nothing else about this podcast. That's one thing we know how to do is elaborate. We can elaborate. Please elaborate. I'll tell you my perspective on this. As part of the transition I think is absolutely critical that tests get away from any semblance of what we've called the dysfunctional loop. There is a more positive way of describing this and perhaps you'll get to it as well. But one of the behaviors we have to get rid of is any semblance of the scapegoating that it is test job to find these bugs. Yes. Dev gets to write the bugs and test gets to find them. Which might be- A lot of vomit in the back of the throat. I mean, there's some places where it is fun. The Easter time, the Easter bunny goes and hides all the eggs and then we have to go and find them. But in this particular case, the Easter bunny doesn't come and slaughter those who didn't find enough eggs. No, what we wanna do to actually move quality upstream is we have to get rid of that safety net and say, no, on code correctness, Dev, you're wholly responsible. Now- And to be clear, Interject, I look at from the Agile quadrants. Of course, modern testing principles are based largely on Agile testing. Brian Merrick's Agile testing quadrants, which were elaborated on quite a bit by Lisa Crispin and Janet Gregory and then others, are a good model to look at to understand the holistic view of testing. And there is code correctness in the quadrants, but if, I could look at it two ways, let me finish both for you, Interject. One, I could say a team where the team I could say a team where the Dev team doesn't own or won't take ownership for code correctness isn't ready for modern testing. Or alternate view is the first thing that a modern tester, someone helping with transition should do is make sure and coach the team and give them the tools and information they need to help own their own code correctness. I'm gonna push back on your first comment because yeah, they're not ready, but they absolutely need to become ready. Yeah, which is kinda led into the second one, which if you're going to, if you do want to evolve and be able to stay in business, frankly, you're going to need to transition to something towards modern testing. I actually think we, the last episode, we talked about my tourniquet metaphor. Yes. Right? And I think if you find that the Dev team is not willing to do this, I actually think that is a viable place to rip the band-aid. Because your other choice is to let it persist. Famously, and I say famously because I saved the screenshot and had it even in my test bash talk was the headline Yahoo Eliminates the Safety Net to improve and to improve code quality or to improve quality. I forget the exact quote, but it was the article from when they got rid of all their testers and figured, well, automatically now, because people don't have testers, they will automatically own quality. And that case, it was a tourniquet. I think, could be either. Yep. Where I think, and the way I'm driving modern testing at Unity is, and the thing I learned from my last two teams at Microsoft where we did transition to Dev teams owning end-to-end quality, more or less, was my big takeaway is that a transition is needed and modern testers and coaches are needed to help the team get there. It won't happen automatically if you get rid of testers. That's the biggest mistake I see teams moving to unified engineering do, is think, okay, nobody's a tester anymore, it'll all work out. You need someone to manage the transition. Oh, oh, oh. Or to lead the transition. The only way where I've seen it actually work is in that regard is you actually eliminate the testers. Because you've visibly gotten rid of the safety net. I remember when Microsoft first started doing this, we tried to come up with a way to sort of ease the balance. And I remember one team, they had a 10 to one, they went from a one to one Dev to test ratio to a 10 to one Dev to test ratio. And they just ended up with a really busy tester, right? Yeah, actually, I think they did. Yeah, actually, no, and this is the reason why, this gets back to why I am so focused on, no, get out of the code correctness game, because your Dev are meant, they have this mental muscle memory, and it is hard to get them out of the frame that they own their code correctness. And I was actually in, I was having lunch, and I was with a Dev and a test, and the Dev looked over at the test guy, who was the one and the 10, looked over at the test guy, and I was like, I don't understand what leadership is doing. How are you going to do all this? And I looked at him, I'm like, he's not, you are. It requires leadership because they didn't know that. Just remember, because we're going through a little bit of this, I'm not going to dive into details, but the main point it reminds me of is, Google had this thing called test ready, which meant, until you can show that you've done everything, and some pretty high levels around code correctness, don't even ask for any help from testing experts. Because you need to show that you can take care of yourself first. I agree with that. I think at Microsoft, There's something to that. I think at Microsoft, I've seen things similar to that, and in different names, I think at Microsoft, what would happen is we would find that the developers would set their test ready bar too low. The thing that I would say on this one is, number one, QA's new value proposition is around the measurement of quality. One of the things that I would do in Xbox, for example, this is the last time I remember doing this, I'm like, we are spending 300 million, not 300 million, but there was one feature that we spent actually a million dollars on. And I'm like, why? What evidence do we have that anyone's going to use this? Turns out I was right, and after we had spent the whole million dollars, the feature got cut, because it relied on a super niche market. And I'm like, we need to get some venture capitalists into some of these decision-making positions, because when it's their money versus Microsoft's money, people are making smarter decisions. On the code correctness, modern testers job is essentially, no, we're here to teach others to fish, okay? So I think the main point to get across is that it's not that we don't care about code correctness, it's absolutely important, but it's, we can't lead modern testing, we can't scale if, and a team essentially can't achieve accelerating the achievable quality without a dev team that really understands and owns code correctness. I've actually gotten to the point where I've realized that the only way to get tests to realize that their role is to teach others to fish is to make it hard for them to give fish, because once that starts, you're inviting the dysfunction loop again. So that in a nutshell, as we have enough more people, the right balances will be struck. But right now, I don't think that's the current shape of the world. Yeah. Just say no. So one thing, I can't remember if you just had a question about this directly, or what the exact question was, and you can look it up while I talk, is the leadership aspect of this, the leadership needed for a person functioning in the modern testing role. It's become, as I've thought about it more and more, and as I've looked at the people in my team who are leading their teams towards this, these, towards these principles, towards this way, I realize it's almost entirely a leadership issue and entirely dependent on their ability to grow as leaders. It's not entirely. I mean, the transition, like if you don't have influencing skills, if there are multiple ways, it's one of the reasons why I say, hey, you need analytical skills. Because a lot of times, if you personally don't have the soft skills to influence, if I'm gonna build a model, a triangle pyramid, whatever, I won't go into the tangent there. I think the basis is, you mentioned it before, you have to have influence skills. The basis is leadership. Everything else builds on top of that. If people don't wanna do what you say, or people don't see your value, or people don't agree with the direction you're trying to go in, good luck. It's gonna be very difficult. Well, no, it's not good luck. Go read a book on the topic. Pick three things from that that resonate, try them. And if this still doesn't work, go get a different book. Like, don't give up. Never surrender. Just like testing where we try new things all the time, as a leader, and not as a manager, as a leader, well, as a manager too, but in this case, talking about leadership, you need to constantly try out new things. If you keep on trying the same thing over and over, like, well, I asked them to do it, and they didn't do it. Well, I asked them to do it, and they didn't do it. Well, I asked them, it's like, dude, you suck, or I guess Brent says go read a book. But seriously, you need to, it is hard. Leadership is, effective leadership is just as hard, or harder than any of the technical stuff that we do as testers. You also have to have a little bravery, I think. I was thinking about the tourniquet stuff. I remember at one point in time, I made a decision of, you know what? Yeah, I got a lot of testers, but the nice thing is they're my testers, not yours, Mr. Dev Guy. So here's the deal. I basically invented something similar to what you talked about with Google, and I said, yeah, we're not available to you. But what about Check-in? Yeah, what about Check-in? And I remember one time, Devleed came to me after I'd done this, and she's like, why wasn't this found before? And I'm like, well, if you wanna set up a meeting, I'm happy to sit down and look through your unit tests and help you answer that question. And she looked at me like, what? I'm like, yeah, I'm here to help. Yeah, there's a, right? She was already pissed when she came to my office. That one comment just blew her over. I'm like, I don't know how to make it more clear. I can't help those that can't help themselves. Right, right. I didn't make up that quote, but it's very applicable here. I don't know where it came from, but we can help those who can help themselves. But that actually started changing her world, because what she did is she did what she has always done when she's getting heat post-ship for a bug. But what we have to be able to do as modern testers is be able to drive that behavior and quality before the crisis comes, before the bad bug comes. Right, we can't wait till, okay, we'll just wait till they fail badly. No, that's ideal, that's ideal, but it's those crisis things where the principles are tested. Yes. And what I did is I said, wow, that's a really crappy situation. And here's a person attacking me as if I'm responsible but I'm not. She wrote the code and so now I'm gonna ignore her emotion. I'm gonna use this as an opportunity to redirect her effort towards something that's productive. Yeah, I'm happy to, you know what? I am a test expert. Let's sit down, I'll set up a meeting tomorrow. Let's sit down, look at your test and figure out the answer to her question. Now, what really pissed her off is because she was hoping to use me as a story for it was Brent's fault, but I had already covered all of those bases. Let me talk about how maybe it could be proactive there. Let's say you're working with a team and they have, you know from looking at data, you're looking around the teams you work with because a lot of times the modern tester is working with multiple teams. There's the quality coach for multiple teams and you look at a team that from the unit test has like 25% code coverage. You could be proactive there and just say, hey Devlead, looking through my data across the team, I see your unit tests only have 25% code coverage. Can we set up a half hour, an hour to kind of walk through and figure out what the plan is to get the code coverage up? That's an A approach. So, and again, I need to preface this by saying code coverage is a wonderful tool, but a horrible metric. You know what I mean? I do. All right, now so go on. That's one way, but. Anyway, what I actually, so my version of that same question is, hey can we set up a meeting? And I'd like to understand why you don't view that as a risk. So what I'm trying to do is, because what I don't want to do is get them to think that, okay, I am the guy, I'm God in the ivory tower and I will happen to look down and say, your magic statistic is too low. No, because I want them to own it. Like, hey, did you consider this? Why don't you view this as a risk? And, because generally that will just be a conversation and it'll end up being, okay, this is a risk. So I like taking the Socratic method on that. But it may not, I mean, again, it's a leadership is, a lot of leadership is just initially, especially, is forging these relationships. You have to have a relationship where we can have a frank and honest conversation about it. Because it may be, well yeah, but my manager is really pushing us to get these features out. So getting those out is more important than reducing risk right now. And then the other thing actually both of our examples help with is teaching that dev leader, hey, there is a way to measure this. There is a way to have a coaching conversation around whether or not you're too low or at risk. And I think also, and I brought that up as an example, but really I'd want to have a more holistic conversation. It's like, hey, can we talk about quality for an hour? And I'm gonna come in with, what are your quality goals? And if we can establish some quality goals together, how are we gonna get there? And co-coverage can come up and risk can come up as part of that. Could also be other risks in the pipeline. Again, looking at everything from developer desktop to deployment, what are some other risks, what are some other things we want to do to improve quality? How are we gonna do that and balance the new feature work? What's important for the customer? How are we going to measure things? The only thing you're missing so far is what's important to them. Because one of the things I've actually discovered if you go, okay, no dev team has enough engineering hours. That's just great. That is absolutely true. Remember the data scientist said that, so it's true. So how do you use what they care about and help them to remove some of these brute force methods? Right, why do they not want to do unit testing? Because the value's not immediately clear to them. They need a way to connect the dots. And it steals their engineering hours because there's this belief that feature development is what builds business value, which it does, but then there's more to that. Like my favorite example, you've heard it from me a million times before, perfect code that no one uses is lower quality than buggy code that everyone uses. Striking that balance and getting trust and getting the guy going, no, look, I'm not stopping being your test guy because of anything with you. It's because the system is broken and I'm a part of that broken system. Me, this handoff between dev and test. No handoffs. No. I would say no handoffs to operations either. I don't like handoffs. Yeah, that's a different topic for a different episode. Different topic for a different episode. So I think there's a lot more to talk about there. I do think that leadership and developing leadership skills and doing that via forging relationships versus asking someone for a top-down mandate is the absolute basis for establishing any sort of modern testing. I think we're gonna go into more detail on each of the principles. Do you wanna start that next episode? Yeah. Okay. So why don't we? One of the things that I'm saying, like leadership is a foundational basis of this. So we'll go into more depth on this topic as we go through the principles. We will. So just for those planning their listening schedule, one of the things Brent and I have been talking about doing and I think we'll be ready to start it with the next episode is we'll spend an episode each on each of the modern testing principles. Maybe two. We'll see how things go. Yeah. And just diving deep with a normal amount of tangents and fitting in some mailbags as needed. But one to explain to the listeners, as anyone who's been listening to the last 10 or more episodes, 20 knows, modern testing is evolving. We flush out a lot of our beliefs on modern testing as we record the podcast. So I think that's what we do. We do as well as through the mailbags and questions. So keep the mailbags coming, join the Slack channel, post questions there. The Slack channel is actually something that I'm extremely fond of because now people will post a question and some other member of the three will respond and I actually like a lot of these responses. It's grown into a self-sustaining community. It is, yeah. Which as a community leader is the ultimate goal. So very proud of that. Thanks Perzy for setting that up. And thank you all three listeners for hanging out there and having the conversations. Okay, we're out of time or more. I'm Alan. I'm Brent. We'll see you next time. Bye. 
