Hi everyone. Howdy. I'm Alan. I'm Brent. We call him Baby Brent today, but he's here with us. Baby Brent. Baby Brent, because in our previous attempt at starting the podcast, you pissed me off. Now you're Baby Brent for a while. What episode are we, Brent? Sixty. Gosh, dang. Sixty, that's awesome. Sixty. Nice round number. If we were a professional podcast, we would have like a special episode every fifth or tenth episode, but Brent, are we a professional podcast? We are not. Or are we? Because we have, if you got into the website, gone to our Slack channel, gone anywhere else, we have a logo. We have a new A B testing logo that isn't something A, Alan, made into word three years ago. I don't know that the logo, so first off, the logo is quite cool. The logo is very cool. But I would argue, right, we still only have three listeners. The logo is just lipstick on a pig. Fake it until you make it. But if you like it, it was done by a wonderful Alexa Horn, who also did my Angry Weasel logo. And so if you're interested in any graphic design work, I'm happy to send you her way. It's great, easy to work with, great job. I'm very, very happy. And that was not a paid promotion. Nope. I have in my hand as well some stickers that Alan has produced. Designed also by Alexa. Designed also by Alexa. It's intended for, if you're one of the three, we have soon to come the ability to proudly emblazon your laptop. Or any place you want to put a sticker. Right. If you are a member of the Slack channel, Alan has already given those folks the opportunity to get one of these stickers sent their way. If you want to join the Slack channel, just contact me. I can send you to Percy on Twitter, but I can just take care of it. I'll get you set up. And then in that, if you're familiar with Slack, there's a channel called I Want a Sticker. And if you want a sticker, you go to I Want a Sticker, put your address, and you get one sent to you. Pretty cool. And soon to come, we'll open this up on the website. We'll figure something out. Yep. We'll get our little A B testing shop going. But the process of getting the new logo first off, I am extremely happy with it. Me too. And the process of it was a lot of fun. It was. It was. I don't want to do like the total Alexa fanboy here. But same with the Angry Weasel logo. She joined the Slack channel. She listened to the podcast. She got an idea of who we were and then used that for inspiration. And we bounced ideas back and forth. It was a super collaborative, really awesome experience. It was. It was. Okie dokie. So you know how else we aren't professional? Well, we're riddled with ADHD. The equipment always fails. No. Well, it's because we're in this crappy Microsoft building where the power outlets don't work. How about you pay your bills? So we have this. We always make a nice little Kanban to do list agenda spur of the moment on the whiteboard. And I realize walking in here as with as much spit and vigor I left leaving 59 ready to continue it. I don't feel very prepared today. But I think that'll probably change as soon as Brent says something that there's like a magic word or phrase you'll use that'll set me off and we'll be back off to the races again. Fugle horn. That's not it. What is a fugle horn? I don't know. I know what a flugel horn is. Ok, that one. I don't know what that is either. Hey, I want to put this out there for all of the three and your friends. If you have a friend who can breathe and whose first name starts with the letter B, I will be accepting applications for someone I can stand. Oh gosh. Alright. One thing I want to talk about. We did negative one. So rather than reorder, we started going backwards. So all anger weasel.com, WAC A B testing where we host this the podcast. I put a I'm drawing with my hand, which is useless on the podcast, but a little web submission form where you can submit a mailbag request. No need for an email address. No need to join the slack challenge. I have a question for these fools. Just go throw it in there. So far I put it up unannounced and I've we've had three submissions to her spam. And one was a regular question, which will probably get to next week, but keep those coming in. If we get a lot, we'll just do an all mailbag show, which we have done once before, right? Once or twice. Yeah, maybe. Wow. Yeah. But this is not technically an all mailbag show, but Brent has insisted that we start off with the mailbag. Yes. So we have been recently talking about what a tester's role is. And that Alan is what that could have been the magic word. Sorry. Go on. Right. So what's the testers role, Alan? It depends who you ask. Ask 10 people, get 11 answers. Yeah. What's the testers role, Alan? Accelerate the achievement of shippable quality. Thank you. Boom. I said, yeah. And Michael Rocha on the Slack channel asked, is it now I have to be, is it Rocha or Roca? He'll go with one of those. All right. Sorry, but the question is the important part. He's asked, can we break down what is accelerating the achievement of shippable quality? Now, last episode, we, towards the end of it, really started to get on a rant on, on, ish a little bit, on sort of what our view of modern testing is. And when considering this question, as well as how we ended the last episode, it occurred to me that in my mind, yeah, they're very much aligned. Can I share with you, Alan, sort of the thoughts on what I think answers this question? Well, because you asked to know. Okay. All right. So then on to topic two. All right. So at a high level, if we break down accelerating the, of shippable quality, as well as sort of merge it with modern testing, the very first and important tenant on this is quality is only defined by the customer. I agree with that. It is not defined by the PM who wrote the requirements. And, but it's, let's just dive into all this stuff because, yeah, because the, I agree with that. But the way I've put that before is software quality is only what's perceived by the customer. That's fine too. The only reason I pause, I pause your definition is the customers don't know what quality is. So they can't actually define it. Right. So it's, so you have to read between the lines a little bit. And it's, I just know that whatever you do, whatever the way I've put this in the past is customers don't give a shit if you have 90 percent code coverage. Nope. Nope. Well, and I think I put that in a presentation. I, oh, I remember I gave a tangent time. I gave a presentation on metrics once and I, and I, I made a big deal out of complaining that customers don't give a crap about those things. But moved into, of course, telemetry and things like that. Anyway, back to the Brent show. To piggyback further on that one. So if quality is only defined by the customer, then what is quality in their point of view? And I like Tom Popendike's guidance on this is quality is a problem solved. I don't want to go too deep on the, the, my view on the tenants of what makes up quality. If people want to hear that or they want to have a view to it, they can go to my blog site. I've written that up. And many, many, many philosophizing on the internet around what is quality, which we don't want to go into. Now, another point of the story where I came to this conclusion was, was back in the day and having multiple arguments within Microsoft around Google and Bing. Google, when it first came out, just shipped bugs all the time. And a lot of the people, this is when I was still in a test role, a lot of the people that I would be talking to, what had the approach that Google was just a fad. Look at all the bugs they shipped. I'm like, if bugs were important to customer quality, then I would be expecting to see Google stock price tank, not a skyrocket, which is what we're observing. It's essentially Google was, was an innovator in terms of solving a problem with respect to search and, uh, an innovator on leveraging their data instrumentation to identify what is causing the most customer pain and reshit. The second tenant that I'll say that this piggy backs. So don't you want, I know you have some, and I always worry when Brent writes things out because, because then Brent just kind of reads and we get lost, but I want to talk about that one. Okay. Because I think. Go ahead. Theoretically, this is your podcast too. I was going ahead. The visualization I came up with, like, it's like our brains have an idea we're going with, but these squirrels and trains and things go by and we look at them and go, oh, look at that. That's cool. Back to this. Yes. The reality is though, it is cool. Yeah. Squirrels are fun to watch. They are. You mentioned a quality is problem solved. Yep. And bugs can get in the way if they block you from solving that problem or make solving that problem unpleasant or more difficult than it should be. Yes, for sure. And we all know that software ships with known bugs. Well, you do now and some of those do block problem solved, but many of them they never customers never see is a bug that a customer never finds a bug. The tree falls in the forest and no one hears it doesn't make a sound is a bug that the customer never finds a bug. They never ever see it. Yeah, it's a bug. Is it one that we care about fixing? No. All right. Depends on your definition. OK, I'm good with that. All right. Go on, Brent, with the rest of your list. In my view, so combining accelerating the shipment or the achievement of shipable quality and modern testing that one of the tenants is shipping and iterating is the best way to get to shipable quality. Say that again. Shipping and iterating. Correct. Is the best way to get to shipable quality. You will initially ship something that you know is not shipable quality. So my goal in working on a team that was shipping every week, I'm a little bit. So in the past, when I've worked on products that have shipped frequently, the goal was always not to make it bug free, not to nor to try and shove a bunch of crap in. The goal was simply ship as often as we can, but every release is better than the one before. Yes. Defined as defined how more, more customer value. Yes. I'm glad you asked that because that's really important. It's like, what is better and more value? And that could be another feature. It could be some important bug fixes, either visible or invisible bug fixes. I think it's important to differentiate. The visible bug fixes can feel much more minor because it could be like a five or ten or fifteen minute fix on the engineering side. But they make a lot of difference in fit and finish to the customer. Whereas you need to balance those with fixing the infrastructure type things like fixing performance issues that may be subtle for customers. But the architecture change may be better in the long run. So you need to balance both the invisible and visible bug fixes and give them value with like, here's a new feature. Value isn't 50 new features. Value isn't just fixing 50 bugs. I want perceivable customer value improvement with every release can be subtle and they may not see it because it's so gradual because they're shipping so often. But that's always my goal. And then you need to end if you ship 50 times a year and every release is better at the end of the year, you should have a. Fairly good product. Yeah. Yeah. I mean, the trick in those things is figuring out how to measure it. It's not the trick. It's to me it's it's it's the fun part. It's it's for sure. This is I know we're going to talk about modern testing in a little bit, but this is sort of a shift in what testers do. And maybe the test rule. What are some things testers do to accelerate the achievement of shipable quality. One of the things we do is to figure out how to measure those things. Yes. Yeah. I mean, let me so let me quickly go through what I think is the next most important tenant on this one. And let's just die deep under the modern. Sure. Sure. Sure. I just I think the train went by a squirrel on top of the balloon. I just kind of went with it. Yeah. I'm envisioning that now. What color balloon. Brett don't think of an elephant. I'm like look at the squirrel. He's carrying a balloon. All right. Calendar time. So the phrase is talking about acceleration. Right. So if we're talking about how do we get to shipable quality. How do we iterate. How do we measure. What is what is what is the key source of signal that we're going to measure. And that's some sort of heuristic around customer satisfaction customer quality. Then you got to realize that calendar time is the single most important resource to any engineering team. Correct. It's a clock on the wall. There's a clock on the wall. And there has to be a sense of urgency a sense of not just velocity that the customer is going to be able to do. But there is velocity. That's why it's acceleration because as you learn more your competition is learning more and you have to take that knowledge and feed it back into the system to be able to scale and increase the rate at which you're adding value to the customer. Yes. Now this is in particular in the services world. Right. This is one of the key problems in the modern world. It is particularly relevant in the services world. There is this item called switching cost. And no matter what you publish to the web. You have a competitor and your customers can go to it near instantly. Everyone does free trials. There's no onboarding costs for for most of these assets. It's really hard to be a monopoly in the service space. You only win by being better. Circle back to the customer quality. Now let's talk about modern testing in this light. Maybe a little bit. I've been thinking just about. I've been thinking while you've been talking. One of the reasons I think we have such healthy competition and so many. I'm going to give an example in a second across so many different things that we use something interesting. I've just come across tonight and I am pairing it with what you're talking about is quality is problem solved is why are there. I'll ask this rhetorically. Why are there six at least or 10 different solutions to solve my problem. Because all of our problems aren't the same. They're not. And it's interesting. So one story I'll share is I am. I do not miss outlook at all. It's very full features. Lots of lots and lots of features but very slow very much a pig and not that much fun to use. So when I left Microsoft I mail it. Unity is all Gmail based but I kind of like having a male class. I wanted a unified male client because I have four different male accounts I use fairly frequently. I'm not going to list them here but there's four and I want a unified male client. So I looked in there were half a dozen or so and some were paying some were free and I tried them all out and found the one that solved my problem. And I love it. It's been great for me. Just yesterday I was having some problems. So that's one example everybody's and somebody will say that doesn't work for me and that's fine. It's really cool. This is where this is why none of those programs will ever become a monopoly. I was having some problems with Evernote. I was looking for alternatives and coincidentally somebody who I've met and talked to a lot over the past. I met him when I say Yahoo and then he went somewhere else and then it went somewhere else. And now he's been it. He sent me a DM. He's at Evernote said I'm going to solve your problem. So a little little side tangent on the power of the network. But I got a lot of great suggestions on alternatives. One's practically an Evernote clone and others are even lighter weight. And of course there's one note from Microsoft. But again I'm not avoiding Microsoft. I'm avoiding fat things. I prefer to work in a web browser most of the time and I honestly I think the web browser experience and all of the other ones is better than the one note web experience. One note as a desktop app it's actually pretty fantastic. It's just a lot for what I need. It just made me I was thinking about as I've looked for like a new tool a new code editor. Why are there 16 different code editors you can use because they solve problems for different people in different ways. Well there's two reasons for that. Right. And we had this problem back in the day when essentially when you and I first met like one of the problems that was dominant around there. You bring 10 testers in a room asking to build a unified test harness. Oh God. You would walk out with at least 12 different designs. But none of those really could claim problem solved. No it's in a problem. The problem is because or the reverse. So we used to have a very vibrant internal tool submission system and multiple times I would find a tool that did 80 percent what I needed but I didn't have source code access. I would ping the owner and they would say no I can't add that feature for you. And if you were lucky and they were still at the company. Yeah. And because I worry about the code base morphing into two different versions I won't give you the source code so you can add this feature and because I'm busy I won't give you the source code and allow you to submit a check in. This is idiocy. The point of the story is the reason why there's multiple versions is a lot of times there's a current solution and someone realizes you know that there's one other thing I need and no one does it. And then they often get forced to rebuild from the ground up. I think it's different though and I want to get to modern testing. I think it's different. I mean internally at Microsoft especially and granted they're much better now at sharing and consolidating. But at that time it was more about I mean there was a checklist item on the career stage profile or latter level at the time for like a senior you've written a test harness. So there was a lot of career advancement motivation more than solving the problem. I think when with a code editor or a mail program or a note taking application I think the founders of those companies they I think we can solve this problem better than anyone else has. Or at least in a different way and they're going to and they create those things for that reason as a founder of a company or if I have an idea that already exists I'm not I think it's ridiculous to think I'm going to take over the market from all my competitors. Every note never set out to order to take over the world from one note. There's a whole bunch of people that need to take notes that I think our solution will be better for. Same thing with mail bird by mail application with sublime text. They solve the problem in a way in a slightly different way that's going to work better for some chunk of people and that's where their business comes from. I think at when you're talking about writing a test harness it's not they're not doing it for that reason that the reason people internally would write a test harness or a tool like that or a data pipeline crap is. Have I said here before that I'll drink here but the data pipeline is the new test harness for sure. So anyway although at least it in my neck of the woods that's tapering way back much faster. No I think yeah I agree. Yeah but you get where I was going so let's pop the stack a little bit. I could elaborate there but you know what I mean. Let me let me let me piggyback off of that and say great. What started with piggybacking is pushing onto the stack. Yeah. Hold on. Do you have a go to coming up. Shut up. Calendar dime is critical like. Oh yeah. We were talking about. We learn. We mean we learn this fascinating email story because I stated calendar time is critical and a big part is switching costs is super cheap. Alan can explore all of the email clients plus the other 50. He hasn't played with yet in easily a span of a day. Right. Correct. Now that's today's world. But what's the world we're heading into. And you can see this progression if you if you look at where we came from. So web 1 0 right. What was that all about. It was about connecting together knowledge content. Right. It was created primarily in colleges so that people could link together ideas. Sure. Web 2 0. Right. What was that. That's typically where the social media is. It's now also linked together not only the ideas but linked together people. Yes. Ideas that aren't written down might be a way of putting it. Great. Now web 3 0 mobile is the is the place where we're at right now. And if you look on the web people are already talking about the beginning of the end for smartphones. Right. The mobile situation is by some people's belief is beginning to already plateau. Right. So what was the point of this aspect is hey you can now connect with knowledge whether it's being linked or whether it's between the ears of your peer group anywhere you want to be. The next thing that's that's that is is already starting to be in flight is dynamic context. It's essentially Allen goes to whatever the hell he wants to go to and he says I want emailing services and in an ML world the system would just go oh okay I know from how Alan's uses pattern that these are the features he's likes. We have connected the dots to you like you need these features. These are all the rest APIs that that exist across the web space. We know how to integrate it together. We know from prior interactions. This is the the the UI that we need to tunnel all of this stuff too and it just works right now that seems nearly impossible today. A lot of it doesn't because I think machine learning could definitely look it's to be different for every person going back to everyone has different needs. There is easily you could look at the way I handle email the wet what I reply to what I delete what I ignore wouldn't take too much machine learning based on my behavior patterns to be very very accurate on predicting exactly what my optimal experiences. I think the next most important advancement that will exist in the testing world is breaking away from least common denominator solutions like the next big thing to come isn't is an active expectation that a vibrant personalization of what I as an individual care about is what's important. That's really interesting because we've heard forever and I've said it for myself when you try and solve all the problems for everyone you solve none of the problems for no one. I said paraphrase on there's many paraphrasements of that. Yep. But it gets away from that says we can use machine learning AI whatever and we can automatically customize your experience with these your software experiences to work the best for you based on how you work and what your problem is go back to problem solved. It's automatic machine driven customization of solving your problem. Yeah. Now easy the machine. How do you test that. How do you test that. And I know the answer. How if you're testing prowess is still based off of 80 percent based off of the learnings you gathered from 1998. How you gonna get there from here. It's not using your brain instead of 10 year old techniques. What is your test automation suite in that world. Actually a very good question because I ranted a little bit on this on Twitter recently. Yep. Because of all this. God I say the word the testing and checking stuff and I think to be successful in testing you absolutely have to be a systems thinker and be able to see the whole picture and know how it all fits together. I see a lot of narrow mindedness in some testing approaches or narrow narrow narrow mind that it sounds like an insult. I'll say narrow views. Many people in testing view test automation as pure test functionality and. It passes or it fails. Actually many people view test automation as UI testing and you do some actions that passes or it fails. I think and I think about most of the automation I've written I've written very little pass fail automation. I've written a lot of tests. I'll call them tests for now and we'll discuss that exercise the system in some way and then when they're done I examine the system in order to figure out if what I did change the state easiest example is all create and delete a thousand objects and then I'll check memory heat fragmentation etc and see if the cleanup things worked. Is that automation or is that a tool. The answer is it doesn't freakin matter. So I think when you're looking at and jump ahead here the answer is not. There's no there's no pass fail automation you're going to write for a system that Brent has described. It's because it's customized how do you tell it's customized. Well I'm going to. I'm going to write a eye to analyze Brent's brain and based on the output of that I'll create an Oracle that tells me whether this is no I'm not going to do that. That's stupid. That's stupid. I am going to go back how do I know I have to ask these questions. This is the fun part I mentioned earlier. How do I know that I'm solving your problem effectively and what can I measure what sort of analytics or instrumentation can I add that may tell you if there is if Brent is trying to do the same act for some examples and we can we could brainstorm these a lot. I want to know how quickly he solves tasks. I want to know how many tasks he tries to do multiple times. I want to know that that made me think of something else but I'll come back to it. I want to know how often does he use the system. I think if he comes back to it a lot he's finding it useful. I'm going to brainstorm a whole bunch of that stuff. I'm going to work. I'm going to design experiments. I'm going to work which ones don't but it's all about analyzing not just Brent but for every single user out there what are some ways I can measure whether or not there is business or customer value from this and that's my testing. That's my testing. I had so I've actually had some experience similar to the world I'm forecasting. So I've mentioned multiple occasions my time in Bing and one of my mentees at that time also ended up working in Bing and before I joined he wanted to meet because he really respected me. He knew me from from my test management roles. And he was a little surprised to hear that I was moving into a dev role but he wanted to pull me aside and said look don't come here and I'm like what why. And he said bugs don't matter. And now he was a fantastic tester. But his view was his job was to find bugs. And in the world where everything changes and there's an ML driven a bug you find today just automatically magically fixed the next day. Right. He would find that he would open bugs like he had done in prior situations and it just wasn't worth Dev's time to investigate what they needed to understand what the services they needed from their testing at that time was what's the systemic bug. Like what are we doing wrong such that this bug appeared not what is the bug. That's an action you can take that does accelerate the achievement of shipable quality. Yes. And not that entering a bug doesn't if it's a risk to that goal to that goal of that role little rhyme there. But yeah you want to take we need to look at taking larger actions. Yes now. Yes. The other thing I want to talk about and then I want to spend some time and quickly go through what we think on a detail focus and testing is different. OK. Tell me what to do. One of the things I want to say is that in the world that we're in today and especially in the world that we are going into I am going to state that the idea that the test job is to provide information is dead. OK. That is dead. Yeah I have the I think you've didn't I. I've pushed back on that before myself anyway. We talked about that on the on. I owe the three another shot because I mentioned that an episode or two ago as well. Yes. I want to say that so there is a model I am working this through for a presentation I'm doing in my job to explain the maturity phases of data and I realize given these topics that it it it falls here. Right. And that model is you collect you inform you recommend and you act. Now that's a maturity model. So if if if you are a inexperienced in testing then yeah maybe what you're trying to do is figure out how do I get the information so that I can inform others. But information is needed for a decision making process and what for the world that we're going into where we are trying to be experts in customer quality and experts in terms of adjusting the system to to make better code and reduce the number of times we need to iterate to achieve shipable quality. You have to be a master of recommending the next action to take. And I think that's the next step for maturity for sure. Yes. And just to hit a little bit on the tester as information provider I think again that model works when the test role is a completely separate and siloed and maybe even not even in the same org building or continent as development. Maybe that's fine. But if we look at I think modern testing involves a tester one who is so sorry to see a changeable quality being an embedded member being the quality and testing expert on a feature team. And I think in that role you cannot be just the information provider. It's it's for one just not pulling your weight but it's not leveraging your expertise to a to an extent that's valuable to the team. And number two in the world and the advancements that we are making on instrumentation and these ideas of a be testing in the cloud like if you are firm that that is test role and you aren't paying attention to what's happening. I am telling you right now today you are moments away from your business team realizing they can automate you away. If all of your role is to inform on what works and what doesn't once these guys realize you all you got to do is hook up an instrumentation stack and ship pilots and betas and hook in continuous integration. Your value proposition is over. And I can tell you that if I came in work for your company I could like that up in a month. It's not that hard. Pay attention if you think that that's what your job is. I think if for any knowledge work role any knowledge work role if you think what you do is only narrowly defined the same story holds true. Yeah I think any knowledge work and testers compare themselves to they think they're not knowledge work sometimes but it is it's like it's a creative role. It's a role that requires a specializing generalist a generalizing specialist a T shape persona tree shape so we ever want to call it if you are purely a quote functional tester. I know what that means but I've seen I've seen it written down before. Yes. But you have to be broad and I'm giving a talk and probably by the time this goes out it'll be tomorrow because this podcast would go out on Monday. The at the online testing conference on technical testing and what that means and at the plug is there only because like if you are only one thing and the talks really about how to be how to have some breadth in what you do and how to provide value to their team. In fact one of the points I'll make is that it doesn't matter how technical you are it matters how valuable you are yet you may need to be more technical to be more valuable and what technical means so box time is that you use the techniques of the craft. That's it. So you should have multiple things you can do to accelerate the achievement of shipable quality which in the way the world is moving means you're able to analyze and understand what the customer experience is. And if you can start by grepping log files that's the collection face. So the four phases collect inform recommend act and to Alan's point like if you are in this informed world and you are a technical tester by his definition great that's a useful skill set and I'm not articulating that the skill set is dead. I am articulating that the role of just informing using that skill set is dead. I fully agree. And this and the wisest and the smartest amongst you if you're in this world are going to start focusing on going to the next maturity level and say okay my technical skill set is now an implementation. It's an implementation detail no one cares what I need to do the way I add value is be the expert on leveraging this data to provide value bulls recommendations with proof like if we do this thing we will improve this thing that you care about by n percent. So it's data driven recommendations yes because that's the next place where you add value and where your specialty as assist greatly. Yeah and again it can be for those of you new to this it can be looking at like oh I wonder if any customers hitting this bug and you go to your Google Analytics and you look it up and you and if the data is not there you go recommend you add the instrumentation or analytics data to find out how many customers are hitting that or if you go. You really need to know these things go recommend that information is added them when you have the right information in order to make a conclusion based on data you can get don't just say here's the data recommend a solution that recommend make a recommendation. I'm just ranting on top of yeah but yeah you get it but the data is there that I don't want to make sure because I completely agree with Ellen's examples but recommend that you add instrumentation and bolster that with what they're going to get out of it. Yeah and as you have to frame your recommendations in terms of the ROI to the business and to be clear if I had and slight tangent maybe not tangent for test automators out there let's stop stop writing automation right you rather recommend it go write that analytics yourself writing adding those instrumentation points yourself is the new test automation. Yeah and that we've covered over and over and I know I know but I'm piecing it into this conference a test harness in my view in the new world is just a load generator all of your validation should be coming from the product instrumentation. Let's go quickly we don't do anything quickly but we'll go for it. So let's talk about something more tactical on what is modern testing. What's what's a key thing that that changes in my mind one key thing is code correctness is no longer anywhere close where someone of a testing skill set is useful to validate code correctness is much more effective. And the domain knowledge is within the dev team. They need to own that. Sure. And it may be a brief in a passing moment in building a team. Maybe it's a testers role to make sure it's being done. Then they're out of the way. When I look at I way I look at it is do you have not going to go into the agile quadrants but a lot of times it's the role of the testing quality expert to make sure it's all covered but absolutely not to try and cover the whole quadrant. Just one of those things that fits into it like I may need to coach my team and writing better unit tests. I may need to hook up some help them or recommend they hook up some static analysis tools or for code correctness recommend unit tests are needed to be done. But I'm going to recommend that hey the lack of unit test is causing us to have to constantly reshape this is causing resources. I'm just saying act provide the set of resources that brings us up and yeah. All I'm saying is the role of the quality and testing expert frequently includes making sure those things get done. Yes. But then it's not but it's never their responsibility to do those things. Yes, it's not their responsibility to do those things it may be their responsibility to make sure that they are done. Yeah, I think it is their responsibility to make sure they're done right and then going back to calendar time is the single most important resource. This this is that's the principle where I'm suggesting this like code correctness needs to be owned by the dev resources at this handoff to to to test. When when understanding how to do a unit test framework and how to do TDD or BDD is not that hard. Right. The the tester should not willingly pick up the codependency loop. We've talked about multiple times because that slows down and you're trying to accelerate. Two bug debt should be zero at all times. Yes bug debt is the single biggest cause of time being wasted in an engineering team. I agree. The second biggest one is dependencies but the number one is bug debt. So I have I'm not because I own like 50 zillion different services. They ever tell you my newfound definition of what services are unity. No, because it's traditional things that you and I think of like ads and analytics. But really it's any parts of unity that don't work when the Internet's down. Any parts of unity that don't work when the Internet is like the website. Anything that connects with anything like our anything in the cloud. Okay. So but so what that means they own a lot of desktop components too. And even in those couple down so far some to go working off zero bugs. If there are bugs active there are actively being worked on. Yes. So they are the bugs are basically on the Kanban board. They're being addressed and fixed. But otherwise they're either are there or they're not. You either have proof you need to fix the bug or you don't. If you don't you defer. When I got the team to really interesting they killed all these old bugs. The reactions were great. It's like I was really scared when we started this. It's like within like five minutes. I really felt bad killing all these. Now it's done. I feel pretty good. Yeah. I mean if they're important they'll come back. Anyway I wanted to show that little story. I fully agree. A bug removed from the database. Is easily 20 minutes. But I guess the point I wanted to make is a lot of teams doing just pure services. Of course they just bug comes in. They deal with it or they forget it. But I think it's absolutely an achievable goal for anything you're working on. I think it's it's part of I think it's part of modern testing is working from zero bugs all the time is part of how we ship. Yes. Absolutely. Automation. What do we automate. Not returning to the cloud. Not rhetorical. What do we automate. All the things that should be automated. A hundred percent of those. Yes. I don't automate anything that's pass fail functional testing. I don't want to automate. I'd rather I'd rather discover that through analytics. I want to or unit tester. We automate unit tests of course. Yep. That that actually is in my view the best high level answer. But we are not. Details of that depend. We are not in the world. We are not in the world where we we say we automate everything. Now unit tests. Yeah unit tests should be automated TDD type things where we're at a very low level code right as you write enough code. If you don't have unit tests to protect you then the is a very good chance that the integration in the tracks between components will fall fall down or fall apart. That needs to be automated. But here's the deal. Dev does that. Of course. Yeah. Right. What other tests. So maybe the question is what do testers or testing quality experts automate. I hold it is no longer true. This this this dogma of we're tests we must automate. Or there's a whole role of I'm a test. Automator I'm a test automation engineer. What do we automate. What we automate up front is quite I mean the next drill downs we automate those things where the ROI of automating up up front is higher than the ROI of relying on our instrumentation systems to tell us where the problem is right now. That's very context sensitive. Yeah. The load tool I mentioned earlier I need to create a thousand ten thousand a million objects and delete them to make sure the system is going to handle this because over the course of a couple days in the real world this will happen. I want to do this tight loop so I can see if there's a problem before our customers eat after two days. You automate those things that would be critical failures cause data loss. Those are things that need to belong in a in a in a production or a preventative suite. You automate those things where knowing too late harms the business again. Calendar time is critical in this world. Got it. Right. But is that everything. No. You do spend your automation again much like what you were talking about the test harness that generates load. You spend your time automating validation off of the instrumentation suite. That's where your value really is in this new world is to understand. Here's a new term new data science the term for the podcast anomaly detection and you create these new signals you hook them up to anomaly detection is the is the current practice. Sure I get where you looking where you saying look we got all this instrumentation get all the signal I'm going to hook it up to ML to tell me where I need to focus my attention. Awesome. What's next tools tool building I think is the next big thing what tools do we build it. Yeah. That I think that ties into the previous bullet pretty well. Depending on where you blur the line between a tool and a test. Well so first and foremost you do not build a test harness in today's world. No no there's a lot and a lot of things you don't build anymore. You don't like it's done it's out there you much like Alan's mail situation you go out and you find a test harness. If you think about what's the problem I need to solve yep and figure out what solves that problem for you the same thing. And there's a there's there is a solution out there for example we didn't talk about it looks like looks like our fine quality way of producing advertising may actually result in a in a sale for cobra ton now cobra ton. This is not intended to be a plug but cobra ton solves a big problem for for in the modern world problem people have right. It was next the last one the last one is knowledge sharing in community is critical in this world why because to reduce calendar time. You cannot rely on each individual higher coming aboard learning it soup to nuts every time they have to be able to connect. Very rapidly with the people who know. And I think it's also and that why of course was an elaborate why not I don't believe you why I think acceleration. Frequently requires innovation and innovation comes from ideas meeting each other and one of the most efficient ways for ideas to meet each other is through community and collaboration. But it's yes it's but it's also well it's it's probably the same thing I'll tell you that today in my job the number one thing that irritates me. Just is I know that I'm about to write code and it's going to take me about three hours and I know someone in this company has already written what I'm about to write. But I'm going to continue to write what I'm about to write because I cannot figure out how I find that guy I know that if I did he'd give me his coat. So what I want is in a world of in a world where maybe it's internal only because the code is proprietary but there are tools that have indexed and walked all the code. In the company and then as I'm writing the code figures out what problem I'm trying to solve and suggest snippets or blocks or functions from around the company. Right but until we have that setting an expectation that you are not to know everything right but you accept for the index of who knows there's that sick feeling of I'm doing this work I know I don't have to I know I know it's been done before. But I have no way to know what I don't know so I have to write it myself. Yeah and then the other one the last thing I have on here is really more of a realization shipping small batches of independent code is in optimizing your test strategy around that in evaluating the customer quality around the small. Batches of code that are continuously shipping is absolutely the best way to accelerate and remove risk from your system interesting you mentioned that I know where is going to be one of our longer podcasts but I know of a team that was shipping a web service weekly. And they got some feedback from customers that said hey we're getting an update every week and it's and it's their buggy. And their response was OK will ship every two weeks my response would have been OK let's ship every day or figure or realistically figure out the what was going on there to slow down the right parts of the system. To make sure they got a customer shouldn't let us a new feature shouldn't notice when they get these updates the fact that they were shipping they're getting more bugs never release made them think I think they took the wrong action I think it's. I think the problem just magnifies if you don't fix the root cause just decide you're going to ship every day that is a horrible decision because now what you have is you've given an additional week for PM's to add a bunch of. And it's not small batches I'm not attacking the problem right right it's a it's a very short sighted and very ill advised and it's going to snowball into further problems until they identify a root cause what I would do in that situation quit is I would say OK let's look at the customer feedback now let's go look at the telemetry and figure out how we detect how they are. How do we detect the signal that correlates to their feedback they say this constantly buggy so how do we detect this in our in our nation if we we look at. The customer we talk to the customer that's actually something new in the modern world testers talk to the customer yeah they talk to the customer. And you figure out what path there is and you look at the instrumentation to figure out how you detect this number one number two you add instrumentation of things are missing. Number three you release daily until you see the signals falling down your releases are only bug fixes and until you see that stable. And it's that zero in there is what talk to the customer what do you mean by buggy. Absolutely I mean that was part of the talking to the customer and you want to look for what are the bugs we can detect in the signal and what are sort of. Aesthetic preferences with the customer and the both are important but I'm imagining in your scenario buggy means things I don't know. Slow or I actually don't type this it could mean it can mean it can mean a lot of things right but there are better modern approaches to this then taking the risk. In if there's anything that I could say that is to any business leader like it is I didn't expect it when I tried it out I was surprised. And then I now understand why like you dramatically reduce the risk of your product by having a strategy that that reacts to instrumentation and by shipping all of the time yes small batches for the win absolutely. All right so thanks for listening. Yes we appreciate that hey I am Alan. And I'm Brent see you next time. 
