Welcome to AB testing podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing leadership, agile, and anything else that comes to mind. Now on with the show. Welcome to the AB testing podcast. I'm Alan and I'm not wearing a shirt. Uh, he is Alan and he's lying. All right. It's for, it's for you to use your imagination. We're back for episode one 35 of the AB testing podcast. This is still season one. Some podcasts have seasons and we have one also. It's just all season one because our topics, if we had seasons, have you listened to podcasts that have seasons? Yeah. A season kind of needs a theme. A theme would require planning an organization, neither of which we do here at AB testing. We are adaptive. We talk about whatever's important at the time we're recording, because it's the thing we can think of to talk about. That's correct. Although if, you know, if there's a high demand for seasons, I would propose we could just, uh, advance it based on years. Yeah. Except whoever proposes it, it's going to have to go back and, and renumber our prior season. Yeah. That it's that seems like work. Work. Yeah. That involves work. Uh, and we're not in, well, I mean, we do a lot of work, right? But I want the podcast to be something that's interesting for people to listen to. So we should talk about things that we think are interesting to us. That sounds very selfish when I put it that way, but hopefully we talk about things that are relevant. And again, if we go back for, if you're listening to your first AB testing, or maybe your second, we are not defining anything new. We are not predicting what's going to happen. We are not prescribing something we think is where we think people should go. We're talking about what we're already seeing, what we're seeing being successful. So if you see change coming, uh, in fact, if I go to Reddit in the testing or quality assurance subreddit once a week, someone is talking about what's happening, changing and testing our team. Our team is getting rid of our testers and everyone's a developer. What does it mean? We're here to help you figure out what that means and how to help you get your team to not screw that up because we've gone through it. And you and I have both seen very closely team screw that up. So where was I going with that? I don't know anything else where I talk about me some more. No, I would, I would even further say we've seen teams screw it up. But here's actually, here's something I'm curious from your point of view. We, we, we both are saying that there should be, when you're doing this, uh, I think we both agree that the benefits far outweigh the, the detriments number one, uh, people should move forward, but do so plan fully. I don't know, but you know, don't plan like a 1998 big design up front waterfall thing, like you'll make mistakes, but adapt. I've talked about the way we plan projects in my org at unity before at this 50% probability of a date. We do that not because to be different, we do it because it's a sweet spot between over-planning and under-planning. And what's happened in our culture is this idea of a P 50 comes up for. Other types of changes as well, not just delivery of code. What's your P 50 plan to do X? So yeah, it's a, you should have a sweet spot between over-planning like we did in the old days, just enough design up front to aim yourself in the right direction and then be ready to learn and adapt. Yeah. But in addition to it, it's like, don't let the fear of screwing up the role out. Abt again. Yeah, you should be okay. Just screw stuff up. So speaking of learning segue, I gave in between the last podcast and this one, I delivered a webinar. Thank you to test project for hosting it. Talking about this again, we've talked about now for three podcasts. So we'll go too deep, but this myth that there is a huge mindset change between developing and testing, and I really just talked about some things talked about before, went into more details on some things I wrote in an article about six weeks ago now, and I was ready to be skewered, but I wasn't. I was pleasantly surprised. The questions were more like we'd address on the podcast. Like, how do I handle this change? How do I, how should I go about this change? What do I do when developers don't want to test? No sorts of things that I'd expect the sort of facts for if developers do testing, how can they do it as well as I do? I'm a specialist, blah, blah, blah. Nobody told me I was full of crap. Although the video recording is up on YouTube and you can find it in my Twitter stream so far, only one downvote. Brent, I have to ask, why did you downvote me? Uh, I just didn't like how you were looking on there. You weren't dapper enough. No, I look like crap. You weren't representing the brand. Brent started the podcast leading way back saying I am staying away from my keyboard. Yeah. Now, now he's bringing up my YouTube video so we can see if he can download it again under a different account. I want to see what the additional, uh, comments might've been. I don't think anyone's commented on the YouTube video. I haven't looked at it in a while, but I could tell you, yeah, the one downvote, but I only have 12 up votes. So it's not, and comments are turned off. So nobody, nobody said anything. So that's what we know so far. All right. Anyway, check it out. If you didn't see the webinar, I don't think it's anything surprising to anyone on here, but, uh, that was a fun one to put together. I feel pretty good about the talk. Uh, of course I'd love feedback. You can go to our slack group at one of the three dot slack.com and get an invitation from our, our website at modern testing.org, pluggity, plug, plug, plug. So yeah, that happened. And, and when I'm really bored, I'm going to go back and look at this video of yours from seven years ago about starting out in security testing. Oh yeah. Yeah. I have a couple of things that was me interviewing Alan Mervold. So we're talking about, there was a time when I was in engineering excellence. I can't believe that was only seven years ago. It feels like a freaking lifetime and a half ago then, or maybe actually it was recorded before then. I just uploaded it then. But the idea was we had realized how much of how much learning happened in the old days when we'd be at, I have offices and we go to work. You just go into someone's office and ask him a question and get 15 minutes of gold knowledge. So what we did for a little while was these are highly staged, but we'd, I only posted a couple of them to YouTube, but we had a dozen or so of these conceptual whiteboard videos where the idea was you'd see me knock on someone's door and then come in and ask a question and they'd explain it to me on the whiteboard, I'd ask questions. That part wasn't staged. We just kind of staged me walking in with the camera running, but then we just recorded them explaining something to me and we put it on, we used them internally, put it on the internet to try and capture that, to try and spread that method of learning a little, little wider. It's a good idea. It was fun at the time. So where was I going? I don't know. Webinar was good. Appreciate your feedback. Why do you, why do you think it ended up being so different than what you expected? Do you think, I think I made my point. Well, also probably some, the people that probably hated what I wrote and hated the concept probably didn't tune in or I don't know, I don't know. I did expect a little bit more, but Alan, you whatever, but that wasn't there. I think people got where I was coming from. I gave, I thought I supported my case pretty well. So I think the thing most people struggle with is not that I felt like in that room, people felt like, okay, developers can do good testing. They're still stuck in the, but my developers don't want to do testing or my developers shouldn't have to do testing. Any one of those flavors, which have different solutions than the mindset thing. I'd like to think I did a fairly good job convincing people that if a developer is given the responsibility of testing, they can do quite well. Yeah. It's it's, those are sort of the top three excuses, right? It's a number one developers can't do it. All right. Well, no, that's false. Number two, they don't want to do it. What's your first pushback on that one? And they don't want to do it. Mm-hmm. Uh, often, often that's not really it. They think they don't want to do it. I it's pairing is almost my answer for all of those questions. I think sometimes they're not wanting to do it is a fear of not knowing how to do it or a fear of seeing value in them doing it. I've found the solution to all that stuff is to begin pairing with them, test things together and they'll figure out, Oh, I can do this on my own. And they understand the value of it in that all of what we do and accelerate the achievement, simple quality. We want to tighten up feedback loop. So that feedback loop involves a pass off, uh, that may involve time zones. Even that's inefficient. Yeah, for me, I would say, well, of course they don't want to do it. Right. We're, we're essentially proposing to take away their safety net. It's scary. For sure. And I think, I think they're just scared, but we have beat this horse to hell. I want to talk about my next rant and, and get some feedback. I am using test project as a platform. They've asked me to write some blogs for them and I haven't written blogs in a while. And if you remember, I wanted to get back into writing some more blogs to go back to our prediction episode. And here I go. Self-fulfilling prophecy. But what I'm writing about and what they don't mind me doing is things that pissed me off. So first I talked about this mindset thing, which I think is crap that a few people have a few, uh, large, I want to say large egos, large influences in the test world talk about this deep into this mindset change, which I think I've proven as crap. The next thing I wrote about was challenging the notion that test automation is done by a separate team from the team developing. I think developers should write the vast majority of test automation. I think that would actually improve design. I made the points in the article. I feel pretty strongly about that. And I'm thinking about what else pisses me off. It's like so many things make me mad, but I thought, well, maybe I'll write something on exploratory test automation, but I've kind of done that before. And I could give some examples. It could be a more techie article and put some code samples in it, how I've used tools to help me test, but then something hit me and I want to run it by you. So you've been in testing a test automation for a long time. What do you think of when I mentioned record and playback automation? Like what's your, what's your first reaction? Good tool when used for it's for what it's good for. How's that for it? Says how would you, well, that's, that's crappy. So let me go ahead and answer that question myself because your answer sucks. You get an F minus record and playback tools. I used to hate, they were flaky as hell. They weren't maintainable, but I was in a different mindset back then of automate all the things or many of the things, at least all the things that should be automated. But if we look at what we agree about, I think the test automation pyramid is a reflection of what happens when you have well-designed code. You have very few of these end to end tests and a few of those tests are UI automation tests. Uh, we used to avoid record and playback tools because there were, the code they generated was flaky. We didn't want to deal with all the crap and they were marketed at not a developers. They were marketed at testers who did not know how to code, who are not the best people to be choosing what needs to be automated in my opinion. Now, if we were to market those record and playback tools to developers, one, they would use very few of them. It would be faster than writing anything in selenium. I have a bunch of people gonna be mad at me. Yes. Selenium is a good framework. It's not an efficient way to write you animation. I prefer test cafe. A lot of people use cypress. A lot of people don't like them for reasons A, B and C. I'm kind of thinking here's the, here's the hill. I'm thinking of climbing. I think given that the good record and playback tools from test to Mabel, Autify and others have eliminated the vast majority of the flakiness from your web automation tests. And I, I assume for UI automation with Appium, there are some similar things, but if that flakiness is gone and we only want to have a few of those tests, why invest a lot of time relative amount, a lot of time of writing tests with selenium when you could just record and playback those suckers and have them stay fairly reliable for a long time problem is they're marketing these things towards testers and not developers, but I kind of think the time is ripe for record and playback to be the default way to do web automation. So I don't, I don't have any current experience on recording and playback, but I think the standard by which I would, I would hold that is essentially it's going to be a combination of, uh, that, well, everything I do is by ROI. Okay. But the problem with the record and playback stuff is if they're intended to persist the traditional record and playback, the problem with that is they're cheap to get out, but you spend, I don't know, a significant more time on the backend of the price tag of this one, like maintenance costs is through the roof. That's the problem with, with record and playback. Now, if you have a, if you have a measuring it by the total price tag, including the maintenance cost over, over the expected lifetime of the automation, if that's been improved, uh, yeah, why not there, there, they're cheap to, to grind out. And if the current record and playback systems have gotten to the point where they can write maintainable code, like, Hey, we noticed you do this same routine over and over again, we've. Well, it doesn't matter. I mean, it used to be like that. Oh, well, the burden playback sucks. We can't reuse code chunks. I don't, I kind of don't care if the code underneath is spaghetti that I can't read and I can't, I can't do anything with manually. I don't even care if I can see it. If I just need something that I can rely on to, to continue to let me know that some scenarios functionality is correct. And it withstands changes underneath, which is what all these, these higher end tools do, I kind of don't care about the code underneath. It doesn't matter to me. How does, how, how does current record and playback tools implement the Oracle? How do you define pass and fail and current playback? Well, it's up to the human to define the Oracle and what, what you want from a scenario. For example, you may want to see a, you have an account creation scenario. It would be a good one. You probably want to test end to end and not just at the integration level. Cause that's, that's critical part of your onboarding of customers. That may be you get, there's two things you do. One is at the UI level. You make sure that you get to the signup successful page. That would be the Oracle is this page exists by test past, but you also want to have a parallel test I would kick off that would verify that the back end matches. Yeah. So I got to tell you often in the web automation world, people may only web verify the front end, like, Oh, the, we got the right page that past. You need to apply a little bit of systems thinking to that, but that's the Oracle. You decide what the outcome is. You want what is the outcome of what you want to do from your test. So where are these tools help over traditional automation in the, in selenium, I would find the web element for, for, to create a login or an account on the site, and then one by one, I'd find the elements, wait for them to appear, add my data to those, press the button, and then verify. I got to again, through web elements, something like the thing I got to, I changed the ID of one of those things. Generally my test breaks. So what happens is, especially this is where this is going back a step to developers owning the test automation. What happens now is the developers change some IDs, move some things around. All of the test engineers, remember on a different team, their stuff all breaks and they have to go figure out why and change their test. That is a, that's an inefficiency. Now what these tools do, audifies my favorite by no lot of people use Mabel or, or what's the other one I mentioned? Test him. I think there are probably 20 others by now. It's a hot market. What they do is they figure out enough other information about the control that you can change all kinds of crap about it, and they will still figure out what control you intended and often let you know, like give you a maintenance step. Okay. This is, is this the same thing we'll update your, we'll update our underlying structure. So they make it harder. They make it harder to screw stuff up by inferring and whether you want to call it AI or ML or just being smart. I think on a side tangent, a lot of the stuff we did as smart monkeys 15 years ago, that people are now calling AI and ML. So it's a fine line. But what these tools do is they allow the underlying code to change and for the tests to still run and, and in most cases help you update those to help them stay stay relevant or stay accurate in the future. So in the face of, of number one, these record and play tools, if the initial technology is such that, uh, that initially the maintenance costs of it, cause as you and I know, like it, that's the, that's the Achilles heel of record plate is made it, if they've made it such that the maintenance cost is extremely low now and then even furthermore, creation cost is low. So I can have a scenario. It may take me four hours to automate and selenium plus ongoing maintenance costs to keep it going. If I can automate that workflow with a record and playback tool in 10 minutes and have that test continue to run for weeks or months or years, why would I invest in creating a bunch of tests in selenium and again, it could get on weirdly if you add hundreds and hundreds of these, but that's the wrong thing to do as well. Uh, yeah. And I agree the, the, at least again, I don't have any recent practical knowledge on this front, but, but based on how you lay out, I don't see the flaw, like in addition, you call out that where a maintenance cost is incurred, these tools are now smarter and telling you and asking, uh, essentially the way you phrase it is saying, Hey, is it okay? Is it okay for us to make this change? Like we're uncertain. Well, what do you plan with? Uh, what is, what is in your hand? This is my child, Brent. So here's my statement. So all that said, give me feedback if you think that's crap, but I kind of think there's something there. I think as far as all the vendors out there marketing tools to testers, if you want to win, if you want to be like, there's a bunch of competition. If you want to get the most customers and, and get more market share, start marketing your stuff to developers. They're going to know how to use your stuff most effectively. Testors are not your long-term audience. Yeah. I completely agree. Uh, the, and in addition to that, I'm wondering, like in this particular topic, like knowing you as I do, right. You're, I don't know, a half an insight away from just going all in and just saying test automation is dumb. You should record them playback. I'm wondering if you shouldn't, uh, set up a special AB episode on that front. Uh, with one of the listeners I'm here, I'm specifically thinking, uh, Percy to, to go through that thought process. I like, there it is an open invitation to anyone of the three, especially if your name is Percy for a, uh, that would be Friday afternoon at six P at Friday evening at 6 PM your time on a Friday of your choice, as long as it aligns with our two week schedule. There it is. Let us know. I'm looking at Slack. Oh, we haven't posted this yet. So we don't know yet, but there it is. And YouTube. We're not live streaming. We're not live streaming. We're live streaming. Did I press record? At least we're recording. Well, that is a step in the right direction. Okay, cool. I want to move on to another topic and I think this is one that's going to involve both some discussion today and maybe some homework for next time. Oh, uh, and I'll see what you think. And I, cause I want to get one, some homework for us to do. And then another one ideas from our listeners on what we should do. So the topic is, it's not just personality tests, but I was reflected talking to my daughter last night and she was sharing a bunch of different, uh, different personality or characteristics tests she's done. And she's done MBTI. She's also done the test that says which, uh, Hogwarts house you would live in. Uh, some other ones as well. Another one called any agreement here. Yeah. Like which Marvel hero would you be? Those are a little out there, but I want to talk about the ones I've done. Cause we share a couple of these together. We both done MBTI. Yep. Myers-Briggs type indicator. That one is, it does give some good insights about yourself. What I found most valuable in that one is understanding. Cause again, it doesn't tell you how you act. It tells you what's difficult and what gives you energy. Uh, but there's some, there's some valuable information there. We have both done a thing was very popular at Microsoft. Not sure if it's still there anymore called insights. Is that still being used there? Uh, not to my knowledge. Remember insights. I need to look it up. I have my book full of evaluations there. You get a color and I think I'm blue, red. I can't quite remember. I'm definitely not green, which means nothing to anybody who hasn't taken it. Yeah. And, and actually I am red, blue and they're nearly equivalent. I am like 0.05. I should look and I believe I may have changed. I think that one may change over time. I'm not sure. My roles are changed a lot. So, so actually I've done this research. I will tell you upfront, uh, the, the color one uses is no different than MBTI. So it doesn't change. It's who you are. Well, it does change because the, the, the problem. Carl Jung would say that with your MBTI, which, which it's based on his work. You don't change. It's your preferences. They're innate. They do not change. Well, right. Carl's Jung's work would say that MB, MBTI is. Actually a far cry from Carl Jung's work. Okay. It's agreed. Although it's based on that, it's based on that knowledge. So there has to be, there has to be something to that statement. But what I'm, what I was trying to close on is MBTI and insights has a direct relationship and, and the insight stuff is actually just using two letters from. MBTI. Okay. And it's I versus E or T versus F. And so if you are an extroverted feeler, you're a yellow, if you are a, yep. And basically, which is don't we have the same MBTI type? We do. That's why I'm, I'm like a little more, little more, which is weird that I am a blue, red, and you're a red, blue, because you're the one in the analytics role. Uh, yeah. But if you look at mine, they're really close to exactly the same. Like there are times I just simply describe myself as purple. Okay. I want to, uh, I will get mine out before next time. I want to dive a little bit deeper. We've also both done strength finders, which I think is a really good one as well. Yes. And so I want to talk again a little bit about those. And the reason is, is I mentioned this in a five for Friday. I think a few of the three have done it, but there's a new one from Pat Lanchoni, who's the author of five dysfunctions of a team and a bunch of other books of which I have read every single one of them, probably at least twice each for now. All of them. I'm a huge fan of his work and his podcast. And he has a new one. He calls working genius. And I'm not going to do a full advertisement for Pat Lanchoni and his working genius, but he says there's six types of six types of sort of leadership genius you bring to the table. Maybe I'll bring mine up. Maybe I will talk about them a little bit. If you haven't done the test, he should not even give you a code for one if you want it, you don't, you didn't have to pay for it. Okay. So he says there's the genius of wonder, which is the natural gift of pondering the possibility of greater potential and opportunity given situation. The genius of invention, natural gift of creating original and novel ideas and solutions, genius of discernment, natural gift of intuitively and instinctively evaluating ideas and situations, galvanizing, which is rallying, inspiring and organizing, enablement, which is providing encouragement and assistance for an idea or project and tenacity, which is pushing projects or tasks to completion or achieve results. I have, I'm curious for your opinions on this. I'll give you more time to look at it before the next podcast. Cause sometimes I like it. Cause the idea is that there's probably two of these that you love doing two that you know how to do and two that you suck at. You need to work with people that can do that to help you get stuff done. But I look at those and I've had to do, well, I suppose it's fair to say I've had to do all of those. So maybe the question is to look at which ones the evaluation I did says that I don't like doing cause they, they didn't realize it didn't like doing them. And there is something to that as well. And from mine, I won't go deeply into these. I won't go into everything, but I am best at discernment and galvanizing. So I am good and enjoy using intuition and instincts to evaluate and assess ideas or plans. That's the N in INTP and I'm galvanizing, which is I get people to do shit together, you're good at and enjoy rallying people and inspiring them to take action around a project task or idea. And what's funny with that is my inspiration for wanting to do that and to get people to rally around that idea, which may lead into another topic here in a minute. So let's put a little pin there is that I read one of his books. I forget which one it was called now, but this idea that when companies are in trouble, like when Microsoft had their security issues back in the early two thousands, the company rallied around improving in security. There was a rallying cry. People were excited about it. They were invested in it. They wanted to get better, learn a bunch of stuff. And they built this culture of security. Well, the concept of this book was you want to build that rallying cry before there's a crisis. You want to have something you align on and the organization wants to do before there's a problem that they're trying to solve. And so that was very inspiring to me. So that's kind of the way I've approached a lot of my leadership since then. So there could be a weird bias in the reason that one's such an important for me, but I do enjoy that. I love seeing organizations come together or people across an organization come together to solve a common problem. And I'm going to give you an example about that in a little bit, but curious, it sounds like you've typed up and are glancing at the, uh, the site right now. Any initial feedback before we maybe dive a little deeper next time? This is interesting in, at least in some regards, obviously I have not ever heard of this before. It's it only came out like literally three months ago. There is, I mean, I could see connections to this to both strength finders and the MBTI. Actually, he brings it up in five dysfunctions of a team that he is a big fan of MBTI and likes to do that across all of the teams he works with. So definitely some influence there. Yeah. The thing I think is the thing I like about MBTI is it really helps promote understanding when used in the right context, right? Um, an example is basically one example I bring up. So I know a lot about MBTI and I'm fairly certain I could teach a course on it. Like way Ts and Fs look at the world is different. Just as an example, if I'm these are intuitive and feeling for those that aren't doing that the MBTI lingo TNF is thinking and feeling. That's right. Thinking and feeling. Yeah. What did I say? Touching and feeling. That's not it. Yeah. No, that's a different letter. And that particular letter it's what it, what it's evaluating is your general preference around how you decide. And so the other letters sort of focus on what sort of things do you bring to the table to make the decision? The TNF is what standard, but is it that you use to decide and T's sort of decide between right and wrong in a, in a logical sense. Fs their standard is good and bad on a, on a more of a morality sense. And one of the things that I've learned from using this is that if you go in and have a conversation as a T with someone who's an F and say you're wrong subconsciously, the way they interpret it is that you just told them they're immoral, right, which will bit flip them. But the other thing, so you and I, we're very different on strength finders, those results. And I just look at this list. I also suspect you and I will be very different. I think so too. Yeah, it's about, it's a little bit more about execution and how you execute with your personality. Anyway, I'm curious. Oh, I'll take it as soon as I can. Yeah. So, I don't know, let me just segue and talk about how I've applied that second one and maybe for next time, if you, if you take it and get some examples like, Oh, Hey, I've done this before. So as our listeners know, a few months ago, maybe six months ago, I expanded my role to cover a lot larger. Well, basically all of the infrastructure support and everything for services at unity. So what that means is I have this team of roughly, roughly 75 people in a variety of different roles, how the team had functioned previously and it worked and I'm sure we all know teams like this, they're very busy. They did important work, but it was, it was just a lot of incremental development across a whole bunch of different areas, which is good. They kept teams running. I mean, our, our slowest deployment pipeline is 40 minutes. So it's not like they were bad at anything. They're actually very effective. But one of the things I wanted to do the rally and cry, and I won't go into any details on what our rallying cries were, but I picked a small number of projects that these are some things we got to keep the business running. We got to keep accelerating our every development team's ability to deliver easier, faster, safer. We talked about that before, but also there are a few big rocks. I want to move. I wanted to ignore as much as possible organizational structure and say, if you need help, ask for it. If there's something else, something happening outside of your direct team, you think you'd have some insight on, please go work on that. Get your day job done, but, but please find ways to collaborate across teams as we try and move these big rocks. So I set the rallying cry and Darna, if it didn't work, I mentioned before you, I manage our documentation team. And one of the things we're working on is spinning up our own customer facing doc portal. If they weren't in my team full of a bunch of people who really, who know how to build pipelines and deploy stuff to web services, that would have been a lot slower process. We just kicked that off short time ago and that pipeline is already built. And it works and it's magic and it's great, but this is multiple teams working together. This is world of Warcraft guide to project management. We brought varying skill sets, regardless of managerial structure. And we're not a holocracy. We have people managers who are responsible for people's growth and finding the right work, but we built a system in which they could easily help each other out and find where they could align and make sure everything got done. So pretty happy with that. Uh, but again, that came from the rallying cry. Like we're going to, we're going to work together on stuff. I'm going to provide some vehicles for you to discover what's happening across the org and these few things are the important things that we're really going to move on and set goals around. So also for those rallying cries, I put some pretty, put some pretty big goals in place so they could have something hard to shoot for what they get from that, which ends up being able to show progress is they get some metrics that actually matter around those ideas. Yeah. I actually think I have a slightly different perspective on what you did, but it's an alignment. I actually think what I, what happened here was, uh, what's this last name, Johnson, Johnson, Steven Johnson, where good ideas come from. Yep. Yeah. What are the books we should just make the modern testing bookshelf with our favorites. Yeah. It's that one where accelerate accelerate how to measure anything. I think those would be, oh, and a lean startup. It probably goes. Yes. That is, that is a very good start. Yes. The, but right. You had constructed, so life had constructed an environment where these two major, but typically separate big ideas were forced to come together. And then you further sort of motivated the birthing of new ideas. By, uh, your rallying cry. And I mean, I, this is from Steven Johnson's book, but it's, it's apropos is this is my job as a manager or as a leader of a large team is to create the right opportunities and a Steven Johnson says chance favors the connected mind, my role is to make these opportunities for people to discover. One find work that's interesting to them and don't worry about organizational boundaries and an org like mine where we don't ship a thing. We make lots and lots of things. Having ownership within a team is going to be a bottleneck. Yeah. The, the, the thing that I think is a strong skill you have, uh, with people on that front is, and I'm nowhere near as skilled as you, uh, on this one is just motivating the discovery. Right. You have, I don't know how many people let's say 50 people underneath you. You have high confidence that they are as smart or smarter than you are. I have actually 75 people in every single one of them is smarter than me. Right. Your job is to get that smartness working together and knowing that once you've achieved the tipping point of the maybe a fifth book, you can get out of the way. Your job is just to get it over, over the tipping point. Well, let me go to another one of my favorite books. This is the modern testing book episode. So the, the, the, the leaders guide to radical management. And this is the, um, what's his Facebook that'll come back to me in a minute. But this was a business writer worked for Rodoloff for Forbes magazine. Still writes for Forbes. Why am I spacing out? Denny, not Denny, maybe Denny. Anyway, uh, he said, create an environment for people to work in and get out of their way. And again, it's all different ways of saying the same thing. I have to give you a funny story here because I frequently tell my team, like, I may have an idea, but please take all my ideas with a grain of salt. You're doing the work. You make the decisions. And I was talking, I can't remember if it was a one-on-one or a meeting. And I mentioned something and one of my newer leads was like, Alan said, we should do this in, in, in a meeting. And everybody else being just laughed at him. Like, it doesn't matter what he says. We make it cause they know, they know they've worked for me for longer, but, uh, largely we have that culture is that I could say, you know what? I think we should only deploy on Saturdays. They would all tell me to go do something anatomically impossible with myself because they know that I am not the one to make decisions like that. I am going, my, my job is to set them up so they can make the best decisions because I, I, they're one, they're smarter than me to the closer to the work. Reminded me of that story. I thought it was funny. I had to go, no, here's what we should do. I could add a kind of phrase of like, I don't want to give all the details of, I said, yeah, there could be a situation where it's, it's true. It's not universally true. I rely on you all to make the decision on what to do here. But most people get that after, you know, you have to, again, going back to L'Ancioni, you communicate the rally and cry. You clarify the rally and cry, and then you communicate the rally and cry again, something like that. It's working well. Oh, one other story. And again, going back to management is again, I'm going to be very fuzzy on details, a lot of times just setting yourself up to take advantage of. You spend a lot of time setting yourself up to be ready for what happens next. Does that make sense? Like you, you. I spend time making sure that I can be adaptive and I had a tiny little change happen with the career director, someone on my team, and it immediately, immediately solved a major, a major, I would say a problem or a challenge, but a structural thing I've been thinking about how to take care of on my team. By one person making a different choice, a bunch of stuff fell into place perfectly and everybody involved is super excited about it. So being ready for those things is what I like to do. Just, I love adapting so much. And, uh, I've told my team this too, is that everything I do in management is agile. It's lean. I try, I will try stuff all the time with the expectation that a big chunk of it won't work and they're getting used to that, which is great. So yeah. And that's what I do. And that's, that's, that's what, that's what energizes me is what allows me to be a manager and not hate it. That yeah. You're in a perfect role. Like for the majority of the time I've known you, you've been anti-management. Right. Well, I didn't want to be a manager at Microsoft. Things are better now. But when at that time at Microsoft, it was on, how would you put it? It wasn't a good thing to want to manage a high performing team. You wanted to manage a team with a few good people, a few, okay, people and a few crappy people, because you had to fit a curve, whether you had a curve or not, cause we had a curve and we didn't have a curve. The curve lasted through several different versions of the review cycle. I didn't want to deal with that. There was too much emotional and bureaucratic and political baggage that came with being a manager at Microsoft for the majority of my career. There I've heard a lot of that's better now. It is a lot better, but I mean, we're not even. So I'd been here now 26 years and that really started when Satya got here and it's still far from solidified the, but it's going in the right direction, but I totally know what you're talking about. Like you were the review system physically incentivize you to construct heroes. Right. 100% correct. Yeah. I mean, obviously heroes are rare and in the world that we're in now, particularly Microsoft and in my case, I'm in Azure that that doesn't scale. Like you, you have too few heroes to come and save the day for all the different problems that we're having. You need rockstar teams. Um, and if, and let me give you an example of, and again, it's, it's a lot of different stuff I know, Mike, I don't, a lot of people have noted my, uh, consistent denouncing of Microsoft. I found it hard to do the work that I loved doing there eventually. You know, one example, like here, if I have, say I have 30 people on my team, just knock it out of the park and all deserve big raises. Let's say HR comes back and says, you don't have the budget for that. At Microsoft you're screwed, right? I mean, I would go talk to our CEO about that and show data that showed what people were doing. And I would get that budget we could take care of them. If people do good work, we can take care of them. Yeah. I don't, I don't, I mean, there is a budget. I can't just lie and say my whole team's great. Everyone gets a 20% raise. I can't do that. No, it's, it's, you can't do that. And even, and even if you could, um, it wouldn't be the right thing to do because the point of the incentive model is to incentivize. And if you were to do something like that, it would, it would probably lead to bad behaviors. Like, yeah. And like another thing, other one for the, the book list is we never call it out explicitly, but a lot of our, I use Dan pink's autonomy, mastery, and purpose model a lot, like our whole review discussion is based on tying the work you do to purpose. So we worry about purpose a lot. And we want people to understand how the work they do impacts the company. And we, of course, as Dan pink says, pay them enough money where money is not the issue and people are motivated by a reward in the short term, but in the longterm they're more motivated by having work that they enjoy and that challenges them, I'm not saying don't pay them. The motivation you get from a pay raise is short lived. It is. And there's been lots of different studies on this that essentially once you've gotten past sort of a bare minimum of fiscal stability, I'll just call it that way, whether, whether it be, you know, you have big dreams of, you know, toys or, or really all you need money to do is to make sure you don't have to worry about it, which is kind of, that's kind of the position I'm in. I, I, I only want to get paid so much as I don't have to ever worry about it again. I had this, this goes back to some pains in my, in growing up that I just like, look, I don't want to ever go back to that. And as long as I have enough to make sure that doesn't happen, it's great. Purpose then becomes the better incentive. And that's exactly it. And I love that our review is, which is we call them three Q's, three questions, three times a year. The first thing we talk about is like, why is what you do important? I do most of my discussion on that. And then some are on mastery. We have a discussion around how it's going. And we ask questions around, usually I ask, what would you like to do more or less of or start doing or stop doing? Like a starfish with respect. Yeah. Yeah. And that, and we have a deep discussion about that. And it's stuff we talk about generally in one-on-ones, but three times a year, we have a longer and deeper and more reflective one-on-one we call our three key process, and it's about making sure we're connected with what people want to do and making sure they enjoy. We are so lucky in this world to do jobs that are hard and challenging and fun and get paid for them. So if anyone's not feeling challenged and having fun, I want to take care of that because that's, that's, that's not good. That's no bueno. See no bueno. Do those words make sense? No, I've heard them before. Spanish, not one of my languages. No bueno means not good. Yeah. Okay. I'm I'm, I am in the ballpark. So cool. So let's go ahead. It's a quick question. Well, what are the three questions again? Oh, um, why is your mission important? What are you doing? How are you doing? Or how's it going? That those are the questions, but my variation, every manager has their own take on those. What I do with the why question is I make everyone tie their mission to the customer. How does this help our customer? So if they don't get there in their, in their first track at it, I walk them there because it's for me, it's really important. They realize that we're at a size company where the work you do affects our customers indirectly, but it affects them. And then what are you doing? And again, I know the work they do. So I want to hear like, what are the, what are you most, what are you most proud of over the last four months? What's something you're really excited when you talk to your parents, talking about your work, what do you talk about? I want to get an idea of that gives me idea of motivation and then how it's going, I mentioned before is the, is the starfish retro. And then I add the, the fourth question, which I think is super valuable is I asked them, what can I do more or less to help you be successful in your mission and it's just my way of getting some feedback from them. Yeah. And I actually think that's an important way to go. I was, I was, I was taking your three questions and then thinking through, like, I, I like to keep these things simple and the more it, the more it's simple, the more you can, you can have a really exciting partnership with your report towards positive goals. Yeah. And that's where book plug again, this is going to be a record, but radical candor says you have to, you care personally challenged directly. So the better relationship I can build with these people that work for me, the easier it is for both of us to give that direct feedback to each other. Yeah. And in that, that involves, I tell my staff, my very first goal is to earn their trust. Absolutely. A million percent agree. All right. Cool. Well, we meandered our way through another AAB testing podcast. And as a teaser for next time, I'd like to talk about the, let's talk about, let's start with working genius. But if you have any other, maybe we'll just kind of walk through our results on the other ones as well. It'll be like the personality type review episode. Is that the homework I got to take working genius and then bring in the other things. Yeah. You can find it just to talk through them. I'm also curious to hear if there are others of these. I may also send you the quiz to help you figure out your Hogwarts house. Cause that may be fun to talk about too. You slithering you, but you're not really, you're not, you're, you're, you're more of a Ravenclaw. I think we'll find out. I'm the blue one. I've done this test. I'd forget which one that is. I haven't done it. I'll have to figure it out. All right. Good. Okay, man. Thank you everyone for listening to yet another episode. Well, by the way, did you, someone came to our slack group again, you can get a link from modern testing.org to join, but someone said they started at episode one and are working their way through. And I can't imagine a more painful introduction to AB testing than starting with episode one, but it happened. I think it's actually very enlightening. I've gone back and listened to it. And it's like you, if you were to binge on it, you very quickly see how we improve and then how we're on our way back down again right now. Well, it could be, could be. We definitely have our ups and downs. All right. Well, this has been fun. Uh, thanks for hanging out with me today, Brent, and I'll see you in a couple of weeks, but thanks everyone else for listening to AB testing. Bye. 
