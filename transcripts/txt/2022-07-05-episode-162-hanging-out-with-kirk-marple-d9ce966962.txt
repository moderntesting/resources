We know Skynet is coming. It's just a matter of when and we see clues all the time. A joke aside, that's why I say I went to data science so that that machines when the machines tick over the world, they look back upon me with fondness. Welcome to A.B. Testing Podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing, leadership, agile and everything else that comes to mind. Now, on with the show. I guess to your Kirk, introduce yourself. Hey, yeah, Kirk, my book on this. Yeah, I'm very cool. Thanks for coming on the A.B. Testing Podcast and hanging out with Brent and I. I didn't bring a beer. I had a bottle of wine here. And there's a reference there. Someone once told us that our podcast sounded like a couple of guys sitting around having a beer, talking about software. And I think we've only had a drink like once. That's when I left Microsoft. We did my last episode when I was there. So it's all right. We kind of hang out. And also we got some feedback when we did our last maybe was our last guest on the show, somebody said the interviewers talk too much, but we're not really interviewers. We're just going to hang out here for a while. That's our plan for today. Yeah, Kirk, Kirk, you weren't expecting an interview, were you? I can go. No, he he's it's Friday. So he's he's listened to a few episodes of the podcast. He know he is getting into it. Yeah. So fair enough. So if you were a color, no, I threatened to ask our last guest that too, because I don't have an answer for that. So not only Kirk is I want to talk about what data and what unstructured data is a little bit, what unstruct does will give you, we'll give you some plug time, but you've also had quite a varied career going back. I think you may have started at Microsoft even before I did. Yeah. What was it? In 94. Yeah. He beat me by a year and worked on some things I heard of. So go ahead. I don't know if Brent did the LinkedIn stalking that I did. So I, in addition to Blackbird, which was some fire, remember, it was some multimedia stuff for MSN way back in the early days of the internet. It was, yeah. Pre HTML, pre pre web, which is like, I mean, pretty interesting. So I mean, those days were pretty crazy. Do you remember? And I think Brent started in 96. So it was like 94, 95, 96. I started January 94. Oh, nevermind. I'm wrong. For some reason I thought I was there. I started before you. I am, I'm dumb. So I remember when we had to get special permission to have internet access from our desktop computers. Oh yeah. I still remember when the first guy in the office got wifi and it was like, he had wifi and the one in the offices, it was like a whole big deal. Like, and I don't remember why he got access to it, but it was, yeah, he had, it was like the first thing. So yeah. And I had, uh, oh, if you remember, we didn't even have TCP, IP. We had, what was our internal protocol we used? Oh, uh, net bias. Or was that, Oh God, you know what I'm talking about, right? But it's not, no, not yet. No, it was, it was net something. He's, he's absolutely right on that one. We had our own very noisy protocol we used ahead of. That we had, you know, net buoy was what was, oh yeah. Then it was, was it land man? Was that part of that? It could have been land man was the thing that Mike, I, you know what I'm talking about, there was a, we had our own protocol. Like when you installed windows, you had to go install this extra protocol off this server somewhere so you could communicate, I can't remember what it was called just, I forgot about it till now, uh, lost cause, but speaking of wifi, I remember also having a window CE device, like in two year, 2000, and having the huge battery pack on the back where he could also put a PCMCIA wifi card in, oh my God. Yeah. I did some contracting for wind CE at like when I came back for a bit, but I was trying to get a startup going and just did some contract on that team. And yeah, it was a, it was a crazy code base. Yeah. Yeah. That was, it was fun. It was fun. I think in some ways that product just couldn't find its niche ahead of, you know, the head of its time. Now there's embedded software everywhere and everything has embedded software. But oh well, we made some stuff. So, uh, go ahead and going crazy trying to remember now, cause I remember losing weeks of my life, trying to like literally weeks of my life, trying to get my damn new computer to get on the frigging network. And I started off at Microsoft network testing. I went 95. So I, I remember it and, uh, but I, I can't remember it. Yeah. Yeah. Yeah. I don't, I don't miss those days. I honestly don't. Right. It trying to figure out how to install OS warp. What the first version of NT came out 50 frigging floppies. And then if you got one that had a bad sector, you had to hunt the rent. I hated those days. All right. So here we are so slow. Here we are in the 21st century. And, uh, so, uh, Kirk, you've done a whole bunch of stuff, um, sort of found your way into, you know, not only unstruct today, but through some jobs with data and analytics. So what was your shift from, and also some product ownership. So you've been, uh, like, yeah, how, how has that been? Is it, is it been interests opportunities, a little bit of both? Like, what's the, what's the story of your, like, I don't want to give you the whole full biography here, but like, how did you get from A to B to C to D? Well, I think it's, it's actually interesting. I mean, so when I was, gosh, 14 or something like that, I was like, okay, I want to go to school for computer graphics. It was like right when that, I mean, it was all coming out and I actually started college really early at 16. And so, but I remember looking and I thought I was like, oh, who has a computer graphics major? Cause that was starting to be the thing and nobody really did. And I started to like looking at computer science and things like that. And so I guess I always, I mean, thinking of that in the media sense, I always kind of started there kind of even, even when I was starting to think about computer science and in undergrad I did, I did a bit of computer graphics, but then when I got out, I ended up doing a, like a surprising number of almost things that are related to today. Like my, one of my first jobs was, it was, I was working for the army Corps of engineers for the basically like doing, oh no, I was like, no, it was the, it was a Marine Corps contractor in DC and doing laser discs of maps for, for the Marines and doing basically plotting graphics on top of laser disc based maps. And so it's like crazy. I think that's almost kind of what we're doing today. Like our app that we have is using the map box and putting graphics on top of map box. And so it's, if you look 30 years ago, I kind of, it kind of started there. So a zigzag path to get back to it, huh? Yeah. I mean, I ended up doing, I was at a really interesting company. I was, I was living in DC for like five years and a company that did all image processing software. And so they were, I mean, there's still around, which is interesting. Really like for a lot of medical use and different things that do some really cool stuff, but I worked on a lot of file format parsers. And that, that kind of common thread of like reading the header of a TIFF file. Like I was doing that when I was 21. And so those are the kind of weird threads that are like still there. Like I'm still writing file parsers and dealing with like that kind of stuff. And when I did my masters before Microsoft, I actually did, it was kind of, it was originally, I was going to go from computer graphics and I kind of pivoted more to multimedia and my thesis is like basically this survey paper of like multimedia technologies, which is like really related to exactly what I'm still doing today. So that's why I think it's always been an interest, but then I always bounced into these jobs, like even at Microsoft. I built the streaming audio control for, for Blackbird, which competed with real audio. And so that, that was kind of what I was working on. Uh, bringing back the memories. I remember real audio, but they're still around, right? Or maybe they finally went out of business. I can't, I can't keep track. Something's there's something there. They bought somebody, lots of memory ticklers today from the old days. Yeah. I was looking at the time and, and, and, uh, in a prior life, I was the QA middle manager for a media center and particularly the AV stack. And I'm, and I'm now wondering if we had a dependency on anything that you coded. Cause we, so I was in, I moved over to, uh, Microsoft research for like three and a half years. And so I was doing 3d virtual worlds for, for like, it was called the virtual worlds group. Um, and that, that was an incredible job. I mean, every, the people in that team were just rock stars. Like summer now, VPs at Microsoft and like almost every, the devs and is now like a CTO somewhere that started a company. And so it was just one of those incredible teams. And the, uh, that was, I mean, just so ahead of its time. I mean, that was like around 96, 90, 96, 98 ish. And you, I mean, that technology is like this whole metaverse concept and all that. Like we were doing that 20 years ago and it was direct, a lot of direct 3d, it was, I mean, network kind of multiplayer technologies and across the network and, um, but that was, I mean, that was honestly one of the best projects I've ever worked on. So, yeah, I was in, I was a dev manager of windows media player at the, like after that, or yeah, I guess when did yeah, we didn't this media player, um, right before I left. And so that was, uh, that was pretty fun too. Yeah. I think we had a dependency on windows media player. And then this other thing that I'm forgetting, oh, the media framework. Uh, which, which cracked me up. Cause it was, uh, the acronym of course was shortened to MF. Uh, yeah. Anyway, good times, old times. I liked the current times, um, much better. So you had to focus on multimedia. All right. And that if, if I go through your LinkedIn stock, then I go, all right. Both to media. Then he went to another multimedia company, like in a reading codex and things like that. And that brings back painful memories streaming. And then suddenly, suddenly VP of product development for stats LLC. Now I started to get, uh, getting really excited. Then that seemed to be like your pivot towards more of, um, more of a, a data as a value product by itself. Why, um, what it was, so I had a video transcoding company for about 10 years. And so that kind of sucked up all of my, like a lot of, a lot of my, my career and then I sold it. And then that's where I kind of had to reinvent a little bit of like, okay, I want to get out of broadcast. I mean, I was in a media entertainment for so long and I ended up at general motors, I mean, that's where they actually wanted somebody that had some video background and they just bought crews and they wanted to build a data pipeline from the data, the LiDAR data, the video data, all that kind of stuff off that they were using, basically recording it, um, for testing and get it in the hands of their data scientists. And that was, it just pulled together like all this, like a little bit of 3d that I'd done and the time series data and all this kind of stuff. And, um, then I, I mean, but the problem is GM is like working for the government and how quickly you can get stuff done. So I got a little bored and then got recruited up to Chicago for that job. And, uh, but I'm a, I'm a big baseball guy. So it was a cool, like working at a sports data company was, uh, was kind of a cool thing. And we're doing like a lot of, um, computer vision, um, doing like media search and stuff like that. So yeah, then I, I did like some drone analytics after that. Um, there was like an IOT digital signage job in the middle there. And so, yeah, I kind of had another sort of five, six years of just finding my way through VPC, T O and CTO jobs at a bunch of places. And then I started another company. So one, one thing just as an aside, do you, you want to hear a question that will make, uh, Alan instantly pass out. So do you think the Mariners are going to get to fit 500 by all star. I all star. Oh, it's going to be close. This has been a painful year. So we, we, we Seattleites say that a lot. Man, I remember I went to like 30 games the year they won a hundred and was it 118 games, 115, a lot of games in the steroid years, maybe who knows. Yeah. And then have them that was post that was, was Johnson still there then? I don't think so. But in our member, like, I remember, gosh, going back to 95, like, yeah, we were in my office with the radio on listening to those games at the end of the year, it was just, it was just. I got sucked into it. And it's, it's tough. It's tough to be a long time baseball fan in Seattle. It is tough. No, I mean, I've, I was a Philly fan growing up because I grew up in Pennsylvania and then the, I got, I'd been in Canada for a couple of years and then came down and then just, I kind of got into that whirlwind of 95. And I'd just been here about a year and I've been America's friend ever since, but it's, yeah, it's a painful life. It is. So for me, so I'm more active, more active, actively watching the Mariners. Right. It was actually for father's day. My, me and my son went out and he genetically picked up his favorite baseball team, being the Dodgers. And we were there and he's like, dad, you know, everything about every one of these players you have it. Dodger's still your favorite team, right? Dad. And I'm like, uh, yes, I just, uh, I just don't get to watch them as much as I do them. Yep. So let me add a quick rant before we go on. So, um, I stopped paying for cable television. Three years ago, I just got tired of Comcast still using for their internet, which is actually getting better. I'm probably not so angry about it, but I don't watch a lot of live TV. Um, so I'm okay. I'm Amazon has standards. I'm big soccer fans. So I can get what I want. Like T-Mobile gives me like free Apple TV plus, uh, which is going to have the MLS next year, so I'm excited about that. But there's not even a streaming, like root sports, which has a monopoly on Kraken and Mariners. There is no streaming option. You can only get it if you have Comcast cable. And I think obviously there's a partnership there. Like there, there's some, there's deals that I don't know about, but man, I would pay, I would, I would pay dollars, like a good chunk. Just to have a streaming root sports team seems reckless is not there. Yeah. Because I lived in LA for a bit. I was in Austin and I was in Chicago and it was great because I wasn't getting blackout. So I could, I could watch MLB TV. And then I come back here and I'm like, I'm like cut the cord too. And I'm like, how the hell am I supposed to watch games? Like I don't want to have to just get TV just for the Mariner. I hear people use VPNs to solve that problem, but I wouldn't know personally. Of course not. Of course not. And that's your answer. I'm sure you're sticking to. That is exactly the answer. You, you had a follow up question that I want to change the subject. Cause I'm curious about something, but go ahead, man. It's all you, Brett. Unless you forgot. No, no, no, I'm ready to back. I could go in and talk about the Mariners baseball. I can talk about the Mariners for the whole rest of the podcast. And the three listeners we have are slowly nodding their, though I would, I'll just say one thing that I will, would love to talk about, but will not. And that is what the hell was that last angels Mariners game? Oh, the brawl, the brawl. Yeah. A I'll just say, I'll just say, I think the angels did that on purpose. Oh, hell yeah. And they were being all whiny. And, and it won because they like their, their manager's out for 10 weeks. Uh, you know, some secondhand starter pitcher is out, who cares? But they basically took out, uh, three of our major players. If I'm being Crawford, Jayrod, and I guess winker, I mean, but he's been doing better, I mean, but yeah, it's, I mean, but honestly, I've seen this happen in other seasons that sometimes this kind of thing fires up the team and it can change the season because we've been playing so poorly that lately that like this may actually set, they, I think I'm hoping the angels like screwed up that now they're going to light a fire under us. And we have kind of a reason to like be pissed and actually do well. And I think the Mariners are just like capable, but they have like just weird problems where they get in their own head and they just underperformed classically. Yeah. It's like the fact that, that we're leading the league in, in, um, players abandoned on base. Yeah. Something about our, our, um, percentage, the average with, uh, bases loaded is like historically low or something like that. It's like hard. And so it's stuff like that is just such a Mariner's thing to do. Oh, so then there's software. Let me, um, let me attempt to put words in your mouth. Maybe I also, you know, you talked about crack and file formats and looking at headers and things. And I worked on, uh, the windows debugger for a while and I got really into cracking the PE looking inside there. I worked on GDI also. So I'm one of the few people that actually in the, maybe not in the world, that's, that's a little bit too pompous, but I know what's inside of a font file. Yeah. So, and I, and I know, you know, I know too much about that and too much about how true type works, et cetera, et cetera. But kind of what you're doing today is you're taking, I mean, if we can, I, let's go ahead and talk about unstruct a little bit. It's just, uh, I did watch some of the videos and things like you are looking, you're grabbing data of any format and often, and what I saw in that, in the videos was videos or pictures, which have a known format where some metadata is there and then adding some more, you know, enhancing that metadata so you can get more information and find more stuff. Is that, is, am I, am I assuming too much or is that kind of what you're doing? You're dead on. I mean, and I think it's, there's always been this classic layer of, there's always technical metadata as we call it in the files. I mean, it's, it's whatever's in the header of the file, the GPS location and your EXIF information from your iPhone and stuff like that. And so, I mean, this isn't that dissimilar than like, um, Adobe Lightroom or your iPhone or things like that. I mean, they're all, they're all doing that today. So what we're doing there isn't unique, but that's where you start from. And then, but the, the next part is we basically are building a knowledge graph from that data. And so from the time, from the geo, the geolocation from maybe author metadata or other metadata that's in the file. Um, but then the second order of what we're doing is running ML on that data. And so we're starting with that technical metadata. We're then doing NLP on documents. We're doing object detection on images. We're doing audio transcription. And then we're kind of recursively like trying to do figure out what the data is perceiving and then kind of keep spidering down from there. So that to me is, I mean, the first order is kind of what used to be called like media management. I mean, it's essentially what I, I thought it was or that kind of thing. But for us, we're kind of taking that to the next step where it's like, okay, what's the data perceiving? And then start to be able to search across that and do analytics on it. Um, and then the third order is what we want to get to is really um, creating inferences between entities and start looking at, oh, you have a cluster of this kind of thing over here, or we can audit, do auto suggestion of, Hey, well these other images look similar and they had all these tags. Here's a bunch of images over here that probably might be tagged with this. And so we could do those kinds of things. So it's, it kind of comes back to you. I mean, data discovery is kind of the best term that's out there right now for a lot of what we're doing. Um, but it's really, I mean, it's, it's semantic search in a way. And the hard part for us is we do, we're kind of full stack because we do ingestion. We do this data management. We do storage management and analytics. Um, so we kind of cover a lot of the different areas and terms that are out there, um, and that's going to be more of a marketing challenge for us. It's like, like, what are we when we do like so much? So I wonder if in something like this, you're, you're really providing a platform for people to understand in your words, unstructured data, which is a good name for that. But I wonder if, if the problems companies or teams or organizations may solve with that are unknown or surprising. I think it's, it's almost like the platform you're giving people. It's, uh, I would think it would enable additional innovation, which would be kind of cool to see. And that's really where I want to go with this is, I mean, it's going to open up a lot of greenfield. I mean, once you can access the data in that way, you can do really innovative analytics and process automation and all this kind of stuff. And what we're finding is it's a lot of people like don't know what they can do with their data. And so now we're having to get over that hump of, Hey, did you know you can search like geospatially over the last three years and find objects? And they're like, we just have this data sitting in SharePoint. Like we can search on file names. And I think we're that, like, we're trying to get, like we're, we know we're a little early, but we're really trying to make a new thing. Uh, is, is hopefully going to unlock some stuff. Well, I'm going to repeat a comment and just a, uh, random thing. That's something I said before, Google makes a wonderful search engine for the internet, but somehow they can't get searched to work well for Google docs. I don't know if the teams just won't talk to each other, but something like this. And again, um, I everything I see is around multi-media multimedia on your site. But when I think actually I can, a couple of public stories I can tell, uh, Microsoft, one cool thing, they build a search engine as well. I forget what it's called. Um, do they still make that Brent is being still a thing. Oh, Brett, Brett's giving me two fingers to say, yes, I'm not sure. I don't know. I don't know. I mean, who would know who would know? I have to ask someone at Microsoft to know. So, so being the thing, but one thing cool at Microsoft was because we have the search technology is there. They index their entire internet, which makes at least discovery possible for a lot of other companies and a Google probably does the same thing for a lot of us. Hmm. Yeah. I'm absolutely I, I, I would bet money on it, even though I don't know their system for a lot of companies, maybe like my company, the places to search are like the data is not only the data is unstructured and separated. Yep. Yeah. So when you want to find out about something, um, you may have to search in multiple places to get to see, see if you can find answers that match your answers in one place and it's, it's interesting. So there, there is some opportunity there as well. I would think. Well, I heard, um, Microsoft did this thing called project cortex that they were talking about for awhile and it was sounded sort of related to what we're doing, but it was probably more like SharePoint SharePoint oriented or something like that, but I mean, just the layer of. Yeah. I mean, we're still, we're not building the ML algorithms to do it. I mean, we're using cognitive services or other vendors or things like that. And so we're not competing in that sense, but we're competing in the, a semantic layer for that data and just providing a really good, I mean, essentially middleware platform, like a pass, um, for that. Yeah. I think as teams, as companies collect more and more data and go through this, this growth of data maturity, okay, data is good. We need to collect it. Then they collect so much and they don't know what to do with it. And Brent has a highly technical term to describe this phenomenon. When there's a massive amount of data, but you have no idea what to do with it. I believe it's called data puke. No, no, no. So data puke is when you have a massive amount of data and you build a dashboard to, to try to, to manually communicate every aspect of it. And so, so imagine, so imagine your worst nightmare of a power BI dashboard with, with like 50 different pie charts and graphs. And you look at it and I look at it and it makes me want to puke. It's essentially, they don't know what they're looking for. So they show up and so they're, they're literally throwing all of the data up on the screen and hope they see something. And honestly, they don't know how to, I mean, they don't know what they're looking for, but they don't know how to figure out what they're looking for. So often they kind of stall there. Yeah. And I would think that Kirk, what you're building, uh, maybe could help with that for some companies. Like we have all this data. We don't know how to look at it. Maybe you can get them over a hump. And that's what we're hoping to do is, I mean, there's, there's that first level of, Hey, we can visualize it and they can search, then it's how do you glean one little thread that actually they could like end up in a trouble ticket? I mean, or something like that. I mean, it's like something that's actionable from that. And so we're, I mean, that's what we want to get to is where hopefully we can, I mean, really in first started furring some information that they can, they can really find material use of. But what the initial use cases, I mean, there, the value is honestly just time and resources. I mean, they spend so much time manually dealing with these files, even if they're just like taking pictures of a maintenance yard on their iPhone. Like if we can save an hour a day, I'm just screwed around with these files, trying to find things like there's immediate ROI. Now, I, in terms of, so, so I'm now a data scientist and within the, within the culture of the team, I'll say like the PM culture hasn't really advanced sufficiently enough to deal with, with data. Right. I'll tell you, look at a PM came to me a few months ago and they said, Brent, I have some exciting news. And I've, I of course like exciting news. And I said, what? He's like, I am going to be able to argue for another 10 heads for your team. And I, I guess everyone at Microsoft is either in a developer or data science role where all empire builders, apparently, right? Because I'm just like, okay, that, what do you want? I immediately went to Spidey, Spidey says tingly. And, and what he asked for is like, okay, but if, if you, if you accept that these 10 heads, this is what you're setting them up for. I want you to build an old school curated primary key, you know, you can think of it as a type one data warehouse for all the customer data for the entire PM org, maintain our ETL pipelines and our Power BI's. This, this was only the second time I had interacted with this PM. So I, I, I definitely informed them in a diplomatic way that no, I was not interested in this wonderful opportunity. Right. Well, I mean, it's a classic, it's a, it brings up a big thing of, I mean, data engineering versus data science and how, I mean, there's such a, I mean, so much of what is just data engineering gets dumped on data scientists or you don't have what you need. And so it's, I mean, you're always waiting on the data and it's such a, I mean, it's such a classic problem these days. And that's why, I mean, there are a lot of companies that are trying to help the data engineering side to let you do your work. So, so I now, so I left QA, went to Devin Bing and in my time in Bing, that's what the search engine is called. Shoot. Thanks. You're number one, Alan, you're number one. Uh, but my time in Bing completely blew my mind in terms of like the types of things that can do with data. And in, I went back to school, pursued the data science spree. Now I honestly believe, and I've, I've shared this with my team. I believe we're at the beginning of the end of the, the, the, the era of the data science sorcerer, right? Where, where data science is really becoming more and more API driven. Right. If, if I can point unstruct at a bunch of reference storage locations and say, no, and it gets 90% the way there, then you know what? That saved me a crap ton of time. It's, I mean, and that's kind of what we're, I mean, we're building on top of the commoditization of a lot of the ML. And that's really what, I mean, our big assumption is, look, I mean, there's going to be a million models out there. There's going to be low code, no code tools to build them. Other great partners we can work with. Um, I mean, it's like, even on just personal, I'm like, we had a data science and start realizing like, we kind of didn't need that role because the partners that we had really filled in a lot of what was needed for, for us. And so we were, I mean, and so it was kind of a, it was a hard thing to do, but we basically decided just to sunset that role and the, where we're heading is now it's more just ML integrations. We are adding in like a auto ML, some auto ML capabilities to like, okay, here you go in and fit your data and our UI, we'll do all the easy button stuff, like, I'll augment it, train it, deploy it, just using Azure auto ML and give it back to you. And just for like identifying things in pictures, but that's just an API thing. I mean, and for me, I'm not a data science at all. And I mean, I don't even know how to code Python hardly at all. So it's like, I'm, I'm still like doing all this stuff in C sharp and able to do like real leading edge ML just with all these tools and platforms. Yeah. No, the, the, the only, the only challenge right now, I would say in today's world on, on the integration, most of it is API calls it, but in today's world, it's, it's analyzing the output where it needs, where it needs some, some data whispering. But I think one of the key things that I think you're tapping into, and I suspect you'll continue to tap into when I go and look at Unstruct, right? I see you pivoting around, Hey, we're going to make a big portion of your data engineering problems go away, which is, which is fantastic. I have many of them and I would love for someone to come in and say, we will make them go away and you won't have to wait 18,000 months in order for it to land and we'll adapt and things like that. But really the industry, as I see it is, is in, is in a space called knowledge management. Data is not really what's important. Transforming that data into actionable knowledge is where it's at and simplifying it in a way that the, like the human beings can consume. That's where I see it's happening. You're dead on. Yeah. I mean, I mean, that's, it's really funny you say that because that, I mean, and this is all marketing problems. It's, I mean, if we say that as a company, like a lot of people think, Oh, you're just doing document management, you're, you're documenting, you're somebody like that. And, um, but we're trying to basically build a holistic knowledge management system that's multimedia. And that's really where, I mean, so we started with this unstructured data term that we've kind of leaned into, but the knowledge management's really where the future is. And we actually, we just added document and email support. Um, or at least we had documents, we added email support and we added like PI detection and a lot more NLP because we're, we're finding is as customers, I thought more document management was more of a solved solution, but they are, what people were talking to actually have a lot of issues that they have a Google-ish kind of search they can do, but they don't have that semantic search for a lot of these folks, but they also wouldn't be like, well, how do I correlate that to a CAD drawing or an image or something like that? And I think it's, yeah, I mean, knowledge management, I've heard, um, Bob Muggly, an ex Microsoft and snowflake guy, um, who I think I actually used to work under his org when I was there, but so did I, he was up, up until the current executive I worked for, he was my favorite executive to ever work for. Yeah. Yeah. But I mean, I, I didn't know him well, but I knew, I knew, I made a decent fit there and I've heard him on a podcast and he is now starting to really talk this knowledge management solution. And it's, um, it's, I think he really gets it and sees like, kind of where the next generation is. And that's, I mean, literally what we're trying to do is tackle that kind of problem that I hear him kind of banging the drum on now. Yeah. I haven't, so you spent five years at, and he's at rational AI. Interesting. Rational tangent, but I just was at a conference and met some of the rational people and they're doing some really interesting knowledge graph stuff for, um, I think like the United nations and stuff like that. Um, but they're not doing at least from what they said quickly is they're not doing a lot of unstructured data, like media data. And so I'm actually meeting with them next week to like, I think there's some really cool stuff that we can do with them. Right. If you're able to, to, to with high confidence, the only, the only thing I'd be concerned, so I do a lot of NLP, I do a lot of time series stuff. And the only thing would be concerned around is the places where you might. Say it do, do a semantic auto tagging, making sure that that is stable over time and that the NLP doesn't suddenly, uh, later on decide, Oh, this is now a different tag. Right. That's a really important thing. I mean, like we're sort of have built in kind of ensembling, or we can pull in models from different like primer and cognitive services and a bunch of different ones and sort of roll them up. And then one of the big things we're looking at is like, how do people, how are people want to can, um, going to handle disambiguation? Like, because that's going to really be one of the things of like, Oh, is this an asset that they've seen in their SAP database or is this some other random term that looks like it or is pretty similar? And so those are the, I think those are going to be a lot of the other pain points we're going to have to start leading into more is to make that tagging work really well is to figure out like how we can offer disambiguation to the users in an easy way. So Alan, I could completely start totally data science geeking out now. No, go for it, man. Yeah. No, I like it. What Kirk's talking about. All right. I am deep in the weeds in some really hard topic modeling problems. We're doing topic modeling. Oh, wait, is topic modeling, are those the models that model clothes for hot topic? Is that I'm not yet, not hot topic. Oh, okay. Sorry. I was confused. Go on. Yeah. And certainly hot topic would not be coming to me for any modeling gig. Damn, Alan, you're talking about topic modeling, topic modeling, your neck deep in topic modeling. He's like visualizing a mall with hot topic in it right now. And he can't get out of it. Yeah. And he and he's in the glass in front trying to figure out how much is myself in the mirror and like, uh, Alan, why don't you put me here? And we're doing it against short texts, which is extremely hard and really relies on sort of probabilistic semantic, uh, understanding. It's a fun, I mean, if it were easy, it wouldn't be fun. Uh, but yeah, I definitely see kind of where you're going and then mixing multimedia types and kind of bringing it together. Yeah. I suspect you in rational. Um, I don't know. I mean, I don't know what your goal is and you're the CEO here. Right. And, but it, it, it almost seems like a, a match made in heaven in some regards. Cause if you can clean up the data, such that it's structured and then to hand it off to them, then there's an end to end story that would. I think, yeah, I mean, I actually have a meeting with, um, I think next week or the week after really did kind of talk because I listened to a talk they gave and I was like, this is super cool. Like, and it looks really complimentary. And so I, I just like, they're one of the few, um, that kind of talk about knowledge management in that way. Um, data.world is another one. Um, it's out there that they're kind of in this knowledge data catalog space. Um, but it's, I think it's good because I'm starting to see more of these companies pop up and people are talking to talk like Muggly and I was talking about the global pandemic and we're talking, like, we're talking, we're talking, like, we're talking, so I think we're positioned, right? But it's just the classic, like, it's going to take a little time for this market to come to bear. So. No, I, I, and I agree. I think, right. You said you, you mentioned you listened to, uh, the podcast. One and a half. And the half is my son. He is 21. And it's his first job, but he's actually doing QA automation right now. So I call him that half. And somehow your company has survived. We actually affirm believers in that sort of transformation. Right? And we now kind of observe that that's kind of the standard practice, that if you realize how to ship software in the current world, you kind of don't need to have specialists to do this. Right? Now, mapping over to the data space, right? There's going to be the leading edge, which I think you're on. But the question is going to be is how quickly does the main group really realize, suddenly click and realize, oh my God, those companies that are doing excellent knowledge management are actually much faster at being able to target what's critical to move their business forward. I spent a lot of my time on customer retention and churn problems. And having clean, crisp data would rapidly accelerate that. Rapidly. It would be a competitive edge. Absolutely. And I see when we've done some of the ML projects, I mean, one of the hardest problems is getting good data to train on. I mean, because we're trying to like, we have drone imagery. We have this. We're like, I bought a drone. And I'm like, oh, man, but like, where in Seattle can I fly a drone and actually fly over something interesting that we would want to detect? And it's just that mundane stuff of like, how do I even get like the, I can run AutoML in an hour, but how do I get a good set of data to even start with? And so there are just all these upstream problems that, I mean, in the data world that are just, I mean, way harder than writing tests to test your UI and react. I mean, it's just a totally different beast. I have a question for, I'm Alan. I come to you. You're talking about this. It brought something to mind. One of the things I've seen happen, fortunately, not a lot, but sometimes when you have a complex ML system looking over a large amount of data, a bug can bug in the ML or in the data can be pretty disastrous and create poisoned data and just screw up a bunch of your stuff. One, I'm curious for both of you, because I just know observationally, I don't work in, although I do like to say that I actually managed one of the first data science teams at Microsoft just long enough for me to hire someone who knew what the hell they were doing. So that's my claim to fame. When I observed this data poisoning phenomena in the industry, like these little, maybe not little, but these bugs that screw up all your good, hard work in practice, because you guys work with stuff a lot more, how often does that happen and how recoverable is it? This is a little bit off the blue, but it piqued my curiosity, because I don't know the answer and I don't know how to Google for that. You start to see, there's these companies doing, they call it data quality, data observability. Yeah, yeah. Then what is data, the QA, the testing thing, they think like, what is data quality? I'm going to shut up. Go on. I think I've been listening to some podcasts and stuff where it's like, hey, we get this report and it's supposed to be updated every 15 minutes. The system realizes it hasn't been updated in three hours or whatever. And so they want to auto trigger and kind of then track the lineage of that upstream to figure that out. And for us, it could be, I mean, we're using a lot of third party models. I mean, some, I mean, and we don't even own them. I mean, we're not even building them. It's like, what if like cognitive services releases a bad model that stops identifying vehicles or something. And so for those two weeks that we, our customers ingest data, there's no vehicles. Like right now, I mean, we would have really difficult time of knowing that other than ad hoc testing. But I think that's something we want to get better about automating is like we, for the same data, to look at what the output is and have basically the anomaly detection, almost like pixels moved on the screen. We should, I mean, right now we don't have this and it's more ad hoc. But we, I mean, once we grow, I really want to do this where every dependency we have, we should have a way to track the input and the output of the model and make sure that that's not changing. Just like you're saying, that's so important. If you, if you figure out how to do that at scale, then right. You earn every penny and a half that you will get from selling this company, if ever that's your plan, right? The, the, so, so not only do, is it important to do anomaly detection against the streams and, and Alan, this is how I prevent it. Right. Because one of the things that happens is if, if you find that your data is poisoned in the way you described, in terms of how to react to it, it's actually, Alan's it's your favorite answer. The answer is it depends. The answer to any sufficiently complex question is it depends, which means my question was good. Yes. So it, it, you have to, you have to then make a decision. Okay. Do I fill in the blanks? Do I cut it off after, after the data is recovered? Yeah. Do, do I do a backfill? A lot of the times backfill, it's not even possible. Like if you're, if you, if you're streaming data at like a 10 minute cadence and there's a data retention policy on the store for maybe two days and you don't detect it for a week later, it's a lost cause. Right. We were literally just talking about backfill today with product team of like, how do we want to handle this? Like, is it time-based backfill? Like, is it a, is it a, is it a, is it a backfill of like, how do we want to handle this? Like, is it time-based backfill? Is it, I mean, different things. I mean, do you want to backfill? Like what if you have your backfilling via different kind of views, do you want to overlap, and then you're kind of double backfilling data and like it's, it gets complicated. But I mean, it's something we're actually going to release probably, I mean, definitely this quarter, but, because it's, I mean, we have to have that. I mean, if somebody updates a model, I mean we have to have some way to go back and do it, but, but the other half of that is how do you avoid poison data? release a crap model that's put in some stupid tag that they don't want anymore. Like we were just talking about like the tag muting and tag disqualification and things like that. Because I think all of that is kind of mixed in. Because it's tied into the ML quality. Even then, so yeah, garbage in garbage out. Right. What I'm more referring to is on the garbage inside is essentially, so for example, let's say, let's say there's this column that normally has five different options. Imagine it's a column that expresses an enum. Right. And then suddenly out of the blue, option four is uncommon. Right. Because not only do you have to do anomaly detection against the data set itself, but then potentially against every column and every value within the column. Yeah. Right. So anomaly detection goes just gets crazy really fast. And what drives so there's one feed that I operate today where my team merges in. I don't know, 50 different data sources all owned by different teams. And then then some random PM out of the blue says, hey, I did this this random query that's important to me and no one else. And then I notice there's this huge spike. What's the cause? And the number of times during the day or during the week where I have to think through, look, my job is to host the data. I do not provide any data quality off of this thing. You're welcome. Like if you want to use it, the risk is blah, blah, blah, blah, blah, blah, because I'm like if I open that door where I kind of go through and answer every one of these questions, that is all I'm going to do. Yeah, you kind of are ending up taking implicit ownership of it at that point. And then people are going to come to you to guarantee the quality. Right. Well, no, no. So I mean, that one's that one's easy for me. You can come to me. The answer is no. Yeah. If if you don't like my data feed, then go. Then you are welcome to take it over or or build your own like. And that's one team. So one team wanted me to treat it like a 24 seven service. And I'm like, I'm like, hey, so one of the feeds I operate is really important to the to the business, but the commitment that I've shared out with the business is is relative correctness, directionally correct. Right. I'm like, yeah, I am not going to go through and and and try to denoise and make sure all my dependencies are trying to denoise themselves. Right. It's that's that's not my it's not my primary job. My primary job is is building data science assets and in any place where this feed where I rely on and it's broken as it relates to the data science. Yeah, I'll fix that. Right. Or if you can make an ROI statement. But the whole point there is it's so burdensome. And if anything like unstruct can come out and and even if they only take take a 20 percent reduction of sort of the manual intervention that I have to pay, then it's then it's going to be it's going to be worth it. Well, that's that's one of the big areas that I'm I see happening in the structure data world, and I'm assuming it's going to happen in kind of our unstructured world of the data quality and observability. Because like, what if what if somebody is putting in 100000 drone images and one percent of them don't have a proper lat long in the metadata because somehow that drone had a firmware glitch and wasn't writing that data in the metadata. Like literally, there's zero chance of them finding it unless you can run like parse that data, show it on a map or show reports or something like that. Or I have I have a set of like 10000 test images that I got from some drones. Like points are one percent of them are black and you would never know. It's a valid file, the file size like you could sort of I mean. And so that's the kind of layer I think there's a ton of value in what we're doing as a data quality platform as well. Even just looking not at it's sort of a flip side of search where it's like, hey, here's all this stuff. But what's all the weird edge stuff around the edges that like is almost not searchable just because it's all clustered together as like black or in zero zero zero lat long or something like that or time is all at the minimum time time value. And and so I've been I don't I mean, not to slag them, but I don't think my PM team gets that part of it yet. I'm trying to teach them about like, look here, I'm seeing this in the other part of the industry. I feel like this is going to be a big thing for us that we need to start thinking about this. And so I'm trying to like warm them up to the idea of like anomaly detection and data quality and and stuff like that. But it's more of a gut feel for me. We don't have data on it. No, no, no. So I'll validate it. It's absolutely absolutely critical. The the so, for example, one of the things that I'm doing is every data point I've taken to creating a new kind of metadata structure. And in terms of your language, imagine imagine tagging it. But with a data quality tag or in a data quality namespace where where you're highlighting. So, for example, I still produce a stream. But if I know if if I know deep down in the bowels that there is a there is a table where a record is missing. Right. I'm going to I'm going to highlight in in this in this. It's basically in my in my implementation to JSON payload. I'm going to highlight that this is a sort of a warning, like the metadata may not be complete on this one. And it's important because certain data science projects I'm working on can deal with noisy data. Others can't. And then this allows me to to sort of pre filter these things. I think you're absolutely right. Now tagging it in terms of a context around how it's used. That's going to be a hard problem. We we map. I mean, so we're actually we're using Cosmos DB today. Gremlin and the sequel, the JSON document store. And so everything's kind of in this entity graph. And we have this concept of observations. And so we can the observations could be a generic tag or it could be a person, place or thing kind of kind of entity. And so we it's fully extensible in that way. That's an interesting we can have like a quality observation on that data that says like, oh, yeah, here, like this is a way to then pivot on that and search for anomalies. I see that as something I want to get. We're too early to like probably a quarter early to deal with it yet. But I think it's something I have a feeling that that's going to be pretty interesting. I would tend to agree. Right. In terms of so here you're providing the way I think of it. If you're doing knowledge management and you're helping people to clean up their minute, their their knowledge, I think it's probably just as important as this is knowledge that may be risky to use or or you shouldn't use at all. Or like it's like almost orphaned or unlinked at some point. And it's funny, like I had a bug where so we have a graph view that we're using this great company, Cambridge Intelligence. It's this really nice graph react component. And I put like a bunch of data into it. And I was like, weird. What are all these satellites around the edges that aren't linked to everything else? And I realized that those are files that didn't get tagged. And so because it was like essentially created this tag cluster. And it's weird because like I didn't realize like if something doesn't get tagged, it's not really a bug, like it's not going to throw an exception anywhere. It's just going to be missing data. And so I was only able to even notice that I'd introduced this bug by seeing it visualized in the in the UI. You know, and so by seeing all that sort of satellite data that was unlinked, I was like, oh, man, that's weird. And then I could like go back and debug it. But those are the kind of I mean, that's essentially data quality visual like visualization of data quality right there. To look for it like everything should be essentially linked to something else or at least a tag per se. Right. If it's if it's just orphan with nothing connected to it, that's a bug. That that's actually the principle that I do. Like this is a lot of times you'll see data entry. Let's say a data column will be empty. Yeah. And I'm like, look, I don't want empties or I want empty to mean one thing. If it's empty because the underlying source is empty. Otherwise, if it's empty because we couldn't find it like we did a left a left out or join or something on those lines. If it's empty in that case, then this thing better say missing or not found or something along those lines. Well, in that area of just that context is I mean, if we're running five ML models on something and each one maybe reports back like a different thing or why it I mean, we store like confidence score and stuff like that, but maybe we had an error. I mean, we couldn't run the model because of some reason. Right now it's in the logs, but it's not in the graph. And so that that information of the why we could or couldn't do something might be something we want to track in our actual graph and start to be like, oh, here's a cluster of issues of oh, we couldn't run this model because it didn't have this a lot long field or something like that or whatever it was, or it was an image of a black image. And so the model just rejected it or something. So there's there's a ton of ideas that I have. Now, once we kind of get we're just at the point now where we're building the graph system, the app, we just launched an Azure marketplace a month ago, but there's just a million ideas for where we can take this. It's that's as it grows. You're in the Azure marketplace. Yeah. Brand has to go look. Oh, Azure, for those of you who don't know, that's like AWS for GCP. They do cloud stuff, but cooler. Yeah, I'm yeah, I have to pick on bread. It's funny, my my dev team, they're all AWS guys. So we were always like picking on each other about like, which is better. So but then I spent all night the other night dealing with I am rules in like Azure AD and all that. And now I'm like, I was kind of pissed at it for like about 24 hours. Yeah, I can. As a current employee of Microsoft, I am just going to respond with no, no comment. I know if you click it, I'll get a I'll get a slack message that somebody subscribes. Oh, OK. Yeah, then our marketing people freak out. Hey, all listeners go to Azure Marketplace and subscribe and let's freak out Kirk's marketing department. Only if you're actually going to use it. Yes, please. Yeah, we did. We're doing like a starter plan through the rest of the year. I mean, it's for us, it's there's just a lot of work consumption based. So it's more data you throw into us, pay more. And then we're just kind of doing like a flat rate for the rest of the year as a starter. Yeah, I think the cool thing and being kind of where you are is you're solving a problem that you're solving an important problem. Looks like well, that companies don't quite yet know they have. Yeah, which is kind of cool. It's I mean, it's like I do a lot of investor discussions right now and it's like it's hard, like some of them just get it. They're like, oh, hell yeah. Like this is like the vision of the future. And other ones are like, call me when you have more customers, more revenue. And I'm like, it really has to be a vision thing where like somebody really has to see where the market's going. And and there's a subset of them that totally do and are super jazzy about it. But it's I mean, I'd give it a year or two and now it's going to be, I mean, even literally just the last six months or something, I've been seeing it talked about a lot more. So I think the wave is starting to happen with this whole knowledge management play. Yeah, I believe it. Definitely. Definitely. I worked on Microsoft Teams, my last org at Microsoft. And one of the things I would say, like our our VP really wanted to like, we're going to go against Slack. And like I said, no, what you but it's all right. It's all right. Because Teams is the collaboration tool for teams that don't know yet that they need a collaboration tool because it's just part because so many companies have offices 365. Slack is a tough learning curve for a lot of these non-tech companies to to get into. But if you can just take this thing that there's a couple of great things Teams does is it brings this collaboration and communication software to people that just, hey, you already have office. You use this stuff. But here's the, it's going to sound a little bit like a jab, but it's true. The back end, like file storage for teams, this may have changed completely, but at the time it was SharePoint on the back end. And I said, I'm not sure. I just listened to a podcast literally about that topic and they said it was, I think it's still so what's happened after, you know, a decade or more of SharePoint is Teams has made SharePoint usable and useful. So kind of cool. One of the areas we're going to look at is do, I mean, because I was so surprised that still I was hearing from like aerial survey companies storing their data in SharePoint. And I was like, really? Like you're not even using an S3 bucket or something like that. Or like it was just like, and I was, I was kind of surprised how prevalent it still was. And we're actually looking at doing a more direct Microsoft Graph API integration that we can start indexing all of that data better from directly from SharePoint. And I think that could end up being a, I mean, a really interesting direction. You know, another thing I thought of, and I know we're over time here and I have to get going, but, but, but it reminds me like before Teams, I worked on a science project and Xbox. And before that I worked on a link, which became Skype for Business. And I remember a conversation way back then saying we really want to do is, I don't think we even knew what ML was. You know, we said, great if we had a way to analyze the text in a recorded meeting and make it searchable. And now though, yeah, but it could get with, you know, show me, you know, got it. You want to go back and revisit how a decision was made. Imagine being able to structure that data from a hundred, like let's do find the evolution of this and see how we got from there to here. And it's actually possible today, which is kind of cool. It's, I mean, I had somebody asked me, like, it was an investor like, Hey, could I dump my zoom meetings into Unstruck and correlate back the companies that were discussed back to my CRM and make sure they're synced. Like I've linked to the time I spoke about them and maybe link to Crunchface or link to whatever, like in those sort of things. And I was like, we almost could do that. Like we're really damn close. Like we are, I mean, if we have the right NLP to make sure we can get the right terms and we, I mean, we, we can already import zoom meetings today and we're already, we can run that stuff. And then it's the data enrichment that's actually this next quarter is really focused on. Like it's like, we're pretty much there. And almost too, if I look at, like most companies, we do an engagement survey twice a year, you can almost get a better view of like, you never know how it's going in between. You work on stuff, you hope it's better. You get the little report, you go, okay, we got better here. It works here. But there's even for like an HR perspective, there's things like sentiment analysis or engagement that you could actually derive from engagement in meetings, how people, the words they're using, the inflection. I was talking with somebody recently who used to work for a comp, a support company and they had ML in real time, listening to their calls and like telling them, okay, you need to provide empathy now. Wow. Yeah. That's sick. That's cool. Like we're on the edge of some really cool, almost weird sci-fi stuff happening, which is kind of exciting. Well, yeah, I, I, that takes to me, I mean your story there takes me to a, a, a ethics place. Yeah. Don the edge, right? It's the uncanny valley of ML. Well, then when you could start having avatars say the words instead, because their conversations are driven by ML. Like that's where they probably want to go with it. Oh my gosh. I realized when I talked to my daughter about it, says, what do you do all day? I mainly talk to people and try and help them make better decisions. AI could totally do that for me. Right. Brent. That is actually how I pitch what my team's job is. Right. It wouldn't do it for you, Alan. The problem is it would do it instead of you. Right. That's, that's where it potentially becomes an ethics thing. Well, again, we've all seen, we know Skynet is coming. It's just a matter of when, and we see clues all the time. A joke aside, that's why I say I went to data science so that, that machines, when the machines stick over the world, they look back upon me with fondness. All right. Well, Kirk, this has been a wonderful conversation. I even picked up on most of the data stuff. I know, I have enough data knowledge to at least know what you're talking about. And I am super excited to see Brent on the cover of the new hot topic catalog. That's going to be fantastic. That's one of my great takeaways. But, um, any, uh, any final words from you, Kirk, anything else you want to share? Shout outs, uh, contact information, et cetera. Yeah. I mean, if, if anybody's interested in just chatting about this kind of data or a company or whatever, um, LinkedIn, great way to get ahold of me. Um, probably the best way to, uh, to learn about world. Cool. All right then. Uh, Brent, anything from you? Hey, uh, Kirk, it's, it's been fantastic. Thanks for joining today. Didn't know what, uh, yeah, I'll just be quite honest. Like when, when, when I saw that I'm like, are we really going to be spending time talking about data engineering really? And then, and then I dove into it. I'm like, Oh no, this guy. And I get it. You're starting with a data engineering thing, but that's not really the space that you're in. You're in knowledge, management. I like pleasantly surprising, Brent, sometimes. But I, but I get it because a lot of people like the companies, they know they have a data engineering problem and they really have a knowledge management problem, but they don't really know it in those terms. Yeah. Yeah. And that, I mean, I think the hardest thing right now is like, I know the product, the product we need to build over the next like 18 months, but it's how to market it and make it click from a terminology standpoint. It's like the hardest part right now. Because I think getting, because people aren't thinking these words, these terms, when we can have a discussion with people and talk like this, they get, they're like, Oh shit, this is like really useful. But I think for technologists and trying to get the marketing speak is, is the hard part right now. But it's, I think, I think we're the, it's not even just us. I think just the market generally is moving this way. And I think it'll be interesting to see other, I mean, hopefully other companies just kind of make the visibility better. Like relation. I mean, I think it's, it's great to see if, if we get more of those types of companies, it just makes the market for everybody. So. All right. Well, thanks again for being here. Thanks Brent for being a pleasant co-host. I'll try not to, I'll try not to cut too much of what you say out of this podcast, Brent. All right. And go Mariners. You need all the, all the thoughts and prayers we can send. All right. See everyone. Take care. Thank you. Bye. Bye. 
