The Modern Testing Podcast. Join your hosts, Alan and Brent. I am mindless agile robot. I must iterate. As we talk about software engineering, software quality, leadership, and whatever else comes to mind. Now on with the show. Hello again, Brent. Howdy, Alan. It's good to see you again. It's good to be seen by you. We're on the summer schedule of A-B testing recording where we don't record as often because crap is going on. Oh my god, speaking of crap. Oh, what episode are we running? 105. 105? That's a cool number. Speaking of numbers. Is it? The number of my house address is changing. I saw that on... I'm freaking moving. Oh my god. And you got a killer stove. Did you see that? I did. That's like a $3,000 stove top in this new place. So, wasn't planning to move. Wasn't looking to move. Had a company picnic a weekend before last. Driving back... I think there's some houses over here. Maybe there's some... Let's just drive over here by this neighborhood. My kids really wanted to live close to their high school. They're both in the same high school next year. And so we drove... Oh, hey look. This house is open house tomorrow. Maybe we'll come back. Oh look, there's a realtor there. Oh, she's going to let us in. Oh, we look around going, you know what? Let's move. So, over email, we negotiate an offer. And buying a house now versus 11 years ago about the last house. Only 11 years. It's like all of... Like the entire negotiation, email and texting. The loan application. I call my loan guy. He's forever. He's great. And get everything taken care of over email and maybe two phone calls. All the real estate documents, all online digital. It's fantastic. It goes super fast. The house is nice. We'll close probably early. We'll probably close in less than 30 days from we made the offer. But all that streamlined stuff. Title companies. The people that do like... The one where you go and you spend eight hours just putting your initials on hundreds of documents. They like traditional testing in many ways. They embraced 1990 like it's nobody's business. It's like one quick story and then we'll go on to even less random stories. They sent me a whole bunch of docs in a random order with weird names and they forgot the names. And I signed... One was a two page thing. Only need signatures on the second page. So, I printed out that page, signed it, sent it back. And they said, hey, I need both pages of that thing. I said, only one page needed signatures. Do you want me to send back the page you sent me as is? The answer? Yes, please. It's like, oh my... It's like I... My whole thing is I abhor inefficiency. I want things to be easy and streamlined and straightforward and that made me mad. But the bureaucrats, Alan, are protecting us from making mistakes. Oh, shush. If you hadn't sent back that first page, you were at risk. Ten years from now, someone would have done a challenge to your title and you'd be out on living in cardboard boxes. I'm curious to see when I edit this later how much thumping comes out from Brent fondling the microphone, but we'll see. So, I want to do a quick... And you get new sexy microphones today. Self-advertisements. I've been working on recording with this exact same microphones in our new recorder here. We'll see how it goes. We should make remote recording easier when we get back to that. I think I've mentioned on the podcast, if not, surprise. I'm working on a modern testing course for Ministry of Testing. I am very excited about it. I want to say I'm a third of the way done, but I'm way more than a third of the way done. I've done all the planning. Everything's all figured out what's in each session. I've recorded the first three. There's an intro, which is sort of the hook on why modern testing isn't as boring and stupid as you think it is. I phrase it much better in the video. I don't think it's boring or stupid. And then I know. And then I've recorded the first episode each on the first two principles. I'm going to do one episode, one course, whatever they're called, per principle, and then a wrap up at the end. So a total of nine episodes. They're all five to seven minutes. So it's enough to, which I guess would be a 45 minute talk if you watched them all at once, which is about right for kind of getting a good overview. But then we'll have lots of discussion with the people that take the course on the dojo. I think it's going to be pretty cool. I don't understand how it's a course. Can you explain those details? Like nine, seven minute things. Those feel more like YouTube nuggets. Remember, we're talking to the YouTube generation. So if you join the Ministry of Testing, if you join the Ministry of Testing, you join the dojo, you have access to all the courses they have online. And they are a whole bunch of these things. And the idea is within these courses, I have within each class, each section of the course, there are one or two or three places where I ask the viewer to pause and reflect or answer some questions or do a little bit of exercises to help reinforce what we're talking about. For example, in principle number two episode, accelerating the team, there's a reflection exercise where I ask them to list the things they're doing that help speed the team up. And the things they're doing that help that are slowing things on their team that are slowing them down. Just so they start thinking about those. So there's things like that. In the first course of this series, the intro, I give them your very first glance for some people at the modern testing principles. I'll just ask them to take some time before they go any farther to think about which ones are which ones like yeah, yeah, yeah, head nod, head nod, which ones they go. I'm going to skip to that right now. See if I want to go any farther. That kind of stuff. So I'm pretty excited. I will. So I'm trying to wrap up this course. The folks at Ministry of Test being very patient with me because I'm behind in getting them things and I'm moving at the same time. And the day job, which will be able to talk about a little bit later, is a bit on the busy side. Now I'll get into that later. But the cool thing is in modern testing, in this course, in doing this course and then watching what's going on, we are seeing and this is the very subtle transition to something more like a podcast than the previous 10 minutes is we're seeing, Brent and I are both seeing more and more people and teams doing things that are a lot more like modern testing. And it's what's the word I'm thinking of how it's awesome, exciting, a little scary, like what if we're wrong? We're not, but we've let the world astray. No, no, no. If we're wrong, then there's a whole bunch of. Then I don't want to be right. Then there's a whole bunch of giants whose shoulders we're standing on who are also wrong. So we won't be alone. But the thing is the chances of that are if this is something that we didn't have any direct experience on, I'd be worried. But we do. It works. And for now, it's right. Yeah, we had I won't name any names, but I want to. But one listener who I hope and I hope gets the job. But they in their interview, they had their interviewer pull up the modern testing principles. Yeah, which is which is awesome. I was going to say brave or I'll go with brave. I don't the way the way they describe the scenario. I'm not so certain. Right. I don't know that I don't remember the details of what they mentioned, but this is a company that didn't have a QA. They were apparently struggling. They looking to hire one person. They knew up front the company, the hiring manager knew up front that they were under heat because QA had a reputation already. That company of slowing things down. And that was something they didn't want. And this listener just immediately went and was like, oh, fantastic. We're aligned here. Let me show you modern testing principles. This is whether or not you hire me. This is what you need to do. Very cool. I was talking to someone. I think it was. I think it was Melissa Eden, who is one of the three for sure. Just gave a talk recently on modern testing, which is the which is kind of cool to see people besides us more and more also talking about this thing called modern testing and in turn taking some of the heat for us, which is pretty cool. Oh, I didn't hear that story. No, I mean, not about her taking heat. I haven't heard any stories about that, but she's talking about modern testing. And I mentioned to her that like when I first I didn't know what to make of the first talk ever on modern testing external, which was at test bash year before last in Brighton. Test bash Brighton because there's 18,000 test bashes a year and a couple of cool things about that talk. One, the coolest thing was afterwards how many people told me, thank you for giving a name to what we're already doing or I want to do that. We just we're not ready, but I want to do that. And I was expecting more, much more hate. I thought it would be much more controversial instead of inspiring. And if I can manage to inspire or lead or provide any sort of value, I feel pretty damn good about what I provided. So I'm excited there. But the other thing I'm excited about with the with that talk was when they scheduled it, they asked me if I do the after lunch slot. I said, you need to you need to we need some of your colorful language. Maybe they said I didn't want to wake up the crowd, wake up the crowd, which I do and did. And in doing so, apparently not the last now because people have taken my lead. But at the time I earned the first ever explicit tag for a Ministry of Test recording. I didn't plan it. It just kind of came out that way. I was excited. These things happen leading into and you can help me. Let's brainstorm on this a bit. Fact. I'm channeling Dwight Schrute from the office. Fact. We're seeing more people do things like modern testing. Fact. Many of them, including myself, are in roles that don't have testing in the title. Fact. I am speaking at Auradev in November. Haven't started working on the talk yet because they have to scrape the information because what I want to talk about the topic. Wait fact. My topic is talking about what people do. You don't watch the office going, Alan, what the. What the are you talking about? I want to tell stories of how people are living in this modern testing world. So I'll need to track some people down, probably via Zoom calls, potentially to be recorded. Guests on our podcast. But over the next few months, I want to start tracking down and talking to a lot of these people who are doing things that are modern test. We had a listener and Brent is never going to get a word in because I have all these ideas on my head. We haven't talked for a month and he gave me coffee. We had a listener who works at a bank on their mobile app. So it's not like oh, who is also got Brad all excited. Is this the one in Germany? Yes. Yes. This is fantastical, which is not German. No, it's not. And I'll let you talk about that in a minute. But the idea is I want to collect all these stories. No, this is really, really happening, which is amazing. And people were and to be clear, this is happening and would have happened without us. What we've provided is a framework to talk about it, some common language, a way to a place to point and say, yes, that's what I'm doing. And so that's the value we provide. We haven't. I don't think we've caused people to go. Well, in most cases, at least to go. This is the way I want to do it. People are going, oh, this is what I do. I have a way to talk about it. But I want to tell those stories to the folks at Oradev who may or may not get what this whole idea is about. So I've been thinking a great deal around what did we do. Right. Because we stole a lot of the modern testing principles from book here, book there, book everywhere. The lucky thing is we read all the same books between Flo and the lean startup and a few other things. The principles emerged fairly easily. And we both executed. But I've been thinking about what is our product. Right. And is it ours or not. Right. And what I realized is what we've done is we've constructed a rallying point. These thoughts were out there, but we gave them a place to converge and grow and be healthy. And that's what I think we've done. Yeah. I am a firm believer in the rallying cry. And that's which is a big thing of Palantioni's work is business novels. And I think, yeah, we provided that probably not just subconsciously. I'm looking forward to I think the next step is when it takes on a life of its own that we couldn't control even if we wanted to. And don't you think if with people pulling it out in interviews and people giving talks about it that aren't you and I that maybe it's on its way there? No. What I'm thinking is the next step is when someone does something like realizes we have a an open source license on it and they go create their own modern testing dot something reference it and start moving on their own. Now, I think just you by saying that that may happen. And to be clear, the modern testing principles are free to do whatever you want with attribution. You can you can make a whole product around them. Oh, that's when we know we've lost when some tool vendor talks about their ability to accelerate their team using modern testing tool. There is one thing we need to do though. And that is we need to modify that license to ensure certain things never happen. For example, I do not ever want to see a certification course for modern testing. Oh, yeah. Let's have this. Actually, spoiler alert coming out next year is the modern tester certification black belt. No, just kidding. Yeah. No. There's there's nothing right now. I mean, maybe you and I should talk off. Okay. There's a lot of risk with our current where we are with how it's going viral and and how open the licenses. Did you want to talk any more about some of the things that you've seen in our slack or elsewhere on people adopting things that are like our modern testing? Well, you brought up the earlier example. One of our listeners, I'm getting to a point where so one of the things I like to do on the select channel, as you know, is when a new person comes in, I like to give them a free taco. Okay. And you only these are these are virtual tacos. They're virtual tacos. But they're tacos and they're awesome. And every person only gets five per five per day to give away. And by the way, shout out the hey taco for helping us out with our hey taco license and keeping us going with tacos on one of the three dot slack dot com. Yeah, they were fantastic on that. And Alan was fantastic on getting them to agree to that. But I'm I do think perhaps even sometime this year that there will be time where I cannot give out. I only have five a day a day. Right. I can't give out all five or I need more than five. Sorry, I can because people are coming in. I love hearing when they when they join the community. I get it. If more than five people join in a day, you run out of tacos. Right. Just connecting the dots for everyone. Sorry. No, I'm good. I'm good. I'm good. Words working not in a love here in their stories. Yes. Yes. There was a there was a new listener that came in. They are applying again MTP like practices. But they they're a German company and they're a bank, which is a highly regulated industry in a country who adores regulations. And so seeing these companies break through what we've been told when we started doing modern testing is oh, you can't do that because of this bureaucracy or that issue. For example, we only do client products on prem. People are pushing the boundary. And I love that the Slack channel is a place where those who are interested are learning how to from each other now. Things are really picking up. Yeah, I think so. And including and we probably don't have enough time to go into it today, but including as things get pick up. Right. The the opinion leaders on the other side are going to start being more visible, more out there and more directly against these concepts. The 80 haters. Yeah. Probably worth mentioning. We'll go too deep into it. But we do. We aren't without our controversy with aren't without those who think what we're doing is harmful to the business or worse harmful to the craft. Remember, modern testing isn't about testing and it's not really that modern. That's our that's our tagline to add somewhere. It's about accelerating the team. It's about building software. It's about it's about helping the team build a sustainable and profitable business or successful business, I should say, because it modern testing works in nonprofits as well. In some regards, though, I'll just speak out. I do think depending on the context, I think in some regards, it is harmful to the craft. Right. Not the testing activity, but the tech testing discipline. And when I hear people say it's harmful to the craft, sometimes they're it's overlooked. Right. Sometimes they're saying, hey, you're going to encourage businesses to ship shit out in the wild. And sometimes they're saying, hey, you're adding adding fuel to the fire towards the direction of removing the discipline. That latter one, I absolutely could see. We're not telling people what to do or what not to do, but we are saying it is a whole team effort and you need to do the right thing regardless of fear. Yeah. Yeah. Let's transition to make this about me. OK. I want to talk a little bit about what I'm doing in the postmodern world, if we want to call it that or the modern world. I don't think it's a postmodern. I think you're post traditional. Post traditional. Post traditional world in my non test, but kind of quality role. I have a weekly meeting. There's a lot of all kinds of weekly meetings. One meeting in particular, I was meeting with marketing and a product manager and my boss and his boss to go over one of the projects a little bit more. One of the new things, a little more interesting, a little more risky. So just kind of a check in. We were talking about and I'm asking you about this. We had a good discussion on the difference between what's the difference in products, Brent, between testing and validation. And these could be terms that are that maybe I just use. So I want to ask you this before I go on. But do those terms mean anything different to you? In my text on me, testing is the super set. Testing is the if I were to break it down, testing is an activity. Validation is one key aspect that you're doing. Another key aspect would be ad hoc or exploratory testing. Validation in the vernacular I used to use is essentially, hey, we expect this to do certain things. Does it actually do those things in a tactical sense, not in the strategic like we're targeting with MT? So when we talk about principle number five, and I didn't phrase this very well, kind of on purpose because I wanted to get that answer before I rephrased it. When we talk about, when we say the customer is the only one able to evaluate, we use the word evaluate, the quality of our product. Yes. And then people freak the hell out because you're just asking customers to test your product because you're lazy and you're harming people and you're a jerk and I don't like you, I'm going to go in my corner. But I don't know about you, but all of those are true. They are. I am lazy. What are what are customers? What is it that we want from customers to do to the product? I am. I'm going to lead you along here. They're not testing it for they're not testing it. We're not trying to use customers to figure out this is the difference I'm driving towards is we don't use customers to test for functional correctness or to or to make sure it's working. We use it to validate that we're building the right thing or what or whether there's product market fit or or to learn more about what we are doing right and wrong with the product. So what I was I did I particularly left the word customer out. But the conversation in this meeting got to a point of like it was a little bit of they're worried like there's a little bit of worry from the exec that the customers were going in this early release. Customers were going to be finding quality issues and absolutely not the case. The quality of this product is very very good. It just is feature lean. OK. But enough it has enough functionality and this is what a minimum viable product is about. This one is actually bigger than that. Actually tangent tangent inception had a nice conversation with my boss yesterday about the concierge MVP. But anyway we it was a good conversation. I made the point that quality is something we build in and a little bit of collaboration how we do that. And one hundred percent what we expect from having a customer use this product is validation of whether we're building the right product not a validation of whether it's going to be a product. Validation of whether it's good enough quality for them to use. Yeah. I'll tell you I just use the language from Reese on this one. I talk about the pivot or persevere meeting. Yes. Yes. And I think that I'm actually driving a lot toward that. I think my hunch is I'm not going to leave this. I'm not. I'm going to wait and see. My hunch is that we will need to pivot that we're building a good product. But the product we're going to build is a subset of what we think we're building. But I'm going to wait and see what happens based on the customer. I think is going to be a nice pivot in there. But the cool point I brought up was I was able to bring up only the customer only the customer is able to evaluate the quality of software in the meeting without anybody freaking out or screaming. But the quality work gets a little over. No. And that's because you're the test guy. And I'm not the tough. No no no. You get number one. That's fantastic. That response number two. So I have as an aside I am formerly from test. My manager is formerly from test and recently my VP was getting heat from his boss around quality issues with respect to co correctness and old school PMs were going to my VP saying we need to get a testing back. Oh now's our opportunity. We need to get a testing back. The VP ran through the motions and was talking to some people around God. Yeah. So both myself and my manager were were in those informationals and my manager and I had multiple private conversations around. OK. First thing we got to do is stop this. We need to make sure that no test manager comes in here. Like we do not want that role here. If we can't succeed in that then we need to absolutely make sure this is not a role either one of us are asked to take on. I have a reputation in certain lights of I call it being punished for being good. I get assigned things that I don't want to sign to me because I have experience in credibility about closing that loop quick. I don't did not want this role. It was an interesting segue. It's funny because when I first joined the monetization team so it's been about 10 weeks now we had a couple of major incidents due to just developer error lack of testing lack of. We normally do very good testing. We just messed up two big ones expensive ones and my boss because I think because I was branded to the team and he knew my background he was threatening to hire a test team. So do I just need to hire a test team to help you and I'm going no no no no and it was an idle threat. It was just like get the team just as it was a conversation starter and through no work of mine through just a general the team does very well. In fact all of our incidents effect can I say this on air most of our incidents now are probably because of. Yes you can say that especially in the Microsoft. I shouldn't say most but a chunk of them so but there's no reason we shouldn't be resilient to the platform or under so that's another stage of maturity and growth for us. But my role is interesting in that I have the way my boss puts it. I have I. How does he put it. I'm the director of program management and this is different than Microsoft program management. I don't know what they do. This is making sure our larger scale projects are nothing actually good. Good describe it is I make sure there are no surprises. Making sure it's a lot of just asking we do I do a status reporting thing just a quick little week log every week to my manager rolls up to a org wide mail. But one of my summaries recently was mainly I just asked a lot of questions. Okay which is kind of like branded at the real estate class. So I just ask people questions and try and lead them in a direction and then I plug myself in and my boss will plug me in the places. Hey this is weird. Go take care of this. That's go figure out what's weird then go take care of it. So I'm at my fingers in just about every part of the org. I run a weekly meeting where I go over not quite a status meeting. It's really more for product management product marketing about release dates to make sure we understand where there's risk to what we're planning to deliver and an org of about 160 engineers to 50 ish overall. There's a lot going on. So keeping things organized making sure a lot of just making sure communications happening. Then I have this quality lever and I don't know if it's a if I don't know if my charter is quality with program management as a tool to get me there or if my role is program management with quality as a lever to get me there. It's been interesting to see how well those two work together. So if I'm looking at a program like how we can deliver this complex project it's getting I think the team's pretty good about most testing but it's like asking questions like so what's the developer experience here? How are they going to use this? What's what's that going to be like? Oh yes kind of crummy and or what are getting them think about load or for latency up front is part of what I do. And then sometimes I realize I get to the end of the week. I didn't think much about testing this week should have I I don't know. No no no no because what you're trying to do. So what you reminded me of is the acceptance review in in scrum. Are you familiar with this concept? Oh yes. So you have a product owner. They set out the requirements. And then an ideal acceptance review occurs after when the developer says it's done. The ideal acceptance review is an interview. The PM's who I know of who do acceptance reviews badly will load up the product and essentially be a test team for that developer and fine bucks. That is not what you want to do. What you want to do is you want to interview them. Don't load the product because that keeps you objective and say OK. So how did you assure that latency isn't going to be an issue for our customers. How did you assure that this is going to scale to an outage. Like for example there's a new end. And this is the PM interviewing the developer or the development team. This is whether or not right. So it's essentially you ask a bunch of questions very similar to what you're doing. You do not load the product. And what you are trying to do is gain confidence that they did the work necessary. Yeah. Right. Makes sense. I have a I have a service that I'm shipping. It's about to be customer facing matter of fact just two weeks ago I shipped my first customer facing model. I'm very excited about that. And but the particular scenario I'm doing only triggers during an outage. OK so there's an outage happening so then this scenario is is intended to ease the customer pain during an outage. And I'll tell you I don't have a good sense on the concurrency that my service is going to be called in the in terms of an outage. And I'm worried about that. So I'm like OK can we figure out how to fake an outage without pissing off customers. Is there a way to do that. Bring back staging environment. Bring back int. We have multiple staging environments and trying to get rid of at least two of them. Yeah. We can go back to another way to describe my job is I am simultaneously trying to accelerate the team and improve quality. And because when we have those outages I don't think you can do that mutually exclusive. I could make the team go super fast and break things or you can't accelerate quality or I could stop deployments that would that would slow down incidents. So ideally I find if I'm successful the quality of the services improve overall and there's tons of ways to measure that with downtime or incidents or whatever. And our ability to deploy quickly and we know from we both read accelerate with another good book from modern testing is there is correlation between deployment velocity and quality which seems weird. But it makes sense if you're if you deploy lots of small changes probably you have a system where you're you can recover from quality issues quality improve. So I another book we still from a lot is accelerate if you read between the lines if you haven't read accelerate we just had a book club on one of the three dot slack dot com where people talked about that. But the quality culture transition guide it was in my head for a long time you've heard this story and if you haven't seen that go to one of the three dot slack dot com it's pinned to the general channel. When I created that it didn't gel in my head until I read accelerate. So you'll find some of you should find you should be able to get to the point where they talk about maturity models and how they work and how they don't work. And they talk about some attributes you'll kind of see hopefully because it happened some influence of how accelerate led to that maturity grid. But let me make that I just finished I finished reading accelerate in the end. One of the things that I found super insightful. So I talk about co correctness and quality of separate things. And co correctness is the bug side and quality is the customer satisfaction side. What I realize where the dots are connected and you just brought it up co correctness is very important but it's very important especially for given. What accelerate is saying in today's modern society modern world is still very important but its purpose is to speed up cycle times. You cannot scale crap right so the purpose of quality is to be able to build things faster. Now building things faster is has never been a traditional testing role but it is all it has been a traditional additional crap job test gets assigned. Now in this world where you are responsible in the service world where speed wins. Yeah you need to make sure you're building a quality product and then you're getting it out there fast. Reading that book what it did for me is it reinvigorated my belief in principle seven because we need to get development understanding how to get their code correct. The first time once that happens testability concerns go out the door right. We've already mentioned it several times accelerate has shown that when test or when dev owns the test automation is correlated with positive business and outcomes. But you can't really focus on that right. A lot of the lot of the old school traditional testing they want us to focus continue to focus on bugs. Speed is important. I've said it over and over again the last several years. But going a hundred million miles an hour and building a product no one wants is perhaps worse than going two miles an hour. Building a product everyone wants. Absolutely. That second piece the customer quality point of view is where I think test is better suited than doing death job for them and helping them to slow down the product. I just yesterday I was reading going through Reddit and I'm subscribed to the software testing subreddit and I should just get off there because it's it's 1990s encapsulated. And there was a question just yesterday someone asked who does the who does the was just unit testing who does the testing on your team. I read an article said that developers do some of the testing and their apply is no on my team the QA is right the unit test the QA is right the unit test the QA is. Yeah. And yeah. There's a whole idea of QA testing bugs the crap out of me. That's a team that's throwing away money. I want to have my dream. I have a dream. I'm not sure how possible this would be given. I think I don't think we're worthy of it but it wouldn't be fun to see if we get Nicole Forsgren on a be testing. It would that would be fantastic. I'm going to work up to that. I got it. I got to work out my nerve get my get get something going to convince her. I have a hunch that she's super super busy. I suspect to maybe we can get her for a half hour. But that book actually has multiple authors so we don't have to just try for Nicole. Yeah but she did all the research. So I'm really interested. So definitely the lead author on that. I want to pick her brain. Yeah. All right. The other thing I was going to mention before we close is that I didn't get to earlier. It was how much coaching I do. And it's interesting. I thought when I joined this team I would be coaching the team on writing tests and on how to think about testing. And they actually understand it pretty well. Oh my God developers know how to test. Oh my God. What kind of crazy world is this. Yeah they do a pretty good job. There's some larger end to end or integration things that they need some help on. And I'm helping with that. But I end up doing most of my coaching although I'm not certified in just how to do something. I mean we call it agile how to do something that makes sense. Too late really bring this up. But things like no that's not a story. It's phrase like a story. So yes it's a story. But that would take three months to finish. So how can we break this down. So things like that just little bits of coaching like how can we. How can we. What if we wanted to ship every week. What would that look like. How could we do that. I don't know if we do. Let's go a little deeper. So lots and lots of things like that just getting people to think about. It's again I am not that smart but sometimes I'm good at coming up with the right questions. The Socratic method of the difference. Yes. The Socratic method is fantastic. Where it feels with me is a lot of times I know the answer and I'm impatient. So I'll go. That's true. Hey Alan have you thought about this. And actually that's a failure in the Socratic method never. You're using a Socratic method never ask a question that can be answered with yes or no. That's the first lesson I'll give on it. Yes. And then if you do use it and you know the answer then and you have a problem with patience as I do. Then I found a little trick and that is make it a game. And that is can you get them to say the correct answer before you lose your patience and say it yourself. So you know how some of those games have little timers. Yeah. Yeah. Another one I've used and we'll go. Well nobody's waiting kicks out of the room yet. Another one I've used is and this is again I don't. Maybe it helps that I have made a lot of mistakes in my life and in my career. It does for sure. Because the other one that works out really well is just asking them like things like if I help someone prep for a meeting yesterday. So let's think about what questions are going to be asked and what questions. What do you mean. Like there's another one of these exec reviews today of this product said. So what are they going to ask about. They're going to ask about how we're going to validate. So they're going to ask about probably going to ask about what customers are we bringing on board. What's your answer there. And just putting you know thinking about. I guess it's a lot like you and a prep for like an exact review. And I guess it's like thinking about the like put yourself in the other person's shoes whether it's the example I gave earlier like what's the developer experience or let's anticipate what questions are going to be asked. That's a better way to prepare and it's like laying out agenda and trying to overwhelm them with information. You know one thing you haven't talked about. And I'm surprised because I remember the last time you did it you came back and you communicated it's hugely successful. That's the hypothesis testing exercise I taught you back in 82. Have you done that yet. Yeah. I'm done a little bit a lot. OK. I even in fact I'm going to talk about the hypothesis testing exercise specifically in the Ministry of Test module course that covers principle five. Nice. All right. So yeah I use that a lot. I even did a workshop on that. I included that at our we had a quality team offsite in January that I attended and gave a presentation that. So yeah. Yeah. No it's a good thing. It's a good way to think about things. The team is. Yeah. They're pretty good at that sort of thing. So I haven't had to do it much in this or. But we talk about it from time to time. That's it in a nutshell. We'll have. We're going to go in two weeks again. Do you think maybe four weeks. Two weeks. All right. We'll figure it out. I have a vacation the third week of August. I think I'm going to England then anyway for work. OK. So we'll figure it out. All right. Thanks for sitting through our planning session. As always I am Alan and I'm Brent. We'll see you next time. 
