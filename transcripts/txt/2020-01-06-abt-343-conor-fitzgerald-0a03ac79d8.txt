This is the AB Testing 343 podcast, a podcast where we ask one of the three listeners of the AB Testing podcast, three questions about almost anything. ABT 343 is a fun slice of what's going on in the world of modern testing. Let's get started. Happy New Year, everyone. We're here on New Year's Eve and today for the 343, I'm joined by Connor Fitzgerald. Say hi, Connor. Hi, Adam. And to all three, the other two listeners, of course. Yes, and the other two listeners, of course. Good. Well, I'm excited to have you here. I have probably said this before on this podcast, but being able to listen to the stories of people who listen to the podcast and have tried some or all of the modern testing principles, it's fun for me. And I'm just touched by the amount of feedback we've gotten about these and people are really enjoying these. So thank you for being the latest volunteer for this thing. The pleasure is all mine. I'm glad to be here to listen to these podcasts quite some time. So it's a absolute pleasure to be interviewed by you today. So thank you. And as you can tell from Connor's accent, he is from the South Eastern America. Just kidding. Where are you from, Connor? I'm from Cork, all the way down the south of Ireland. I had a coworker. What was I forgotten his name? Someone I talked with a lot at Microsoft from Cork. Yeah, very similar accent. Oh, it's escaping me. Brent knows him. Brent actually introduced us. But so never mind. Tangent number one completed. So let's get started. Why don't we do a good warm up question today? Tell me all about your life, but limit the limited to your testing career and your current role. OK, perfect. So in the last 15 years, I've been working as a tester. So this is my 15th year. And I guess some stuff that you might find interesting or I might flip through a couple of different things. So the various titles I've had as a consultant as the lead manager as well. Industries wise, I've worked in finance. So things like hedge fund, insurance, banking, a little bit of fraud prevention at one stage as well. And then I've been telecoms as the other major area I've been in for a number of years, which was working with embedded systems like Intel and also working on a platform for telcos as well. The likes of Vodafone would use that particular platform I worked on. Then companies wise, I've kind of tried all the different sizes. So I've worked on a startup for somebody who was essentially out of their living room. And that's where we used to have the monthly meetings. Worked for mid-sized companies all the way up to enterprise, like I was saying, Intel. And also when I was in consultancy, I was on site with companies like ING. Location wise, we chatted about Ireland already, but most of my experience actually being in Ireland, so Cork the last number of years. But I actually started in Shannon in County Clare in the west of Ireland and also did a stint in Dublin on the east side. And I did a year in Sydney. So when I was doing a round world trip a number of years ago, stopped off in Sydney for a year to replenish the funds before I continued the second part of that trip. And also as well, I suppose I've been lucky to travel quite a bit as well. Places like London, Bangalore, and you'll be glad to know Seattle as well twice in the past. So the most current role then I've been working with Popolo for the last three years. And I've been working there as a senior software tester. Previously been working as a manager and I kind of flipped back to being probably more of a hands on software tester. So Popolo then is a mid-sized company based in here in Cork. And often people like myself who work in Ireland work for American companies and the satellite offices is here. So Popolo is actually reverse of that where it's a Cork company started here as a startup and is a mid-sized company now. It's a really interesting space. It's very different to any of the industries I've worked on before because it's internal communications or also can be termed as employee engagement. So the Popolo platform allows large firms to plan, target, monitor the impact of internal communications all in a single platform. So what that means is like to see you have a company may want to send out an email or maybe mobile content to all the employees or subset of employees. And then they want to get some analytics to show what were the most popular posts where people actually reading the content and then they can do things like basic campaigns around that. So like say they want to increase awareness around something and they can maybe match that into an email campaign as well. So it's been very interesting working the space last three years. So the last 18 months I've been working with two teams, a mobile team and then a content team. So just kind of working with those two teams, working from the idea phase to the customer rollout phase all the way through. So whatever that involves workshops, planning sessions, facilitating bug bashes, joining and impairing or mobbing. And also the teams do a level three support. So essentially it's level one or level two customer support can figure out a problem or fix it. Then it comes to us and we'll root cause it and resolve it as quickly as we can. And it's been really interesting working with those two teams. One team I've been working with for three years, ever since it started. And the second team I've been working with for 18 months. So from the very beginning, and it's really interesting to see those teams mature and change over time. And it's been excellent journey today at any rate. It sounds very modern tested ish. So is your role sort of a, it's, am I kind of reading between the lies that you're working with teams, is it sort of a internal consultant kind of role or coach, or how would you describe the role you have? Yeah, that's interesting. Yeah. Like how would I describe, I describe myself as a tester, but I guess other people who would have done the role, maybe would have called themselves more of a coach because I'm working with two teams. I'm trying to influence them. So often I'd probably turn myself more facilitator. So what I'm trying to do is influence the quality on, on both teams and kind of folks very much on encouraging communication and collaboration, which isn't that difficult because the people I get to work with are excellent that way. So, um, I call myself a tester, but I guess I could call myself a coach or maybe some sort of a test facilitator maybe. Yeah, it doesn't matter. But, uh, the, what you call yourself doesn't matter, but it's the role and what you do, and you can't, of course, can't hear it, but I'm not nodding my head quite a bit because it's very similar to what I've done and what I've been doing for years. It's just sometimes you can cause a lot of change just by asking the right questions and you mentioned facilitating and that's the bulk of what I do. I get people to talk with each other. I get them to pause and think and ask the right questions and think about what they're doing and it ends up making quality improvements or process improvements or just some, or I should phrase it though that facilitation and asking those questions accelerates the achieve in the shape of a quality. Indeed. I fully agree. I love that. So very cool. And this is yet another example of you've, well, I'll loop back to, I'm not gonna put words in your mouth. So I'll ask another question instead. Okay. Plenty of words to put in your mouth, but obviously you didn't yell no. When I said this sounds very modern testing-ish, but what led to your interest in modern testing? I guess it starts like a lot of the listeners, the interviews so far and from being a conference is on speaking to people who also enjoy modern testing, they say they got to the point like myself, there must be a better way to test or even simply there must be a better way to develop software and deploy software. So the majority of my early career was very traditional testing. So you named traditional testing and I was probably doing it. So test plans, test scripts, waterfall. In all honestly, when I reflect back on it, I was probably the bottleneck or safety network teams, very certification focused. I did my STQB exams, but the big turning point for me was I worked on two projects that failed. The first probably wasn't such a large one, but the second one I've been working on for over two years. And when I saw that second project fail, it really was a catalyst to find a better way of doing things, even probably just beyond testing or quality, but even how you deliver software from start to finish and how do you make it successful. And there was one particular company that I worked with, which was actually the Seattle based company that I was working out at the Cork office for. And there I saw what success looked like. And to me, culture was the key in that company. There was a lot of collaboration, communication was at the heart of everything we did, for example, instead of writing a big, huge test plan, I'd write a one page test plan and then the test team and the people who were going to develop the software reviewed that test plan with me. And then that just drove a lot of how we did our testing, things like that. There was a DevOps culture there probably before a lot of companies were doing that and a concrete example of that was that we began to release software for the first time. So prior to that, I would have tested the software development team and then somebody else would have released the software, but we took responsibility in that company for releasing the software to production. And we had the, what I term, torturous use of automation and tools. So we looked at what was the critical things to automate and what were the best tools to use. And also that was the place where I began to learn about Bug Bash, to use Bug Bash and in a way begin to learn how to facilitate. So I guess that was a way of, I'd seen how possibly it could be done. But I needed to kind of pat it, like, learn, lead out a path for myself. And a journey it took was that I began to read about context-based testing. Now I know we don't talk about schools of testing anymore, but I just simply read an article in context-based testing. And that led me on to probably what I think is the best book I've ever read on the topic of software testing, which is lessons learned in software testing. That didn't turn into the rapid software testing course, which was led by Michael Bolton, that was at Test Bash Manchester 2017. And because Explorer testing was such a big part of that, then that led me onto the excellent book, Explorit. And then I got to the point where I was starting to look at the wider context of quality. You know, like I was saying, testing just not as an activity, but you know, testing as an activity rather than a rule, I mean, sorry. And then that's when I encountered modern testing and I've been playing around with a lot of the ideas before I'd ever seen the modern test and principles like a lot of the listeners say, but when I encountered the principles, it was the fact that it was so clearly articulated by Brent and yourself, and it was quite obvious that you'd put a huge one to talk into it. So I particularly love the deep dive of the modern testing principles. I think it's episode 67 to 93. Well-remembered. You go into real detail in those episodes. And I just want to read, and for any listeners don't just stop at 93, of course, to some great episodes after that time before, but that's where the real deep dive occurred. And yeah, I've just loved it. And then I suppose just, you know, began to think then how could I apply modern testing in my role and began to talk to others about how they were doing that, I guess. Yeah. But it's been a journey over maybe three or four years and it's been interesting once at least. Yeah. It sounds like, and so much to unpack there, so many things I want to poke in on. It sounds like you had kind of, like a lot of people had kind of been doing something very modern testing-ish before you discovered the principles and the work that Brent and I have done on the topic, which is cool and great and probably the way it's going to happen more often. One thing, and just to reiterate, and you know this, but maybe the listeners don't know, but Brent and I, we didn't write these from scratch. Like we're going to make a new thing. We just, really the principle started off as let's try and, we had been talking about modern testing for a year or more before then. In fact, we had our, I think our debate of the modern tester, modern test manager versus a traditional test manager was somewhere around 57 or 60 or somewhere in there. And we had been talking about this concept of modern testing, which again, not that modern, nothing to do with testing. It's about building a quality culture and quality Oregon and accelerating the delivery of customer value. We had been talking about that for a long, long time. So when we wrote the principles and iterated on them and got them, finally got them right, the process was just writing down what we had already, trying to document what we had already been talking about. And they've kind of stuck. I'm actually, it's one of my, the pieces of work in my entire career, I'm most proud of because these just make sense. Then you mentioned also a DevOps culture. And one thing I've noticed is I spent more and more time in DevOps and coming from the modern testing background and doing DevOps roles and being in running DevOps teams is that a DevOps culture and modern testing have a lot of overlap as well. I mean, DevOps DevOps is a lot about accelerating the achieve and shippable quality. So it's, it's, it's fun to see that come up and more and more we see, we're seeing testers speak at DevOps conferences. Lisa Crispin and Abby Bangs are, are both speaking at a delivery conf in Seattle next month. And probably if you've heard the AB testing end of year extravaganza, I think that was 112, maybe 111. I can't quite remember. I listened to it. I predicted I'm probably just going to do one conference again, this year. Probably it very, very likely will not be a test conference. It will be a dev conference to make it back to or dev or probably even better. If I could start going into the DevOps tour of duty, it might be kind of fun. So lots of overlaps there. And then one other thing to unpack is it's a two part is I love that you've mentioned all these books and classes that sort of the lead your own opinion of testing. And that's what I tell people is if you get all your information about testing from one place, you are going to have a sort of a skewed view of the testing world. And what I like from you is you have pulled in information from all kinds of sources, uh, you've read different books from different authors, taking classes, and you've used those to form your own opinions about how testing should be done or how quality should be built into a product. So I applaud you for that. And then the one book I wanted to drill in on was, I think an underrated book, not read by enough testers, especially people moving in the test coach roles is Elizabeth Henderson's explore it book, because not only is it good, a good book on exploratory testing, but it talks about coaching developers to do exploratory testing and stories about that. And that to me was maybe not directly, but definitely subconsciously. One of the things that told me I could, that it was, so when that came out, I don't think I had, if I remember right, I was still working on the Xbox team when that came out. I'll have to look up the years. Uh, my whole, my whole life, my, my history of life is what was I working on at the time this event and history happened? So that's how I remember years, but it wasn't until after I left that team that to my next, every job I've had since then, I've been in a role of, or at least part of my job was to coach developers how to test. So definitely some influence from that book and how they recommended. So thanks for bringing that up. Yeah. Cause I think I'm the same. It's the, when I started to talk to developers about exploratory testing, initially I didn't think I had the correct language or approach to do, but after reading that book, there was so many examples I could bring to them. And every time I brought an example from that book to developers, they found it incredibly interesting and it was something they could add to their testing skillset, which was really interesting for me. Yeah. And just the idea, like so many of the testers, when I came to Unity, uh, I'll back up a little bit on my previous teams. I did a lot of bug bashes with teams and sat with them in a room and, and tested things together. And we do some learning from that. But when I joined Unity as a quality director or a quality manager, QA manager for the first time in a long time, I took a little different approach and encouraged them to pair test as much as possible versus being the bottleneck because there would be some knowledge transfer that would happen. And I'm blown away by how much just getting in a habit of pair testing changes together will help developers learn how to test. And in fact, one of the best people on my team that was really good at pairing and, and making sure that he would say, I don't need the tests that you've done, you do diligence or help them test when they needed it. That team no longer has or needs a dedicated testing specialist. That pairing really, really helped. So again, glad you're doing that. How they recommended. Oh, thank you. Principal seven in place. So yeah. And you had mentioned a little sub question here, not on our list, but when we wrote the principles, we thought number seven, of course, would be the most controversial, but it ends up it's not because teams, it isn't the get rid of testers principle. It's the, you may get so good, you may not need a tester principle, but you're working a lot with data in your current role and the, the data principle isn't the controversial one. It's the one where we say that the customers are the only ones that can evaluate quality. People don't like that. So what's been your experience with that principle? It sounds like you've probably touched a little bit on that at Popo at Popo. Yeah. So like principle five round customer, was you like the only one who's capable to judge the judge and evaluate the quality of the product essentially as. Yeah. What's been your experience with that? Yeah. So I guess one thing. Sorry, sorry. Not one of your prepared questions, but I'm curious because of your background and the company you're working at. Oh yeah, sure. So like, so we've probably matured more as, as teams around this like, so the first thing they're working with over three years, we've probably learned to over time, more and more to engage with the customer as early as possible. To the point that we got pretty good at finding beta customers to work on new features with to ensure that we were going in the right direction. But based on some of the successes there, we actually started to look at if we're doing major bug fixes and it was important to maybe a couple of customers, we asked them to come on board as beta customers and that's worked really well as well. So they're often giving us really early feedback about how the fix is going to work. And sometimes it can be different, which is an infrastructure, things like that. So they're often giving us a really good quality feedback. And so because we're just working maybe with five or six key customers, sometimes we can get solutions to very good standard before we roll it out to everyone. And we can also give feed. We can get things in place for the support team as well. So that they know what may be coming based on feedback from customers. So that's been really good. And with the second team, I was working with the mobile team. That was really interesting because we went from the idea of we're going to have a mobile app to a sale in six months. So again, there we were highly engaged with a number of customers. Again, it was around approximately six customers who worked with us to develop that mobile app. And the feedback we got from the customers was invaluable. And it was really a partnership. And that's really the thing. If you can find customers you can work with who know things aren't perfect, but you're iterating towards something better. And they can have an early influence on that product. Great things can happen. So I've seen on both teams how engaging with the customer and the importance of the customer. And I think, for example, five actually links back to one. Because if you're going to make money, you have to... People are going to have to pay for it, whether it's like they're going to upgrade or they're going to buy something new from you. And internally, you can run things past people and they'll think it's great. But it's not until you get in front of customers that you got that feedback. But also, there are people who have to sign the dotted line to say we're going to pay money for this. So I think five and one to me are very highly interlinked. Yes, yes. And one of the things... And honestly, we realized this a little bit after the fact, after we had been vetting them. In fact, it's probably we realized it while we were doing the deep dives is how much they support each other. There is some intertwointment, which I guess we're just trying to capture the things that we believe are the driving factors of modern testing. That makes sense. But I'm pleasantly surprised how well they work together. Indeed. So how else have you used modern testing in your current role? Well, I wrote down a couple of examples and I'll share a couple of with you and then maybe you can dip in and out of the ones that maybe you find interesting. Sure. And so one thing that I was thinking about was that the key thing is to accelerate the achievement of shipability. So one thing I did one of the teams recently working with the team lead was we use a tool called split and split is how we manage our feature toggles. So I took it on as a mini project at one stage to review all the various feature toggles we had in place. And what spurred this was after read and accelerate, I wanted to figure out once the software is committed, like once it's merged and it's ready to go, how long does it take to get rolled out to production and actually get into the hands of customers? And what I kind of started to figure out that is if I could track the status of different feature toggles, I could track where the code was and how long it was taken to get the customers. And through that, by tracking the status of all the various feature toggles, we started to realize that there were some feature toggles we got rid of. There was some instances where, you know, software could have got out to customers sooner. And some instances like where stuff was ready for rollout, it's just because sometimes you're managing a lot of them. So based on my learnings from that, I shared with the department, we do concept of show and tell and Wednesday mornings where we share ideas like this. And other teams then got interested in this topic as well and began to track their feature toggles in more detail as well. So it may seem like a simple thing to keep track of your feature toggles and where they are, but it really plays a critical part of accelerating, you know, the achievement of acceptable quality and also, I think, really interlinks with the book, Accelerate as well. Yeah, I that's I had some flashbacks on my last project at Microsoft. We had we had feature toggles in place and they if you don't watch them, they can create some tech debt. And I frequently, I won't say constantly, but frequently had to go follow up with toggles that have been around for months sometimes like, oh, yeah, we're not going to ship that. That was an experiment that failed. We'll take take the toggle out. There's a we had a culture of shipping all the new features behind a feature toggle. And then once it was done and ready for wider use, we turn that toggle basically on by default or remove the toggle. But we if for whatever reasons, I'm sure it was the implementation or what are the other culture. But we kept a lot of the old ones around. And we had a massive list of these feature toggles. So tracking those is a good idea. And I'm it sounds like I'm pretty excited about the company you're working at, because not only doing some things that may feature flags certainly aren't brand new or elite, but they're so powerful, such a powerful tool to use to release safely. If we look at testing is one thing we could do to reduce risk. But another big lever we have in the services side is the operability having safety in the way we deploy, the way we monitor, the way we observe and the way we can roll back. And so it's exciting to hear. It's exciting to hear that you're using tools like that. And it sounds like also your team works together really well. You mentioned earlier looping in support so they know what's coming up. It sounds like a good company. Oh, yeah, it's really excellent. I can do as culturally of companies I've worked with, it is the best I've worked with. So it's it's, you know, when Sunday night comes around, you look forward to going into work on Monday morning, which is a nice thing to be able to say. I like that. That's probably one of the reasons I left Microsoft is I wasn't excited to go to work anymore. And I am again, even though most days I work from my home office, which is about 20 feet from my bedroom. So it's I don't have to maintain my excitement to go to work for a long. Very good. Did you have another example? Yeah, there's a couple of more there. Like I put a conscious effort in moving away from being a bottleneck and a gatekeeper and focusing more on whole team testing. So I particularly started to use Kanban to tackle the acceptance to bottleneck that a lot of people encounter. And probably more books there to probably influence that was Kanban in action, which I think is probably the best book in Kanban. The goal, which I'd read a couple of years previously, and then only started to use. And then, of course, the work of Janet, Gregory and Lisa Crispin had an influence in that. So it was just beginning to use, you know, the Kanban principles to see that the acceptance cue was the bottleneck and that I was probably being a gatekeeper there in the early days of the first team. And then we began to share the responsibility of that acceptance cue. We began to do things earlier. And we actually started to move to the point that we started to pair more and mob more, so we kind of started moving early towards one piece workflow for a finish where in both teams now they pretty much mob the whole time, which means there's only one item being worked on at any one point in time. And of course, then you get all the quality benefits from that. So, similar to the future toggles might only seem like a small thing, but I think more people, if they learn about Kanban, I think it can have a huge impact on quality. Yeah, I and you know, for listening to the show that Brent and I both massively prefer Kanban as a project tracking method over Scrum and its variants. I can do a whole other podcast telling you why, but we'll save that for another day. Maybe Brent and I should talk about that. Why we prefer Kanban? Yeah, if we haven't already, it's probably come up in bits and pieces. I don't know if you saw over the weekend, I reread. Actually, I've forgotten what day it is because I'm on vacation, but sometime in the past few days, I reread Shape Up, the book by Basecamp. It's an e-book about how they do project estimation. And I'm not a huge fan of the book, but this of my second read through. I found a couple of gems I liked and one about project tracking, but one in the end of the book, it's it seems how they relevant to what we're talking about. And I tweeted it, but I'm going to read it out loud also. Therefore, we think of QA as a level up, not a gate or checkpoint that all work must go through. We're much better off with QA than without it, but we don't depend on QA to ship quality features that work as they should. And I, as you and I know, there are some testers that still like being the gatekeeper, but I really liked that phrasing of that. They're not the bottleneck. They definitely having that test specialist can add some for almost every team can add some extra oomph and extra quality and find some of those edge cases. But they don't let that person be a bottleneck, which I think is great. And elsewhere, elsewhere in that chapter, they said that they have the time of book was written. They had one QA for their entire engineering team. 40 or 50 people. So I think it's I like that ratio. Excellent. That's a really good quote. I think it fits well. Any other examples you want to go through before we wrap up? What else? The developers do the majority of the testing now. They did a majority of automation. We had SRE team join us and that's had a big effect in observability. So we do a lot of monitoring and alerting now. We use synthetic tests. And another tool that we start to use a century as well to get our JavaScript errors, which has been really cool. We're very much into learning. So we're doing retros all the way through and we've ensured our retros. Postmortems is a new way of us learning. We've been in the know it about six months and they've been working really well. And what else? We use analytics. So we try to use analytics to start to figure out should we be using something or not? And then again, use analytics at the end to figure out if something was successful or not, and we try to use analytics to remove features as well as possible. And I am smiling and nodding my head so much. This is what an amazing gig. Yeah, no, it's really good. And yeah, I think I covered the customer engagement parts already and yeah. And the pairing and mobbing we discussed already as well. So yeah, where possible try to do it. Often like in conferences, people go away from a conference talk thinking they're not doing things right. And often maybe in a podcast, sometimes people think I'm not doing all those things. But I guess day to day, this is over three years. I've been trying to do these different things. They've been small experiments, some things work, some things haven't. And it's just trying to do, you know, best effort all the time. We don't always get a race and we don't always do all these things perfectly, but it's very important for people to know to. It's important to try things, have those experiments and see what works and just do your best to keep to the principles, I guess. Everyone's at a different place on their journey. And there is a good reason why one of the principles is to be a driver for continuous improvement and just get a little better all the time. Retro's and Brent and I have both said how much we love retro's because even if you just do one improvement from a retro that you have every two weeks, I mean, that's not even enough in my opinion, but that's 26 improvements to your process and your product over the year, which isn't horrible, but then think, oh, we can spread that out. And it would be silly to do that little. We should do each team should do a retro. Maybe she should do it every week. And those improvements add up, those tiny improvements add up. And you end up building a lot of great stuff. You think about what's slowing us down. What do we need to keep on doing? What can we stop doing? However you want to format that discussion and great ideas come out of it. And also it follows some lean practices of everyone has a voice. You get everyone on the team involved and anyone can have ideas for how to improve. Yeah, definitely. Like lean is a great thing to learn about, I guess, as a rule, you know, as a rule of thumb to learn about, you know, the likes of Toyota and how they became successful. Because, you know, like you're saying, you know, in the lean startup and books like that, there's so much to learn there because I know you lead to yourselves is that a lot of that is the groundwork for a lot of the seven principles. Yes. Yes. And then I was just just made this connection. But Toyota with with everyone being empowered and anyone can shut down the assembly line, it is a precursor to whole team quality. And when I was listening to what you were talking about, you truly have whole team quality. As I mentioned on the podcast and probably elsewhere, I read an article a while back where someone said, I really believe in whole team quality. It's so important. In fact, our tester, our developers write unit tests for almost everything before they give it to the test team. I think that's not whole team quality. What you're talking about, Connor, that's whole team quality. Everyone, everyone cares and everyone's doing their part. Mobbing is about whole team quality, one feature at a time. Getting everyone to talk to each other about what's going on with the product. That's whole team quality. So it's it's I think it's a good example. Thank you. This has been I say this to everyone, I think, but I love hearing the story. I don't think I've smiled this much in one of these so far because there's so much I like and so much I'm just excited that you get to be a part of. It's a little bit of jealousy. You may have a lot of people wanting to move to Cork and join you. But I know it's a small company, probably not hiring a lot, but they're very, very exciting. Thank you very much for being for volunteering and and being here and sharing your stories and giving me a lot of help to the community. I sometimes and popular actually growing. We got an investment this time last year. So if people do want to the Cork and want to do work in Buffalo, get in touch. All right. And I know we can get ahold of you on Twitter at Connor F. And I'll put that in the show notes. Any other blogs or any other things you want me to link to a blog? I can't find Connor Fai as well. So pardon those blogs as much as I'd like to. Maybe in the near I try to blog a bit more, but that's you'll find some of the blogs there on software testing and I do a little bit of blogging on public speaking. There, too. All right. I will put it in the notes and we'll get this out probably in the next week or so. So congratulations being the first ABT 343 of 2020. That's a lot of numbers and letters, but I think everybody understands and have a have a good evening. Have a good new year and we'll talk and interact with you soon. Yeah, have a new year to take care. 
