Good morning everybody. Howdy. Hey, I'm Alan. I'm Brent. And we're here for episode 41 of AB testing. Woo hoo! AB stands for Alan and Brent. It's also the name of a popular experimentation method of comparing two versions or treatments of a product, website, etc. Yes, and it occurs to me for the first time since we had this new drinking game, here's something I think we haven't talked about previously. We haven't talked about the fact that I just got back from not just but last week, I gave my talk on experimentation and AB testing at Better Software. I feel like it went pretty well. I promoted the podcast. Hopefully we have a new member of the three. Hopefully. Yeah. Wasn't this one the workshop model? Yeah, it was. So tell me how it goes. I'm interested. We talked about what it was, who was doing it, nobody, what they knew about it, nothing. We played one of the games that I graciously was able to borrow from Ronnie Guevara, who's done a lot of experimentation at Microsoft on how your intuition on which treatment is better is probably wrong. And then we emphasized a lot that data trumps intuition, so how to get that data, things to do, ways to do experiments. We talked a little bit about statistical significance and the math that goes into it. We talked about designing experiments. I even went and used Google Analytics to set up an AB experiment on angryweasel.com of just two different web pages and show how, if you want to do very low scale AB testing, it's how simple it is to set up. And then we also talked about the architecture on how you would do sort of a flighting system to do, not sort of, a flighting system or something to do experimentation on a much larger scale. Examples, we tried some things out. We had a lot of discussion back and forth. And believe it or not, there was enough details and nuance and all that. We talked a lot about the caveats of how to look at the data coming in and different things you can get from this, et cetera, et cetera. That was kind of it. That's one of my favorite discussions, the caveats. Like Wikipedia has this fantastic link. If you look up cognitive biases, it's like seven pages of individual biases. And I go back and I look at that frequently and judge which of these am I committing today? Have you seen whichtestone.com? Treatment A, treatment B, guess which one's better, and nobody can get more than two in a row right ever. Whichtestone has pretty much one of those every day for around the industry. Whichtestone.com? Whichtestone.com. Very cool. I'll be looking at that. Yeah, it's a pretty cool site. I get to my feed list and maybe it's not once a day, but it's multiple times a week. Super interesting. What I find is more fascinating or more not fascinating, where I get my most learning from that, I go, huh, I wouldn't have guessed that. But the comment section below has people that don't know what they're doing. A lot of people who actually do know what they're doing who kind of helped me get some insights I wouldn't have been able to get without them. So it's kind of fun. Something I find fascinating with, particularly Ronnie's explanation of the game. Last time I saw it was a couple years ago. And he does a really good job for the ones that he's able to address explaining why so that you can learn where your intuition went into the weeds. But that's very difficult. One of the things I brought up is that experimentation and the framework behind, the framework you use behind it will tell you which test won, but it doesn't really tell you why. You can use data to get a good idea of what's why, but you don't know why. Not initially, no. You'll have to do a lot of extra digging. But Ronnie has been using that deck for a long time. Yeah, and he's modified it a lot. He's some other experimenters. I like the which test won because now I can get some non-Microsoft experiments. They're more relevant, or at least more recent. The final thing I got into, and I didn't mention in my previous shortened synopsis, is the idea of building an experimentation culture where everything's an experiment. It's a journey to get there, but when you get a team that has this idea of everything's an experiment, it's a pretty fascinating place, a pretty fun place to be as far as software development goes. Yes, it aligns with Patrick's, Tess Pappy's presentation, where he was talking about ignorance and knowledge. Yeah. And when you are able to create a culture of experimentation where you essentially only value knowledge that is validated. Yeah. It is pretty cool. It is cool. It's really fun. I always like talking about, with you, when I actually do stuff in the data side of the world, just to show you I can dip my foot over there. Y'all know that's ... I try and prove to you that I really am a generalist. Specializing generalist. I have my specialties, but I don't mind dipping in deep in areas that I find. Not only that I find interesting, but I think are critical to making good software. So now you have successfully impressed me enough on this front. Now what I'd like to see you get to, it is review time, so here's your next priority. And that's use it. All right. Thanks, Brent. And we can start with once a quarter. There's a lot more. We could go a lot deeper there, but I don't want to go too far off the books. Hey, you mentioned I'm going to go back to ... Actually, I'll do a brief announcement, and then we'll actually talk about the fact that it is review season. I looked up my calendar, and just like, sometime in the last year, I put a date on my calendar, so I would remember to mention it on the AB testing show, and maybe in a tweet. But tomorrow, June 18th, which may be yesterday by the time you hear this, is the 12-year anniversary of my very first blog post. Congratulations. I don't know if it's an anniversary we're celebrating, just more of a milestone, but it was on the old MSDN blog site. Actually it still exists. What was the title? The title of my ... Hi, I'm Alan. You know what? I don't know. I think the title was I'm Not a Blogger. That was the title of one of my very first few blog posts. The context was I was about to present at a embedded developers conference on how to use the tools that we had written, the test framework we had written in Windows CE, so that ISVs or IHVs, independent hardware vendors, could write tests for their drivers and their firmware for Windows CE devices. So, Wnts was still alive 12 years ago? It feels like much longer than it was killed. It was around for a while. I ended up doing a bunch of those over a couple years. So I wanted a place to be able to answer post conference questions and to post slides and samples, et cetera. So I started it for that and then eventually I said, I'm going to use this to actually practice writing, learn how to write and be careful because then you end up writing books and shit and life goes crazy. But anyway, 12 years since my very first blog post, which was weird because it was almost ... It was like a week after my nine year anniversary at Microsoft. June, a time of many milestones. I started blogging for exactly the same reason, improved writing, improved communication. I would say having gone back recently and looked through some of my old stuff, I would say I've achieved that but my pace is slow. If you go to ... If you want a real treat, you can go to blogs.msdn.com, Wack Alum P-A and the A-L-A-N-P-A. I think the actual URL has changed as they've gone through different formats but my old posts are still there and it's like a child wrote them. Yeah. All right. And the other thing, first off, it feels like it's been 12 years since I've written my last blog. I've been working on one off and on for the last month. It's hard to do all this writing and go to school and do work. And stay employed. And still know what my children look like. Hey, speaking of staying employed, what do we call it now? It's sort of reward, performance, feedback. We separated at Microsoft the individual discussion on how you're doing with the rewards, although they're still kind of tied just unofficially. But it's time for rewards and I want to talk a little bit about how ... I don't even know if it's unofficial. Really what the thing is is that we're not allowed to talk ... Essentially, we're not supposed to or allowed to talk about performance during the reward discussion. Blah, blah, blah. Exactly. So, I'm on my fourth or so different way we review people at Microsoft and I want to talk a little bit about it and the games people play and the tools I use to get around that. It's always an interesting time. I'm a manager now of just a few people, but even when I wasn't a manager, I was always invited to be part of these review discussions because generally my role is someone that works across the org. I have a good insight, I have a sort of an objective insight across the org on how I know more people. Given the job I do, I tend to know what more people do in the org than any of my peers. We had our discussion last week and like every other discussion we've had about people and any team at Microsoft over the last 10 or 12 years I've been doing this was ... It was interesting and people played games. For the most part, the right things happened. But I want to talk about some of the games they play. What games have you seen people play during these discussions? Oh, I see all sorts of games. You tell me one or I'll tell you one. I will tell you the game I use most often and that is ... First and foremost, before I go into that, you ever study the prisoner's dilemma? Yeah. It's the prisoner's dilemma that's being played out where there is really no benefit to collaboration. I'll say the best review discussion I've ever had was the last time I was a middle manager and I was able to successfully convince my leads that for the review discussion they are not to defend their people. What they are to do is to put themselves in my shoes and assume that their people are part of our business team and we're going to focus on what needs to be done going forward. And I'm simplifying here. You're sure and you're being very philosophical about it. But it worked very well. The people started ... I remember one lead was like, hey, that person really needs consistency on this, this and this in order to really get to the next level. I have that work. How about we talk about me moving that work to your team and you have that guy do it. It was very productive. Okay, so let me ... I'm going to take over because you're not getting where I want you to get to. I'm about to go there. Let me ... it took too long. Okay. I'll tie this into something else I brought up in my workshop. I've read most of the Dan Ariale books on irrational ... why people do irrational things. I forget the names. Irrational yours or something. Predictably irrational is the first book. Yes. So there's a story in one of those books, I believe. I didn't fact check, but I'm sure I read it there, where they did a study where people went to a movie theater and they had two sizes of popcorn available, a small for $3 and a large for $7. Most people bought the $3 popcorn and when asked why, they all said the $7 seems pretty expensive for popcorn. It's too much. Different movie theater or same movie theater, different night. They had three sizes of popcorn, a small for $3, a medium for $6.50 and a large for $7. Most people bought the large and when asked why, they said because it's such a good deal, it's only $0.50 more than the medium. Yep. Interesting the way the brain works. So let me tell you a game I saw and two ways I know to break this game, but I want to kind of hear what you thought. So the game I saw played is very similar to that game where you take, as a manager, you bring in the employee that you ... we have this rating scale that goes from $100 is the middle and it goes down from there and up from there from $0 to $200. That's sort of your scale where we put people on for rewards. Okay. There is a person from one manager who is above average but not way above average. My ratings are usually right in line with where they come in. So that's where they're seeing my peers as well. In other words, you bully your peers into ... No, no, no. I hardly say anything. But anyway, this person came in at $200. This guy is a rock star, walks on water. $200 is ... Jesus looks up to him. Something like that. Got it. And I immediately recognized it as the game. Because what happens there is ... generally ... It's the ... talk me ... He's not a $200. You'll talk me down to the value I want. And I think he got talked down to a value above where perhaps he should have been. And then everybody just happens. Like, okay, I feel better about this. Same game, interesting way. I have two ways. One very good way I've done this in the past. But I'm curious what you've done. What do you do when you see someone playing that game? So the last time I really had to deal with that particular game, the best solution that I've had for that is ... you know what? We're going to reschedule this meeting. Each manager needs to come in balanced. It's not a great one. No. It's a horrible thing because I do not believe that every team has to be balanced. I think there can be better teams than other teams. Oh, and I completely agree. But what we do ... so once they come in balanced, then we have a discussion around who moves up. So this exercise works exactly the same or very similar to the previous bucket systems I've had at Microsoft, no matter which bucket system it was. You want to put, as far as rewards go, like who are they like. So two ways I've fought this in the past. I'll give you the actual one that works 100% of the time in this case. One way to stop this game is that anywhere where there's a discussion or a discrepancy, anywhere, whether it's by 20% or 60%, if there's a discussion in the room for even a minute on ... I don't think that's right for that person. That person drops to zero, not permanently, but they get dropped to zero and they're left there until the rest of the discussion's done. So what happens is ... this is another game too, but played a different way. What happens is managers come into that room, putting people into the spot, into their bucket where they don't expect to have any controversy. Where there's controversy, those things go to zero, and then at the end of the discussion where you decided where everybody else go, then you can play the is like a game and put them in the right spot where they belong, not where they were put in. So if you put someone in an obviously bad place, you force a hard discussion around that person at the end of the conversation, everybody else has been agreed on. That one works very, very well. I have never heard that one. That's really good. You got to play it for managers, play that game. That's an anti-game. And you can even argue it. You can say, hey, look, what we want to do is go quickly through the ones we agree on and cue up all the discussion at the end. Let me tell you about another game people play. I'm going to talk about the fallacy. Peak in May. The peak in May game means, is because we do reward discussions in June, that what you do in July, August, September, pretty much all the way to March or April, doesn't matter as much as what you do in May. Yes. Oh, peak in May. Peak in May. I was thinking peak is in like, no, peak in on someone. So how do you fight the peak in May game? I take that one head on. I go, OK, great. He did a great job in May. So remind us all what he did in the prior quarters. I always ask, what were the results in the first half of the year? That discussion comes up a lot. When there's discussion around people, in time there's like controversy, what did they do in the first half of the year? Well, they had a really rough first half and blah, blah, blah. I said, then they're not. That's fine. They had a great second half, but they're not 180 or whatever. You have to take the whole year into account. And that's more of a management issue. And I tend to, I'm not like, I have the smallest team in our org. But I've sat in, I probably sat in on more review discussions because I used to sit in on other teams review discussions. I probably sat in on more of those than any person in our org. So I can tend to say the right things to make the right things happen. The other thing I think that is wise in terms of how you are presenting that question. You said, what were the results? Because I have seen over and over again, and it's part of the discussions generally do around Agile. Humans are built for relative, not concrete results. So you could actually really get yourself a rock star review by tanking, purposefully tanking the first half of the year so that, and then take your manager's feedback to heart and rocket it up back to say normal. And the manager is going to basically say, number one, wow, I really helped this guy improve. And look at the differential, even though the raw results at the end of the day was nothing special. Cool. One final challenge to bring up in there here is the Microsoft, I think probably any company who has some Agile values in them, values collaboration and helping others as being very important, especially at more senior levels. So say I work for you, Brent. Okay. And I'm going to come to you and I'm going to give you, again, peaking in May, I'm going to give you a bunch of information about how I've helped other people on the team in hopes that you see that as a way to boost me up. How do you know how much I've helped versus my peers? Like say there's a group of 10 of us in my level band and you're trying to figure out you have, results are more tangible and measurable, but what's intangible and more difficult to measure is how well my influence or how much my influence has helped the team get better, either through general leadership, coaching, diving in and helping others, et cetera. How do you compare what I've done in that intangible area versus my peers? How do you know if I help more than someone else or as much or less? The only way to really do that is a sucky way. And that's peer feedback. Our peer feedback system at Microsoft currently sucks. Just to clarify that for the listeners. The system is, I'm going to send out a list of, I'm going to send out an invitation to give feedback to all the people I know who will say good things about me. Oh, and yeah, because in addition, even if someone had something not good to say about you, they're not motivated. The last two review cycles, my entire team has gotten 100% strongly agrees in all their peer feedback. And I think people are like, well, I don't want to have a discussion with this guy's manager, so I'll just do five, five, five, five, five. What was the question I was asking? Oh, how to deal with this. The other general strategy that I see is it's the lifeboat game. So when you're talking about collaboration against your peers, it's more about sorting it by who pissed off others the most, because that's clearly visible versus who's not always. Not always. I think there are many ways, plenty of ways for me to sabotage your career for the benefit of mine without you knowing. Oh, for sure. Oh, the game of throwing people under the bus. Oh, yeah. Let me tell you what I've done to combat this in the past and in the end on this team. But if I stick around, perhaps we will. We don't stack rank anymore at Microsoft. I should clarify this. Managers don't stack rank their teams as part of the review process. One thing, say I have, I'm the leader of a team and I have like 15 people or 20 people or some manageable number of people in some level band. I have a good idea of their output, but all the intangibles, as much as I pay attention to them day to day, I want to get an idea of how much they help each other. What I do is I have every single one of you stack rank that your peers and you. So I'll do 10 for an even number. I ask each of you to stack rank the 10 of you along with your peers. So I get an idea of where you think you fit in and I get an idea of where you think others are contributing to the team. When I can, and this is a heuristic, I'm going to pull these things together and look for surprises. If I see one person consistently at the top, I go, and I thought they were pretty good. I'm now, yeah, they are awesome. And the rest of the team thinks they're awesome. And that's actually very, very important to me. The same, and I'll say anything at the bottom. And then surprises are interesting. That tells you where someone's playing the game. Like if I think like Brent, I think you are the best person on my team. And then I do this little exercise where I have you and all your peers rank you one through 10 and you end up at the bottom of everyone's list. I go, wait a minute. Brent's nice to me, but I think he's an asshole. And the opposite can happen too, because managers, again, through the same irrational behavior, I can think I could have flipped the bit on you. I thought, you know what? I don't know that Brent's contributing. He gets work gets done, but he says he's helping others, but I don't believe him. And to get past that, it's anecdotal. It's subjective data. But when I see you show up at the top of everyone's list, it forces me to think, you know what? Maybe I was wrong. I got to I got to figure out what I'm doing wrong here. There's a ton of value in that exercise. And it tells you who's playing the game versus who is truly valuable. I don't know about playing the game, right? It's essentially what you've essentially done is is kind of created a source of of data to help battle intuition. It's a cheap way to handle the fact that our peer feedback system is pretty much non-existent. I have used that technique and the surprise situation where where you think I was your rock star, but the rest of the group thinks I'm sucktastic. I have found seven times out of 10 that that is an indication of a hero in the org because heroes generally damn the torpedoes. Yep. Nope. I'm here to get things done. The rest of you get out of my way or I'll stomp on you all on my way there. Yeah. So the thing I don't like is just to close on this a little bit is with a lack of a peer feedback system, there's a little bit too much weight and responsibility and error proneness of having the manager of a large team just figure out without much data where everybody belongs, not just on output, but output and collaboration and growth. The people discussion that I went through for my team was much simpler. We have a team of about 30 people and the goal was each manager went in with three impact. So you said zero to 200, I believe. Yeah. You can think of us as having zero to two. Okay. Okay. And we were encouraged to discuss business results and just simply listen. So the decision is mine. But the listening part is my peers tell me what they observe with that individual that I'm not observing. I don't have to change my score. I can keep it the way it is. But we go in with those three values and even our rewards were at those at three distinct setting of the values and it will bubble up to my manager to do the differentiation and then to his manager to do the differentiation. Too complicated. It's simpler than the system you've been going through. It really is. I just sit there and I talk about my guys. So it's better for me to work for someone who can speak better of me regardless of the work I do. Yes. That's the lead I should gravitate towards. Absolutely. The one who likes me the most. Alright. Career tips from Brit. No but that's a true technique. It is. Right. The in these models right you have to be able to defend against if you're not the most charismatic guy in the room you better have techniques to defend against that guy. So here's one gaming system that I would love to see how you've dealt with. Yeah. People say it the adjective only manager which is doesn't communicate results just communicates spews out a chain of adjectives. My guy is a rock star. The developer says this is the best dude ever. That sort of thing. I know you've had you've experienced that guy. I was reminded of our drinking game. We've talked about this before drinking game because for a while at our people discussion the description of everyone started off with he's a great guy or she's a great she's a great woman. Great. And it would HR rep stepped up to band that phrase from the discussion. Alright I think we have time for a little bit of answering of questions. So let me reach deep into the mailbag. Always good to have a mailbag. Steve Rowe. Thornken. Thornken. What is Thornken? I actually asked him of course he did and he said that was his handle when he started way back in the day on BBS's. That is hilarious. And when he told me that because I spent a lot of time on BBS's. Oh me too. And I'm like I'm wondering how many of the three don't even know what the hell that is. But I was like is Steve old enough to have spent time on BBS's? He looks young. There used to be a paper a free newspaper called computer user or something in the Northwest. I used to pick it up and there were articles in there lots of ads for computers. But in the back was a whole list of phone numbers of all kinds of different BBS's. And I used to kind of go through there and try some out. Some were really lame and some had stuff. Then I found the friend of mine at Mittysoft. My company before I worked at Microsoft found me I forget the name of the BBS but in this BBS I could actually get to a thing called the Internet. But this is this was I guess Mosaic was just coming around but I could go to Gofer and search for things. And search was so slow. It was really bad. But a memory. So the reason I brought up Thornkin is Thornkin is Steve Rose's name on our Slack channel. One of the three O N E O F T H R E E at slack.com. So anyway Steve asked a question. You might you might think that gee I thought this was the mailbag thing and they're just randomly talking about BBS's. That's the way we roll. Steve asked about what the role of program management is in unified engineering. What functions do they serve and what what shifts to development. Right. It's part of that. What what work that is currently on a program manager's plate that moves to dev tangent time. Yeah totally forgot this at the beginning. I'm very very sorry. But speaking of unified engineering. Yes I cheated on you. Again it happened. Sorry. No I'm fine. Yeah I I strayed. OK. But I had a good time and I do it again. As well you should. I was a guest on the testing show this week for their podcast unreleased at this point. So I'll mention it again when it comes out. Talking about I wasn't sure what it was about at first because Matt Heuser referred to as concurrent engineering but he meant like combined engineering unified engineering you know the phrase works it's doing the activities are all happening concurrently. And we talked about a little bit about that. So unified engineering reminded me that I forgot to mention it earlier. So shout out Michael Larson Justin Norman perzy is on the testing show. And Matt Heuser we had a good little talk about as good a talk as you can have at an even earlier hour than this. Yeah if they record at eight their time that's five you know I did a 5 a.m. Talk. So answering Steve's question as if you thought we'd never get there. Brent on a unified engineering team. What the hell do program managers do. The role of a of a PM should be essentially get more customers identify the features the requirements the opportunities that will acquire more happy customers for the feature set of that team. That's it in a nutshell. They they should not not be doing schedule management. They may have a role in certain design functions but even then if you have a designer I would prefer to go talk to those guys then to program management in a nutshell. They're there to take on the customer focus wing of software development. I like the idea you brought up they don't own the schedule from an agile point of view. Those familiar with the role of a P.O. product owner. I think the there on a unified engineering team you could pretty much just look up any definition of a product owner and be pretty well so that's you jumped way ahead. I was going to talk about and it's good thing. I was going to talk about schedule a little bit and what happened there and then I was going to make a profound statement and say a profound question and ask Brent. Do you think the P.M. role is moving to much more of an agile project owner role. Yes product owner role and I think you're right. I do my best to push it there. I don't think it's moving there at least in my world it's not moving there. So let me tell you in the Skype consumer org that's the role of the P.M. It's a P.O. We have product owners. Yep. Like it like it. My team is still a little bit in the old world of project management which is interesting because I will tell you I don't have any problems putting laundry on the table. Love the product. Love the team. There comes a point when you're moving as we're moving where P.M. in the old role tends to get in the way a little bit. It does. It does. And and they should be enabling. They are. P.M. is another function that of command and control that's still trying to keep itself alive. It is. So I think our good friend Jim Moore once described the role of test or often described the role of test as the remainder. Dev do the implementation and coding P.M. do the design and scheduling and the remainder is done by test. Whatever and other you know Dev and P.M. may pick up more of that but test does the remainder. Unified engineering team the engineering team owns scheduling and implementation and testing and quality and deployment etcetera. The P.O. you could think of as the new owner of the remainder. No the P.O. I like I like the idea of the P.M. moving to the P.O. Yes it's a distinct set of stuff that a P.O. does. It's not the remainder. Right. The pro the transitional problem is as bluntly we have too many P.M.s. Yes. So in and the we don't need as many P.M. as we have for the P.O. role. So I want to have them pick up the remainder purely as job security for my friends still stuck in that role. Yeah and I'm going to say screw that. I'm going to say screw that in the long run but that's that the reason I brought up as the remainder is because there's a transitional period in similar to how there's been in test where testers have figured out former testers have figured out where to sort of belong in the new world and most of them have. And I guess in the same way we had to go through it. P.M. should figure out where they belong in that world too. I completely agree. The I'll walk through. So I first encountered this this this P.M. drama about five years ago when when I went to Bing and as as I wrote on my blog post at Microsoft the U.E. trans transformation started in Bing literally just a month before I joined in one of the I'd be funded several P.M. is there and there was a period where they were confused in terms of what value they had and then some started focusing on customer stuff and started succeeding on this but even those guys felt that it wasn't going to persist right. The because a lot of the data they were looking at so Bing's rich in data and a lot of the data they were looking at was essentially which customers are were pissing off. They were focusing on D sat not C sat and they're like I don't know what I'm to do because we're busily building up all of these great data tools and shortly the dev will be able to figure out what's pissing off customers readily without me. And I'm like right. And that's the way it should be. But dev can't focus on what are the new opportunities that we have no code for. Right. That's what you should be doing. That's you should be designing a B experiments. You should be thinking through what are opportunities that we can either grow our market or increase revenue from our existing market and use your quality intuition skills to generate a list of opportunities that we can run through and test very quickly. I agree. I agree. And so what that means it goes into Steve's third question about shifts to development or shifts to the members of the unified engineering team is that there are some old some skills some tasks activities some activities previously done by PM belong on the dev team. And like you thought the devs didn't want to do testing they don't want to do PM crap. It's more efficient for it to be there. And it further to me it further emphasizes the need for generalizing specialists and specializing generalists on your unified engineering team. Some people are going to be good at different things. Some people just want to be told what to do. Some people want to help figure out what to do. There is room for all of those. In fact you need all of those. You need a mix of skills and a mix of interests to have a successful unified engineering team. I think it's much more efficient when you start moving a chunk of that previously owned of the activities when you move a chunk of the previously owned activities from PM to a unified engineering team. So I'm full behind that. I don't know exactly how that transition happens because right now the unfortunately in most parts of Microsoft the PM role is much about wanting to get their thumbs in a lot of different areas and having that wide exposure. The program management role is a lot about I coordinate I schedule I know you don't. You know I think I think it's the next step. I think it's in a team like Microsoft or organization like Microsoft or in a team structure like these Microsoft orgs with it's not like we have like a one to one PM to dev ratio but we I would say in general we have far more than we need. I would and we need to think are far more and I think that's the next step in our evolution as a company. We made a lot of strides in evolution how we make software. I'm across most orgs at Microsoft. I'm very proud. I talked to no assessment shout out. Thank you. Thank you. Thank you again Noah. Noah was at Etsy. He helped. He helped take Etsy from the sort of the shipping weekly monthly to continuous deployment. He knows what he's doing and I talked to him about what we're doing wanted to get his feedback and and I feel like we're actually making software like a real company on my team. I'm very excited about that. I like Oh my God we're doing it right. We're getting there. Very exciting for me. And I think despite despite I don't want to throw all PMs under the bus they mean well but I think the next stage in the evolution of engineering at Microsoft at least is to evolve that PM role into something more supportive of a efficient unified engineering team. One of the one of the PM's that I worked with a lot in being left teams four years ago and he asked me to come in and help train up his current team on agile practices in particular Kanban and I met him couple of months ago. I happened to do a presentation for H.R. And he was in the room and he's like dude we need to sync up. I've gotten my whole 300 person org executing using Kanban. And it's the happiest I've seen him. Right. It's the PM role when when they simultaneously do things let go of command and control and stay consistent on maximize business value and customer value. It ends up being a lot of work for them. They keep themselves busy. You're able to prune off the crappy folks and when we get rid of command and control right. They become so in P.O. you know the role of acceptance testing. So what is an acceptance test in a scrum model. The P.O. does the acceptance test. Okay. Do they do any testing. Sure. No. None. None. None. This is something that is very the best acceptance test. The product isn't even there. So the role of the P.O. right when when you're doing agile the P.O. Sets down and says these are the outcomes these are the use cases I want solved and then we'll generate a definition of done that is not the ideally not framed in. No I get that. Right. Well so then the dev team wants to claim that they're done and so they do they sit with the P.O. The best P.O.s will do a bunch of precision questioning around how did you assure that this fulfilled that requirement. The worst P.O.s will open up the product and retested themselves. The Intel has gone through that acceptance review and it's more of a precision. Alan explained to me so this this particular situation the outcome was it needed to scale out very high. Right. And then let's say I mad it needs to scale out to end instances of whatever is important to your team. Okay. How did you prove it did that. I launched a bunch of VMs and VMs and deployed and it was fun. For how long. Just like a minute. What's right. And so and so that is an end. I don't struggle with that from the idea of like that's the right thing to do. I struggle with that is those. I was talking before about the transition from P.M. to P.O. Most the P.M. I know can't become that P.O. it's a different person. They can't. There are there are several that that I am aware of that are working in most several others are. Let me put it this way. I think it's a difficult transition. The thing I'm addressing because the one pattern that I saw in my last team when when P.M. started going through this thing. That it literally was P.M. becoming the remainder. And I I saw it from day one. The the codependency loop being reconstructed. Yeah I could see that and I would go to P.M. and say when you are signing off on this thing. You should not even have the product in the room. I see where you're coming from and there is an interesting parallel here. A lot of teams at Microsoft. When moving to the unified engineering they decided that OK most you testers are now data scientists. And some of them embraced it and and figured it out and became really good data scientists and a bunch of others just waving their hands around trying to figure out what it means. I see the same thing happening in the P.M. to P.O. some will get it and embrace it and be excellent at it and there'll be a bunch more kind of way of their hands and figure it out what it means. On my current team the majority of the P.M.s. There are I would say it's 75 25 25 percent are still holding on with white knuckles to the old world but the majority of the P.M.s that I'm I'm working with. They're coming to me and say hey Brent I want to I want to sit with you and see if we have the data because I have this idea for a business opportunity. And I want to see if the data sort of aligns with this intuition. And for example there's there's one new feature that we're going to fund in Azure that directly came from me and this one P.M. working together and realizing that yeah it does look fine. So he then I then pulled in a list of internal customers on Azure and he's now going through and doing a road show and testing out whether or not the internals. I just said he was testing. He's testing out whether or not this would be a valuable thing to those guys. Remember Brent words have meaning. They do. They run a time. Fun episode 41. It was a fun 41. Next one's up is 42. All right. I'm Alan. I'm Brett. 
