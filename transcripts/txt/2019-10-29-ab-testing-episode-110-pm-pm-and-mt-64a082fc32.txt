Welcome to AV testing podcast your modern testing podcast your hosts Alan and Brent will be here to guide you through topics on testing leadership agile and anything else that comes to mind now on with the show we're back hey right how's it going? How you been? Overworked under exciting yes I did a big presentation to Scott Guthrie I've heard of that man since we last talked and he is now referring other teams to me by name I'm not certain if it's good or bad that Scott Guthrie knows my name. Brent a mover and a shaker on his way up. I'm definitely moving and shaking things but now I've got teams coming to me at a at a rate around three teams per employee on my team so I have a I have a scale problem coming very quickly. Well the trick to that is just say no Nancy Reagan. I was just gonna say no. You don't like the Nancy Reagan add-on? No I don't. So anyway so it's exciting times I will figure out a way to scale don't know how yet but I have confidence that my background thinker processor will some at some moment at 3 a.m. over the next week will wake me up and go this is the answer and I'll go oh thank you background processor I will execute that immediately how about you you've been traveling I did a quick trip to Montreal I left on Sunday came back Wednesday night got up early for my Thursday morning meeting luckily got lots of sleep last night so yeah yeah a little quick trip to Montreal I'm home for two weeks then I am off to Sweden to wait Norway Sweden we think Sweden I am I am less geographically challenged than most Americans I'm just stupid so yeah I'm off to Sweden to give a talk on reflections on modern testing so the point of that talk is a a brief little intro to modern testing and then I want to share some stories from some people that have done some modern testing like things in the world we'll talk about what's next should be fun I will share slides once I complete them on our one of the three dot slack comm so that's exciting I also recently if you're listening to this and listen to our feed you go what the hell is this weird new age jazz stuff and Alan just talking kicked off the intro episode of the ABT three-for-three podcast which is a will be a sporadic but interesting and shorter version of a testing because there's no Brent there's just Alan talking to members of the three members of the community I have one show ready to post probably go out next week I have another one recorded just needs some editing and a few others lined up to record so those should be fun they're all about 20 minutes long just a brief little peek into a slice of life and people doing things that are modern testing like you have one released I just the intro you release the okay so that's coming up and that's it that's my travel for the end of the year and then I'm going to London for like eight days in eight or nine days in January cool and that's as far ahead as I can think so that's what's going on with me where I haven't turned around where are we starting on today's podcast journey I would start you want to start with the letter or do you want to start with oh it might go into that so let me give some context here yeah it's not really a letter I guess it's I thinking mailbag but it's not a mailbag it was a slack message and I need to give some context all of our mailbags are slack messages oh you're right you're right this is the different one so this is gonna take a while to work into you correct me where I veer off course or I'm wrong so most of you know I work at Unity I recently six months ago changed jobs away from being a director of QA I became that ever tell you my title it's stupid I wanted to be director of delivery so back that's what I do I make sure that we can deliver and the way I describe it without saying accelerate the achievement of quality because I say I help our organization about growing 200 plus engineers how to balance quality and frequent releases and shipping faster how can we go faster and with higher quality because those things balance each other out I think I might have mentioned that on the podcast before I'll take a shot at tequila right now hold on okay I'm back and that's what I do in the art my title is director of program management which has a which is hard for me because at Microsoft where I spent a big chunk of my life the program manager role is such a bastardized bit of the remainder that's different from org to org and is actually nothing like the program management I do now that it hurts a little bit but I'm getting it I'm getting I'm getting past it are you managing a program I'm managing all of our programs okay it seems like a relevant title it's much more when I actually read books it's funny how often you like do internet circuits on program management and you end up with project management either a name or a description but what I've done some reading and research on program management it's actually it's actually what I do I make sure all of our programs execute it's very highly related to quality and actually my full title is director of program management and quality so I still have that keyword in there but anyway I still like director of delivery they drive much better what I do but they wouldn't they wouldn't make up a new title for me so a lot of syllables for essentially a release p.m. oh but it's but it's much more than that we'll get to that later how does ship room go at unity oh we don't have a ship why would how could you have a ship room I'm not redoing the the old-school and actually versus new no but actually the editor engine team does has something kind of like a ship room but when we're shipping we have stuff shipping I mean we have 90 services that make up monetization alone and those are shipping all the time yeah that's actually an interesting criteria right in the old days just the existence of a ship room was necessary because you need to get everyone together because the decision to ship was a unanimous decision and also irreversible decision it wasn't it was I mean for all intents and purposes yes it was irreversible it would be extremely expensive and and as someone who holds accountability for allowing a recall class bug to go out yeah I'm very sensitive to those type of things I do think on as an aside shipping a recall class bug it's something you have no choice to do but once but is a super valuable learning lesson so here's an interesting stage where we're at I want to and we'll get to a point here eventually we have we're talking about the SRE team earlier before we started recording at Unity and we have incidents errors in production something goes wrong we've had a few that were just oops bad code but those really they happen but they're super rare right now what we have is the flavor of error of oh wow it's actually I'll put it this way we have our errors of complexity and what I call complexity or complexity due to integration complexity dude like I make a change in this thing and then this unexpected thing downstream does something unexpected based on that yeah so that error of complexity we have a lot of those and we'll work on getting more resilient towards those we have some errors because a third-party provider our cloud platform for example may have an issue okay there's this course or stuff with failover you can do there that would help their longer term and then there are we had an interesting issue I don't know if I can go into it too much but I'll put it this I can this way we had an interesting error took us a while to track down we had one of our metrics was what we expected for something we were we had rolled out to a small percentage of our of our users and the metrics were off the monitoring like this something's weird here off as in going in the wrong direction wrong direction wrong direction this something it we had something we expected to be at a level when we rolled out to assert to 10% and it was not at that level and it was well below it was not horrible but there was something going on and we dug and we dug like we looked at our community re-reviewed code undoing the change completely would not have been a valid case because it was Jesus so hard to talk abstractly about anyway we couldn't believe me we couldn't just undo the change we needed to figure out why the change we made was causing problems lots of reviews of three etc etc we found out that it was just a malicious player running some scripts okay so your system wasn't flawed in any way or did well change the system was flawed in that it was no no no there's something they didn't get anything but the fact that they were trying caused our numbers to go off so it's that anomaly detection which is as you know as a data scientist is a kind of a hard thing no anomaly detection is not hard not all really yeah root causing the source okay that's hard okay fair enough so anyway my role oh so getting back on track as some of the three know I send a newsletter ease to my used to be my team then it was my former team and then eventually I just called the unit eventually turned to something we call the unity quality newsletter and I send it to a whole bunch of people at unity and various people sign up including my manager current current manus per at both okay so lots of people read this thing I just sent one out interestingly about accelerate okay and kind of going into the book the BS of the word okay just the word the letter a is beautiful good it's also a stand for Alan not coincidentally stands for shut the crap up so anyway I got a message from my boss I got your newsletter great mail and then he said I have another idea for you for your next news like great now he's giving me I'm paraphrasing this brunch actually read the quote but I'm paraphrasing the the text he's a very eloquent speaker and writer but and I hope it's this podcast but wait you said you hope he doesn't okay I have nothing bad to say about it actually I don't and as you know I've hated a lot of managers yeah well mostly my last one at Microsoft but it's others is to you know you I will throw any names under the bus I thought Chris I liked Chris the next box he was good anyway all right then so and this ties into my program manager and quality role and everything else I do he says blah blah blah imagine a quality imagine as a quality leader as you are as mean of our maneuver lessons are if you got control of the program management aspect of the organization which I have and made some changes Halloween I was trying to what changes would you make and how would they be measured and measured stuff but what changes would you make and how would they be measured and then there they says in your experience in this role what have you learned about having that control of the activity of a product organization so far and what matters but that first half I think is kind of interesting I'm gonna throw to you and I'd love to throw to the listeners as well but as someone with quality experience as a quality and testing specialist and rephrasing this in the terms we use in modern testing if you're given a role where you're in charge of program management of an organization we want to look at it from a quality and testing specialist no I don't well care to do that okay as a quality leader all right okay which is I think me as a testing specialist how would you tell what would you do for PM yeah well I think it's important I think a lot of people a lot of non-modern testers may approach this challenge with a hammer I would have mandates and these things and I don't think it's necessarily the right way to do it but I can give some context about the org and the way we work as we go but I'm just going to throw this to you I'll rephrase the question one more time so as a quality leader which you are even though you're in data science so as a quality leader you're in charge of program management for a large organization making sure all their programs rumbling talk about what that means as needed what are some things you do and how would you measure them or how would you measure success okay I was kind of hoping you'd answer the second question do you want me to do that first yeah I know actually I think that's better unless the answer is nothing no the answer is not nothing so the second half is in so in my experience what have I learned about having the control of that and so far of what matters but I think that that's more of a follow-up to the discussion from you I can talk about what I've done and what I haven't done because there's you know how you are you're in a new role and you're figuring it out you feel like yeah yeah I'm getting this I'm getting this I'm getting this and you get to a point look back and go oh I could have done that stuff a lot better I'm kind of at that stage to do this longer would be easy the first thing the first thing I would do is essentially start executing against a Reese style playbook program management program management project management PM mate their job is to help you grow the business by creating happy customers okay sounds a lot like accelerate the achievement of shipable quality except in my view the PM role should be at least if not more one milestone ahead of the rest of the engineering team so I think you're confusing a little bit I mean let's let's get our PM roles here that's a set context so we have product managers who are and that's more like what our program manager roles were at Microsoft a little bit more like the product owner so the project the product manager they're the ones who talk to the customer figure out what we're building establish the where a requirement doc or better stories that define what the feature is what the things are we're building okay they figure out they're the ones figuring out what's the hypothesis for the experiments were running I change my view of this text and just put in release manager well no because that's and I know you're yeah so when you're talking about PM that so product managers do those sorts of things so my role is there is some release management in there and I've done that role before but it's a lot more risk management a lot of my role is making sure that people are doing the right things to plan their work to make sure I mean we do p50 estimates which are surprisingly good I can go into those later I make sure that 50% probability will hit the estimate on this date so take a chunk out of hubroids how to measure anything book we're gonna try and estimate with a 50% probability that will hit this date okay that's what we made by feet of p50 estimates so I'm making sure people are talking to each other making sure coordination between teams is working as needed are like most teams when a sub team needs to ship something they do it very well and their predictions are very accurate and quality is high as features or functionality begins to span multiple groups of teams is when more risk is introduced I'll say so I try and make sure I can identify where those areas are get the right people talking ask a lot of good questions I'm good at precision questioning or just asking like what's the worst thing that could go wrong if you ship this tomorrow and they freak out and create a whole big list of work items to go investigate which is good so things like that so I'm really just trying to resolve constantly trying to mitigate risk across the organization so that's my role mitigate or prevent prevent if I can I'll take mitigate okay and the risk you perceive is what it's primarily lack of communication the risk is complexity or lack of understanding of complexity when you look at estimates estimates really are just an evaluation of complexity so I try and make sure that we have more context to evaluate that complexity so that we can make better choices and or trade-offs or communicate downstream or etc etc so that's the difference in and that's fairly constant in what I've read in the books that I found that have been about program management and the writing online about program management versus product or project management okay so in the context of the role you're talking about the decision around what to ship what we're gonna start working on is already made your your role the way you describe is now around reducing the risk and increasing predictability of the delivery is that right sure part of it which is I think why it ties into quality quite well if the outcome variables so in some regards like what you describe you keep not care about quality at all right your job is someone made a decision to ship this we're gonna ship this damn it no no no I never said that no no so how does quality play into this what what what pivots do you have in your role as a quality leader here or is that maybe the question how do we introduce quality into this the answers to your question are sort of endless I guess the reason for the trying to explain a little bit what I do was to provide you context because in your answer to the question you are falling into the traditional Microsoft definition of program management which isn't program management no no no so I here's what I have in my head right I think of a train station okay you're the you're the the guy that's standing on outside of the train station with the stopwatch making sure the train leaves on time but someone else has already decided what what the train schedule is what the destination no no no that that's not it at all so I'm trying to map it I mean you're right there's a very different is there actually maybe we should just go this way is there a role at Microsoft that is directly is there a title at Microsoft that's directly to the thing that you're talking about no okay so that we're gonna have to go to get on the more complex so yeah so the product manager when does when does their jurisdiction stop in your start that's a good way to ask that I would say that they're defining what we build the engineering team and I'm part of engineering okay the engineering team builds it I work a lot on how what about when when is up to the engineering team so one thing I really like about our org is we have a mantra or a statement only engineers can give dates so a product manager can say here's what we want to build here's the stories and they'll say okay it'll be done by June tober 15th I say oh no we need something sooner I say okay fine cut scope more re-estimate otherwise June tober is alright so your job is to address how I would say largely how yes how okay which is why your release manager thing isn't too far off but typically that role is only effective or only prominent later in the cycle do you do you have the ability to declare we've made a decision and we are stopping this work no but I can and have made recommendations along those lines which are generally followed okay all right so I leave it I leave that decision to the product team I give them the information so that I'm not I'm not a I'm not the guy with the stopwatch and the whistle deciding when trains leave all right so when we talk about quality are we are we talking about how would I now term code correctness no not at all I don't I don't I only cover that if someone's really messed up and I really have to okay so in terms of the role that you described to me it seems like quality what you have the ability to influence is make sure as part of the requirements a definition of quality is the is considered right telemetry monitors you guys operate a service knowledge around I constantly think of the pivot and persevere meetings that's in the the Reese's book yeah right your job is going to be to make sure that post ship those questions and those meetings are efficient effective and informed yes but part of if you control how and I guess to some degree what then to me the the number one metric that matters is going to be it's going to involve cycle time mm-hmm and I'm thinking something along the lines of some ratio of errors to cycle time sure right it's because you want to be able to a key part of your role is how do you iterate and move faster yes but how do you do that without incidence I it's so funny that after all that roundabout way like I didn't know where we were gonna go you kind of ended up with the right answer anyway okay so cycle time is important if we go back to I can remember who put it this way one of the three but I bring it up all the time it's in fact I brought it up in a conversation yesterday about a related subject it was a conversation about how we manage programs and for context I wanted to manage my proposal was that we manage feature or functionality or product work differently than we manage experiments but well I make the experiments visible so anyway but I kept my role is to this is stolen right from one of the three I wish I remember two but I give you full credit pat yourself on the back if you're listening our goal is to reduce the time from product hypothesis to customer feedback so that's the cycle timer cycle time in Jason yeah going back to accelerate the look one of the metrics they say is an indicator or a predictor of quality is deployment frequency we reduce time from product hypothesis to customer feedback if you yeah if you have a case your job oh this is this is not gonna get really interesting so the KPI and our listeners are going thank God it's getting interesting so the this is in your job is to measure and increase adaptability yes I want to write that one down keep going because reduce time from product hypothesis to customer feedback right first and foremost love it so it basically says okay my stuff you know my I might even classify that now you know what I'm gonna do I'm gonna I'm gonna specify an additional thing on that one reduce time and waste from product hypothesis to customer feedback because what I was I was thinking through is you want to reduce the cycle time your job is to fail faster and faster yes yes that's the exact see very much the scope of my conversation about experiments my argument was that I I met with one of our data scientists are really got smart data scientists in Montreal and we had a long conversation about how our experiments take too long why do they lots of reasons I'm not good lots of reasons that are not fair for me to go into on a public podcast we have we assume their unity technical IP reasons in some cases yes it's also our data science team and monetization is fairly new but anyway most of our experiments come from our data science team but I want to use what they're doing and accelerating their ability to experiment to build a culture of experimentation across the other teams as well so that's all part of my master plan yeah so I'm heavily in a bunch of experiments of course so right now right and the the team I'm interacting with we have not implemented any sort of experimentation framework doing an experiment takes a developer to hand code and hand separate the sample we have many of the same issues and and even then once it rolls out you'll you'll very quickly discover crap the two experiments aren't clean enough because they both share this one variable that invalidates the experiment and then you have to do it all over again yeah it's not it's it's not friction-free yeah I get that so yeah so many of the same issues so not going into details yes there's many things that can slow down your ability to experiment and fail fast so yes now now we're getting somewhere this is I like it when conversations go this way we're actually eventually reaching some agreement there's a time over cost that is important in your role and I'm trying to figure out how to put that in right you can fail faster and faster and faster right you let's say presumably you could put a backlog of hypotheses that can all be independently tested as experiments it's probably not true but let's let's assume okay but the way you do that is it takes a thousand man-hours every day to pull it off that's too expensive that's way expensive yes right and so it's it's it needs to be easy to experiment it's the the price tag associated with improvements in cycle times is something to take on does this flow I'm trying to recall if low has a KPI here that you can it's interesting because I was thinking about this I'm not obviously in Mike in Microsoft recording this but right on my desk at home there's usually one book I'm referring to on my desk in front of me and the one I pulled off the shelf last night to think about this was flow it's on my desk better know I'm gonna look it up yeah a lot of a lot of what he does is justify the principles and and explains it with science why that's the principle right so obviously smaller smaller chunks is one way you're gonna do this yes one hypothesis at a time is a great way to build smaller and smaller chunks right the one hypothesis at a time per feature right I would it would I would not be failing fast enough if we were on one experiment at a time overall no the sequencing will kill you yeah a lot of teams will do pairwise deployments as long as the the features are mutually exclusive and don't mess or step on each other right so there's an art and science to how do you how do you deploy these multiple different experiments into one payload so that you can reduce calendar time on this putting in feature flags and putting in the mechanism that track because people today deal with DLL hell as well as version hell and then when you add in flights that are making very real I took me a while dot feature the DLL is that library thing on Windows it's really shitty right DLL have an equivalent in every operating system it is a it is a versioned package whether you get it from new git DLL Python package whatever I guarantee you guys have it and it's a problem the yeah so let me go let me back up here and get and and I want to get back towards I think we're on to something with the measurements and some things I've been thinking about so as a quality leader and I think quality leader we've thought of things like flow we've read accelerate we have ideas on what it takes to get quality software to our customers and we know it's not all testing that goes back to your functional correctness thing it's how do we make sure we're delivering something valuable or something it's we can get valuable feedback from that gives value to our customers and we had mentioned error rate frequent to look at deployment frequency incidence divided by deployment frequency we get an error rate that should decrease if quality is going up and then a little bit about velocity no no I'm gonna push back on that okay so I'm thinking about Reese okay so I think one of the mistakes that a lot of people make in this case is that they they go back to what you say they track error rate what you need is a positive metric to improve versus one and I like is more powerful and the reason the reason I bring up error rate errors per deployment is that's one of the predictors of quality that's listed in accelerate so what would be the inverse of that what will be what metric will be try to improve rather than do you remember the episode we talked about where where I taught you my little hypothesis process yes very much so it's completely ripped off and re-embodied in the principal five six principal six forgot them all episode of the ministry of testing course which is coming out next week anyway go on plug so Reese came here to Microsoft and I managed to grab a seat when he came in and he talks about you grabbed his seat no AC okay I have all right I would have maybe and I don't want to summarize the whole first three well I do want to summarize the whole first three chapters so he worked on this he was talking about the initial when you when you're in an entrepreneurial role which may not always be the case and in in the topic that you're talking about when you're trying to break into a brand-new market you think you have created something that is an innovation and he always recommends first see if anyone cares yes right and he talked through an alternative strategy to the critical failure that caused him to write lean startup okay I'm not gonna summarize the book but I am gonna summarize the book I'm just not gonna go into detail and then I'll and then I'll they they had a large number of people who worked 40 hour days for six months straight because they had an idea and they were convinced it would have been awesome and the day came to ship and they got 12 people to download the project that they and several other people had killed themselves over the last six months to deliver okay far from a solid ROI mm-hmm statement we've all yep that that's a common story in software and so he spent time in and we've we talked about this in retrospective episodes but he spent time thinking about what could I have done differently to come to the same conclusion sooner yeah but this is going back to the now let's talk about errors and failures let me let you finish up and we're talking I think we're talking about two different things but you may wrap it up and make some magic connection go I I will and you realize what he could have done is created a website advertising his product with a button that says click here to download in at the seminar he stated I would have learned exactly the same thing but that would have been infinitely cheaper to produce I just gave that same suggestion to someone last week we're talking about a we're not sure which of these three or four features we're going to add next to this component I said add buttons for all of them and track who clicks what and then he said it in the seminar by the way he asked the crowd would I have had to implement that download button or would a 404 been fine answer of course 404 would be fine right and so that's where that's what triggered on the the error cases because in that case you would have a sharp rise in 404 it's an ex I'm not talking about I mean it's expected error it's an expected error so there's a huge difference overall we're using an error to track product market fit and which is fine I'm talking about errors that cause what we call an incident what some companies call an oops I don't know it's something that causes people to go fix something in production Reese has no intent of going to fix those 404s in that case there's there's no reason to we just want to track the data it's a it's a cheap way what he's done the 404 is cheap telemetry you can track the 404 on the page request and that's most most web servers will track that for you quite easily it's super smart but what I'm talking about your he turned an error code into a value code so looping this back into my original statement okay is that yes they're going to there could be caveats depending on things that are experiments or information gathering or trying to evaluate product market fit versus actual errors that impact customer success now so now and my original question was how do you turn that metric of errors per deployment and these are errors as I described a moment ago into something that can be measured as a positive thing you want to improve I or maybe we don't maybe maybe I just push back and say actually counting errors per deployment and reducing that is something we do want to improve errors per deployment is a vanity metric you know that I do know it I know it's totally gameable but I do know and trust the research by Forsgren et al and accelerate that says it it along with their other four key metrics they came up with our predictors of quality and I think they could be it's tough I forget what they're called you can tell me what they're called when they measure they find that all these things are indicative of high quality and services but correlates but but the moment you begin measuring them they become less valuable what's that called that's Hawthorne's effect it really is it's Hawthorne's effect because the second you the people's behavior changes in accordance to how they're being measured okay so let's and anytime you measure those error rates it eventually comes down to who's accountable to reducing them so let's go back to once again to the original question so as a quality leader what are some things what I don't know about changes as tough you don't have the context but what sorts of things would you measure to know that your program effectiveness of my program I mean your ability to get if we go back to I forget the quote but our ability to get feedback get value to our customers and get feedback from them what are some other things that you would measure to know if that was being effective or something that would be less gameable you could track against and watch improve so here's something that's actually the way you've now framed the question is extremely relevant I have been asked to solve that problem for Azure Azure compute specifically great we have the same job we just figured that out we don't have this I'm gonna do it a fun way you're gonna do it by cat hurting or vice versa we'll see what happens here's another way of phrasing it so so I will say I have ideas on this one as it relates to compute but here's how I will generalize it in the absence of surveys how do you measure a particular type of error and its individual impact to see set don't you measure it without sending surveys don't surveys are revenue lost revenue lost is a negative metric but if you revenue not lost no so I'll tell you have you heard of the CLB common language verification no no customer lifetime oh oh we just call it ltv lifetime value but okay sure makes sense my current my current thinking is that I would expect to see if we're going in the right direction I would expect to see CLB go up of course the problem is you need time to be able to see those trends maybe yep and so you want to be able to you want to be able to say actually I'm gonna go back to my experience and being we had the ability in being because we had everything till we had everything hooked up we could track changes to we could track changes to an individual check-in and because every query generates ad revenue in that particular case it was very easy to look and draw connections to Alan you're checking yesterday cost the business one thousand two hundred and eighty four dollars and fifteen cents we had that ability in big is because both the revenue side and the telemetry side are near real-time okay our our ability to measure that is lagging that as is mine support request is another heuristic what you trying to do yes so okay you have no choice I've talked myself into it all right until you have a near real-time signal that expresses a positive happy point and maybe it's usage which is common in in an experiential stuff right like the website we talked about then yeah you have to you have to do two things you have you will have to measure error rate but then you have to have something else that is running lagged behind that quantifies the importance of that error rate when it first rolls out you're gonna have to treat ever ever every error rate has equivalently important to some degree but then over time you're gonna have to do a correlation and be able to determine which errors are actually more important than others yes going out of time here and I apologize I'll I will try and fix this in editing as I always do but over the course of the podcast Brent has moved himself farther and farther and farther away from the microphone he actually moved the microphone closer to his body and then slumped way back so he's I'll fix it as I always do but I would push back and say I there have to be and there must be and there are things that I could measure without that that's obviously a very good measure to without having real time feedback on the cost of every change your job is to reduce the time between product hypotheses and customer feedback you need both elements of customer feedback because what you're trying to grow have you heard of NPS what what is Microsoft call NPS no it's not Microsoft F you it's the world all right NPS is net promoter score yes I have heard of net promoter score okay you're trying to find that and that one is the world you were trying to find a telemetric version of net promoter score right if you just track error rates then you you aren't going to be able to accurately measure the net benefit yes error rates went up but that's because this thing is super popular yeah and the people who love what yeah once again we're talking about different things so error rates are in I know you're talking about and oh my god and I don't know what to do with it just I think tire put you in a straight jacket and just put your headset on you so we have error rate of course for 404s for example all the different things for what I'm talking about for errors to deployment because yeah you can have like an intermittent error that you get a million users on you know more a bunch more hits on doesn't mean quality went down quality actually went up because you have more usage and people are using your application I'm talking about things that cause errors at the level they cause what you could call an outage or an incident or something that requires intervention something that may require something that may cause either a inability to move forward or in worst case a revenue loss that level of error per deployment is what I want to reduce oh okay so I'm sent turns out I'm already central to that for core compute I don't if the question is how would you change the program manager role essentially to reduce outages that's one part of it emmer out of time okay but I I think we need to continue this discussion because I think we had a lot of miscommunication on definitions it took a while to get on the same page and I think we got there and I think what I've discovered is that in our roles we do a lot of the same things which I find interesting from a oh my mr. hyperactive hands over here it just accidentally shoved his microphone down his pants so from a modern testing perspective it's cool that we are largely in the same place but I that is interesting and from a program manager perspective is just making sure people are doing really it's making sure people are doing the right things you may have been in the test architect group 15 years ago when I described my role as I stop people from being stupid and well that's well I phrase it differently now but it's like it's my more mature result is I try and help people make good choices so it's a parental role sure all right I gotta go okay you gotta go we're gonna talk more about this next time I'm well I'm definitely I'm definitely not Brent and I am okay 
