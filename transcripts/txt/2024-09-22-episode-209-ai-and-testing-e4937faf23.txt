Welcome to AB testing podcast, your modern testing podcast. Your hosts, Alan and Brent will be here to guide you through topics on testing, leadership, agile, and anything else that comes to mind. Now on with the show. Hey everybody. It's Alan. It's Brent, mostly the anything else that comes to mind. That's really, that's kind of the story here. We're not a great podcast, but we're a fun podcast. Yeah. You know, I miss the days where it was literally, well, we only did it a couple of times, the vodka episodes. Not the vodka. Oh, those are fun too. And every time I open Riverside, I look at the live streaming option. I think, you know, we should just, no, we shouldn't. So we always can though. We can't, but those shouldn't involve drinking. So anything before I get into my story here and why I'm just freaking delirious today and not in a good way. Anything new, exciting with you need to share? I think last time I had just recovered from COVID. No, no, really good. So I did the opposite of had COVID I did for, I did this last year. You'll remember as well. I'll probably write a blog post about it again tomorrow, but I just finished walking around Mount Rainier again. Okay. I took seven days to walk 95 miles around a big freaking mountain, which is fun. So anyway, let me just get all the way to the end. I do that. I have, was it snowy at the level you were at? I assume not. No, no. The highest point of the trail is about 6,600 feet, 6,800 feet. And that was, I could see there, there was snow around me that on the trail. Anyway, I did this trail seven days. It's kind of fast. I had a couple of big days in there. I get to the last day. I start with a big climb. The weather is beautiful. I have great pictures. I'm cruising along. This is the last day. I'm feeling good. I have breakfast, kind of a late, I like to walk like four or five miles before I eat. And I had breakfast and then I said, I'm going to the car. And I booked, I walked seven miles without stopping at all. Except for a couple of places, I had a little drink of water and I just booked it. And I get into the parking lot and there's nobody there cheering for me, but I'm cheering for myself. Yay. And I walk over to my car and I can't get in because it is completely dead so much. I can't even get the key fob to unlock the door. Oh, someone left their lights on? No, it's just a, it's, I, time to upgrade someone's car. Time to upgrade someone's 12 volt battery in their car for sure. So anyway, that was a little. So I ended up not even making it home last night, but getting home early this morning. When, when, when was the, the, oh crap, my car screwed. What time did that occur? Well, I got to the parking lot at two and it took me. PM. PM. Okay. And it took me a couple hours to get to the, I can't get out of this. This won't, this won't jumpstart. This won't nothing. And yeah, but anyway, I'm just, it's great. The hike was great. I have to get rid of the gray cloud over it. I'm working on that. So by the way, and I'll leave this part in the podcast with my phone rings. I have to answer it because I have a couple of calls in to get it fixed because it's still, my car is still way the F out in the mountains and I have to go rescue it. How the hell did you get home? Mrs. Weasel came and picked me up. I see. Okay. So yeah, so that happens. I'm a little tired. So yeah, yeah. And then that's it. And then today I spent the entire day as one does when they take a week off, catching up on messages in Slack and Teams and email because why have one solid and effective communication platform when you can have three mediocre methods to choose from? Okay. You just pause there. I'm sure it picked up fine, but I had no idea what you just said. It'll be in the recording. Don't worry. I was making a crack around too many methods of communication. Ah, yeah. So that's an excellent point, Alan. Yes. Yes. Speaking of excellent points, have you played with, and I had both my boss at work and Jason Arbin had awaiting pings for me today asking if I've played with notebook LM. Have you played with notebook LM yet? No. So we're going to go into the AI portion here. I brand had to ask some topics. So let me just, before you go dive in there, before you go dive in there, let me see if I can play something for you, if I can find it here. I'll just cut to the middle here if I can. Okay. So are you ready to dive into something kind of controversial today? Always up for a good debate, especially when it comes to the ever-changing world of software development. Right. And this one is a hot button issue. The idea that maybe we don't need dedicated software testers anymore. Yeah. That's definitely a statement that's going to ruffle some feathers. You got that right. And the person shaking things up is Alan Page. Oh yeah. I know Alan very well respected in the testing world. Yeah. You could say he wrote the book on it literally. We're looking at a couple of his articles where he really throws down the gauntlet claiming that most software teams don't need dedicated testers anymore. Interesting. And that most stood out to me too, because he's not saying all teams, right? Yeah. And that's an important distinction right off the bat, because he's acknowledging that there are always going to be exceptions. Like what? I'm going to stop there. And this incredible podcast you've been listening to was generated by Notebook LM by me feeding it two of my blog posts. Holy crap. I mean, I'm fascinated. So I did it with some, you know, very few of my posts are about testing, but I want to bring that up in context of today. And just like, oh, well, that's interesting. So, um, I gave it, I did feed it all the rest of my blog posts and I asked it a couple of questions here. And it's actually kind of interesting. It's only based on. You make like a notebook, like a Jupiter notebook. And it, oh my God, I'm just kind of blown away by it. So I asked it, I gave it all my blog posts and I asked two questions. Question number one, why do managers suck? I won't read the whole thing, but it's, um, you know, it, it, it's, you know, it's like the answers are written by me, uh, micromanagement and failure to delegate lack of focus on people and engagement. And that's my big one ineffective communication and lack of clarity, failure to adapt and embrace change, toxic behaviors and lack of accountability. I mean, those are, I mean, you know, those are me, but those are pretty good answers too. I was there all tried and true. So I said, what attributes to the best managers have, I haven't looked at the answer yet, so we just come up here. Okay. They prioritize employee engagement and wellbeing. They trust their teams and empower them to make decisions. They are strong communicators and promote transparency. And they cultivate a culture of learning and continuous improvement. Again, great answers again, all, all based on me. So of course I'm going to like them, but, uh, it's, there's a whole bunch of texts below that, that goes in there, but oh my God, the, what it can do. Already is kind of scaring me. This, uh, the idea that I could create auto create a whole series of blog posts. Like I have, you know, I've, I've written 80 articles on anger weasel.substack.com. Now, uh, I could have 80 blog posts. It would be funny if I did that, those AI generated ones and they got more listeners than AB testing did. I, I don't, not only do I not think it's funny, I think it's reasonable. And I'm like, you know, that would save an hour of my time every two weeks. Right. We get a little deeper and they're a little too, you know, chat GPT is a little too positive and always knows the right answer. I think the, the, this does the same thing in a way. It's a little too, I need it to not be me. I need it to be influenced by me. Well, yeah. But like the, when you played, when you played the two characters, right? I'm still thinking about, about the, the one like, like, Oh yeah. I know Alan. He, he literally wrote the, like, like the way they phrase it. So much about me, but the thing that they phrased it was sort of like, um, it came across like, um, a vague recollection, right? Wasn't he the guy that literally wrote the book on testing? Like, I'm like, okay. Yeah. Someone who doesn't know Alan might do that. And immediately thought of like, yeah. Whitaker would probably argue about that. So would Kainer, but you know, uh, but yeah, it took me, this is sort of the state of affairs, right? It took me a little while, like literally maybe 10 seconds before you turned it off. I'm like, Oh, wait a minute. This is, this is AI generated audio. It's gotta be right. And, um, it's why it's wild though. Right. Yeah. And, and honestly, like there was something and I still can't put my finger on it. That was something that was odd that led me to believe it was AI generated. Right. Yeah. But we're really approaching uncanny Valley. Yeah. Yeah. I don't know what uncanny Valley is, but I'm going to perfectly agree. Uncanny Valley. It's from, I think it's from Valley of the dolls like in the 60s of book. It's where it's like, maybe I think it's something different now. Um, but it's, we're creeped out. Humans are creeped out. Like we can see androids that look very androidy, like in the movies and that's, that's fine. They could look exactly human like they do in some movies and that's fine. But when they're right on that line, it's, it's creepy. And they call that uncanny Valley where it's, it's kind of here where it's like, this is, oh wait, this isn't real or is it real or is it not real? Right. Right. Um, yeah, like it's happening. It's happening. Yeah, we're getting there. I've, I've been, uh, uh, interviewing folks on, uh, for AI, for some AI related positions at, at, at current job, not in my org, but it's interesting. The folks I talked to, they get it. They get like, is a weird, we're going to go onto some of your topics, but what I've noticed is, uh, and the reason I brought this up again, lack of sleep, uh, AI isn't magic. AI today isn't even really AI and people want it to be magic and they want it to be real AI and I think there are a set of realists who kind of get it. Who know what it can do, what problems it can solve, what problems it can't solve. So I've been talking to a few of those folks, uh, as well. Um, anyway, uh, AI can solve everything. It can't solve hardly anything. So, uh, here's what I would like to do. I would like, I think you had a couple topics you were going to bring up, which I think is better for you, for me to comment and you to give us the topic ideas than for me to think of topics because I'm a little, a little not at a hundred percent. Oh, okay. So the topic that this. Let's we, we probably, we've probably talked about this a bunch of times, but let me, let me just phrase it this way. I've come to learn of, of certain organizations that when they went too fast and got rid of tests, right? Piggybacking off of your idea, uh, as per the, you know, the, the, the snippet from the other podcasts you just played, right? That, that test doesn't need or test, uh, test specialists aren't necessarily needed. Right. The, one of the things that was observed from this organization is they did that. They did not follow the advice, um, that maybe you and I have had around, Hey, be cautious here. Make, make the change carefully, uh, because you don't want to lose the knowledge. Right. Um, the advice I have most organizations have ignored anyway. Uh, the advice I have is if you want to do this, right. Right. Obviously to me, it's make your assets, your, your, your key go-to people. Cause I find teaching them how to design and code properly is for some odd reason, way harder than teaching people how that don't want to test how to test. Right. And so I often will, you know, I still stand by I've experienced it. If you're in this phase, right. My view focus on your assets. Cause I think they're your way forward. Well, I, I, hold on. Cause I was going to just go on from that, but you don't have, you don't have Estet's in your org. No, no, no, I'm, I'm whining back. Oh, you're, you're back on how we made the transition. How we made the transition. Yeah. But now, but now there's a situation, um, where this organization also made that transition a long time ago, managed to, to muddle through it. But now we have quote unquote, I'll call them generations of dev leadership, all who, which never really understood test, never really understood agile, never really understood TDD. And now you have sort of this, this culture of dysfunction, um, that we're trying to unwind and the, the people that came and talked to me were like, yeah, we are going to swing the pendulum all the way back to 1998. And they asked me this one question. Yeah. And I, and they did it this way. And I'm going to ask you, Alan. When was the last time you even saw a test plan last week? Did you really? Yeah. We make, I make, I make the devs write them. Oh, all right. Yeah. That wasn't my response. All right. My response was, uh, 2007. And I hope to keep that status. Well, it depends if you're talking about the, like some of those old big tome test plans versus let's please have a strategy of how we're going to test this thing that we're building. That was the context. They want us. What do I consider? They want to know 50 page piece of shit that no one reads. Well, uh, Microsoft has always been very good about swinging the pendulum hard. Right. Um, so I know, um, it could happen. It, I mean, but actually, but maybe less so now, maybe more other places. I think people freak out. Actually, I shouldn't even put Microsoft in there. The industry has been infatuated with swinging the pendulum hard as a reaction. And this is a common, be careful of that band date. It may be a turnip good thing. When people, when teams made the transition to not having dedicated testers, uh, you know, I used to talk about this a lot. Like you can, you, if you don't have a plan, you're going to fail. And the plan includes how do you get as Elizabeth Henderson used to say, how do you get the development team test infected? How do you get them? And now you get them to take testing seriously. And the way I did it, my first couple of teams, as we eliminated the role was a lot of coaching and mentoring and teaching devs how to test, giving them permission to test, making them feel like they knew what testing, what testing was, and they could do it without some dedicated, uh, person to help them. And as Brent's looking up by, he's actually probably AI generating his response while I talk here. So if you never had someone to teach developers more about testing that you're not going to get free for, they have to have at least some ideas. And it's funny. It reminds me, uh, many, many, many years ago, someone asked me about this exact situation, like what happened? That's like, it's the quality culture maturity guide. Let's say you get to this level where of team owned quality and the developers get testing and there's, and there's good testing in place. And, and, uh, the culture is correct for that to happen. But how do you maintain that? Say the coach leaves, uh, how do you need the developers to now become the coaches for the other developers? And when that begins to fail, you end up with a bunch of developers who have never been given any idea how to test and they don't, and again, it's not, oh shit, here's more controversy. It's not like testing's hard, but it's, it's, it's, it is. And I put it this way, coding's not hard either, but it's the ability to go low level heads down, figure how to implement this thing and then step out the 10,000 feet and ask yourself, does it work in which ways will this not work? Let me, let me write some more code to evaluate my hypothesis on whether this works and which ways it won't work. And it's just getting used to that workflow, which is different. It's different from when you're building a application in college for your professor versus you're building an application at a company for 10,000, 100,000 million users. Um, before you go on, I want to, I want to, um, see if I can with my best voice impressions, act out a little bit of a skit on this very topic. It needs to feel like they own quality. Absolutely. It can't be a one person job. It's got to be baked into the DNA of how the entire organization operates from the product managers to the developers, to the folks in operations. Everyone has a role to play. Okay. So we're painting this picture of a more collaborative data driven way of building software and getting away from this codependent relationship. What are some practical things teams can do to make that shift happen? Well, for starters, it's about embracing that coaching mentality. Yeah. We can't just expect developers to magically transform into testing experts overnight. Right. It's like any skill you don't become a master chef just by reading a cookbook. You need somebody to show you the ropes, teach you the techniques, help you troubleshoot when things go wrong. Couldn't have said it better myself. Yeah. So I'm just like, you know what? We should just do the podcast and then transcript it and then feed it to these two, please suck. You know what? I bet you even by today, there are podcasts posted with these two talking about something. Oh, I'm sure somebody's way ahead of my idea. So I'm going to wait till there's more voices out there. But anyway, it, I, I listened to the whole, it's a nine minute podcast they made based on my testing articles, but I knew there are some stuff about this somewhere and I literally dropped the needle and that's what we got. So let me, let me ask you this. So the answer is not. If you're the answer is not to swing the pendulum, go back to night 1992 and, uh, create a one to one test to dev ratio. Not the right answer. No. What advice would you give a team who comes? I mean, they come to you, they came to you with a solution, not a problem. They came to you with a solution to say here, we don't understand our problem, but the solution is we should have a full test team. What do you say to them? I, um, well, so a full test team, I would go like that's, um, dumb, dumb, asinine, idiotic, I would do all sorts of adjectives along those lines. Right. The, um, I'll tell you what I did tell these folks. Um, so when people come in and ask me these type of questions, like my answer is often in the strategic realm because I'm like, okay, what are the two key things that are two or three, the small number of simple key things as a strategy that they can execute that would generate the self-sustaining momentum. Right. For me, it's, it's right. The, the strategy around, oh, let's teach. Right. Even if it, even if that happened at the beginning, you're exactly right. If it doesn't continue, then you get new people coming in because this is still not a topic that's really trained in college. Right. So you, and if you let that happen a couple generations, even if it is trained in college, right, they'll come in and their manager was like, what are you doing? Why do you, why are you spending all your time running this test code? Just ship it. Right. I don't know if that's what they do, but yeah. Right. For me, I'm like, look, for me, it was when they came to me, I said, look, there are two practices that, that I would encourage you guys to implement immediately. Everything else will come as a result of these two practices. This is from my experience. Number one, immediately implement and require TDD. Number two, instead of creating practices that, um, go back to the prevention model, go the, um, essentially go the opposite direction. Instead of like creating practices where you deploy once every 10 months, go the other way, deploy 10 times a day, because doing that successfully requires the correct amount of testing, the correct amount of muscle memory. Uh, and it then becomes a lot easier, small batch. Yeah. And I can build on two points in there. First is, and I kind of hinted at it in my setup for the question. I like to remind people, it says it doesn't, it doesn't surprise me. They came with a solution without understanding the problem, ask them what problems they're solving because they may not even know. And, and that, and there's a reason I bring that up, not because it's, I wanted to poop on the rest of your answer. It's because it's a, it's a stepping stone of critical thinking and testing in large part is just critical thinking, looking at things from different ways, looking at your things, critical thinking, or looking at your application from different directions to understand in what scenarios and situations will it work and not work, developing hypotheses around that. And then implementing more code to help you understand if necessary or telemetry to help you understand those hypotheses. So understand the problem, dive in there. You can't just say, I know you, I know you didn't mean it. This way, we can just say implement TDD because if I just tell a dev dev team or a developer to go do TDD, they're probably going to do it in a really dumb way just to satisfy the requirement of must do TDD, uh, what, and again, TDD, it, it really isn't about testing. It's about writing testable code and about, and, but it does handle the critical thinking part of makes you think about how the code should work before you write it. So you begin to think about code differently than just implementing the thing over and over and over. Now, I think there's still more to it because as you and I have talked, dev teams are 100% reliable for functional correctness, but I don't think the limitation that that's the minimum bar, the minimum bar for a dev team is to ensure functional correctness. They can do more. Yes. They can, they can do their own security testing. They can do their own performance testing. They can, if you want to hire me as a very highly paid consultant to help help this team out, um, I'll get it. I'll see if I can get a moonlighting exception and add to my retirement portfolio, but they may need a coach for a little while. Uh, but what I found, and I found this as you know, repeated for everybody here for many, many hundreds of developers, uh, in affecting them with the ability to test really well is far more common than you might think, like in a team of example of a hundred developers, I'm going to, going to, uh, mentally merge some things here, but in the team of a hundred developers, if I am giving them coaching and I, and coaching is such an overblown word now too, if I'm helping them, my goal is to help them become better testers. About 10 of those developers are going to be better testers than 95% of the testers I know they just pick it up. They get it. They have the right, um, mental capacity, whether they've, you know, just the way they think about things, they get it and they go, oh my God, and they're, they're great. Um, another 50 or 50 of those are really, really good at it. Not, not, you know, phenomenal at that higher level, but they're, they're good testers. The kind of testers that if you're a tester, you would always get, you could always find a job. The remaining 40 are going to, there's going to be 10 or 15 at the bottom who just don't get it. And those are your low performers who probably get fired anyway, because you can find, you can find developers who not only know how to test, but want to test their own code. Well, just fire the shit heads and hire those folks to be done with them. And then I left about 10 or 15 in the middle who are probably kind of struggling a little bit, but they're coachable and fixable over time. And in general, I'm, I'm, I'm amalgating amp. Is that the word I'm combining in my head, a bunch of different teams I work with, but they just need a little hand holding. And, you know, in the coaching, again, overused word, the coaching you give them need to be, you need someone who can be adaptive. It can't be a very rote process. It's always, you need that bigger, critical thinking of what's working, what's not working. Let's try this. Let's try that. Let's let's disappoint them. Lovely. Can absorb and help them understand that they are actually going back to feedback loops, I'll shut up for a second here. They are actually faster. Yes. If they do more of their own testing and. Well, and again, this is why, and I agree with your comments on TDD, but not only, like I said, not only should they, they, they do their own testing, like accelerate, blah, blah, blah. See, see, you know, 70% of the last 100 podcasts we've done. Right. Um, the, but if they don't know how I absolutely believe, uh, the two key things are TDD and small batch. Right. Yeah. And, and, and, and I agree with you that TDD isn't necessarily around covering, but it does, it does kind of kill two birds with one stone. It makes a better design, which, which not only makes it more testable, but, um, uh, the better design makes it a lot easier to add features to it later. Yeah. Um, the, but I think the, the second part of that, I'm going to interrupt because the second part of that is more important and Brian Finster, I had never really articulate like he had. And I use this all the time now. Do CD or figure out what will be involved to do continuous deployment, because even that, even the thought experiment will help highlight everything blocking you from doing CD. So if you go to a team that requests this and they say, uh, we want to go hire a bunch of testers to solve a problem we don't understand. And their problem is once you dig in that, well, we're, we have a bunch of bugs that are getting to customers. And so we need, we need more time to bake them in or whatever it is. Say what's, what's missing. If you need to do continuous deployment, we're going to need to have, you know, more tests, great. Why can't your developers read those tests? As just anyway, do CD, even if you'd never ever do it, go through the process. I still, uh, yeah. Okay. Number one, you're right. Go through the process because that will probably inspire things that you realize that you can do. You can't just say, no, we can't do CD. Right. The thing is you will never do CD if you do not ever start thinking about how to do CD, right. In order to finish it, you have to start. So don't use, don't use your perception of it to avoid starting. Like it's sort of the same thing. I, I encountered with agile people, people like we talked about it many times before and many years ago, like my hatred of the manifesto, because people use it as a shortcut. They don't spend time thinking about it. Intellectual lazy kicks, kicks in. And one side says, Oh, agile will never work here. And the other side abuses it to dysfunction. Fragile as, as you call it. Right. Um, to me, it's, it's encouraged people to. Every six months, figure out a way to reduce by an order of magnitude, your deployment time, right. It, it, and, um, a lot of the strategies around preventing error to the customer. Um, that we used to do, well, certainly a lot of the strategies we used to do in the old package, pre-packaged software world, um, just start dumb today. Right. Yeah, you should prevent. Okay. But don't over prevent, right? How, well, how do we figure that out? Well, you know, flight control is a great system. That's an exposure controls, a great system for this. It's better to improve your observability to understand the errors in flight than it is to, um, when you're operating the service, then to put all of that energy into your prevention test code that you can't reuse once operating. Like the other thing, just to briefly rant around testing, um, the, that the same organization talked to me and they're like, but Brent, we really want to prevent regressions. And I'm like, what do you mean? Well, we had a bug that was in the product for seven years and it just hit us, uh, and cause an outage. And I'm like, okay, but it had no impact for seven years. And so now you want to, to spawn up a needle in the haystack exercise where you don't even know if there's a needle in the haystack and your rate at which you can go through the haystacks is significantly slower in which, in terms of the rate at which new haystacks are for me, I'm like, you're asking. The old ways will not scale to solve the problem you have today. What you need to do going to your point around what's the goal. What you need to do is a minimize the amount of time the customer is in, is in pain, not move all of that energy to, we must prevent pain. Okay. Brent, it is 2024. Yes, I agree. Any, any problem as you're describing your sounds really hard. Easy answer. Always AI. LLM. Tell, tell, tell them to use an LLM to solve it. That's just what you can do because as we know, AI is magic. What you do is you feed, you make an LLM out of your code and then you ask a prompt where the needle is and it just tells you, cause it's AI and it's magic. Right. Actually. And we can just do multi-agent AI and just have the AI talk to each other. Yeah. Have one that generates the code. In fact, why don't we get rid of the customers and just deliver our software to AI? And it'll tell us if there are bugs because it's AI and it's magic. Right. And that's an excellent point. And like, tell me how we would even get, I mean, how would we get a faster feedback loop than that? We can't. And as we know, fast feedback loops and, and, but obviously we are being facetious. But I do know that you had another story we talked about briefly in the, in the, in the intro, I'm going to put you on the spot for a bit. Um, and it goes back to people thinking AI is magic. And there's, I forget the details now, but there is something around a team wanting to use AI or LLMs to solve one of the longest known problems in testing. Yes. And, and I will set it up a little bit. Cause I didn't remember as I was talking, I went to GTAC, the Google test automation conference in 2008. And I saw speakers from reputable, high quality software engineering companies across the industry. And I remember I tweeted, which is the thing we did back when, um, used to be a platform called Twitter that was fun for it's like, you know, sharing ideas with the little community there. It was pretty popular for a while. I think it's been dead for a while, but on Twitter, I tweeted about the fact that they should rename the conference, the flaky test conference. Cause of these illuminaries from around Facebooks and Netflix's and Google's every single talk had at least one section, not a large section talking about flaky tests. Yes. The, the primary bane, uh, in existence, like just to go back to like, but one of my experiences around, and I think you've done speeches on this, so you can probably rattle off all the causes. I've done a great deal of time trying to repress, um, my time as a tester. Um, but one of the causes that kind of goes into the last topic around overuse of the prevention techniques is, um, One of the main reasons why software doesn't work when it goes to production is because production is dirty. And when we test in a local thing, we try to, we kind of do it in a clean room exercise, some teams will do things like, you know, they, they have the ability to dirty up their current, their current environment, but it eventually gets out of sync and, um, that's why they should just test in production. Agreed. Um, the, but that's also one of the causes of flakiness. Right. It's, oh, well, it's because you haven't really set up your test. Uh, in my experience, you haven't really set up, set up the initial sets of, uh, the test before you engage the Oracle part of it to, to guarantee that there isn't variance in the Oracle bit of it. It's like, no, right. Sometimes system goes slower and, and, and if you're waiting for this event to trigger within five seconds, like sometimes it doesn't do that. Right. Um, anyway, I'm going to, so I guess the question is, okay, to what degree could we apply LLM today? That's the question I have is can we, it's flaky tests, age old problem. As you know, as I just stated five minutes ago, anytime we have a hard problem, it's unsolvable. All we do is all we do is throw AI at it and it's solved. It's magic. Right. Okay. Is that it? That's it. So, so is that true? Tell me why that is or isn't true. Oh, um, I'm not entirely certain at this point in time, because I do think, I do think, to me, I do think we could apply LLM to solve that problem. Um, but at the end of it, what I'm trying to load up, uh, into it using my intuition is, okay, but is the ROI of that approach going to be lesser? Is it going to be worth it? Right. Because I could go, all right. Well, we could chain together a bulge of multi-agent LLMs, one covering each other, one, you know, we run code. And then we go, Oh, how do we make this, um, less flaky, right? And it's going to be around setting up the environment. A lot of the times from my experience. Um, but then, right. What happens at the LLM, um, goes after, goes after the same hack we used to do in tests and, you know, Oh, just change the sleep from five to 15. Sure. Sure. More sleeps. That's the solution and more retries. Right. More retries, more sleeps. Uh, if, if we want to implement that, then absolutely, then I'm going to change my answer as far less complex, absolutely. L L and could solve all of those problems. It falls into that, you know, it's just, it's not magic. That's, that's really the answer. It's not magic. You'd have to look at, I'm not even sure what data you would feed into the LLM to help solve the flaky test problem flaky tests that, I mean, there's multiple, multiple criteria to define flaky at the very least level, the very smallest level, uh, it's tests that either fail or pass when they shouldn't based on the condition of the system. The, the approach I would take initially. What essentially, um, the way I'm thinking of it, it's probably simpler ways of doing it, but run, uh, take the code, run it through, um, LLM with a series of prompts that are related to testing the code, similar to like a code review. Hey, Hey, LLM evaluate, uh, to what degree this, this test case is going to be resilient, um, or flaky, right. And then propose fix. Again, it's, it's too magic key because when testers write or developers, you know, back in this century, write test code, they run that test and it gives them the proper result, the flaky part comes from, uh, almost a drift into failure of Sydney Decker type situation. It, a test that should pass or a test that should fail passes because of multiple different system factors happening at the same time, this test always passes in less run while memory is below 60% and time is changing over from 11 59 am to 12 p.m. Okay. Uh, your L how is your LLM going to find that bit of it? I guess you could note again, I wouldn't use an LLM for this is for traditionally maybe cause maybe cause I don't know AI or LLMs well enough, but this is traditionally where we'd write a model to try it like a scoring to look at. Okay. This test has timing dependencies. This test has memory dependencies. They make it more prone to flakiness and I could, I would want to reduce my flakiness score. Um, but I think every test, like I have a test that does, you know, verify that two divided by two equals one, the flakiness of that test, the flakiness factor is zero. If I have a test that's very complicated, that relies on a bunch of systems things, I would say the flakiness factor is a much higher number. If it's zero to one, it's probably somewhere in the point, you know, seven, 5.8 area, uh, and that, but that's just an algorithm. That's not AI. I still don't understand how the LLM is going to help you understand and fix flaky tests other than a wish for magic. Yeah. Uh, I don't, I, I'm, I'm, for the listeners, I am sharing screen. I've gotten the Google scholar look, just did a search for LLM solving flaky tests. And there are a bunch of articles on this topic, at least the top four seem to be on that. So, um, it's, it isn't magic. It's probably going to be work. And now I am fascinated by this and I'll be typing into it. I'm not going to try to read it and regurgitate it here. Um, but again, they talked about something called a neuro symbolic technique. Great, great. Here's, I'm going to go back 10 years to what I know and what I know is not. I mean, I know probably more about AI than the average software engineer, but I know enough to know, I know very, very little, but as you know, I, I, I fake things quite well in the old days. Yeah. As you remember at the time, I worked a lot with not cheating. I got the pond over an MSR doing a bunch of, he did, he did all the heavy work. I just helped him review and think of ideas. Um, but, uh, in that old MSR software development model, they would take, they would analyze every test that's ever been labeled as flaky and come up with factors for what made them flaky, why they were, you know, what, what factors were in there and they would build a model that would generate a flakiness score. Uh, to me that, and again, a flaky test isn't a bad test. Um, it, but it requires more human time to understand if the results are something you need to care about or not. Uh, you could, I mean, you can, once you know, a test is flaky through an algorithm, you could refactor it and make it into two less flaky tests or, or three, even less flaky tests as possible. You could put constraints around some of the external inputs to reduce that flakiness. But I, given how long I've been in software, given what I know about how LLMs are not magic, I don't think the flaky test today, at least with the technology we have today, the flaky test problem is not a solvable problem. Not, not, not a hundred percent solvable problem. Meaning do all the work you want with LLMs and burning up the environment. You will still have flaky tests fewer. You'll still have them. That is the closest thing I have heard ever from you shutting, shutting the door on a topic. That's fast. That by itself is fascinating to me. Um, yeah, but the, the thing is, um, you're right, the tech may not be there today. Right. Um, the, but like one of the things, one of the things I, um, just noticed, like one of the papers that I found on Google scholar, they, it's a small sample size, but they tried, um, they tried using their algorithm to fix 79 tests, uh, on, on public get hubs and 19 of them were accepted. Right. So yeah, that's, that's your point. It's not a hundred percent, but, um, 25% pears down the problem, uh, in a way that is helpful. Here's another study I want, uh, again, I, I'm just going to lean on what I know. Uh, I wonder if, and going back to TDD and how TDD makes more testable code, uh, like we talked about earlier, I would also like to look at a code base and understand from a code base, how light, like there's a likelihood. I don't know how to measure it yet, but there is a likelihood based on the characteristics of a code base or a code, a code module of how likely it is that flaky tests exists for that module. For example, if you have a, an API that takes 13 input parameters and they interact with each other, uh, I would, there's a good chance that you will have flaky tests for that module. Why? I mean, it's an interesting hypothesis and, and I'm probably well positioned. No, I don't have yet automated access to a sort of get hub app, but for me, it's, it's more around timing, right? Clicky test comes from there's some element of non-deterministic behavior. Uh, in the system you're evaluating or actually in the test code itself, it's that non-determinism that causes flakiness. Maybe. Yeah. I think there's something there. Anyway, I think this is truly one of those hard problems that, well, actually, I'm going to change my answer in the middle of it before you even heard what my answer was, I think this is a hard problem in test. I think it will remain a hard problem, but because I think you and I know this flaky tests can often, in fact, I won't even say from time to time can often actually provide value, even though they're flaky, I don't know if this is a problem I talked about not being solvable today. Is it a problem worth solving? Probably to some degree, right? We talked about testing in production. Wait, we want to test in dirty systems, right? Dirty systems is one of the sources of non-deterministic behavior, right? Um, like the alternative is not test and let outages occur and react that way. I'm not certain that that's right, but I definitely don't think we're going to resolve flaky tests that occur in the prevention side of it. I mean, that one should be resolvable, I think. Like, but part of me is just like, yeah, quit trying to create pre-production environments where you try to test how well this stuff works in the real world when it's not even related to the real world. Yeah, well, I think it's worth it. I know we need to close here in a second, but I worry, especially based on the first hypothesis you brought, um, and it could be, I'm worrying because my brain is sleep deprived, but I'm going to come up with a generality, which I, which I think is true here for me at least. Uh, and I want to fix this problem. Uh, why do we continuously try and solve the wrong problem or have a solution for the wrong problem? Uh, and again, not this isn't a big problem. We should try and solve. I think, uh, there's a lot more systems thinking needed in software for it to advance. How do all the pieces fit together? Is our flaky tests a problem? Yes. Are they my biggest problem for satisfying my customers and improving retention and stuff? Mmm, maybe, maybe not. Uh, here I think about Jimbo and his, his last work. I remember, right. It's he created a tool called the riskitizer. Yeah. Which, and it's just like, like my answer to your question is, well, it depends on what it's testing and like how much that represents, uh, risk to our definition of customer quality, right? The, the ability for the customer to solve their problem, right? The, if it's a small risk, then flakiness doesn't really matter. Yeah. And, and if, if you, maybe it is, maybe, yeah, we want our feedback loops from our CI to be, uh, they gotta be much more accurate because we lost a million dollars yesterday because one of our flaky tests passed, it should have failed. It'll be made a bad deployment. Right. Uh, then yeah, you gotta go fix that and you don't, and again, uh, theory of constraints, you don't eliminate the problem completely. You mitigate the problem until it's no longer your biggest sticking point, which you could do then, which you would solve enough of it. You would make that test less flaky. You would temporarily remove that test till you could refactor it. Whatever. Um, write a smaller test that gave you, you know, 90% of the act three that you could trust. Whatever the solution is, you're trying to mitigate that problem. So you can solve your overall problem. But I think people just look at, Oh, this is, this is broken. Let me go fix this. Software developers aren't just infatuated with solving a problem. Well, no, my friends that differently software developers in general are infatuated with developing solutions for perceived problems, not wallowing in the problem they want to solve and not weighing and debating to see if that's the biggest problem in their system that needs to be solved. That's where we, and that's where LLMs are not going to help us. It requires humans. Maybe an LLM can do this. I could I feed an LLM here are all my known problems of my software development. The problem is you can't know them all, but you have to find the one that matters the most and work on that one first. And then just God, people just like dive right to the, I'm going to implement an algorithm for that or put AI on it. It's magic. There is, there is, and I wouldn't be surprised. I'll, I'll save my, my check on Google scholar until after the podcast. Good. But there is a, um, to me, it does feel like there is. Some room for, you know, the next doctorate, um, PhD. I have a topic for them. All right. It's in, it's in there somewhere in systems thinking. It's, it's okay. System thinking is super important, but when the system grows, um, orders of magnitude beyond the cognitive limitations of the human, right? How do we, how do we as humans go about solving that? Right? The, the, like there's simple things all the time that I am dealing with and I can put out fires too. Like people come to me all the times. They break. Can you give me advice on this KPI? And then I'm like, yeah, I can. Right. Um, the problem with that KPI though, is that there's this other KPI that's battling against it. And if you guys don't work together to try to solve both of your KPI's and one's going to continuously cannibalize the other, and you're just going to go in circles because you're not evaluating the system. So this is, and we got it. We got to end cause I got to go to another meeting, but this is where like the software developers, they got to help them not teaching them testing. I think that's relatively easy testers. Don't get mad at me. There's there's yes, there's nuance. It's systems thinking, critical thinking. It's those skills, those, those, those thinking skills that I see missing the most from software developers. Teach them that. Uh, even more so teach them to the degree that they create, in my view, create automated systems that help you analyze what you were about to do. Now, what that means. I don't, I don't know. Look before you leap a little bit, stop this ready fire aim stuff already. Already fire. No. So, so like here is a, here is a problem with a, with a large enough, uh, system. Right. You have one dev team way over here has no idea that after, after goes through the entire complex domino chain, that his check-in could, could break this product. He's never even heard of, right? It's it's how, how, how is it even possible for us to proactively determine that? And if it's not possible, which I would argue that not possible in linear time anyway, um, then what we need to do is figure out how do we detect and react to it? Right. To me, I think it's pointless to focus so much on prevention. It's no, you need to detect and react to it optimized for reaction. That's my view. Sounds good. All right. We got to, you got to close this thing. That's close. This has been AI Alan. And just Brent. 
