Welcome to AV testing podcast your modern testing podcast your hosts Alan and Brent will be here to guide you through Topics on testing leadership agile and anything else that comes to mind now on with the show. Hey everyone I'm Alan and Brent's not here. I decided as threatened Previously on a podcast that if Brent wasn't here on time I would just start without him and you could hear just how bad a solo podcast can be Got a couple good topics lined up today. None of them have anything to do with sports nor politics nor the weather I don't think all I want to say is Covid is not a political thing despite the fact that Far too many people see it as such, you know, my preference is that everyone gets vaccinated I know there are some people that don't want to for various reasons What I do ask is that for everyone look past the misinformation that's out there find out what's right for you but please be careful and realize that this is a real thing and That it does get people sick and just do your best to be careful Put some effort into being careful to make sure you're not being part of the spread of this and then maybe someday that light At the end of the tunnel will begin to come back into view again All right. Well, let's get on Brent's gonna get on here He's gonna go blah blah blah and talk about stuff I'm gonna make fun of him a little bit and then we get going with the podcast But I do have somewhere an agenda for today And I may as well start without him and we'll catch him up when he gets here the first thing is I Think it was the last podcast within the last few we talked about the wasn't the last one for sure. Oh, hey Brent's here Brand I've already pressed record. I started the podcast without you. Welcome to the welcome to the A B testing podcast Brent Thank you, Alan. I thought maybe we could do just one more without you. This is oh Anyway, let's pause. I can go right back into where I was because Brent knows my agenda. How's it going? I will survive I will survive Things are a foot That will have a negative impact on my life That I am not quite ready to share publicly on the podcast Understood understood as much as we do leave a lot of things out there Brett and I both leave a few Things to our private lives so I get that we can chat about that a little at the end if we have time So yeah, that sucks. Um Already talked about kovat. You missed that. Oh Did you yeah any I'm not gonna add anything else. Do you have any short? Statements you want to make about the current surge in Delta variants, etc No The Delta the Delta variant surge concerns me far less than what's happening with Afghanistan. I Think it wise for us to just skip over the political I will just say this I'll just say I won't mention anything specifically and we'll move on between everything going on sometimes even Optimistic Alan has to wonder if the end of the world is coming. That's all I have to say about that Yeah, so, um, yeah, let's talk about stuff about the podcast. We have a podcast and What's interesting about this podcast is called modern testing Brent and I are two old people So we're not that modern and we don't really talk about testing very much, but we do and we will today We talk about a lot about Agile and agile flavors and we talk a lot about How to improve quality? In fact, I would say I can safely say that Brent and I care Deeply about quality but not so much about testing No, no testing. I don't understand why we would talk so much about testing since it is clearly Dev's job Alright, you got to stop chewing and bang on your desk just saying that but you're right Oh, and I I want to read now I threw with Chatham House rules I'm not this so this is so another slack I'm on says And I'm gonna breaker. I'm gonna try not to break a rule by paraphrasing. See if I can find the channel here someone was asking about QA teams, I'm not gonna mention the slack or the people and I'm gonna paraphrase I think I'm I think I'm gonna be okay here then cuz I'm not outing anybody But someone was asking about how QA works at different orgs. This is a Cross company industry slack. That's as far as I'll go and what's interesting is all of the replies in this thread Saying we don't have QA, but we do lots of testing these people these and the thing is with modern testing We're describing what's already happening in the world. These people haven't been indoctrinated by modern testing They're doing this because it's the right thing to do Response to that is same here. No dedicated QA Teams own entire services also on the testing of that service someone else says for what it's worth for us It's similar to what they said The function of QA is owned by engineering There is no dedicated QA team three people three different companies Somebody else says QA is a skill set and present on every team or I work so it says QA What's QA and then in parentheses says engineers write automated tests and do manual aggression testing and it goes on and on and on It's a long thread and there's good discussion on it, too I don't want to go into that but this is something This is a thread like you expect to see on one of the three dot slack comm plug You can go to modern testing org enjoying that but to see this pop up The thing we're talking about with modern testing It's happening frickin everywhere and a lot of people are just blind to it and it bugs me So here is my question Two and I think I've asked this before to our listeners and by the way if you're listening to this podcast Thank you I don't really know who would but you're cool and I like you and I appreciate you listening because it just blows me away that About a thousand of you a month listen every month and go. Hey, I'm gonna do this again in two more weeks I I Feel like there's a chunk and maybe it's the title of the podcast which are not gonna change But I feel like there's a chunk of people who would benefit from this podcast a big chunk that we're not reaching And I don't know how to reach them Actually, I do have some ideas, but I'm gonna pause there. How do we reach? First you agree with me. Is there a chunk of people that were not reaching that would find this beneficial and then part two If so, what are some ways you can think of that we should reach those folks? So there absolutely is a chunk right all you have to all you have to do to prove that And you in particular can do this. I'm not certain I can but you in particular good. All you have to do is go to Twitter and Just just copy paste one of the principles in particular like principle five like even today Maybe that could be a heuristic when Alan Quarterly will will copy paste principle five into his Twitter feed and then we just count how many hate responses Alan gets and and see if that starts to temper down, you know, I should do it I should just post a principle a week and I should probably do some more Conferences but not test conferences and talk about the podcast there. I never really fully made that transition I was kind of in the process of it and then Then Covid so that's something else like I should really I'm gonna actually put that into my to-do list here Speaking of actual dev or DevOps conference and I may have to start over like I got so lazy with test conferences I get at least a request a week to speak at some test conference I say no to almost all of them, but it's easy. Oh, that one's cool Or I like that, you know ministry of testing looking at you. I like you. I'll always when you ask I'll always do it Right. Honestly, I'm I'm a little bit in the echo chamber there and I probably just need especially now things are remote I don't have to travel I Should probably just actually go through the old-school process of applying to speak at some dev conferences and just talk about Quality cultures whatever I probably a lot of things I could talk about but we yeah Even some modern testing a B testing stuff. I'm actually wondering like based on what you just said I'm wondering if that's even needed Right because if you talk about Devco things like the stats that you just shared I mean, it's not really stats. It's it's sort of a small sample set essentially The universe is moving on in the direction that we've been talking about for who knows how many friggin years now It might already be past the tipping right it we've had conversations We have we've had conversations around How big is the bubble we're in? Right in and now I'm actually wondering. Okay are are are we actually in the bubble or We just think we're in the bubble because we have We're bumping up against the bubble But are we the ones that are in it or is it is it the traditionalists that are now in the bubble and that bubble? shrinking because as Like I'm seeing over and over again that the world is in fact not functioning The way it did 20 years ago. No, no, no, no, no, no, no, no, no, no It's actually segues really well into the first topic how long we've been talking about this Well, we didn't know what modern testing was We don't were sure exactly what we were gonna talk about when the podcast formed very early on like in the first Few episodes we began talking about the ways which this works we explored it We talked about it as I was learning sort of how to make that transition myself But then five years ago, which I got a note in Twitter that five years ago Someone asked me about what I thought the future of test was actually asked about ten people what they thought the future of test was and Because Covid has blurred it. I thought blurred time. I Thought five years ago. Jeez, what was I was that Xbox? When was that? And no five years ago was towards the end of my time at Microsoft I've been at Unity now about four and a half years actually a little bit more But they asked me five years ago. What was the question? The question I was asked five years ago was Blah blah blah preface preface. What will software development look like in one three or five years? How will that impact testing approaches? I guess I chose five and I'll skip all the other people that my answer is actually in hindsight is contradictory because I said at first I said Nothing super different I expect that more companies even those not shipping weekly or more often will still move toward more agile ish engineering approaches Smaller aeration smaller batches build measure learn data collection and analysis will also continue to grow in importance and use I think I wrote that from a little bit of a bubble because those things have definitely taken off and Maybe I assumed there was more of those around than I thought from a Microsoft bubble bit outside. Definitely see more of that stuff Also said to me. It means more teams forget about what to call people Integrated or unified engineering teams become more of the norm and dedicated test teams become rarer and rarer I think I nailed that one The good testers will be fine on those teams as I expect is more teams move in this direction They recognize the need for a quality specialist among an engineering team So I feel like we're seeing more and more of that this idea of a test coach Which I had been doing for a while at that time is almost mainstream Mm-hmm. I said, I don't know that any of that changes testing approaches and even five years ago I said software developers are certainly capable of writing good unit functional integration tests and more and they'll be doing a lot more of This and if you recall a few months ago and I wrote an article talking about how they could do this the hate I took About well our developers don't want to do that and I'm better at it. So I should do it anyway, etc Etc, but all this stuff is happening and I think it was it was pretty much I didn't take a risk in the prediction I don't think you I still so I didn't I didn't take a risk at all. All these things I think were pretty well predictable and In hindsight, I'm a little surprised we haven't moved even farther I think sometimes we think five years is a long long time for change big changes to happen across the industry But they just don't go like that. So we could I could be well well into retirement and sipping a senior special my tie at a beach somewhere watching the planet burn and There will still be somebody spinning up a test automation team totally disconnected from the dev team so they can do their automated freakin checking Yeah, actually I'm looking at the website I do think their predictions are spot-on I notice that other people in here like Jerry Weinberg's in here. I miss Jerry I do too I don't want to I'm trying to scan it and it's not For the listeners because I probably forget to put a link in the show notes because I'm freaking lazy My five for Friday today does include a link to this you can check it out there And then merit had a really good follow-up approach merit who was on episode. I'm gonna guess 128 Somewhere last year wrote a blog post about following up on her predictions and and you know, I feel a little I have to say When someone who's English is a second language emeritus a self-professed polyglot She she speaks and write several languages She writes better than I do as a native English speaker and it's just I don't say intimidated I'm not sure what the word is how I feel about that, but it's I feel Definitely impressive and awesome and I feel a little like why can't I be a better writer? I bet she's very prolific too and she practices but she's really good I read all of her blog articles and this one particularly good. So unplanned plug go merit Yeah, I'm looking like it's interesting. I'd like to see her follow up Yeah, it's also also in my five for Friday link so you can check that out and then Bach is in here as well Yep, everybody has lots of I love it. Look got lots of opinions, but worth talking about today I wanted to bring that up what happens in the next five years. What do you think? Is it just more of the same? What's something I'm gonna ask you is on the spot What's something that's gonna happen in the next couple of years? You don't have to time bound it that People may find surprising. What's something that you think? May happen in the next couple of years one to three to five years that maybe a lot of people aren't expecting Yeah, you know what this I'm suffering a A flashback because I feel like we've done this on the podcast. Well, we do our we do our prediction episode Like I I'm actually seriously thinking are we now on the on the path to To start developing the postmodern testing principle. No, no, I think next five years My I definitely see momentum continuing in this direction Because I do think I do think what is the there was this one book that Al Shalloway talked to me about Right, but it's essentially We're now talking about things that are mainstream not we're not on the leading edge with them MTP not at all when we started talking about this we were we were kind of more on the leading edge Cuz we were observing and when I first talked about it at test bash a couple years ago When we first heard the public talk about it, I actually used I used the what's the curve? Yeah The the the pace of innovation curve, which is actually from somebody else's work But anyway, I showed that modern testing was out towards the front But it's not it's not totally mainstream, but it's definitely to the big part of the curve. It's not a bizarre idea There's too many people There's too many people doing things we describe as modern testing without even knowing what the hell modern testing is and that's fine It's absolutely fine. And then what I'm saying is it's far more mainstream today than it was when we started Working on what are the principles? Conversation I just reading from another slack where oh, yeah devs owning this stuff and understanding they can and teams being data driven and Shipping in small chunks to get feedback from customers all that stuff is happening. Yeah so the thing is like in the context of testing or in automated Checking I As you already covered, I don't know what's happening on the front the front lines of that. I'm sure things are But in terms of testing or checking Honestly, I don't care. Oh, yeah That's so far outside. I don't want to get into that stuff. I've kind of decided that with With checking versus testing with if you find value in it, but what the heck go ahead I don't but that's me you be you know, but in terms of in in terms of the question you asked like the prediction I mean, maybe you have a better one. Yeah Well nobody's what I'm basically saying is I don't think I'm currently in a position where I would be able to actually Sort of say put my finger in the air and say what's coming next as it relates to testing Right. Oh, yeah, it's a release of quality Yeah, let's let's let's have that discussion. But as it relates to testing, I don't have that visibility It's it's it's out of my life. Yeah Yeah Where where we came from before it was still very much in both of our lives and we had seen much better Techniques and and and actually even experienced them and rolled them out. We were definitely Coming from a point of experience and we developed the the MTP. Yeah. Yeah So I want to pause you there because everything is segueing so well and I want to jump in at the segue point Okay, you don't know how these things segue I send because There was a time when Brent and I would both contribute to a bolded list We prioritize but Brett now phones in the podcast and I send them a list and tell them what we're talking about in advance But going from there I've been listening to The testing peers podcast and you should check it out It's they get three or four people to talk about I have a nice conversation about different things around testing. I like it There is the one before last they actually brought up principle number five and they didn't poo-poo it They said they said oh, yeah that makes sense. It fit in with their thoughts that ultimately the customer The customer determines what quality is which goes back to something I wrote in an article which is I think is worth Again, I just kind of tweet these little sound bites more often to get people thinking about what we do here with modern testing But the point I brought up in that article and I'm all over the place Gotta loop it all in and meanwhile my dog decides she wants to play just brought her toy so she could play while I talk That's gonna be fun. Somebody in that podcast this last one Said made the statement. We're talking about quality They said well at least nobody brought up that quality is conformance to requirements Actually, I believe that quality is conformance to requirements But only the customer can define those requirements not some PM somewhere, right? And if you put that twist on it all of a sudden, yeah Crosby's right quality is conformist to requirements. So ha Anyway, they that was the one about quality this last one. They did a cool thing I don't want to steal their bit entire like as an ongoing thing, but I'm gonna steal it today I'm gonna tell you what they did talk about one of the conversations they had and then we're gonna do it ourselves You have no idea what any of that means. I'm gonna walk you through it It's kind of like when you're playing one of those complicated games for the first time you have no idea what to do. So Let me talk about test sphere. Have you seen test sphere cards? No, these are from ministry of testing They're super freaking cool and what they are is a bunch of cards and different kinds of topics and I just have the standard set here and Just discussion starters around quality and testing. I can't find the exact card. I may not have it They have like a because they're all cool. They have like a bonus deck full of stuff But what they did is they kind of spun a wheel or they chose random cards to talk about it was pretty cool but one thing they talked about and I know we've talked about this a long time in the past but It's something that I'm you know serendipitously or coincidentally. I'm not even sure what the difference is It's come up with me a couple times recently is they were talking about Something about you creates a regression test for a bug that was found Three years later. You're still running that regression test Because once we especially with automated tests once we write them they run forever These tests never the situation were that regression test you wrote that test for a bug that was in the product It's impossible for it to happen again You can't actually make that bug happen again because has something to do with like a particular version of Chrome That's three years old or whatever and that regression test remains even though it provides no value It can never actually find another bug some of those my words One thing they didn't get to they talked about They had a lot of things to talk about it But one thing that doesn't get talked about enough in the industry is Because I've been I've listened in on two or three of these things via Twitter and LinkedIn in this podcast Is the idea of test selection this idea of specifically choosing certain tests to run in a certain situation and thus choosing other tests not to run and I wonder if Well one with developers writing the bulk or most or all of these tests Do you think they'll fall into that same trap of running every regression test and ever release? Will they be quicker to retire tests or the or will they do what I think you and I have both done in The past and write a smart test selection engine to make sure they're selecting the proper tests for whatever they're changing I think none of the above Oh better. What what happens? I think by the time that that becomes problematic Right the the thing that's happened What were the friends that we just talked to rocket? I think that that company I think by the time that matters Actually, the tooling will improve to automatically do that. It'll become part of the tool Right. So the one benefit that that the industry that the industry has in my view around test That the the testing responsibility primarily be being shouldered by Developer is as as I actually said in that podcast developers are inherently lazy and Of course with with with the the advent of cloud like one of the reasons that that We had we had to do this test selection thing is because like our our code Depending on which team was doing it, right? It had varying degrees of being able to parallelize Everyone had like it back in the day. Everyone had labs and and Those labs were of limited resources So we had to go we ended up being forced to go to test selection because we our test pass had to complete by this date We now had too many tests to run them all Now and with the cloud the story is going to get changed We're not going to hit the issue of having Too few resources to run it because the the cloud offers sort of unlimited capacity Well, I mean you still pay for that. You're still that's what I'm going to get to but I compute You pay for both commute and compute That's where I was getting to like it becomes a new constraint and it's going to be it's the cost So now actually test selection said turns into more of an roi statement. Okay Given the cost of this test What's the predicted value of the of the result of the test? Cost is readily computable today It's the the relative value of that that needs to be operationalized And that to me is just a matter of time my brain was spinning from the beginning when you said the tools were going to get there because I'm tempted to quit my job today. I could I I have done so much with test selection and I am so confident I could write a generic thing that people could use to plug in and select tests based on I mean, I just want to make my startup right now and make a test selection tool for the industry Do you want to stop talking about it so that no i'm good i'm good I'm probably that's not really going to happen because i'm i'm far too lazy to do that. I like the day-to-day challenges of of working at my current job, but honestly if You know that that's a there's a business there and I am happy to be i'm on the advisory board for one Test automation company and so if you want to spin this up Put me on your board and i'll just take some stock for me to help you make your billion dollar test selection business Give me a ping because I I know i've done this multiple times. I know how to do it I think I could make we could make it in a way that could be applied at multiple companies Um, but yeah i'm available But definitely on a board consulting type Uh position more than quitting my day job because I like getting I like getting a twice a month paycheck So yeah, yeah, but but again if you just have to share ideas and you know Free money of any sort rolls in you know net positive Yes, so What's interesting in the modern so that was? Listening then talk about it. It was cool because that came up They never talked about test selection and I don't hear people talk about it enough So they just wanted to plant that seed for all three of the listeners. So, uh, thanks for that But it made me think it might be fun today To for me to randomly pick some of these cards here Because what I found was this is not just like oh, I want to copy them What I found was for every question they asked They were answering it like a I don't want to say a traditional tester But more like you'd expect a tester to answer it and not the way you and I would answer it Oh picking up what i'm laying down Yeah So this is not quite traditional I mean we could do this and we could do the traditional manager versus agile manager But we're not going to do that or modern modern test manager I do want to pick a couple random cards And i'll actually random within a A subset so there's some blue cards which are quality aspects. There's green cards which techniques Probably do one of a couple of these see if we can do it I grabbed a quality aspects card, which is blue and what they do is they have a statement at the top and then some Supporting statements below and we'll just kind of see where this takes us. And if it's done, we'll talk about something else This one is the usability aspect Will users find the product convenient and easy to work with but more important will they enjoy it? Hey, I picked a principle five call card And the state supportive statements below are what makes an app attractive speed ease of use reliability question mark Are the most important features available in the middle of your screen? Have your app. I track tested to find out people easily find what they're looking for Some things are intrusive yet. People love them. For example the group on space cat logo. I haven't seen it This makes your app unique identify memorable So i'm gonna go back to the top and have you answer this from a eric reese modern testing point of view Okay, how do you know if users are finding the product convenient and easy to work with? well, my my my 50 000 level answer is that they are the They are showing that they are solving the problem That they're solving problems with it with a ui app I think the way I would look at it is Is from a kpi standpoint and I would look at uh, what's called the dow mau uh Uh equation is basically am i've seen Am I seeing repeat visits from the same customer? Am I am I am I observing that there is stickiness with? With the customer. Yeah in general So in short data, let's talk about that. This is where we differ like this is where this is I think my answer is going to be data on every one of these Let's talk about how we'd use data here because this middle statement says are the most important features available in the middle of your screen Well, it isn't for the tester to decide but those most important features are Right, you can you can gather that from data are the things people using the most most easily accessible Or how how difficult are some of the things that uh, they're using a lot to find Well, and even then like there is there is a concept that's coming up in the data science space That's that for me been coming up more and more Right. How do you define most? The you that user's most because here's here's one thing i'll call out that a lot of At that when you start doing basic sort of data analysis, it's very attractive To go after and target the average Okay, but here's the deal and there's multiple stories on on this that I could share but There's actually no one that is That fits into the average persona. Uh-huh. You're right Nobody no pilot is the actually the average height for a pilot Actually, yeah. Yeah, uh, there was a recent story that On that same topic where back in world war something Where they designed the cockpits And they designed the competes for the average Uh pilot right they they measured height they measured weight they measured chest size etc. Okay and of course No one actually fit in the plane and in hindsight 2020 They looked at and that was one of the main causes for why there were accidents in the plane Because the pilots themselves like literally no one fit correct when you look at humans Right. There's a certain studies where you'll like the average human Is going to have half of their body parts be male and the other half of their body parts be female And it turns out there's no actual human that fulfills that criteria That's one of the problems with averages and and when you try to I would I would restate the statement like Is what? each individual To what degree are each individual customer? Finding their needs and finding that the ui Is where they expect those things to be right in some places it and sometimes it may not actually be Um, like for example in in the azure portal we will we have actually multiple entry points into certain aspects uh that we know are popular because we We want we don't want to have to try to train the customer on where to find how we hard-coded it We want to shortcut To okay. What are their needs here here? This is how you get to it Yeah, I I think um, the important thing here is for that question is And i've actually looked through a couple of these while you've been talking and they're all going to end up in data So I may not go through many more unless I find a good one but A lot of testers think like we have to act like the customer will use personas and you can't you're not the customers That's what principle five is about you have to use data and I have and I have to tell you I haven't used an office app in Four years, but I assume office still has the ribbon Yeah, okay. So when the ribbon came out it pissed a lot of people off And a lot of people liked it. It was a good often when you innovate Uh, you can tell you're innovating when half the people love it and half people hate it. You get a polarized Response, but the reason they did that and I heard all kinds of theories I was at a conference once and somebody told me They had some wild conspiracy story on why the ribbon was the way the ribbon was and it was I was A little shocked that someone could have that imagination but the reason the ribbon existed and they came out with it was A whole bunch of customers were requesting features that already existed in office apps. They just couldn't find them And the ribbon was about trying to make sure that every feature was Two clicks or three clicks away and taking those most important features of putting them one click away The next level down two clicks away just making it easier And if you have been using office for a long time, you'll remember their attempt before Was to do this thing where they'd hide menu items. You remember this? They would hide the menu items that you hadn't used recently. So you go to look and it was that was more confusing That was a horrible solution for that customer problem But they're able to discover that through data and lots of feedback that that wasn't working They thought okay. We'll limit your view to the stuff you use But there's it was there was a pain in the butt for those things you only use once in a while like mail merge You have to go dig around and remember which menu it was in to find it. It was just ridiculous So all that's data driven, you know, love or hate office. I haven't used it because I haven't needed it Although honestly as a tangent if I had a windows machine, I would have excel Just so I could run power bi Other than that, but I don't so I don't and it's all right anyway Data to understand what Challenges the customers are having how are they using the product? right, I grabbed a card from the The green which are techniques, but the first one I grabbed I Is very similar to the test selection one. It's a different car But i'm gonna go ahead and read it for you and then i'll grab and maybe i'll grab another not sure It's on the sampling technique when faced with unlimited possibilities testers select the most likely options the one most prone to risks Okay, so we wait what? when faced with unlimited possibilities Testers select the most likely options the ones most prone to risks. This is saying wait Wait, is that a is there a comma in there or no? I had to add I had to add commas to make it make sense I will read it as written when faced with unlimited possibilities testers select the most likely options the ones most prone to risks What they're saying is we do risk-based that we The number of ways I could test something is indeed basically infinite But we do risk-based testing. Okay, I just tried to think through what What they mean by the most likely but I got it All testing is sampling. We can't run all tests. They say checks. I'm going to say tests. So we settle for a subset Being honest about this keep this in mind. What keeps us humble? Yes, of course risk-based testing means you can't test everything Choosing which set you will test is very important So this is all they call this sampling technique, but really it's risk-based testing And the last one is not going to read that whole second one Let's test more different variations of ordering english language books as 99 of our businesses in that module So actually like this one at the end because i'm going to assume that's based on data and yeah You can Use all kinds of demographic data to figure out how much time you're going to spend testing if you have a feature of your application that is A very small subset of your customers find useful you can spend less time Both adding new features to that section or testing that section for the product But i'm not sure what else to add to that one other than I don't know any comments on that one before I grab one from The most the most likely the ones Right. What was the last statement the something around most likely though? What they're saying is this one isn't super well written, but but what they're basically saying is because 99 of our business is in ordering english language books Let's wait our testing in that area Oh, okay Yeah, to me that makes sense So if you have a limited test selection, you're going to want to prioritize The way I always phrase it is you're going to want to prioritize the tests That's kind of gonna okay gonna have the biggest oh shit factor if they fail we're gonna do a lightning round All right. I'm gonna read you a heuristic and you're gonna say Tell me whether you can solve that problem with data Or not data. So the game is data and not data. I just made this up. All right. All right data Yes, all right. What's the next question data a product that is hard to explain might be too hard for a user to understand Data We all have expectations of the world around us. Our app should be no exception I can't parse that one. Okay, fine I'll just default to data. There is much to learn from earlier versions. What did they tell you about risk and failure? Oh data is your product true to the image and reputation you or the app's company wishes to project I've been saying mix on that. Okay. Are your competitors doing things different maybe even in a better way? Uh data if you have it That's going to be another mix. I'm I'm I'm I'm uh, created a third choice. Okay What's the third choice mix Yeah data no data mix. Okay, so i'm basically viewing it now as quantitative qualitative or both All right. Can your product live up to the hype? That's got to be both. Okay What are the user's desires and does the product live up to them? That's both does the product solve a problem will it fulfill its ultimate mission data? Your app will have to adhere to standards norms regulations or the law Um that for sure is both. Yep What can you learn from a workflow by executing it in opposite order or a different order? I I can't parse that one. I think that's data. What they want to know is Is if someone goes off the happy path will they still be successful? Yeah, the way I interpret workflow is like a sequence of steps and how do you execute it in reverse order, right? It's a sequence How does your app handle zeros and negatives? Okay, i'm inventing A fourth option. All right, man. Don't care. Don't care. Great Dev's job I think I think we have another one for category four blank fields too many records or exactly one input can lead to horrible errors Yeah, dev When we look at uh, i'm gonna do there's four more things i'll do for heresics and we'll stop but all right But i'll pause there because when we one thing we can look at which should exist is Error rates what errors are customers seeing? Yes. How many are they seeing? Uh, it's it's a great denominator. So you say you might see oh we had 10 000 users last month and A thousand of them encountered errors and a hundred of them encountered more than 20 errors per session Yep, we can go look at that and figure out what to do What is wrong with these people and they all live in texas or something and and desantis shut off their internet who knows? uh, but we can If we have those areas we can dig in and debug and i've talked about this before but Worth a pause here before we finish this up is I love to debug from data I like to look at an anomaly and then dig in and figure out why what's unique about this anomaly? What's going on here? Why? Did this customer get this error? Oh, that is absolutely my favorite thing to do And just to be clear. I don't think brent's being sarcastic No, it's not. It's not I'll tell you why because once I find the answer to that question and then I can generalize it to okay How well does this broaden in terms of impact? I will say 50 of the time it allows me to To do my even more favorite thing to do and that is go to the executives and inform them that their business is not Operating the way they think it is. Yeah, but I even did that that's that's on big scale But I remember even on we had so much data on even on the xbox console that We would hear customers having an error with something So we go look in the data and find out who was having that error Then we check on all the different things about them and slowly like what's in common It's just like debugging with the debugger, but we're just kind of going through we're doing sql queries instead of debugger commands And in the end you find out if they're on a certain version of the console and they have this game installed and they have More than two controllers set up they get this error. Yeah Love that so much fun There is so I think I may have shared with them the podcast that one of one of my team's key missions is around Uh, we call it support supportability One of the kpis that manages sort of success and supportability is self-help solution Okay, and that's to the degree Do we publish documentation and bring that documentation to the customer? On their ability to to understand the docket and actually solve it themselves like they don't actually need to engage support Actually, it's funny that you brought the errors thing up Because literally just this week I was in a meeting on a very similar context and the kpi Just it's very simply measured Like the numerator is the number of people that that deflected Uh a case that we deflected them from entering a case. That's the numerator and then the denominator is the number of people That went into that that particular problem space that that were deflection opportunities and one of the things I stated for the first time actually this week that was controversial, uh because Everybody everybody is focused on the numerator the numerator. How do we get that numerator to 100%? How do we deflect all of these things and I said, yeah, I just want to let you know I'm I'm actually no longer interested in the numerator. I care about the denominator. I want to drive that to zero How do I get the customers? No longer having problems in this space Because if I get them to know how to have no longer having problems then I don't care if we even have documentation Right. They don't need to go and click it. How do we improve our platform? To be smarter to actually solve it in the first place. Yep. I use the left shift Language like our goal is to bring solutions to the customers. It's the absolute most tragic of a fail If a customer does go through Through support and walks away feeling like they did not get a solution but we've been A lot of companies and microsoft included has sort of been Optimizing for a safety net around around the support staff But that's expensive and that's manually driven and and it it at some point in time It fails to scale. Yeah, if you if your customer has to call support you're losing money In our case, not so much. All right. Well in general, I would say that's true Yeah in general, I would say that's potentially true like with microsoft right now for For for the majority of our support topic or support plans These are add-ons that that customers actually need to buy And so people who are going in and creating creating these support cases in some degree They've already decided that they're going to be active on the on the platform but Even if you want to keep that model Right you want then more people to to funnel into it. How do we get more people buying? finding so much value and it's worthwhile to uh Buy a support plan. I get it. I get it. So One of the thing I wanted to mention so I got excited thinking about debugging through data I didn't know we're gonna get here when we started but i'm glad I remembered that but it reminded me a thing we did even on teams There's something I liked a lot about what we did on teams and it was a A dirt simple solution and almost lazy, but it was great We had a in the beta We had a form where a message care how it worked We had a way for any customer using our product internal external to just give plain text feedback, of course, we had we had data on error rates on the server side that people could give us Any sort of feedback we wanted and they and we dump it into a database And we'd never look at it directly, at least never on purpose but What we got is we just wrapped an elastic search Elastic search around that data and then when somebody internally would say hey i'm seeing something weird where the uh The fonts are really small. I go we go. I wonder if customers are seeing this Small font and we see okay. Here's five reports. What's going on there? and we get a much more context and information on why that may happen or daily usually someone would just type crash And see if if if a lot of customers reporting crashes, but it's a pretty It ended up being a pretty cheap way to kind of check on the pulse of what of customer feedback Because there's no way to look at all of it, but I wanted but we collected it all so we'd have it for Often you get something from like satya's office. You want to know is this a butts and chairs figure this out or a A one-off thing. We just need to go over and babysit someone through and having being able to pull from A thousand entries a day on random feedback from customers gave us a way to see if those are things that are really happening But you you seem like you don't like this No, no, no, I I like it very much Uh, but question I think I missed it. Did you say that's in teams or in unity? No, we did that in teams Okay, do you do something? No, not yet But okay all right, so Do you have a pad and paper? I'm going to give you some terms. I want you to go talk to your data science team and Do your charismatic hand wavy? Fucking do the shit that alan asked you to do thing. Okay, i'm listening You need to get a feedback stream going just like you did in teams That's number one like obviously your data science team is not going to be able to do that You're going to have to talk to your desk. I I can populate that. Yeah, okay Then what you want? Is against that stream You want your data science team to produce a daily? TF idf Vector Okay, and what a tf idf is a is an nlp algorithm and what a basic fuck it's damn I don't know what it means. No it's term frequency. It stands for term frequency That's cool. I got it. I got what else verse document, okay Then what you want to do is do anomaly detection against that vector Okay, and wouldn't you have done that? What you will now have Is a feed that tells you in near real time when certain terms Are anomalous and starting to appear in your feedback stream In other words instead of what you did with teams where Customers provided feedback and you looked at it You know when when reactively when someone decided to care you're now going to have a feed that tells you Hey something weird is happening and the customers know about it before we do You turn it to near real time reaction instead of hmm. I wonder if anyone's ever talked about this That is cool. So now what I wanted so another thing no i'm excited Okay, we don't have a lot of time to talk about this. But another thing I want to do is Again xbox live use this i'm sure a lot of teams did this. This is actually similar to this is um Just a sentiment analysis on all the different feedback streams where people can give feedback. What's your what's your view on sentiment analysis? Sentiment analysis. So if you have the right data set and if you have customers that are providing sentiment Um, it's it's handy. Uh, but but what i'm what i'm hearing in your voice is what I really want is the ti Tfidf vector Well, so that one's easy and it should work with your data set. That's the problem is you need like You need near office level degree of usage Because customers don't generally provide feedback, right? Uh, the the response rate is rather low like two three five seven percent. Yeah, I thought more like two I think is is is Going going well So to do something useful with data science on that one you need a lot of users And I I don't know if unity is at that space in terms of where they have customer feedback stuff, but Yeah, certainly And you know no product plans here i'll do this for the part of the product that I ship which is um documentation But it's a place where right feedback and the other thing that is problematic with sentiment analysis is Do you remember back when you were at microsoft when we did, uh the peer feedback? where where You know you you had to ask 20 people to give you peer feedback and then they would answer Uh, these these questions that scaled from one to five and and then some random text that like ms 360 Right to what degree did you find that useful? To it. I I found it. I found it useful to a degree a very slight degree, right? then the problem the reason why is um Anything that was sort of constructive feedback A lot of the times end up being overblown constructive, right? Because it's somebody that actually hates your guts and now their interaction with you was personal Um, but over overall the majority of time you you get oh sure 555 allen is perfect in every way Which doesn't help you grow doesn't help you improve right, so What what I got from from mine was and we do need to close her i'm late for another meeting is uh I I met with the hr person to go over with me and they said alan everyone your work with Thinks you do great work and they love how you get it done But they kind of think you're an asshole And this was very early in my microsoft career. All my role models were assholes And that's when I began. Oh I can you're right But I was kind of almost told that was the way I had to do it That's that it was at that point. I began to actually learn How to read people and build an emotional quotient and think about how I was coming across Which is the only reason i'm still employed thought I kept on being an asshole to get stuff done and left bodies in my wake And not been and burned bridges right and left or to get done what I wanted to get done I'd be in a different place in my career Yeah, but when we both started at microsoft, I mean, that's how you get shit done Yep, exactly back in the day. All right, man. Uh We better close it up here so I can get going but this was fun Uh always good surprising you with agenda items the last minute and see how you react but again you get an a plus Data And the the answer is all the data. Yeah. All right. Uh, thanks everyone for listening. I am Alan and I am brent. All right, man. See you later 
