Well, I don't know. I don't know your view. Have you heard of a band called The Beatles? Okay, that's a bit of an exaggeration. Welcome to AB Testing Podcast, your modern testing podcast. Your hosts, Alan and Brent, will be here to guide you through topics on testing, leadership, Agile, and anything else that comes to mind. Now, on with the show. Welcome back to the AB Testing Podcast. Thank you. Hey, man. How's it going? Good. Good. Here we go again. You know, this is not a secret, but I'm going to let the first time listeners know that we are not one of those podcasts, the carefully scripted script, carefully written script that we use to wow you with our ability to go deep into the topics that matter. That's not us. No, the days that we accidentally wow you, that was entirely unscripted. Yeah. And what we do, as they say, even a broken clock is right two times a day. Correct. Unless it's a 24 hour clock, but they get the point. They get the point. I was talking with someone about the podcast and I think I speak for you, like why we do it. Do we do it for fame? No. Do we do it to piss people off? Actually, no. You think so, but maybe no. This is, and the person I was talking to understood this perfectly, but I don't want to put words in your mouth, but this podcast is for me, as well as my Saturday blog posts, both are forms of therapy for me. You know, with my, my regular therapist, I talk through what's going on and what I'm struggling with, but more personal inward stuff for things about relationships with the people around me. But you and I talk about oftentimes it's, I'm talking through things that I'm thinking about that I haven't really figured out yet. And it's, or things I want to talk about, but haven't found anyone to talk about with. You can always talk about any random prep with uncle Brent. Yeah. Cause uncle Brent, it really is. He's not really all there. So we just talk, but you know, we got a lot of, I'm going to talk about two things, two forms of therapy. Sometimes we do a podcast. I know this is your feeling. Someone listens to a podcast and goes, what the hell did I just listen to? Sometimes I finished recording a podcast and I go, what the hell did Brett and I just do? But those two aren't necessarily correlated. And sometimes we can do a crap podcast from my point of view. And a bunch of people go, Ooh, that was good. Thank you for that episode, Alan and Brent. And I just shrugged my shoulders and stare at the sky and say, you're welcome. Really? Those drive me crazy, but it's maybe what did you like about that? What, what could you have liked? No, it's the, um, in the same thing happened with, with blogging, right? The ones with, I sat down and I was careful and I spent eight hours writing up and judging every single word. Those are the ones where not even crickets, uh, give me feedback, but the ones that I just sit down and barf out. Oh, that's fantastic. Well, my deal is that's, I'm glad you brought that up. Cause I only barf out. I write a, for now 34 weeks or so in a row, I counted. I have written a blog post every weekend, mostly on Saturday, sometimes on Sunday. I do not know what I'm going to write about when I sit down to write. I just pick something. I have a couple of things that kind of sit in the drafts folder. Like if I, like this week, I had an idea of something I've already forgot when it was. I wrote a little blurb in a draft post it's sitting there, but most of the time I'll just sit down and I'll start. And last week I wrote about, um, how tech interviews are horrible at most places. I got just a massive amount of feedback on that and then tags and all kinds of things. So it's weird. Of course, if I talk about how dedicated testers may not be needed, a whole different crowd reaches out and drives my numbers up. Or for the most part, I'm just writing about stuff I'm thinking about. I'm not trying to make a, a change in the industry or make some big thing go on. I just want to dump my thoughts before I forget what I'm thinking because I'm old and that happens. At least get it in the to-do list. If there's anything that you need to follow up on, I get that. Yeah. So anyway, that's a really back in. That's what we're doing with the podcast. We're going to talk today about stuff that I have dumped into our shared Slack channel. If you want to be in the same, uh, hang out with Brent and I and I'll almost a thousand others, including Patrick Pearl, test Pappy, who listened to our last episode and gave us some good feedback and comments. You can go to modern testing.org and click the link to join the one of the three dot slack.com. We had probably half a dozen people join just this week alone. So good little trickle coming in. The countdown to 1000 is steadily getting closer. I'm not sure what happens when we get a thousand users, but we have a little party, but we're at 916 now. I don't know what that means. So I know. Is it, is it, it's 91.6% of the way to 1000. We could round up. I mean, I mean, I'm fine with that. I mean, we've, we've already, we've already rounded up significantly with, with the three. What's what's what's a thousand. All right. Do we, we have talked already since I went to Taylor Swift, right? Did we, did we talk about that? No, actually you were going to. Oh, that's right. And I said, if I didn't come back, you know what happened? I went to Taylor Swift. Not only was it good, and I have been to at least a hundred concerts in my life. It very likely is the best concert I ever went to. Really? Really. I do hear that, that she does put on the show. Did she bring out a guest? No, no. But I am, you know, I've seen, I've seen Bruce Springsteen play for three and a half hours. And some of the time he's phoning it in a little bit. I mean, he's a great performer. I've seen him three or four times. I have been to a lot of, I've seen the Rolling Stones play three hours. I've seen a lot of groups play long shows, but watching her so happy to perform the same songs she's been doing for all year, two or three shows every weekend and love performing them so much and performing them so well. And just, it was very, very good. Very good. So that was good. And I survived this weekend to see fair. So once again, another weekend to stay out of Seattle. Seafair for those not from Seattle. This is the weekend when big loud boats race on the lake, big loud planes fly in the sky. And a bunch of people sit on boats, sit on the beach and get really drunk while all it's going on. Did I sum up seafare? Yeah. Didn't miss anything. No. All right. Do you want to talk? Do you have anything to share with the crowd before we get into a topic or two? I'm actually pretty excited to find out that seafares this weekend. I didn't realize that. Brent, you say, so just for listeners at home, Brent is already very quiet. I'm going to try and fix that in post-broad. But then when you talk softly at your mic levels, you just get cut off completely. And I'm going to add words in there. I'm going to do one of those things where it's Brent's voice saying, hello, I like doggies and puppies than flowers. Yeah. With olive oil and garlic. No, that's number one. It's very exciting to see that. I don't think my daughter's ever been there. I may brave Seattle. All right. Do you want to talk about stuff people care about? Sure. We're coming up on that 10 minute mark. But once I edit a little out and then add a little intro music right at the 10 minute mark where people like to skip forward to, like I have a lot of podcasts, by the way, I'm going to screw it up now, where I use Pocket Cast and it allows me to start a podcast like 30 seconds in, a minute in. And I think if you have Pocket Cast and you listen to AB testing, just set it to start 10 minutes in. That's like a productivity hack. Hey, by the way, there is a podcast that I discovered accidentally this last week. All right. Is it DrumGagel? No. No. It reminds me of you. I don't know why. I don't know why. Please be someone smart. Please be someone smart. Please be someone smart. The name of the podcast is called 60 songs that explained the 90s. Oh, I don't know this podcast, but it sounds good. But and he'll pick a song and it's an hour and a half and he spends the first half hour off of it's just some random tangent. And then he segues into the song and then he has a guest. Like right now, the one I'm listening to is smells like teen spirit. And he managed to land Courtney Love as a guest. Oh my God. It is fascinating. Yeah. I I've gone through like several of the songs and I'm a huge band of, uh, do you know the band? They might be giants. Uh, yeah. I do know the band. They might be giants. I'm not like, like, do you think I went to live on the moon during the 90s? Like, what do you think? Well, I don't know. I don't know your view. Have you heard of a band called the Beatles? They okay. That's that's a bit of an exaggeration. But okay. Hey, Brent, have you heard of this one band? They're really cool. They're called Van Halen. I have. All right. Anyway, go on. I have all of their albums. Now. So so a bunch of a bunch of my like he's actually claiming that smells like teen spirit actually killed 80s pop. Like we would have still gone on with 80s pop if it hadn't been for that song. That's his argument. I'm like, okay, that's a fascinating topic. It is. But let me okay. Screw the 10 minute thing. I'm sorry. I'm going to tell you my story. So, you know, I was in my twenties and the nineties and listening to music and I was not listening to any 80s pop. I mean, I was into 80s pop. Certainly I know I could I can play song quiz on Alexa on 80s and get 100% till the cows come home. But there were a lot of bands that were nowhere near 80s pop before. Nevermind came out. And I remember I went to Silver Platters in must have been fall of 90. I don't know what it's 92 year was that I was on my way to go teach a marching band and I wanted some music for the road. So I grabbed the brand new albums from Pearl Jam and Nirvana on the same day and listened to them. I also like the Pearl Jam still today better, but they're both like, but they weren't like they're oh my God, this is so different. This is the brand new thing. I didn't I don't remember feeling that way at all. It was maybe it's because of the sea. I was in the Seattle crowd listening to a lot of those bands and seeing all those bands live. But I did not feel in any way like I was listening to some brand new thing that was going to be the hit that it was. It was just it was just more of the same I had heard, but done and produced very well. Do you remember? Do you remember? Do you remember any song from winger or striper or even poison after I remember I remember after this. Yeah, because a lot of those came out some of that came out before then there was that big thing on glam rock. There was warrant and striper and all those bands before then for sure. But even going back 10 years before then, I mean, the 80s had Motley crew and bands like that. So that sort of glam rock, I think more of David Bowie, but that glam metal had been around for 10 years before Pearl Jam and Nirvana. So yeah, I remember those bands, but they're different because those bands sucked. Yeah, the difference for me was when those when those songs came out for the first time in MTV, I was right there with you. I'm like, people buy this. Oh, my God. Yeah, I mean, it was horrible. So anyway, we should get no, no, no, no, we're going to go to the music a little bit longer. I'm going to actually explain myself with the Taylor Swift. Okay. I was thinking, oh, Taylor Swift, she's just so popular. So I am there. I mean, above everything else. And my interest in music is very much on the lyrics. I love the singer songwriters. That's why I've seen Bruce Springsteen a bunch of times. That's why I'm a big fan of like John Prine and Dylan, the people that can tell a story in their songs. And that's a big reason why I like listening to Taylor Swift, not her pop stuff. I really don't like like the reputation set. I was it was actually performed well, although I was going to go take a bathroom break. But she, man, I mean, she knows how to tell a story with a song. So that's flat out it. Even Pearl Jam, Pearl Jam, especially, you know, how to tell a story with a song. They're not they're not like a not comparing Pearl Jam to Bob Dylan by any means. But if you listen to like Jeremy, the lyrics have some substance warrant by the way, warrant by the poison. There's no substance of the songs. It's just it's just crap. The words don't even matter. Courtney Love on the podcast said that she loves Jeremy more than any song Kurt ever did. Well, you get the point. I want the music I love has meaning. I was listening to Elliot Smith last week in the shower a little while ago today. So I don't know one of my favorites from the 90s, so much, you know, much mellower, very depressed version of these lyrics. But anyway, interested. So now that you've heard five minutes of this, after you skip the first 10 minutes, I'm going to jump us right into a mailbag question. Holy shit. Oh, what a mailbag question. Okay. Make sure make sure you check out that podcast. One of our best friends, Percy in the slack group, he said this, Brent, this is probably a thought exercise more than a mailbag item challenge accepted, but would appreciate what Alan and Brent think about this. How would you go about calculating the ROI of automated tests? I think it's a great question. Let's say in a year, your org is automated and equivalent of 30 years with a test execution. Would you put a value per test? What is the value with the amount of time and material savings instead? What about diminishing return? So I have many, many thoughts on this. I'm just going to go to I'm going to summarize this whole thing as how do you calculate the ROI of your automation? How do you know? Like a lot of the poorly written articles about test automation say you write this because it saves you so much time in the future and you can focus more on other kinds of testing, which may be true. It may not be true, but I have so many thoughts to go on that. I'm going to let you start. Brent, how would you go about calculating the ROI of automated tests? And for those of you, sorry, sorry, one thing for the people who don't know, like ROI, what is this strange abbreviation Alan's using? Seriously? No, it's return on investment. Go, you're on. Yeah. So for me, for me, right, the return of an automated test is sort of risk reduction, right? You automate a test. So it validates a part of the product and you're trying to reduce the risk in a faster amount of calendar time, right? Like the way I would interpret ROI of a product, the way I think I would interpret it is number one, the amount of calendar time saved, times sort of the probability that you would have executed it if it had been a manual test case. And I think that's it. Let me go through the thought exercise part of this because I think that's a confusing answer, but it's partially at least correct from my point of view. Maybe there is no correct right or wrong answer to this question. So Brent, is it possible for me to write an automated test that provides no value? Yes. Is it possible for me to write an automated test that has equal positive value for the entire lifetime of the test? Equal. So every time you- Is it just as valuable today when I run it as it is a year when I run it? I don't agree with that one. No, I don't either. You get the right answer. So the thought exercise here is some tests are more valuable than others. Some automated tests are more valuable than others. We agree on that. And the value of all automated tests generally decreases over time. Well, yes, it generally does. So why bother? Because there's that period in time, there's that short period of time where it is valuable. So let me answer this question differently than Percy asked it. Because I love this. And the reason I love this is I think so many companies, and I'm talking to you listener, but probably not you, but maybe you, you are running a bunch of automated tests that you've always run spending hours or days running tests and you're not, you're getting minimal value from that effort. But Alan, they're automated. They just run. They're free. Let me ask another question. Let's bring in the whole flaky test into the formula. Let's say that test is pretty valuable, but it gives me false positives 20% of the time. So that makes it less valuable, right? That makes it I get less ROI on that because I'm spending time maintaining. Right. That one. When I when I think of this whole exercise, I think you were slipped Microsoft when he was hawking this, this approach. I kind of, I kind of view him as as really in spirit, the first data science scientist in test. Who we talking about? That's Jimbo Pfeiffer. Jimbo. Yeah. There's a name I heard. He's still at Microsoft. I was just looking that up. I'm not finding this LinkedIn profile. Oh, well, maybe you could have like an address book or something that would let you figure out if someone still work there. But he had this tool that he built called the Prioritizer. Yep. And I have written versions of that myself. And you you saw where I was going. Well, that's where I was going. Yeah. To give people context on our main our mind meld here. What we said before was true. Some tests are more valuable than others. All tests in general lose value over time. While sometimes in their length of their time, they may be more valuable than they were in previous times. So what you want to do and there's other reasons around flakiness, around length of the test, around how much coverage one particular test gives you that can give you different levels of value for a single test. So what the way Jimbo and I and others and probably everybody else should solve this is by prioritizing tests. The way I look at it, for example, is like fast tests. They're going to be more valuable, going to get more away from slower tests if they provide the same other value. If I have a set of tests, it takes 15 minutes to run. And they're very low to zero false positives. And they give me good code coverage and they find regressions really well. Those tests are very valuable when compared to a set of slow tests that are flaky, that do not find issues. Like for example, how many of you rhetorical question are running that test, that automated test every day for the last year, because it's automated and you need to run it every day, even though you haven't found a bug with that test ever. Most people, most people, because because we're afraid testers are afraid that the moment they turn off that test, they'll be your regression and they'll miss it. So the way that I'll just go in a second here. So the way we did prioritization was you would prioritize and you could eventually you lop off the bottom. You don't run tests from one to end in prioritized order. You're from one to a hundred and skip the rest. If you, when you, if you choose, if the prioritizer chooses to skip running a test on day one, that test has slightly more value to run on day two. If I skip it on day two, it has slightly more value on day three and so on. And eventually if you write a good waiting system, it will run the right tests at the right time to give you sort of an optimal ROI. Yeah, that, that is just untiming, right? And that is if you as a tester view the code you're covering as a black box, right? That makes sense. If you view it as a white box, which is what Jim did, you're like, okay, you know what? That code module that has not changed in the year, as well as the modules that call that code module, they haven't changed. Well, what is the actual risk if we skip that test? Almost zero. Almost zero. But at some point in time, it's about getting crap done. So I do want to mention that I've written code coverage based prioritization as far back as like 2002 or 2003. Just choosing which tests to run based on like, you look at what, what code changed in the set of check-ins and you run the tests that test that the challenge for us wasn't an operating system is anytime anything low level changed, like in the IP stack are not so mature prioritization engine said you need to run everything, because everything eventually everything goes through the low level stuff, which I guess isn't horrible, but not it kind of lose the effect. But yeah, I mean, that, that as a level of prioritization based around the code and the code that the tests run, that's been around forever. Right, right. And this is, this is kind of what I was leading to when I say, okay, the potential return of the test, like in my view, the point of it has the way you evaluate return on on investment, the point of a test is to save manual labor. If you whether or not you automate, right, if you are in a position where you have to run that test pass before we ship, whether or not you automate, you're going to run that test pass before we ship. And if you if you don't have it automated, you're going to do it manually. The whole purpose of the automated tests in that front is, is essentially to save yourself on that manual time cost to help your team scale. And that has a very real cost, right? Everybody on your team has a salary and is a very real cost. So when I, when I try to compare, when I try to compare the value of automated tests versus a manual test, to me, it's like, okay, the way you can determine what the ROI is, is let's say you didn't have it automated, which of these things would you have to run manually? And which of these things would you not the investment of it, of course, there's a one time hit. And then you have the time, the execution cost. And then as you called out, the additional cost is like flaky tests, if you have to keep paying that one time cost over and over again, then the amount of risk that your automation test is validating as stay static. And if the cost goes up, then yeah, the ROI is going to decrease. So let me throw one more lens on this, as I think about this, and then we can, we have actually do two mailbags today. If maybe we'll try. No assessment. He did a drawing of the test automation pyramid, but turned it upside down and imagined it as a filter. And basically saying like unit test, the only bugs found here are the bugs that can't be found by unit tests. The only bugs found here are the ones that can't be found by integration tests. The only bugs that happen at the top of the pyramid at the UI, the ones that can only be found at the UI automation level. So I would say one way to calculate, I'm just, just again, therapy, thinking through this as I talk to calculate the ROI of your automation is for the bugs you're finding not in automation, is it helping you focus on finding the bugs that can only be found without automation? Like I wonder if you looked at the bugs your team is finding and being reported by customers, I would expect a high ROI automation suite to have a lower number of bugs that you don't think could have been caught with a reasonable automated test. I feel like you said that sentence with a triple negative. So I'm having a hard time. Okay, let me, let me try this again. Let me try this again. I wonder if one way to measure ROI is to count the number of bugs you're finding overall that could not be found with a reasonable automation effort. In other words, when I find a bug, did I just, is that one? Yeah, that's a rough one. I suppose we could have written automation would be very difficult. Or are we finding stuff? Yeah, we just didn't write that test. Yep. We didn't write that test. Actually, I'm not sure I like that, but I'm not certain I do either, but it did trigger an additional thought that we haven't put into our sort of equation. Right. And that is an automated test. Let's say it's not a flaky test, automated test. And it's, it's, it's finding a bug, a different bug, maybe once a quarter. What we haven't evaluated is how important that bug is. Yeah. One more thing too, is effort. Go to the bug first. I've been talking about effort after that. Right. So we've talked about it regarding MTP, right? That the data analysis principle, the customer principle, if a bug ships in a product that a customer never hits, we can't do that. Do we care? I don't care. I don't care. And that's maybe the difference. Going off one of our tangents here, one difference between you and I and a lot of testers, we care about quality, not testing, which means you and I don't particularly care about bugs that no customer has ever found, even if they're in the product. Yeah, actually, literally to get, to get on my board as a bug. As you know, for years, I haven't had a bug database. To get on my board as a bug, you either have to articulate very succinctly that is absolutely a bug. The low level engineer wrote a wild true loop. Okay, that's a bug. Right. It's just a better time before an infinite loop triggers. But without that sort of succinct, I literally don't care unless there's customer impact. And not even theoretical. Like I want the type of bugs that I would fix don't really happen anymore that don't have customer impact. Like, hey, we accidentally deleted being the customer's entire store. That's a bug. That's a bug, but that's not going to ship in under my watch. Things like weird things happen if you use a weird symbol exactly at mid nine on February 29. Yeah, I'm probably going to ship that bug every day, all day long, I probably not even going to test that condition. So now, I yeah, we're getting somewhere here. Let me talk about effort too. So, Percy mentioned the equivalent of 30 years where the test execution. Well, one, what does that mean? But two, but more importantly is how much effort went into those. And this is the argument that I always argue for the newer record and playback tools for web automation is they're quick. They're smart enough to not be flaky. And if they ever get to a point where you've changed the UI so much, they can't figure it out and keep going. Just record them again, because it's low effort. People talk about the low code part, but it's low effort. I get a lot of ROI out of that because the effort is low. If I am spending, as I did in the past many, many years ago, days to weeks, I'll tell you about I'll tell you a story here in a second to write test automation. And I get very little return from that. I don't find any bugs or the bugs I find things we're going to fix. You know, maybe that's not right. So let me give you an example. Very, very early in my career, I wrote UI animation, I haven't since then, I've looked at a little bit, but not necessarily the UI animation, the folks get all excited about today, I've automated Win 32 by putting messages into message loops and things like that. But Windows 98, I was working on networking, and particularly making sure that the new networking stack across our different supported networks would handle code sets and all of the the Asian languages. And the thing with Asian languages at the time Windows 9x, the consumer windows of the time, was that they were mixed by meaning the special and like a Japanese, your kanji characters or Korean, your hangul characters, they were a two byte character. So which caused all kinds of problems in network names that they're parsed, weird, etc, etc. And, you know, Windows 95, but it's some ad hoc checking of characters we knew would be bad and tried them and but didn't try every character for sure. So one thing I did probably took me two full weeks to write was I wrote a little application that would cycle through every single character in a given code page. Even if that code page had thousands of characters, then do the canonical list of network connections, create a share with those characters, connect to a server and create and create folders with those characters, copy files with those characters around the varying lengths, all kinds of different stuff to try and see if stuff would break. It was fun, you know, it took forever to run, as you can imagine. Yeah. On some systems. And there was ways to give it like I had like a at the time, remember any files in any in Israel, YAML before YAML, but they're not but they're not markup dot I files. Remember those? Oh, yeah, they still exist. I know they do in Windows. I haven't seen one in years. So you could put little parameters in mind to make it like start and stop at certain code points. We didn't want to run the whole thing handy. But it found, you know, a good number of bugs across different networking components across characters. I mean, and the majority of those got fixed because they were they were potentially real world characters. I think the telemetry would tell you that most Asian users just used English characters in their share name. So maybe not that exciting. But just looking back, looking back here, I wrote a pretty complex thing. It was super cool. We ran it was probably run thousands of times across the delivery of when 98 and Windows Millennium. Sorry, everybody. I still don't think we got the ROI out of that effort. In hindsight, I like if I would go back in time, I would not write anything that complex. This is young programmer Alan trying to do something cool. But in the big picture, there was probably a more efficient way to do that. That would have there are higher ROIs for me to do to get the results of that testing I did. Right in comparison to the type of things you can do nowadays in two weeks, there's certainly there's certainly a long list of higher ROI items. But I don't know. That's a very long way of saying we got to look at effort also. So you got to look at effort, you got to look at the value of the bug, you got to look at some aspect of prioritization. I don't know what else to recap. I tell my team, you talk about effort. And when I when I go and talk to my team, and they're like, okay, well, what's your, how do you prioritize things? And I say, oh, that's very clear. Like do the most important thing first. That's technology. I basically say the number one thing, you need to come up with ways to articulate that your effort is adding business value. Not a freshman college essay answer, be real, add business value. It's not just reducing costs. It's more the ROI question. My second priority is reduce calendar time. That is the only thing that I have not figured out how to scale. Right? Once you've lost calendar time, it's gone. Third, reduce engineering time. That's kind of the effort. Right. So for for me, I don't know. I think if I were your manager at that time, and I go, okay, these mixed bite things, these are weird. Is it worthwhile for for it was the Wild West on that team. They said, Alan, go write some tests. Everything was wild west back then. Back in those days. Let me give you another way to think about this. We can't do an AB test. Ideally, in a perfect world, you'd have one team that wrote a bunch of automation, we have product code at the same time, and then shipped. And one team that didn't write any automation at all and worked on a product, well, very little, and worked on a product, and then shipped, then shipped. You get more ROI if you're able to ship faster, or with higher quality, or both with that automation. But that's not totally true either, because it adds automation retains value for the next version of that product as well. Yeah, when I'm exploring when I'm shipping an actual MVP, right, there was a phase in the world where MVP was just a synonym for beta. When I'm shipping an actual MVP, right now my team is doing a concierge MVP. Love it. Love the concierge MVP. And you know what? We're not spending a whole lot of time on automation, right? Because that's not what's important. You wouldn't get a return on that investment. That's not going to be adding business value at this time. What's important at this phase is understanding, do we actually have a product? Do we have something people want to use? In terms of the one product that I'm thinking about right now, I'll tell you, we did this big announcement event last week, and we absolutely do. Right? I have people coming to me out of the blue, trying to figure out how to be on my early adopters list. And I'm like, yeah, I can't scale to support you. I don't want to go too fast. Once I validated it, then it's a more around lockdown, get execution and make sure we're able to operationalize that execution. Small bugs, I really don't care about us being sloppy on check ins and things like that. That will hurt our reputation. So the recap short answer for Percy is actually, I think he said this sounds more like a thought experiment, which our answer says it kind of is. But there's a lot of things to think about that can help you figure out if you're getting good or bad, or if you could get better or worse ROI on that investment. For me in a nutshell is compare it to your top two and three alternatives. And that's going to tell you what the ROI is. Yeah. And Percy's been in the industry long enough. It is. But this but this answer is for everyone. Percy asks, we're giving it to everyone, I think. And again, I said this last time, and I restate it again, I think our industry is has an infatuation with UI automation. And I would think a lot of folks and who are writing a crap, the teams are specialized automation teams writing a crap ton of selenium, I can guarantee you, they're not getting full, they're not getting ROI from those tests, they're getting value, value is different from ROI, they're getting value from those tests. They're not getting value from the effort they're putting into writing those tests, because they're a pain in the ass to write. And they're probably not finding a great set of bugs. And they're probably not giving you a bunch of value over time, because they're flaky. Yeah, the thing the thing that I've kind of saved until now, which probably is unfortunate. I don't know that I agree that the point of an automated test is the fine bug. No, and I don't think I will. I don't think we said that we did say what kind of bugs does it bind? Well, it's a safety net, which we hate, but it is in a way. People talk about unit tests giving you confidence in your ability to change the code, because you know, if you screwed up anything, you'll know. But all the tests give you, you want the test to let you know if you screwed up. That's one, you'll also want the tests, you want to be able to trust the tests and that if they pass, they're actually passing. Right? Sure. And I think we may have talked about at Microsoft, there was this one, extremely huge team ran through their whole suite without actually having the product even installed and some ungodly number of their tests pass. Famous story. Right. All right. Well, hopefully that's some that's some enough food for thought to get you thinking, Percy, and get somebody else. And all right, we got to get going. I got a thing to go to. And Brent's probably got a thing to go to. And it is hot out in Seattle and my dog's hungry and I got to go. And I'm Alan. I'm Brent. And we will talk to you next time on A B testing. 
