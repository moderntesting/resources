Hey everybody. Howdy. I'm Alan. I'm Brent. And we're back for AAB testing episode 46. Hey Brent. Yes, I don't think we're alone now. We are not. Today we have a special guest. Is he special or just a guest? He's a special guest. A special guest. Steve Rowe is a data scientist manager in the Windows team. His journey has been much like myself. Why don't you say a few words, Steve, about how you got here? Sure. So I'm Steve Rowe. I came through the same similar channel to Brent and perhaps Alan. I was a test developer for a long time in the Windows team. I did multimedia for a long time and then I did some developer service stuff and then made a changeover to data science as I saw things moving and changing and I thought that there was a lot more power in figuring out whether we were doing the right thing or not and bringing quality through data rather than quality through repeating testing over and over and over and over again. What's wrong with that? People are now worried for their jobs. Don't freak them out. There is, yes, there's some of that. There's going to be, what was it called, creative destruction. I think would be Schumpeter's turn that is going on in some of this. So I don't think you'll get rid of testers but I think that there's definitely a reduction in the need for raw testers as they were replaced by data. Just the same way that there was kind of a reduction in the need for manual testers as test development picked up. You'll see if you follow Steve Rowe's blogs, you'll definitely see that he's cut from the same cloth as most of the three. Actually, it occurs to me. We have two of the three in the room today. Wait. Yeah. I think I'm one. Yeah, but Alan doesn't listen to the podcast. I do. I have multiple personalities. Those guys are smart. Oh, God. Brent, Brent, Brent. So I have a question. I've been waiting to have a guest to do this because like testing, if you ask two testers what they do, there's a lot of overlap Venn diagram but there's differences. I think I bet my hunch is it's the same thing in the data science world. So you've listened, you know Brent really well, you know what he does. What are some things that maybe differ in what your approach and what you do and then what Brent does? I don't quite know what Brent does well enough to tell you exactly what. Yeah, Brent doesn't do anything well. Well, then I do things well. So there's the difference. I do perception management really well, apparently. So my team is kind of a split team. Half my team is what I would call a data engineering team and the other half is a data science team. And so we spend some of our time figuring out how to move data and process data at volume and then some of our time trying to actually analyze data and figure out what the insights are. I think the big difference is organizationally we're probably at different spots in an organization. Windows is still trying to figure out how to use data. And so as you guys have been talking the last few times about the process through data, we're clearly past the data oblivious phase. We know no data exists but we're still in this phase that I don't think you quite mentioned but kind of a confirmatory data stage where half the time we want to use data but only if it confirms our intuition. And we don't really want to be challenged by the data. I've heard some people around me say, you know, I only want this result if it's going to show me what I want because if it's not what I think is true, then I know that the data is wrong, which is clearly an interesting stage to be in. So a lot of our job is spent trying to get the organization to pay attention to data, to become more data literate and more understanding of what data should be used for. So we have less opinions and more data-driven decisions. That's actually very interesting. There's something we talked about that data aware stage, which was interesting and maybe it's somewhere in there. I think you're much farther along than my org, for example. But there's a data trustworthy. Remember we used to deal with, used to, like last week with test trustworthiness. Can I trust that this test failure is a product failure? Or can I trust that this test passing means the product's working correctly? We kind of, not kind of, there's a huge parallel with data. Can I, is this data telling me, do I believe this data? Do I not believe this data? Right. And I think that's, that's accurately a question people ask because the one thing I keep telling my team is when you're writing programs and you get them wrong, oftentimes they crash or they go clearly incorrect. If you're using math, it doesn't generally crash. You can divide by zero. You can get some imaginary numbers, but not in numbers, but otherwise it doesn't crash. It will happily tell you that the average house price is $2 billion. And if you believe it, you know, that's the case, right? It won't tell you, sorry, you gave me the wrong input. It'll just tell you the average house price and value is $2 billion, which is off by at least one zero. At least yeah. So I just had this happen yesterday. I, I got the phone call, the IM, summoning me to the boss's rooms, the VP's room. And yeah, we looked at the bug data and some just going back a decade looking at bug data and incoming rate was dropping way down. We're at the part of our product, right? Where it should and he looked, I don't believe it's dropped down that far. I said, well, that's what the data says. I'm gonna go take a look at it. I, that it is a little bit too good to be true, but I think we're over that corner. And sure enough, I go back to the data. Someone had mucked with a query underneath me and got the, and screwed it up and they're off by about half. So the numbers were not good, but it's like, I think anytime what my lesson learned there was trust, but verify just, I think with, yeah, I think that was the wise wisdom from way back. I've definitely seen it happen both ways. It's still true here. People find data that they want to believe and they, they believe it to be true. I remember years and years ago, somebody did some analysis, not like data analysis, but some analysis of a, we had this big system called WTT windows test technology or whatever. And there's all these jobs that ran tests, like thousands and thousands of jobs that ran tests. And somebody came in one day to a develop directors meeting and they said, half of these jobs don't work. I ran my tool and I found out that half of them don't work. They just can't run it all. And they were beating us up because we were all idiots for not having working jobs. And we go back and we're like, well, they actually do work. So what was going on? Well, their tool had forgotten how to parse or they hadn't built it, how to parse variables. And so there might be a directory like, you know, SQL and back just windows. And then we would say winder in as a variable and it wouldn't parse the directory and go, well, there's no directory called winder. So therefore this can't possibly be accessing anything on the system. They were happily trusting their data instead of verifying it. And they were, you know, and that happens both what happens that way, or it happens on the dev side or the PM side sometimes where they don't want the data to tell them something wrong. Like I don't trust it no matter what, because it's not matching my intuition. You do have a lot of those problems in, in the data science space. I'll give you an example. Just yesterday, we have a new data scientist that ramped up he's come from office, where they have a much more well documented curated list of data sets. And he's like, Hey, I want to do is essentially, hey, I want to do data science. Can you point me to the documentation for our data sources? And I'm like, okay, dude, you and I need to meet. Because there are literally hundreds of them, none documented. Right, we have, we have this one system that does kind of automate away all the data movement for us. And they able to use reflection to get essentially a reflection to get the schema and publish the schema. But you have you have things like a widget, goo it in a widget ID. Are these things the same thing? Or are they not, right? We you have a lot of exceptions in the data because the data doesn't come to us clean. You have a lot of people who go to the raw data directly directly operate under the belief that they do actually understand everything about the data that they need to, which is I think similar to what Steve was talking about, but they don't. So part of the things I do now is, hey, I'll point you to the data. If you present any analyses you do without vetting it through me to make sure you've got the tribal knowledge, right, dude, you're on your own. And when someone comes to me and says, hey, your numbers don't line up with Brent's. I'm telling you upfront, you're the one that's going to do the the the digging to figure out why. So what do you think is the right level of documentation to do on data sources? Like I've seen people that want a white pages of data, right? I want to be able to like point you at every single data source. I want to have them all fully documented. And then I've seen teams that operate more like what you're talking about where it's there's a bunch of data and you can kind of tell the names, but you can't tell anything else. Where's the right balance? Because you can get it right. You can easily over index. So I'll tell you the approach that my team's taking now. We are building what we call the CDW, which is a common data warehouse. And it's initially we had planned it to be something more directive where we would build it up and then democratize it. Now what we are doing because I so I flatted that as a proposal. And what it turns out is my own team, Dev and PM, they're at the phase of Yeah, that could be really useful as long as we don't have to build it. And so I think the ROI of us doing that upfront is rather low, because it's uncertain who who are going to be my customers if I build it. But I need the data curated for I need a subset of it for my purposes. So what my team is going to do is every time we ingest a new data set, we'll curate it and then we'll document it and we'll do it one at a time. I'd say right now, most of 80% of what I'm working with is against 20 different data sets. That's easy enough to document. There's there's a lot of asks for a lot more. But it's not high in my backlog today. So when it becomes when it becomes it and I then I pull the data in, I'll document it. In the meantime, what I will do is document and police standards of practice. So those those others who want a different data set, I'll say, Hey, you're welcome to add to the CDW turn it into sort of a shared source type project. And but you need to you will follow these rules. Because if you don't, and I discover it, I will remove your data from the CDW. Right? So I turned it into much more of a shared source, but moderated to prevent broken window theory. So you have a few sources, a limited number of sources that are commonly used, they're all documented, and then everything else is kind of still the wild west. Absolutely. What are some examples of data sources you use for data sets? You can you can generify for the external audience. Until right now, I'm spending a large portion of my time focused on availability concerns and reliability concerns. So we have a transaction feed for the heartbeat of VMs is the VM up or down. We have a ton of metadata feeds, we have another transaction feed that that we use quite often, which is our we call it our control plane. It's essentially, hey, how many times the customer says create a VM? Do we actually create a VM? Because now those two together, we can then come up with new insights like hey, if a if a VM goes down, can we determine if it was because the customer asked for it to be down, which the heartbeat feed won't necessarily tell you, but the control plane feed will. And then we have a just a crap ton of dimension feeds, which is essentially metadata about the nouns that we see in the transaction feeds. So we'll have a VM dimension, it tells us everything about where that VM lived and who it belongs to, we have customer dimensions that say, hey, this customer is Walmart, for example, and their headquarters are over and wherever. Isn't that personally identifiable information? It is not. That is tenant, what's known as tenant identifiable information. And that is fine. I said that because we've had the same discussion often in my team. So what about you, Steve, what data sets is your team primarily working? So we have quite a I work in the developer space. So I work with a lot of application development and application lifecycle kind of data. So we have a lot of streams that show us like what applications this is for people that send us data or in the beta program, not the general machines out there. But we have data that tells us like what applications were installed or attempted to install and which ones succeeded or didn't succeed what the application lifecycles were, was it launched? Did it go into suspend? When was it killed? Or was it closed down? That kind of stuff. And so we can see whether or not applications are successful or not. We also have interesting data, we can watch what API's various apps call. And so we can use that data to do cool things like figure out whether or not the build of the operating system regressed an app. So let's say that maybe the Facebook app calls this networking API 40 times in the first minute or whatever. If I can actually watch that over time and say, Oh, on this day, it dropped down to 20, we probably just broke that application. What we do wrong, I don't know yet, we'll go figure it out. But we can actually see in the data that that application broke on a given build of Windows because that it behaves very differently than it used to behave. That's cool. Wasn't Windows recently in the news on this data thing? Isn't it wasn't at Windows, the where we announced or where it was announced that we are following all of you? I don't think so. No, there was some news. I don't actually follow the news. We don't follow everybody. We certainly don't. We have very strict restrictions on pride, personally identifiable information and all kinds of stuff that we do to make sure that we can't track people. You can also turn it off. And, you know, that kind of stuff. So if you're in the beta program, we tend to watch more because it'll check boxes that tend to be on by default. But a typical person using Windows has a lot more control over it. And we don't do anything that we would identify you as anything. So so you do not have the ability to see what websites Alan is looking at. We go out of our way not to be. Just mostly I shop on Amazon. Just total tangent because that's part of the show. I was talking to neighbor my works at Amazon and I said, man, if I got a discount there, oh, it says we get a discount. And but it's like, it's not really much they get like 10 percent up to a thousand dollars. Yeah. So they get one hundred dollars a year. Yeah. So basically they get a hundred. It's not much. And you worked there for two years. So really you got about two hundred dollars as you. It's more than that. But it's like a really when you do the math, he's really excited about it. The math. I go, wait a minute. Yeah, it is something like that. I've heard friends talk about it. So that's their equivalent of the company store program. I guess because yeah, I'm I don't like shopping in stores. So I'll buy clothes in stores. And obviously I don't buy clothes very often, but everything else I buy. Eventually they wear out. Well, you don't have to stop wearing them when they wear out. That is a common misconception. You can just keep on wearing them. This T-shirt I bought in 1998. Still little colors falling off. Windows 95 on it or something. No, I was wearing this when I was working on Windows 95. And that's the nice thing, right? With with if he just waits long enough, he'll have his team feed him new clothes. Although no one commonly gives out T-shirts or common, but no one's doing pants. No, I haven't. I don't know that I've ever gotten pants. I have me neither. No, I'm I'm kind of upset. I think I saw a pair of shorts once. That's an oversight. I don't nothing shirt. Lots of T-shirts. Jackets. I just got to meet yet another jacket. I've got two jackets on this team, a sweatshirt and a jacket. So maybe you should propose when your when your product finally ships, you get a pair of sweat pants with the product name right on the butt. Like those pink ones. Right. Yeah. Juicy. That's the name of our product. They just take things for a leak in our product game. Microsoft Juicy for business. Actually, I saw PR for your product recently. So you guys are announced now. No, we're not. I didn't see anything. Nothing. Nothing. Nothing. I saw something. There's nothing nothing condoned by our organization has been released on the interwebs. Gotcha. And that's where we're going to keep that one. So I've done enough things to piss off management. I'm not going to go there. So Steve's here. Yeah. Hey. So back to Steve. What are we going to say? How about now? Sabre. Go ahead. So Steve. Yes, Brad. Um, I anxiously await your question. I anxiously wait his question too. So I've, I've gone through and sort of walk through how I got to the position I'm in right now. Uh, a lot of our listeners are, I'll say data curious, right? Um, and it's very, very clear that they are operating in worlds that are behind us in some degree, very similar to us, uh, 10, 15 years ago. Or three years ago. How would you, how would you guide from, from your experience, how would you guide people, uh, making this transition? I think it depends on what course you're going to take. So there's two courses to try to get your team to be more data driven. Uh, one is a top down approach and the other one is kind of a bottoms up approach. Um, so you can either try to find or convince management that this is a good thing. And then management can create the edict that we will go do this and, uh, then started building up the organizational skills from there. Or you can try to make your little section of the world more data driven and do that in a way that also gets your other jobs done and try to become a center of excellence. And people might mock, might follow you, uh, over time and emulate you. And those are both possible routes. Um, I suspect most of our readers, listeners, whatever, uh, we'll have to be in the second bucket because probably most of them are not directors or, you know, CEOs or whatever, and they don't get to come down with the, with the edict, they can try to convince management, but it's not always easy to convince management to do anything new and different because it's scary and dangerous. Um, and so I think that the pattern is to try to start at the bottom. Uh, a lot of times as unfortunate as that is, I think it's a harder road to go, but it's also the only road available a lot of times. Um, and I think the right way to do that is to figure out where, where you can replace current functionality with this data or supplement it with data. I don't think that you can just say, we're not going to test anymore. We're going to use data. Cause I don't think anybody's going to believe that that is effective enough in order to let you spend the time doing that. They're going to still expect their test passes, but you can start running data analysis on the test passes. You can start running data. You can try to get some instrumentation into the builds. A lot of people have at least logs or something along those lines, convince the devs or go at it yourself, depending on where you are organizationally and what's allowed, and then use that to try to understand, uh, whether, um, your, the quality is high or not. I think there's two major uses of data, um, in an organization. One use is to understand the health of the product and the other one is to understand what I call the success of the product. So the first one is to figure out whether or not what you built is doing what you expected it to do. And the other one is figuring out, figure out whether or not the users are appreciating what you built. Um, and I think it's much easier to bootstrap the first one, the health metrics than it is the other ones, because it's more of a drop in replacement for, or a supplement for traditional testing. And, uh, I have a couple of blog posts from way back talking about data driven quality and kind of moving in that direction, which focused mostly on those health metrics. And so I think one thing you can do is you can start monitoring the percentage of time that your test passes fit, pass or fail. You can start monitoring data about the machines or the builds or the, the environments that they're in and try to tease out data from there. Um, and then you can over time, maybe start getting data from real instances of the application, either through logs or through some kind of, uh, you know, data API that you have. If you're in a server, you can do it much more easily than if you're, you're running distributed software. Um, and then figure out whether or not I can give you some sense for the quality of your product, uh, for the, the health of your product. And then maybe you can turn down the dial on testing and turn up the dial on data. What about your journey personally? So how long ago were you in a, a, let's just say a pure traditional test manager role? About three years ago. It was the fall, about three years ago, right? When we started the work on, on windows 10 that I moved from a traditional test developer job to, uh, a data science job. When, when, uh, so windows was essentially the first of your two ways to get there. If I, if I recall correctly, you, in other words, it wasn't a bunch of small satellite teams trying to convince upper management upper men, as this is my perception from outside of windows, it was upper management said, no, we're doing. It was definitely a, a, a student body left movement. Like everybody, uh, has to go this direction. We took what was a standard test developer role and said that role no longer exists. We now have a quality role, which is not the same as a data science role, but this thing we call the quality role, which is just a label we put on it, um, was all about using data to understand the quality of our product. So we had in that group, we have people that are more traditional developers whose job it is to instrument the builds and then do basic analysis on those builds and, and understand the area very well. And then we have data scientists whose job it is to help bring actual math to the, to the problems, um, in, in non-basic ways. Um, I'd like to think of, there's a Venn diagram that's pretty popular out there that says data science is kind of a combination of area knowledge, programming knowledge, and statistics or math. And I tend to think that no one person really has all of those. And so we've almost ended up in a world where the area knowledge and the programming knowledge are owned by one group of people. That's the quality teams. And then the, the data science knowledge or the, the statistics knowledge, and then maybe some of the programming knowledge as well is owned by the data science teams. And so the combination ends up with a sweet spot in the middle. So when, when they pulled this trigger, were you anxious or excited about it? I was personally excited about it. I'd thought about it for a while. I'd actually been looking for ways to try to go do this in my, my old job. The difficulty that I had as I was on the team that was developing what was called Windows runtime, which is the API model for all of the, the Windows apps, uh, the app store apps, the hard part about that is trying to use data in something that has a very long pipeline is super difficult. You can't exactly A B test an API. You can't send it out, you know, one version of the API to it, to developer a and one version of developer B and see which one works better. You know, you can't really watch for applications to be submitted to a store somewhere six months after you introduce the API and then start gathering data. So it was the, one of the more difficult places to go use data. And so I hadn't figured out yet the right way to go use it in that space until the edict came down. Um, and then we moved, but even there we ended up in a world where although upper management said, go do this upper management didn't really know exactly what they wanted to do with it. Either they knew they wanted to get there, but they didn't know they'd never done it before. And so they didn't know what parts to appreciate or not appreciate. And there was a lot of, of trying and failing and learning that happened along the way. And so we had a lot of we're getting much better, but we had a lot of, uh, early pressure from the top to go do this. And then various teams tried different things. And so you ended up with kind of this hybrid model where there was a lot of local, uh, idea development and talent development and, and ways to use stuff that was developed and then kind of pushed out to other people as centers of excellence. So it was a couple of things on you, uh, touch on that you talked about one was, uh, going way back the, um, this is why I edit. You were talking about, uh, you can either have an edict from top or sort of, you know, grassroots and, you know, try stuff and do it gradually. Uh, generally that latter approach in my experience sticks more. If it's harder to do driving from the bottom up, but it, in my experience in any sort of organizational changes sort of sticks more, um, edicts are interesting and they can often work. They're often difficult to get, you know, all the time I see people, they go to a VP or somebody high up and go, you should demand that the whole org does this. And they rarely ever do that because they, it's not always a good thing to make an edict. Uh, but one thing was interesting, glad you touched on it with the windows one is, is they said we're making this change and the, how does it happen and what does it mean was all left in the air. So was why, no, no, why was there? Why was not partially there. I'll go more than what or how. So there was this and either it was, and Steve and I may be on two sides of this, but it may be either genius or idiocy of this. Just let a thousand flowers bloom. And the end of worked out. All right. Regardless of the how it got there, let a thousand flowers bloom and the strong flowers will guide the way for the slash and burn all the white ones, keep the red ones. Is that you describe it like we were sitting on a hill and we, they showed us this picture across the valley of another city on another hill. And he said, we want to get there. And you have this valley laid out in front of you covered with clouds, you know, and fog. And they said, go over to that other hill. And you're like, well, what do I need? Do I need like a camel? Do I need a parka? Do I need a boat? Like, how am I getting there? Like, we don't know, just go, you know. And so we all tried. And those of us that chose camels, you know, ended up poor in the, in the, you know, middle of the Arctic or and you get halfway there. And some people say, let's turn back. This isn't working. We're not all the way to the city yet. Let's turn back. And so we had those people that said, let's go back to the traditional ways of doing things. I miss this part that I had before. And then some of us pushed on and we're still in the process. We haven't gotten all the way to the city. Maybe a few. I think that's true across a lot of Microsoft. I think a lot of people have made it to the city. There are a ton of people who just want to go back to their old comfortable city that that comfortable city that's it's now been raised to the ground. No, it's full. It's full of great 90s grunge music and other things from the 90s, many other things from the 90s. Yes. The problem is that people don't realize that the old way was just not fast enough. You're guaranteed to be beaten by your competition. That's what I was saying. All the old ways. That's what I was saying when when this first rolled out, like, I think we were still doing a we we were doing the podcast then one of the biggest things that both Alan and I discovered during those phases, explaining to people why from a business context, because the number one question we got, at least I got from from individuals in the Windows org was no one's explaining why the old system is broken today. Right. It's kind of a precursor towards these folks who if if they're forced to journey are likely to want to go back. Right. I think that they're there. We could have been better about that. If you go back, there's some blog posts I have on kind of the development of tests or the evolution of testing and we kind of walk through. I think the important thing to recognize is that the people who have made it to data science, you know, Island or Hill or whatever, sometimes look back and go, we were all idiots. That was a terrible way to do things. It was never a good way. You know, and I think that's wrong. I think there was certainly a point in time and certain requirements in the in the environment and around the ecosystem that made that the right way to go do things and still to this day makes it the right way way to do certain kinds of work. I think you can't say that data science and data driven quality is the only possible way to handle quality and it's probably not the end way that will handle quality. There are probably other ways that will be invented later that will replace those a lot of it. But to say that manual testing was always a bad idea or that S-net work was always a bad idea is to ignore what was actually happening in the industry at the time. And I think those were actually very good things to happen at points in time when you had certain requirements from the ecosystem. If you don't have the ability to quickly push new software out, if you have to get it into a box and get it onto somebody's desk, you can't use the data driven world that we have. If you're not connected, you can't use a data driven system. And so there was a point in time when that was exactly the right solution. We've just moved past that time. Right. I think I totally agree. And one bit of advice for pretty much anything you're doing is look for efficiency and don't fall into that. That's the way we've always done it. Trap. And don't fall into that. That's the new way to do a trap either. Sometimes it's the new shiny way. And so everything has to be rewritten or redone to be the new shiny way. And it doesn't always apply to every part of every industry, right? If you're working on healthcare equipment, I'm not sure that data driven stuff is the best way to go. Like, oh, we killed three people. We should probably change that. It depends on the area of health, right? If it's in some areas, yeah, but there may be some, uh, I'm trying to think while I talk here, but, uh, some more preventative things. Maybe there's a better way to measure your blood pressure. I always see every time I go to the doctor, they have like some better device measure my blood pressure and, and they have like scales that are networked. And there's a lot of, there is room for, I think even in healthcare to have more data science and to learn how doctors are using things and to make tweaks in the equipment. Yeah, you got to figure out where the places are. I think it's always a question of, of how long the damage is going to be sustained and how big the damage is. Right. And that's how you figure out whether you need, whether you need to scale things. And so if I, if I, the advantage of using data is that I can release something into the wild for a little while, and I can quickly fix it, which means that the data is the damage that is doing, if it's wrong, isn't sustained very long in the old world, it had to be sustained for months or years because of the release cycles. Now you can have it out there for a day or half a day and fix it, which is all awesome until you have an infinite cost, right? If, if one second of damage or one split second of damage, it causes a death or causes major harm and you can't afford to let that go. And you have to fix it before you possibly, or even worse breaks the Facebook app. Yes. Yeah. That's, that's unconscionable. We are definitely, we are definitely in a world where we have a lot more options than we did back in the day. I do, to your point, like there is a lot of revisionism in a lot of the people that I talked to, right? It's easy to predict the future in hindsight. Yeah. Especially to create it. Right. The lot of the world and back in those days, a lot of the world and in a lot of our listeners, I think I still are still in what we refer to as the old days. Yeah, I think they are. They're, they're missing knowledge. They're missing assets that we've built up over, over the years. Um, the, the positive thing though, is in today's world, there's a lot of other examples to learn from. You don't have to treat it as theoretical. I know back in the windows 95 days, if I had brought up anything along the lines of what I do today, they would look at me like I had three heads. Right. And it's just because the ideas of this type of work that we do and the benefits of it were sort of academic theoretical at that point in time. Well, at that point, that would have been impossible to carry out. But even if assuming it was possible, you're right. They would have looked at you like, this doesn't make any sense because they were just moving to a large test organization at that point in time. Like that was kind of when a lot of the independent test organizations are starting to spin up. Um, I think around that time, maybe today, just been up not too long before that. Now back to your personal journey. So you've been, you've been doing this for three years. Correct. What did you do to prepare yourself to be the head of a, of a data science that engineering team, did you have that skillset beforehand? No, I didn't at all. I, in fact, I've never even taken a statistics course when I was in college. So I was uniquely on skilled in this particular, in this particular thing. So what I did is what I always do. And I started reading, I started reading blogs, I started reading books, I started reading papers. Um, and then around that time, all the, the MOOCs, the Massly online open courses, like Coursera and edX, et cetera, uh, started existing. And so I took a lot of the courses. There's a whole series of, of almost collegiate level courses that you can take for, for free or almost for free. Um, and so I went through a lot of those, um, over time, I had to learn a lot of the techniques that I had to go learn. So you have to kind of become rigorous and go back to fundamentals and learn the basics so you can figure out what to apply. Um, and then you have to read, you have to figure out where it applies and where it doesn't apply. But a lot of it, you have to understand enough techniques in order to go back and be able to figure out which ones to apply and not apply. Is there one thing if you were to say, if you only do one to start from your experience, what is the one concrete thing you would suggest? If you're trying to, is there a data scientist? Right. Yeah. So there's there's a series of MOOCs, about nine, eight or nine of them, depending on the take the caps in from, uh, Johns Hopkins University, um, which is called, I think like data science or whatever. Um, it's on Coursera and there's a series of, of they're offered every month or one per month. And I would suggest going through that set of courses. It's a very good set of courses. Most of them aren't too hard. There's a few of them that are quite rigorous. Um, but when you get to the end of it, you'll have a very good sense for things. You won't be a PhD by any stretch of the imagination, but you will have a very strong sense of things. And how often have you been finding yourself using what you've learned in your job? Daily, probably. Right. So my job today is as a manager. So I don't do a lot of the direct, uh, data analysis, but I overview a lot of with the data analysis. And so I will oftentimes meet with my teams and they'll say, we're going to go report this. And I'm like, well, what about that? You've missed this corner over here. Maybe you should use this technique. And so those all become useful to ask questions to make suggestions, um, and to understand what is possible and what's not a lot of data science can feel like magic and but it can't do everything. And so you have to understand where the constraints are in order to be able to figure out what to tell your teams to go do or to try to go do. Um, one of the things I think is important is to realize that you also need to be willing to make mistakes and willing to fail. And so there's definitely times when I have sent my team off to go do something and it turned out it wasn't possible to do like we went and tried really hard and then realized, you know, PhDs in the corners of universities haven't yet figured out how to go do this thing. So it's probably not possible. And then it's your reaction to that. That's important. You have to go like, okay, cool. Now we know and we'll go down a different path. No way they can't figure out you fire them all. Get some smarter people in the room. That is sometimes the management strategy. That's not the one that I recommend. Um, just just throwing spitballs. I think we're very much in a world. Uh, one thing that I've noticed is the world of, of test development and to some extent, the world of development is a much more constrained world, meaning you generally don't send people to go do something that you don't know is possible to go do. It does happen. Um, and you can go read like show stopper about the building of NT and there were people that were sent off to go build a graphic system that they didn't even know was possible. And they went off and I think lived on a boat for like three months and came back with a, with, with GDI or whatever, uh, out of the system. But, um, that's rare. Most of the time you're sent to go do something that everybody knows is doable. It's just a question of how long it takes. When you're in the world of data science, you don't know if there's enough signal among all the noise in order to be able to generate what you need. You sometimes will go, we'll get something and realize all I can tell you is there's no conclusive answer and that has to be okay. There's another tangent. This is my job to make tangents today cause I don't know nothing about no data science. I know a little just enough to freak Brent out, but for the younger of the three listeners, there's only one left of two are here. Um, one thing I wanted to call out, it wasn't just that the guys went off and built GDI, but it, you know, in seclusion, but imagine if you will building complex software without the internet. Yeah. Without stack overflow, without Google or Bing, you can't just ask somebody how to go do it. Uh, what an archaic world. My job today, trees that will give you a hint. Yeah. My job today would, would have been so much harder to get to. And, um, I'll double quote master without the internet for sure. One thing. And yeah, everything is faster. Uh, the reason that we're data science as a role is cause we're moving so much faster, both for necessity and for, and because, uh, possibility. One of the great moves about moving to a lot more open source at Microsoft using things like Git for source control is all of a sudden, uh, we used to have like horrible internal documentation for our internal tools, if they were documented at all. But now that we're using a lot more stuff, open source answers are on stack overflow answers are documented in blog posts. It's, it's, it's been, it's accelerated. At least one fantastic thing our team does, I pick on our lack of data science, but, uh, we have a very good modern engineering system. It's all builds in Azure. I find them red. Woo hoo. Use a lot of VSTS get first source control, uh, the whole crap ton of open source and enables us to move pretty fast despite ourselves. Yeah. It's, it's interesting to leverage the new world, but it's also changed the way that people learn. It used to be, you went and you got a book and you read and you got, and that kind of forced you to get an overview of the whole area and kind of get deep and realize how the pieces are connected. And now you almost have, um, a paging system where you don't even have the data until you need it. And then you go to stack overflow or you go to a search engine and you find it. And so you kind of piecemeal build this thing up in your head, but you don't understand the big picture sometimes almost ever. You just learn the pieces you absolutely need to learn. And then you move on and that's good and bad. Yeah, I agree too. I call it just in time learning the, actually to that point. So you've been here at least 15 years, right? About 19. Okay. So almost catching up with you guys. The, the, the situation you went through three years ago, where you were, um, told to go into this space, which was fine. Cause that's where you wanted to go anyway. If that had happened 15 years ago, what do you think would have happened to you? It would have been different, right? It would have been, you know, go find books and read a lot of books and then, and then figure out how to go do it from there. Um, which is a lot harder. The hardest part actually would 15 years ago is there's no tooling. So 15 years ago, everything was proprietary tooling. Everything costs money or the company had to go build it. And so today I can just go and I can get our, or I can get Python and I can have basically every data tool I want at my fingertips and a bunch of documentation on how to go use it. And examples everywhere. And examples everywhere. Back then you probably would have to go get a book that told you here's how logistic regression is implemented in math and you would have had to turn that into an algorithm and write your own logistic regression routine. I, as, as our listeners know, I've gone back to school on this. Um, I don't, if, if this had happened to me 15 years ago, I don't think I'd be in this role. Right. Cause it, it's, um, it would have been a lot harder to, to quickly, first and foremost, online academia didn't exist. Right. Um, so I would have had to gone to U-Dub or some other, um, travel situation, which would made my life a lot harder. Uh, so much so I may not have done it. Yeah. I mean, you kind of had to, I actually had to some extent that I was strangely enough, I was, uh, in law school when I started at Microsoft. Um, and it didn't have much in the programming background. So I went to U-Dub's website that existed, but didn't have much resources on it. And I, I figured out what books you read for a, uh, CS degree. And I read those books. Um, I went to, you know, the local borders and I bought them and then I read those books. So I have like half of a CS degree cause I didn't do any of the homework. C plus plus programming in 21 days. Probably not. Not so much that. Yeah. This was Hennessy and Patterson and a bunch of those books. So I was talking with one of my children the other day. Um, actually they were listening to a podcast and they were talking about you are so cruel to your kids. And I'm leaving that statement in the podcast. They think it's cool. They think Allen's cool. Yeah. So that you drug them too. That's awesome, Brent. Nice fathering. And they were surprised cause there was something you brought up. I think it was your music major. And they were like, Allen's a music major. I'm like, oh yeah. I mean, there are, um, I don't know what the stats are, but there are just a ton of people here that didn't grow up through the prescribed and defined academic route. Yeah, there's a good fraction of us that are that way. It's still a much harder route to go through than going through a computer science degree, kind of traditional route to get here. Um, both organizationally, like it's hard to get through the gauntlets. If you don't have the right degrees, um, and it's hard to get the right level of knowledge. Self taught people tend to have big gaps in their knowledge a lot of times. And so it's much easier to have a better sense of how everything works together. Um, if you, if you're forced to learn all the pieces in between, um, that usually means you don't know the, the details of any given piece as much, but you know how they work together. The self taught people today, um, the way I've seen, I don't see a whole lot of self taught people today in the resumes that I look at, but the ones that I do see have been, have a, a large number of examples of things that they've shipped and applied. This goes back to what Steve was talking about earlier. I think the way Steve and I self taught ourselves gave us those overviews that are missing with the way people are self taught today, which is the just in time learning of just a little chunk here and there. So I think it's more difficult if you're learning that way, cause you really do need that overview cause systems thinking, whether it's data science or testing is so important. I think Joel Spolsky used to talk about, uh, he's created stack overflow. One of the guys who created stack overflow. Um, and he's talked about abstractions and how abstraction layers are useful, but they always leak and you have to understand the level below that abstraction layer. And I think the problem with a lot of today's learning models is you only learn the top level of the abstraction and you don't learn anything down below, you know, and that, that shows, I've seen people in interviews that will do things like they need to concatenate two numbers together. So they will turn them into strings, add the strings together and then turn them back into numbers. I'm like, yeah, that works. But what, how much work just happened in the system in order to make that simple thing happen, you could have multiplied by 10 and you know, added. And instead you, you know, did a whole bunch of parsing and conversion and then re parsing. Yeah. But you told me to concatenate so I concatenated. So technically it got the job done, but not in a way that was, I'll save the long explanation so they can get onto the bag. But I'll just say this for the first time in my career, I'm working on a team full of JavaScript programmers. They're a different breed. Yeah, they are. It's a trip that they're at an abstraction level way higher than I've ever worked with. Hey, it is time. And I've never done this in front of anyone but Brent before. Mail bag. I always wondered if you did that live or if that was just in post. Apparently it's live. Live and a little reverb later. It'd be good. Yeah. It's really hard for him to get the the reverb across. He's he's been working on this for a long time. Yeah. All right. So we have two mail bag questions. We'll see if we can get both of them. I'll go in order. And Brent wrote down the names for me because I always get them wrong. So Abraham Lincoln said, no, or score. And I memorized that in seventh grade, I think, and still have like the first two or three lines memorized, but I won't do them today. Percy asks, where or how do we start with this data democratization thing? That's the word Brent made up last time. That's apparently people had heard of. My company has BI data and analytics group, but it seems that only senior management are the ones getting insight from what information they provide. So how do you start? I'll just bring it to your word. How do you start with this data democratization? Well, you sweet. I am retroactively be going down as record as the guy who invented data democratization. That's fantastic. It's one of those time travel movies where you invented it in the future and then brought it to the past. Right. Just second, my mind settle after that. Okay, go on. So Percy has a situation where they have a data science BI team that essentially services the executives and he wants to know how does he get into data democratization? Well, so the data democratization means you publish your data, you make your data available. I think he wants to be a consumer of it. So what he needs to do is start off with what actions does he want to improve the productivity or efficiency? He is a manager. What actions does he want to take to improve the productivity efficiency of either his team or his product? So you start off with what are the, where do you think you can make better decisions and take stronger, faster actions. Then what you do is you go and you just talk with that team, say, hey, I have this concrete situation that I think you have the data for. Do you? If they don't, then that's also going to be a good team to get advice from on the data engineering front. In order for you to democratize data, you have to have the data. If your BI team has it, then work with them to figure out how to share it. Now, I'll tell you, a lot of these guys will view themselves as, oh, we were the guys that build the scorecard for the execs. So you may need to be willing to pony up resources in order to share that data. The thing is that most of the data doesn't exist. It's not created by the BI team. It's collected by the BI team. Yep. So one possibility is to go to the team that they're collecting it from and say, hey, you have this database with this information in it, or you have this Hadoop server with this information in it, or you have this, you know, spreadsheet with this information in it, whatever your form of information is, could you please share that out company wide or to a much larger group of people, you know, not just me, because that doesn't really, that's not data democratization. If you say you have this data, can you share it with my team? That's not really democratizing things. That's making yourself part of the special, you know, part of the elite. The better model is to say, Hey, I need access to this data. There's others that probably do too. Could you make it more broadly available? Could you lower the access rights on that server and start having that conversation plus any data that you control demonstrate how it works, make it openly available. A lot of times people are scared to open their data because what might happen if somebody gets ahold of this. And I think too often we are, there's no advantage to opening and there might be some risks somewhere, some existential corner case risk where we're like, okay, I don't want to share it because it might cause, it might leak and this might happen. And oftentimes it's overblown. And so I think it should take a little bit of risk, but not much. And just, the situation that I encounter is, is I do encounter that the teams that I know that lock their data, I would say the primary reason is because they know there's a lot of tribal knowledge baked in and how to use it correctly. And they don't want to be randomized by people doing it wrong. Right. Which is why I basically say what I told earlier in this, in this episode. I will say that if you're getting the data in this, in this space, you, you do want to, so Steve is right. The data is being gathered by the BI team from some place, but the BI team most likely has already done the work to clean and check that data. And to encode the tribe, the correct tribal knowledge, start there first, see if they will share it out. If they don't, then yeah, you may have to go back and reinvent the wheel, but you should figure out how to make sure you can bootstrap that domain knowledge and not reproduce it. Right. It is always difficult. Data democratization solves the access to the data, but doesn't solve the ability to use it correctly. Right. And so it still really requires a domain expert in order to be able to use it correctly. Agreed. Hey, we have two male bait questions. We do, but I only sing the song once. Henry Golding, who says, so chaps, how long do you think it will be before data science becomes just another tool in the specializing generalist box rather than a separate thing you need to spin up a big team of specialists for, which I think is a really interesting question. I will say, uh, my answer for that is actually pretty short. Um, it ran. You have never had a short answer for anything in 46 episodes of AB testing. I can't wait. It will be a very long time. The reason why is, uh, data scientists, uh, in the tooling is advancing, but the rate at which we're acquiring tooling and the rate at which we are acquiring data science is not keeping pace with the rate at which new data sources are coming in and not keeping pace with, with our need to go deeper on data science problems. So we are currently just scratching the surface and we have a lot of tooling. Um, but as more and more data comes in, we will have the need to improve data engineering. There's this sense that, uh, at least in my world, that there's a sense that we're always six inches just underneath the top of the water. Yeah. I think that you'll, it will not go away and just become a tool, but I think it will become, I think you'll always need data scientists be pushing the edge and to do the more complicated things. Um, for the primary reason that the thought processes of programming and doing data science are so different and the knowledge bases are so big that you can't keep both of them. You cannot be an expert in both at the same time. That said, I think that the pieces of data science will move more and more easily into the basic toolkit of your generalizing specialists and your specializing generalists. Um, and so they will be able to do more and more parts of it, but they won't be able to do some of the more advanced parts. And I think what it defines an advanced part and a basic part will over time, that boundary will continue to move forward. You know, it used to be five years ago, if you were data science, you had to be a PhD because you had to go invent all your own algorithms. If you wanted to go, um, do, uh, some deep learning, you had to go invent your own neural net and you had to go know how a back propagation worked and all that kind of stuff. Today, you just go and use the package in Python or a package in R or you go to Azure machine learning and you just say like, please train a neural net for me with, you know, 18 levels go and it will just do it for you. Now, understanding what that is doing and how to tweak it is difficult, but it's easier to go do. And if I just want to go figure out the median for some values or I want to go and create like a simple regression, it is very easy to go do. And it doesn't take a lot of knowledge to be able to go do the basic versions of that. And that will move over time up the stack. Yes. And so what we'll find is a lot of that there'll be a broadening that a lot of the simple problems will be readily automated away. But to Henry's question, there probably will be, I don't know, I'll just make up a number five years from now that the tooling will exist that the generalizing specialists or specializing generalists will be able to solve 80% of the problems. And there will be environments where solving 80% of the problems is sufficient. I think all the years of today's problems. But I think, yeah, as always, you'll get to more and more complex problems as you can solve the easy problems. It will depend on on the competitive advantage like it. The as right now we're all competing on the basis of speed in the world. And I think once everyone is kind of going at the same pace, then there's going to be other things that that are the differentiator in the market. And I think that's where the deep data scientists will come into play. I think you're both wrong. Not totally wrong. I think I think there's another spin on it that I'll get to before I run out of time. I think both happen. You guys are talking right. I sit back and listen. It's like you're talking like Microsoft data scientists. But remember, if I'm on a team of 20 people making some cool app, absolutely. There's a twist on Henry's question. I need you're not going to have a data scientist on the team probably in 20 people, maybe not even 40 people. But you absolutely as part of a team of specializing judge generalizing specialists, you need someone with some data science chops, someone who's gone through a couple of course, not a man, an expert, not a PhD for sure, but gone through the course or classes, you know, read lean analytics. For example, they have some idea of what to do. This is the role I play a lot on my team. So I can advocate a little bit there is I am not a data scientist. I know enough about it that I can tell them we're doing stupid things or where there's things that were obviously needed to be doing. We are now ramping up with some data science people in the org to help cover some of the things that I can't get to. But I think you need both. At some point, you may need a team of data scientists we as you get bigger, and you're making more impact, you have more data coming in. But absolutely, I wouldn't say I wouldn't say there'll be a time when I think the time when data science needs to be a tool in your toolbox is now. Right. I think the time when it's only tool in your toolbox, and you don't have specialists is a long ways away. But I think it's becoming more and more democratized. There's a subtle every year. I think it becomes a tool now. It'll be. And so I want to make that point. But I think definitely it's probably never just that, especially, it'll always be with scale, but it'll always be moving on. So I'll invent another new term today. This is Brent's continuing short answer. Expert systems. So an expert system is a new newish asset in which you apply data science, automation, machine learning, and you apply in a rigorous fashion domain knowledge from particular teams. And I will say, for example, 60% of what I do is analyzing histograms. And there's a very succinct period or a rigor around how I do that, what I discover. And that's, I would say, readily automatable. Right. So once an expert system or something similar comes into play that can generate these things, do the if conditions determine what further analysis needs to be done and then output, go do this. This is what appears to be the problem. I think that's certainly doable in the next five years. And there's tools that are making it easier and easier all the time. Like the new version of Excel has histograms built in in a reasonable way, which it never has had before. Power BI is now fairly democratized. You know, there's other platforms out there that you can use to analyze your data and get a lot of sense for it just from the beginning with it with very easy tools to use. Very cool. Hey guys, we're out of time. Hey, thanks Steve, which means shut up. Thanks Steve for coming. Yeah. I guess you just showed up anyway. It does. If I didn't show up, you would have been what? As testing? What's that ass testing? You need me. All right, then. All right, I'm Alan. I'm Brent. And I am neither. Awesome. See you next time. Bye guys. 
