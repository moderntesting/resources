Hello, buddy. Hello. Hey, I'm Alan. I'm Brent. Hey, Brent, what episode is this? 25? 25! We're a quarter of a way to 100. Yes. By the way, this is the AAB testing podcast. Welcome, everybody. And Brent is a little distracted because I have my laptop plugged into the TV monitor in this conference room, and Brent can't stop staring at the little sound files going by. Yes. So I'm thinking I should unplug this so you can actually focus. That would be fantastic. Okay, there we go. Not on the monitor anymore. Back in the room. Ah, doesn't exist. Brent's one of those people who just, wait, look at that. Squirrel! So, gosh, we just got off a long phone call with talking to some folks at FedEx IT about adopting Agile. I had fun. Did you have fun? That was a lot of fun. One of the things just I want to talk about briefly, briefly, is a realization. I saw some, you know how you see something happen, but it isn't until the fourth or fifth time you see it, they actually recognize the trend. And when you take a team that has not done just any sort of culture change, but in this case, Agile, they haven't done it, they kind of want to know about it. And there's a point where you give them just a little bit too much information where they kind of wet themselves. Did you feel that? Yeah, my filter for avoiding that just doesn't function. You are, you are as to what? It was a lot of fun. The thing I picked up, and the thing I picked up is how similar a lot of the questions and the problems that they were asking is based on the stuff we've been talking here. They sent ahead sort of a sample of their questions and it's like, yep, yep, talked about that, talked about that. So I should send them to the podcast and they could learn it slowly. I don't know. Yeah, we can get the fourth listener. I like that my life is complete when the one of the three hashtag actually trends. So that's never going to happen. But so my life will never be complete. But that's a topic for another day. Did it trend? No. One person had a hashtag one of the three. I thought, what a great hashtag. So that hashtag will be when we do our AB testing t-shirts on the back will be, that's what we'll say, be hashtag one of the three. One of three? One of the three. Sure. All right. Is it one or spelled out? As you can tell, these details will work out later. Yeah. Working off the carefully crafted script that we've prepared today. Not. We haven't talked in, it's been like maybe three weeks because job, busy stuff, and we're, and Brett and I are separated by eight miles now. So it's a little harder. Yeah, we haven't completely worked out the details. Thank you for coming back to my building again. Yeah. You'll come to me one of these times. So my new job, just a little checking because I haven't had a chance to blog much about it, but it's, I'm still really digging it. As you guys know, I have joined a team with no testers. We have about 70 developers and there's me to be the quality guy, the chief risk officer. Sometimes there's a great McConnell book from one of his books where it talks about a product needing a chief risk officer who is part chicken little, the sky is falling and part Eeyore, it'll never work. So I play that role sometimes. And I spend a lot of time sort of highlighting the obvious going, Hey, how about this? How about this? Work a lot with program management. We have. That's unexpected. Why? Why? Because my job, I better abstract for a minute. I, what I've done forever in my career at Microsoft is figure out what's not, not getting done. That needs to be done and make sure it gets done. Our data we are work item tracking bug tracking, et cetera, is in visual studio online TFS. And the data is one inaccurate and two not up to date. So maybe that's, those are the same things. So I put together a lot of massively cool pivot tables. Um, well actually those are, that's, those, that's an oxymoron, right? No. Okay. There are such thing as cool pivot tables. So I made some pivot tables. I highly doubt it based on the, in your case, based on the content. So what I wanted to do, um, one thing that wasn't getting done was reasonable triage, uh, uh, to work tracking hygiene, et cetera. People weren't doing the right work in that system. Eventually I'd like to get rid of most of it, but in the meantime, we're not tracking our work very well. And so I put together, uh, just like a little, you know, pivot table spreadsheet, pulling data from TFS that, uh, just sort of grabs things like, like what, from the teams we have, what are their, what bugs do they have? Do they have old bugs? Cause old bugs pissed me off. Um, yeah, yeah. They pissed Brent off too. He just made a grunting sound. Yeah. So, um, I really wanted, and again, it's about pro culture change process change. It's about boiling frogs. You turn the heat up a little at a time. The first thing I need to do is kind of highlight what is actually happening on the team. And I say, and I'm going to track this and I'm going to assign colors to these things. And you don't want to be red because red means I chase, I, you know, I do whatever it makes do. People don't want to be red. They're going to yell that by other people. So teams bought off on this. I, I, I put together these metrics. I said, Hey, here is, uh, what I think we should drive for. And those numbers are going to change over time. I'm going to drive the team down to very low bug numbers that they carry right now. They're carrying way too many. Also at the same time, the data is invalid. Like we have N number of bugs across the product. I think the actual number of known product bugs is less than half of that because the team has done such a horrible job, uh, following up and closing bugs as things change, et cetera, et cetera. Same with, you know, not marking work items is done so that the data is untrustworthy. But the fact that it shows up as red on my screen. Means what? It means that there's work to do there and they can get rid of it. If you have, like if you have 42 bugs in your area, that could either mean you have 42 bugs in your way in debt or oh yeah, these, none of these 35 of these aren't valid anymore. So, uh, I can go take care of those really easily. Well, this is going to drive initially something like this as an example, we'll drive is getting the data cleaned up and then I can drive the proper engineering behavior to happen. So as one example, what's your goal? The goal is to ship, uh, initially it's to clean up. And then I, as we mentioned in the last hour in our other call, I'm a big fan of having that product ready to ship within a day or two days ready to ship, ship. And you can't do that when you're carrying a large bug, a large bug debt. One thing we do right now is other thing. One of the other things I'm in charge of, I kind of took over is, uh, daily quality. We have, we don't have a production environment yet, but we have a continuous integration environment and a dog food environment. And I, we have some very minimal gates on our CI environment. Um, we have a set of automated tests that have to pass and then we have some additional tests, including some, uh, exploratory verification that has to happen before we go to our dog food environment. And we're at a point now where at least once a day, there will be, not at least once, but at least once a day, there's a break in the CI environment, which could have been prevented by someone running those tests, uh, beforehand. They were lazy. And couple of times a week, we have a break in trying to promote to our dog food environment because somebody, again, things that are preventable farther upstream. So, uh, one example, what I'm doing there is one shifting that right. I hate that term. I should have said that. Uh, I want to get all those preventable bugs, uh, taken care of, but at the same time, I want to over time raise those quality bars. I think the bars that currently we have on getting through our CI environment and up through to our dog food environment are too low. They're bare minimums. I want to raise those while updating the frequency. When should we update our dog food environment? For example, when I joined the team, they were updating. The goal was to update their dog food environment once a week. I'm sorry. Sorry. Once a day. And for a lot of people on that team who come from other teams, I was like, they didn't want to do that. That was too much. And my goal is to get to twice a day, uh, until we raise those bars and make that whole process, uh, very easy to go through. We're not going to get there. So, so what is the KPI that you're judging this thing by? What's this thing? So you guys can't see it. Um, Alan's got his spreadsheet chart. It is a beautiful pivot table. Um, but I hate everything about it. Yeah, I figured you would. We could talk more about it later. I brought it up so you could, you could, uh, yell at me about it later. No, this is temporary. I'll, that's what I assume. Excel is great for tracking. Uh, this, I put you this, I just want to be able to track this data. Not even track. I want to be able to show the accurate data based on the current state of what our, uh, database showed about our product. Eventually, this will move to some sort of reporting site. So I can at one, make it look a little better to get trends, which I think are really important, et cetera, et cetera. I think you're missing the one key goal from this though. And which I don't think is, I don't think you went to this team to, to become their glorified release manager. Oh no, no. Right. So do you envision some period in time, which I hope is relatively soon, um, where this table just goes away? Absolutely. This is basically the highlight where we're are and to, and to use it to drive behavior change. So what is the behavior you want? I want us to be able to trust our data and to see how, and for the one trust our data, but to the eventual behavior is, uh, some transparency across the team to know where we are, what we have to do. That's a good step. I think, I think there's another one there as well. Like you mentioned that you want to be, um, much more near real time in terms of your deployment model. So I'm wondering, have you considered using this as an opportunity for that? Um, in other words, rewarding individual teams for, uh, the teams that are able to get to the highest number of zero or end days without a bug in the database. Right. Cause what you want to encourage is them being much more near real time when they're, and that just goes to a philosophical thing around, uh, carrots and sticks around metrics. And I rarely, um, especially not in a large group individually, I may give you a stick meaning, Hey buddy, you gotta clean this up below a block, but publicly I'm all about the carrots. Meaning, uh, as T as teams are doing really well at something going to celebrate that loud and transparent. Now, when I look at this and the behaviors that I've seen from traditional dev teams, right? This looks like every or any, it's much prettier than, than others I've seen, but every team has an RM that pulls together a bug chart similar to this. Um, I don't think in terms of you wanting to achieve your goals, I don't think you're going to do that. If this turns into another facility that, that your dev teams used to that creates that co-dependency between an RM and dev teams. Like you want to have a goal where you're celebrating those teams that are, this is a bootstrapping table. All right. This says our current assets, we got to pay down this technical debt, but the ultimate goal is the teams that are able to RTM their component on a daily basis. When yes. Right. And if, if this creates that co-dependency where they're not paying attention to their bugs because they know Allen is you'll not get there. Yeah, that, that is not the plan at all. Yeah. I don't even, I won't even own this. This was just as an example, something not being done through together. So here we're going to use this to, there has to be a way to get each individual team to care. Yes. Right. Um, do you guys have a stabilization period? No, I almost, um, I almost used a bad word there. I almost up the rating of, uh, AB pot, AB podcast rating. If there is one, uh, no, I don't like, um, on one note, not a plan stabilization period. Eventually I talked about that dog food and CI environments. We will add a prod environment at some point soon, um, which will update at first and probably for a while, less frequently than dog food. Uh, because we'll want to bake something a little bit longer, look for more edge cases, things that, because right now with our dog food environment, for example, with our dog food environment updating every day, uh, we actually don't know what it's like to run on one server and one, one server and one client for longer than a day. The going back to your table, one of the things that I was thinking through, if you were to measure the, the average age in which each individual teams knocks out a bug and then say, we're not done until everybody here consistently, every bug that needs to be fixed is fixed within 48 hours of identification. So measure their team on their turnaround time, not on their account. Again, one step at a time. I, I hate counting things, uh, but the snapshot is helpful. One thing for context on this team, um, and it's just so, uh, we're all on the same page. I actually unplugged the monitor again. So Brandon can no longer see the pretty spreadsheet. This team didn't exist or just turn over when they formed three months ago. Uh, brand new product from scratch. You've been there a month, a month now. Okay. So a lot of this is, uh, they did the thing where you're a brand new team and you kind of start off a little cowboy style and nothing wrong with cowboy completely. But, uh, time to, uh, I thought he's a bad metaphor about putting a saddle on the horse, but I don't know. Stormin Norman Foreman performing. Okay. Uh, so anyway, other than that, uh, one other thing I did right off the bat was, uh, we did just as I was joining the team, they did a demo for a former executive Microsoft and he really liked it and, and some other, uh, executing our CEO and was there and they said, Oh, we want to use this. So, uh, one of the things was the first things he had to do was, Hey, Alan, a couple of really important people want to start using our product. Uh, can you, can you make sure we have the right quality for whatever we're going to give them, figure out what features they need, what level of quality we need and take care of that. And, uh, as of last week, uh, they are invited and, uh, and we have very important people using our product and liking it, which is really cool. So I'm excited about that. Uh, and we'll get more on board soon. So, uh, it's kind of fun. When, when do, uh, lowly people like me get to use, you know, I could tell you, but then I would have to kill you. Not because you, you would know him, but I, I, I probably not allowed to, uh, to tell people. So I'm going to, I'm going to defer that, but let's just say very soon, every, every bit of feedback we've gotten, we've received gotten who the hell says gotten every bit of feedback we've received. I just did, uh, has been not about, Oh, you need to do feature X. Oh, you need to be feature Y. It's like, how can you reel this in? How can you get this done? Polish and, and get it out sooner. So I like that. Uh, I like that direction. So soon. The answer is soon. What problem is itself? Are you allowed to communicate? Let's talk about that later. Okay. All right. Uh, just to be, just to be safe here. It's, I think you'll dig it. So in addition to pulling together bug charts and such, we've been there a month. Yeah. What are you finding as, as big challenges or, or what are the unexpected challenges that you're finding that you didn't expect or things that you expected that are much easier? Well, I'm going to answer a different question because, uh, I don't know if anything was totally unexpected. This is a team of, I mentioned 70 developers. There are exactly three of us that came from a test background and the other two guys work mostly on build pipeline and including some of the, getting some of the automated tests into the CI, things like that, uh, building that quality culture among developers and getting them to understand. This is going to tie right into another question I want to talk about today, but getting developers to understand what it means to own quality and for your feature and test it. And, uh, one of the things to talk a lot about with my new manager is, is how do we build that quality culture? How do we build us? People get like, so they get the idea that, um, they need to really own not breaking crap. There's a great threat as an example, great threat I had last week with one of the developers on the team around. He came into work and said, Hey, our CI build hasn't updated in eight hours. This is supposed to have the latest and greatest all the time. And I tell him, well, the tests haven't passed. He says, why should that block CI? I want the features in there. We can fix those tests later. And part of me, there's the part of me who wants to go. But, and then, and then part of me wants to like go grab Martin Fowler's article from 15 years ago on CI, explain how CI includes a rich set of tests, tests passing. We have this minimal set. And then when I realized those are the first two thoughts went through my head, like in five seconds. And then I realized there's really just a lack of education and education is the wrong word there. Maybe it's, uh, awareness, transparency, like what exactly do these things mean? And like, there isn't a shared understanding of what our CI environment is for. Uh, and we're not talking about unexpected. Um, I'm probably most surprised by so many things that I find, you know, some transparency around definitions of what things mean. That is when a team is fairly small, there's not a lot of shared understanding of like what environments are for what things mean, what, uh, how, you know, some given that they're highly cowboy, I guess that isn't unexpected just yet. Yeah. And they're not real. And again, I think the cowboy has been reeled in mostly. There was a big push to get this minimum viable product out there. And I don't know if it's what you intended to say, but one of the things that you said that I found very interesting is no, the CI should have happened. Um, this, this code base is supposed to have the latest and greatest, but what you said was we can fix the tests later. I didn't say that. That's what, that's what I heard. In other words, Oh, yeah. And then the, the, the features work. If the tests are failing, it's because the tests are wrong. In some cases, yes. But the tests are wrong because the developer changed a feature that in a way that required a test update that they own. It's been years since I've had to deal with this particular product. For me, it's been about three days. Yeah. Um, so yeah, interesting. So, so a lot of, uh, you're doing a, uh, cool new sexy product, right. Uh, modern age product. With what? 2003 execution model. Hey, are you even that high? Uh, considering the last team I was on was about 1993 execution model. I'm pretty ecstatic and I figure with this size of a team and my, and my, I'm confident I can bring this team up to the right decade within the next, even within the next couple of months. That'd be great. So that's, I'm, I'm totally confident on that. Uh, part of it is, you know, you just don't, again, I've talked a lot about with my manager around building that quality culture and you can just demand it from the top, but you need, but that's really risky and you want to find ways to, in my experience, finding ways to kind of find pockets, you know, celebrate things are going well, promote it organically. There's all kinds of low hanging fruit that I think will help, uh, each one of these could bring the team forward a full year. And it's, and it's a practice, but I think we can get there. So the key thing is to, is to measure what the business wants and not necessarily measure the symptoms. Correct. The behavior of the business wants in this particular case is, I think that, that developer freaking out around the test failing and, um, his response isn't, uh, Oh, Alan, we can, silly Alan, we can fix the test later. Right. Uh, his response should be, Oh crap. Really? To me, it's, it really comes down to, uh, this is a more junior developer, but a very vocal one, uh, really comes down to lack of that lack of shared understanding what the environment was for. He, uh, it became very clear in our conversation, the, he was using the CI build as his private test bed rather than actually building and running it locally on his machine for that. He wanted to be able to go to someone and say, Hey, check out this feature. What do you think? And use CI for that. I don't, I don't see a problem with that. As long as you, well, as long as you understand what it's for, go ahead and do it. Um, um, this is something my, my dear old dad used to say is, uh, not going to condone any sort of, uh, behavior. You, you make up your own mind in terms of your, your, what you want to do and why you want to do it, but you must always be willing to pay the Piper. So if you're going to do something that sort of is leapfrogging over the, the rationale for the existence of the system, go ahead. Maybe you'll find some innovative way to, uh, move things forward. But if it fails, you need to be the one accountable for making that, that decision. Um, All right, you're done. I am. So one thing, uh, that we touched on that I want to talk about is last week, no earlier in the month, two weeks ago, there was an article on SD times, um, by a Martin Mudge and I'm not going to read the whole article, but the title is why developers shouldn't perform software testing. And the article was about, you know, it's a lot of these, what I call myths I've heard before. And I had a prominent software test for Tommy. It's not a myth. It's true. So the, the concept is, is like, I, you wrote this code. You couldn't possibly write a test for it. And there's a summary of the article. Uh, and to me, I think because you wrote that code, you absolutely should know how to test it because you, you know better than anyone, how it works. I think where, let me finish for you, for you to make that, uh, that judge there, that, that jump there is in a profession where software testing where people value themselves on their critical thinking. I'm sort of blown away again by this, uh, black and white binary view on developer testing. Developer testing doesn't mean that they do absolutely every bit end to end 100% of, of all the testing needed for that component and how it fits into the product, et cetera, et cetera, et cetera. Uh, that is a little crazy. And although I, I, after saying that there are, there are places where that is absolutely correct where you can do that and then get everything else you need from, uh, telemetry, et cetera, et cetera. But I think in the bulk of cases, we absolutely, I, as a tester, as the only sort of quality test guy on this team of 70 absolutely need every single developer to own, uh, unit functional integration tests for their component. I think they're highly capable and there is, it's absolutely not a myth that they absolutely are the, maybe not even capable of doing that, but in many cases, the best people to do that. And if not, we could help them get better at that. But the, the, uh, well, I'm still up on my soapbox. This binary view of either developers do no testing or every single bit of testing is ridiculous. And even last time we talked about this role of the functional tester going away. And by functional tester, I mean the guy that writes all those unit and functional integration tests for the developer, uh, that absolutely, those tests absolutely need to live on the developer's plate as far as making sure the product works and making sure the screen doesn't turn pink and doing exploratory testing. Absolutely. Probably not the right people. We inspired actually our last, uh, podcast inspired a, uh, blog. Yeah, Josh. Yeah. Although right now it's not, uh, his message seemed to be aligned with kind of both points of view. Anyway, back to your, your system test. It seems like it's an important thing. Great. So with, with my data scientists hat on, like why wouldn't we want devs, um, testing their own code? Uh, and it's, uh, in a nutshell, it's, it's, um, confirmation bias would be the main reason why we wouldn't want them doing that. Right. As they wrote their code, they had certain assumptions in their mind and their tests are going to carry those same assumptions. But then the, so I, I see where those folks are coming from. I don't agree with it, but I see where they're coming from. Well, again, I'm going to throw out the, the, the critical thinking card here for a moment and use a technique. Uh, I'm sure it's a name for this that I'm going to get lambasted for not knowing, but whenever I look at a problem that someone treats as binary, whether it's waterfall versus agile, whether it's, you know, developer testing versus no developer testing, take a moment to examine both sides of that, uh, argument. Cause the answer is usually in the middle, right? So what would happen if the developer was in charge of every single bit of testing? Well, some user experience stuff may, may get through and you could brainstorm about, you know, you know, there's some integration points that automated tests can't find all fine, all valid points. So then think what about the other extreme? What about the test? The developer does no testing. What happens? Well, the tester spends most of his time finding functional bugs. Uh, you know, it's, you'd spend a lot of time going back and forth between bug fix and fix for the bug fix and fix for that bug fix and the emaggressions and related things. So that's not super ha super great either. So for that point, you have to, you have to realize the answer is somewhere in the middle. Yes. And, uh, I think it's very, and again, my exercises, when you think, when you see a binary argument is look at both extremes, think about what the middle looks like and think about, it doesn't have to be exactly in the middle. In my case, it's as far as that functional side. And I've used, uh, you familiar with, uh, the agile quadrants that are in, uh, Lisa and Janet's agile testing book that they took from Brian. Yeah. I'm familiar with them. I don't have them memorized. Look them up. I'm not going to draw them on the table here, but when I look at, uh, the agile quadrants, I think you did a blog on this. I did, but no, but no, actually I think more people now listen to our podcasts and read my blog. But that left side of the quadrant is all about the developer things. The developers should be doing unit testing. Those are their functional integration all the way up to even using, uh, tools like, um, uh, static analysis tools, dynamic analysis tools, all on the developers, uh, the developers should own that testing. Traditionally test has done a lot of that, um, if not all, but the right side where those user facing things, where you're doing exploratory testing, performance security, again, some of those can be done by the developer be really those things often fall in that testing world. But again, they'll vary from product to product, context to context, team to team, where you may have specialists on the development team that own those things. But the key thing is, is I think it's just ridiculous beyond like, uh, an oversight, but ridiculous things that developers can't own any testing at all. So let me give you another, let me get up in my soapbox. No, I completely agree. The person that argued with me, I don't think he, as he often does, he sort of, uh, I think jumped on my tweet out of context a little bit, which is fine. Um, I do that all the time as well. So no, no big deal there. But he, um, made a comparison of having someone proofreader edit your work. Like here's, you know, Brent, if I give you a paper I've written and I want feedback on it, that's sort of the exploratory testing of my, of my document. You're going to give me sort of that I don't, I've done what I can, but I can't find everything. I, because I'm, I'm too close to that work. Totally get that. But in the meantime, I have heavily edited my doc by hand. I've run spell check. I've run grammar check. I've, uh, you know, done a massive amount of, I haven't just done a stream of thoughts spit and said here, Brent, would you review this for my, for whatever, you know, I wouldn't, I would do that work beforehand. So that's a, uh, for me, that's a parallel. Like when I write a paper for, uh, whether it's a chapter for a book or a article for a magazine or a paper for a technical journal, I do a heavy amount of editing before I give it to that editor. So that is to me, the equivalent of, uh, the developer owning a lot of quality of that paper, but at some point passing it off for that second set of eyes to kind of get, to get it over that, uh, over that next hump. One of the, so again, going back to the developer contest or not, right. One of the key principles that they bring this up is again, they want to defeat confirmation bias. So then the question is, is there, if that's true or, or it's actually two reasons. You would want someone else to do the testing for you in order to defeat confirmation bias, right? Um, I can't tell you when I do a blog, um, I will post it immediately, but then I will, I will reread it over the course of the next month. And I'm generally doing edits every single time, because when I first read through it, there are things that I didn't, there are things that I don't see as flaws in, in what I've written. That's kind of an example of using, uh, uh, MVP for your blog. Like here's the basic concepts out there and whether it's reaction to feedback, your own editing, you're going to tweak that as you go along. Sure. Honestly, cause I don't have Alan's talent. Alan can write up a brilliant blog posts within around 20 minutes to a half hour. Um, me, it takes closer to four hours. So usually when I post it, I'm just done with that for today. What I've, this is an interesting anecdotal metric. I find that the, the less time I spend writing a blog post, the more feedback and hits it gets. I've noticed that as well. Every time I posted a blog and I'm like, uh, that is the worst one ever. That's the one I get all the, the five star comments and, and, and I'm like, okay, I, I just don't understand how to do this blog thing. Anyway, confirmation bias is one of the things. The other one is that there's a specific domain of knowledge that, that the developer doesn't have. Okay. Now that second one is often broadened into well, testing is a specific domain of knowledge and it freaking is not right. Perf that's a specific domain of knowledge, uh, security. That's a specific domain of knowledge. Uh, now this will sound odd, but even doing integration across multiple components is a specific domain of knowledge, even though it's sort of global scope, it takes a lot of work to sort of understand the business context around how everything should integrate on a sufficiently complex system. Now the question is, is can we develop a testing strategy that involves devs only or mostly devs that removes the, the dependency or removes the need to avoid confirmation bias and, or specific domain of knowledge. And the domain of knowledge, I don't really know of a very good strategy for that one. Uh, uh, you could hire a couple people, vendor it out. If you have a sufficient deployment system, you can even do, you know, fighting with exposure rings. Um, on security testing, you probably don't want to do that. Right. Um, but to defeat confirmation bias, right, at the folks that go, well, much like the editor example, um, the problem with that is that there is a assumption around how stuff gets done editing. You can't do pre writing, but testing, you can do pre writing. If, if there was a way for me to edit your blog post before you've written the blog post, then we could probably get rid of your editor as well, because if you could do it, you could be your own editor before the blog and then, um, start writing the blog. Um, does that make sense? You're staring at me like, um, yeah, I get again babbling on endlessly. Well, that's just how I look at you, Brent. Ah, gotcha. Hey, um, one of the thing I want to get up in my soapbox about, and then I want to go on to the, um, the, the, the fun item is, uh, you're talking about testing. What was your comment? So testing is not, um, uh, shoot, it's not a domain expertise. So he said, most of it. No. Oh, it's, it's, in other words, um, it's not a deep domain expert. Yeah. And, and to be clear, it's, uh, uh, people go, oh, well, my testing was important. Absolutely. Testing skills. Very important. One thing that really, uh, uh, just kind of pushes my buttons is, uh, I see tweets on this blog posts on this subject at least once a week. Uh, and I, again, this comes from a history of testing being very much sort of thought of as sort of this factory process. Like you go to the testers, go through it and the bug finders go through it and they find the bugs and they get fixed. It's a very factories. And there's this, there's this sort of inferiority complex almost that comes out of a lot of testers around trying to show that their work is, you know, uh, brain engaged and require, and with that, they're absolutely right. It is brain engaged. And requires critical thinking absolutely does. But the, the, the posts I see almost imply that, uh, like they, I'm not getting to my point very well, but they'll say some very insightful thing, not even insightful, it's very truistic thing about what testing is, which is really true for any sort of knowledge work. So I, and I dunno, I guess I shouldn't let it bug me, but yes, the world, we get it testing software testing is not a mindless activity that is like putting bottle caps on bottles in a factory. Uh, it actually requires thinking as does most knowledge work. If you don't know what knowledge work is, go read, uh, uh, like Dan Pink's a whole new mind or search on knowledge work on the internet. Cause everything you find there will be true of testing as well as anything that isn't working in a factory. Thank you very much off of myself. Box just, just, just had to, your, your comment made me think of that. I go, you know what? That pisses me off and things that piss me off somehow find their way into Alan's rants on AB testing. We have talked about this on numerous occasions that we have the similar opinion that the best testers are ones that have a strong suit in systems thinking. Yeah. Absolutely. And the big picture, the forest for the trees, et cetera, et cetera. What with that in mind, like the, the, I don't know that this is what bothers you, but it does bother me when I see a tester and I expect them to be strong in system thinking, but they can't apply it to the system of testing. It were, where it's this myopic view of I have spent the last 15 years convincing myself that this is the single most important job in on the planet. And I will defend it vigorously, but with out really any sort of consideration around the alternatives. That goes back to critical thinking. The factory model, you brought it up like one of the techniques that modern factories do is that they know how to control quality in line. For example, let's say you have a FUBAR, FUBAR widget, and your job is to apply four screws to the FUBAR widget, and then you let it pass that further down into the chain. One of the techniques that one of the factory workers will do is they will actually, the first step is they will take four screws, put it in a little box right in front of them. Okay. And then from that box, they put it, they apply the screws to the FUBAR widget. Okay. Now, if that FUBAR widget goes down and the person goes put four more screws into their little box and they realize there is a screw already there, they're going to stop the chain right there and then, because there's a defect already moving down. Whereas the versus the old model, where they don't do that intermediary step, which I argue is the equivalent of TDD, they don't do that immediate step. They just go screw one, screw, screw. They screw the four screws directly to the thing without any intermediary step, and they don't have a signal to tell them that they screwed up. And so that model, you require a QC engineer. Is that the origin of the term screwed up? Yes. We'll call that a- I don't actually know. I doubt it is, but for now, today, hey, that's where screwed up came from. I'm forgetting a screw on the FUBAR widget factory line. Yes. All right. They failed to screw down, therefore it was screwed up. Oh, God. All right. I'm going to shut you up. You know why? Why, Alan? You know why? Why, Alan? Mailbag. All right, Brent. It's time for the mailbag. Yes. And what's on the mailbag today? Hey, I'm going to pronounce the name wrong, but on Twitter, pairs of ABBA, pairs. P-E-R-Z-E, pairs. I'm going to need him to send a sound byte with his name pronounced correctly. I apologize. But he had a question about tension metrics. And in one of Brent's blabs, maybe it was the last episode? I don't know. I think so. He talked about tension metrics. And the question is, WTF? What the hell are tension metrics? So, Brent, WTF, what the hell are tension metrics? Tension metrics, in essence, are groups of metrics that check and balance each other with the intent of trang- I'm being too verby- with the intent of- Use fewer, smaller words. You use multiple metrics that work with each other such that not only do you have metrics that are all intended to say light up green or light up red, but also to encourage the correct behavior for the entire organization. A common example is several services nowadays are talking about their service reliability. Reliability, in essence, is the rate at which people use it and have a failed transaction. So, reliability typically is measured as successful transactions divided by total transactions. Remarkably similar to test pass rate from the old days. Now, the problem with just using reliability as a metric is there is a way to game it. You could have, first and foremost, you can have your success transactions be calculated much more rapidly. You could over-publicize success. Or, for example, a common, just a much more simple way of dealing with success transactions is if you see a transaction that's en route to fail, stop it mid-flight and issue a retry. Just retrying it until it succeeds. That would allow people, if you had a dev team that implemented a retry facility to that degree, the reliability metric would be 100% reliable. We never fail. The problem with that one, of course, is the latency perception of your customers is that okay, when it goes into one of these retry storms, the latency is extremely slow. There is a metric that combines these two, latency and reliability, called availability, where it's around the time on a transaction that you're spending failing. An example, if you had success transactions take about 100 milliseconds and a failure transaction take 10 seconds, this is a real-world example, and you only fail one at every 10 times, that means you would be spending, you would have 90% of your time, 90% reliable service, but 7% of your time is spent succeeding. Now, if you've dealt with any services along those lines, hey, it works, but it takes a half hour, it doesn't feel like a functioning service. Availability enhances reliability, but you can, again, game it by multiple different facets, and you create an additional metric that locks down how that prior metric can be gained. So, you could give a team a retry, or the easiest one is every retry you just count as a failure. Anytime a transaction takes a particular too long of a time, you count that as a failure as well, regardless of whether it was originally tracked as a success or failure. A easier way of processing this, I think, is another example. A lot of teams in the Agile space today are trying to deploy faster. Now, one of the old-school techniques, particularly in testing, a lot of the preventative testing blocks us from being able to deploy faster, and as we get more and more of our test suite and the preventative suite, what I mean is the suite we run before we check in or release, that makes deployment times slower. In this new world, we don't want slower deployment times. One of the metrics that I've started flooding on my particular team is something I call time to block. So, I don't want to validate that the product is working, I want to validate that the product is crap, which is easier. So, I'm calculating how long it takes for me to block that release, but I don't want that slowing down my ability to get to stable production code. So, I have something known as a production cycle time. What that means is, if I'm going to reduce the time it takes for me to block, that's likely going to mean I'm going to be focusing on the most important test cases, and unimportant test cases aren't necessarily going to be run. I'm going to release the friction to getting the deployment out. But, my goal is to measure the distance between when the dev started writing code and when the code was in production and considered stable. So, if I have an easier time getting through the preventative side of the cycle, then I need to then account for the fact that I'm likely going to be doing QFEs or things along those lines. And, I need to measure, I need to be able to go which more successful, investing in the preventative side or investing in flighting and quickly reacting, because my goal is the length of time it takes to get from initial deployment to when it's stable. And then, the last metric is the rate at which, oh, that's actually the one I just talked about, that's deployment throughput, and then production cycle time is the delta between the stage start time and the product. Oh, how long is it? I think this is what happens when Brent prepares notes for an A-B testing podcast. We work much better kind of off the cuff. I'm going to close this. So, in a nutshell, and we're out of time, in a nutshell, what you want to do, attention metric, is you start with what you think is your initial metric. Earlier in the podcast, Alan was talking about bug counts, right? His initial metric is probably going to be bug counts per team or something along those lines. Then, you start thinking through, it's like testing, how can this metric cause the wrong behavior? What behaviors are going to be common with this metric? Hawthorne's principle, once this is published, it's going to change people's behavior. How is it going to change it in the way you don't want? What is a new metric to add that also judges the system such that there's no way for both of these metrics to pass and not get the behavior you want? If that's not sufficient, then you add a third or a fourth. Or tweak the metrics as needed. If Brent, that chain's going on and on and on and on, you probably just didn't design very good measurements in the first place. Once you get to around four, then four is sort of my boundary line where I go, okay, we need to rethink of this whole new thing. Eric Ries talks about a topic called the one metric that matters. Once I get to around four metrics, I rethink those four metrics and try to figure out a way to combine them into a single one that gets me the behavior that I want. All right. Very cool. Thanks, talking about tension metrics, Brent. But you're right, we're out of time. I got to run to a meeting and we got to get going. I got to pack my stuff. Okay. All right. I am Alan. And I am not. All right. We'll see you next time for episode 26. All right. Bye, guys. 
