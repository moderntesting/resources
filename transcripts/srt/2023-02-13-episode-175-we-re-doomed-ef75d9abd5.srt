1
00:00:00,000 --> 00:00:02,560
Can we just do no, we can't.

2
00:00:02,800 --> 00:00:05,120
But couldn't we just know we can't.

3
00:00:05,120 --> 00:00:06,480
Oh, my God. Why do you?

4
00:00:06,480 --> 00:00:09,320
Why did you just set up a daily one on one with me?

5
00:00:09,320 --> 00:00:11,800
Oh, my God. No, stop. Stop.

6
00:00:15,490 --> 00:00:16,610
Welcome to A.B.

7
00:00:16,610 --> 00:00:19,650
Testing podcast, your modern testing podcast.

8
00:00:19,970 --> 00:00:24,530
Your hosts, Alan and Brent will be here to guide you through topics

9
00:00:24,530 --> 00:00:28,650
on testing, leadership, agile and anything else that comes to mind.

10
00:00:28,850 --> 00:00:30,450
Now on with the show.

11
00:00:30,450 --> 00:00:34,330
I worry that we've already had all of the interesting discussions

12
00:00:34,330 --> 00:00:35,330
in the afternoon.

13
00:00:35,330 --> 00:00:37,690
We have if we're going to get out.

14
00:00:38,130 --> 00:00:41,490
Yeah, it's it wasn't that I forgot to press record.

15
00:00:41,570 --> 00:00:43,850
We were just talking and it was interesting.

16
00:00:43,850 --> 00:00:48,010
And then I thought, let's just press record and continue the conversation

17
00:00:48,010 --> 00:00:51,250
in the middle, because I don't know what I said five minutes ago.

18
00:00:51,250 --> 00:00:53,730
As anybody who works with me will remind you.

19
00:00:53,970 --> 00:00:56,210
I'm Alan and he's Brent.

20
00:00:56,210 --> 00:00:59,690
And together we make up the Alan Brent testing podcast,

21
00:00:59,770 --> 00:01:04,010
where we rarely, if ever, talk about testing, but we do talk about stuff.

22
00:01:04,800 --> 00:01:08,760
Yeah, we do talk about Alan and Brent quite often.

23
00:01:09,040 --> 00:01:13,680
Yes. Mostly Alan talking about Alan and Brent talking about Brent,

24
00:01:13,680 --> 00:01:17,000
because it's our podcast and we're the centers of attention.

25
00:01:18,480 --> 00:01:20,200
I was going to go a different angle.

26
00:01:20,200 --> 00:01:24,480
It's more normally Alan bitching about Brent and vice versa.

27
00:01:24,640 --> 00:01:26,840
Oh, I can fire you.

28
00:01:27,240 --> 00:01:29,520
A B could be Alan bitching.

29
00:01:29,520 --> 00:01:31,490
Bye bye. All right.

30
00:01:31,490 --> 00:01:32,290
Talk to you.

31
00:01:32,730 --> 00:01:34,010
Brent has hung up.

32
00:01:34,010 --> 00:01:37,450
I'm leaving.

33
00:01:38,890 --> 00:01:41,170
I mean, boy, oh boy.

34
00:01:41,170 --> 00:01:44,650
One of my favorite things to do when I'm just done with a telephone

35
00:01:44,650 --> 00:01:46,850
conversation, it reminds me of it.

36
00:01:46,850 --> 00:01:49,450
I will go, oh, hey, Alan.

37
00:01:50,490 --> 00:01:52,090
Do you know what I have?

38
00:01:52,090 --> 00:01:53,770
I have a pop quiz for you.

39
00:01:53,770 --> 00:01:56,210
And if I do it right, you'll go, what?

40
00:01:57,020 --> 00:02:00,900
And I'm like, can you guess what is this sound?

41
00:02:01,180 --> 00:02:03,640
And then I just hang up.

42
00:02:03,640 --> 00:02:04,760
Sound of silence.

43
00:02:04,760 --> 00:02:06,360
Did we talk about the last of us already?

44
00:02:06,360 --> 00:02:08,560
You've been watching that? No.

45
00:02:08,560 --> 00:02:10,200
Oh, it's good. You got to watch it.

46
00:02:10,200 --> 00:02:14,000
Yeah, I am up to date on Doom Patrol.

47
00:02:14,000 --> 00:02:15,320
I am up to date.

48
00:02:15,320 --> 00:02:17,120
I mean, and none of this really matters.

49
00:02:17,120 --> 00:02:19,880
No, it doesn't. Anyway, I got to I got to get back on Doom Patrol.

50
00:02:19,880 --> 00:02:22,240
I watched a bunch of that when I was stranded in Buffalo.

51
00:02:22,640 --> 00:02:23,600
But I have to get back on.

52
00:02:23,600 --> 00:02:26,440
And then I got distracted by White Lotus and and it was better.

53
00:02:26,600 --> 00:02:28,080
But I want to get back on that.

54
00:02:28,080 --> 00:02:30,080
What is last of us on with streaming?

55
00:02:30,080 --> 00:02:31,640
HBO, HBO.

56
00:02:31,640 --> 00:02:33,360
OK, maybe I'll check that.

57
00:02:33,360 --> 00:02:34,160
Yeah, I have that one.

58
00:02:34,160 --> 00:02:37,160
And I think I maybe didn't say it's on the podcast before.

59
00:02:37,440 --> 00:02:41,280
But I normally do not like well, because they're not very good.

60
00:02:41,760 --> 00:02:47,190
Usually the movies or series that are modeled up your video games

61
00:02:47,190 --> 00:02:49,460
are just not good.

62
00:02:49,460 --> 00:02:51,260
Yeah, I don't have to list them all off.

63
00:02:51,260 --> 00:02:52,980
But I watched it anyway.

64
00:02:52,980 --> 00:02:55,820
And I was surprised. It's very good. Very, very good.

65
00:02:56,420 --> 00:03:00,300
I am hopeful that the movie that they're going to do for fallout

66
00:03:00,300 --> 00:03:03,770
will not follow that path.

67
00:03:04,610 --> 00:03:06,330
Oh, that's right. We did talk about this a little bit.

68
00:03:06,330 --> 00:03:08,810
OK, let's we are forgetful old men.

69
00:03:09,210 --> 00:03:12,930
I don't know how old I am, but it's old. It's super old.

70
00:03:13,710 --> 00:03:17,430
All right. I want to talk about I mentioned it on the three of my five for Friday.

71
00:03:17,670 --> 00:03:21,190
I mentioned this invention

72
00:03:21,930 --> 00:03:23,890
and we've talked about it before on the podcast.

73
00:03:23,890 --> 00:03:26,170
Everyone drink. I have coffee. I'm not brand has.

74
00:03:26,770 --> 00:03:28,010
I have a coke.

75
00:03:28,010 --> 00:03:31,370
Let's talk about again, yet again,

76
00:03:32,330 --> 00:03:35,250
because it never goes away and it's getting more interesting.

77
00:03:35,250 --> 00:03:39,090
And actually, it takes a lot to make Microsoft interesting.

78
00:03:39,090 --> 00:03:41,890
Satya has fixed a lot of things wrong with Microsoft.

79
00:03:42,330 --> 00:03:44,130
Oh, by the way, glad to see you're still employed.

80
00:03:44,130 --> 00:03:46,530
I see another round launch today.

81
00:03:46,530 --> 00:03:48,850
Yesterday. Oh, my God.

82
00:03:49,370 --> 00:03:52,170
He's done some stuff. He has really made Microsoft interesting.

83
00:03:52,530 --> 00:03:55,250
Microsoft caters to corporations.

84
00:03:55,250 --> 00:03:59,610
People buying tens of thousands of licenses of office and windows.

85
00:03:59,930 --> 00:04:02,690
And although there are consumers use that stuff, it's for corporations.

86
00:04:02,690 --> 00:04:06,810
And it's boring. But Microsoft is getting less boring.

87
00:04:06,810 --> 00:04:11,370
You know why? And in my eyes, as a non Microsofty, former Microsofty,

88
00:04:11,370 --> 00:04:15,010
it's getting more interesting because I'm seeing our old friend chat

89
00:04:15,010 --> 00:04:19,410
GPT pop up all over the place and press releases from Microsoft.

90
00:04:19,410 --> 00:04:21,880
We know about the Bing one.

91
00:04:21,880 --> 00:04:27,200
And we've also seen chat GPT integrated with Microsoft Teams.

92
00:04:27,720 --> 00:04:29,040
And I'm going to ask you a question.

93
00:04:29,040 --> 00:04:31,520
I know you want to ask us to talk about chat GPT,

94
00:04:31,520 --> 00:04:34,720
but I think it'll generate some interest.

95
00:04:34,720 --> 00:04:38,600
So Microsoft invests heavily in chat GPT, right?

96
00:04:39,280 --> 00:04:41,680
But I know that that part you can answer.

97
00:04:41,680 --> 00:04:47,120
So to be clear, there will be no accidental release of Microsoft

98
00:04:47,120 --> 00:04:50,240
IP today. OK, no, no, no.

99
00:04:50,360 --> 00:04:55,400
But I do have a request and I'm almost afraid to ask because if it's

100
00:04:55,840 --> 00:04:59,480
if it's something that you can't answer either, then I'll tell you,

101
00:04:59,480 --> 00:05:01,040
I can't answer. All right.

102
00:05:01,040 --> 00:05:02,440
But here's my suggestion for you.

103
00:05:02,440 --> 00:05:05,040
Here's my here's my top notch suggestion for you.

104
00:05:05,640 --> 00:05:11,120
Can I just get myself rather than go through chat GPT?

105
00:05:12,290 --> 00:05:17,170
Can I just get my own Azure VM or Azure cluster

106
00:05:17,570 --> 00:05:22,450
with GPT through technology so I can just run my own language processing service

107
00:05:23,010 --> 00:05:25,530
so you can without GPT?

108
00:05:25,570 --> 00:05:29,970
No, well, with GPT, like what I'm saying is here's my tip is

109
00:05:30,290 --> 00:05:35,610
build in GPT through technology into pre build it into some Azure VM

110
00:05:35,610 --> 00:05:39,130
so I can just huck training material at something that I check out

111
00:05:39,130 --> 00:05:41,490
in his mind and I pay you for and everybody's happy.

112
00:05:41,490 --> 00:05:45,450
Well, so that's that is officially.

113
00:05:46,130 --> 00:05:48,010
Well, I guess I don't understand the distinction.

114
00:05:48,010 --> 00:05:51,450
So maybe I should double check if this is public.

115
00:05:51,730 --> 00:05:55,370
So maybe maybe maybe I'm just going to say that because it's something

116
00:05:55,370 --> 00:06:00,450
that should be available. I think it's it's Microsoft will absolutely

117
00:06:00,890 --> 00:06:07,490
enable or directly enable a scenario similar to what you describe.

118
00:06:07,570 --> 00:06:12,170
OK, so really all I'm saying is is Microsoft is all of a sudden

119
00:06:12,170 --> 00:06:17,250
in an interesting place in making by making this technology accessible,

120
00:06:17,250 --> 00:06:23,530
it makes some of its old stodgy business, the applications and and Bing,

121
00:06:23,530 --> 00:06:28,650
which I will soon probably have to I will have to stop reminding people

122
00:06:28,650 --> 00:06:31,930
that being as a search engine like Google and that you can use it

123
00:06:31,930 --> 00:06:34,370
to Google stuff because I think it's going to become relevant.

124
00:06:34,370 --> 00:06:37,210
I think a lot of interesting things are happening.

125
00:06:37,450 --> 00:06:38,690
And then you have stuff you want to say.

126
00:06:38,690 --> 00:06:42,450
But the other thing for my side is I posted an article today

127
00:06:42,450 --> 00:06:46,290
showing how chat GPT was helping people.

128
00:06:46,290 --> 00:06:49,410
And I like that. I think a lot of the photo, a lot of the comments

129
00:06:49,410 --> 00:06:54,130
I see on social media and articles are people showing, oh, look,

130
00:06:54,130 --> 00:06:57,570
chat GPT sucks for this. It sucks for this because they're using it

131
00:06:57,570 --> 00:07:00,010
in things it's not designed for.

132
00:07:00,010 --> 00:07:03,530
Or someone pointed out you can gaslight chat GPT

133
00:07:04,480 --> 00:07:06,920
as you can with any AI.

134
00:07:06,920 --> 00:07:10,320
They're trying to rather than trying to use it for good.

135
00:07:10,320 --> 00:07:12,720
They're trying to find ways that it won't work, which are endless.

136
00:07:12,720 --> 00:07:14,560
It's endless. It's infallible.

137
00:07:14,560 --> 00:07:19,840
I think it is not Skynet, nor does it claim to be.

138
00:07:20,160 --> 00:07:24,600
I would say it's probably closer to a very advanced version of that thing

139
00:07:24,600 --> 00:07:28,000
in your text app as Brent is texting and in Gmail

140
00:07:28,000 --> 00:07:30,200
that tries to predict what your next word is going to be.

141
00:07:30,440 --> 00:07:34,160
No, I'm trying to quickly scan the article that you sent out.

142
00:07:34,160 --> 00:07:36,680
Oh, just why do you look for it?

143
00:07:36,680 --> 00:07:39,920
I'll talk about how I have ended up using chat GPT.

144
00:07:40,200 --> 00:07:42,420
I use it for brainstorming.

145
00:07:42,420 --> 00:07:44,500
I know that's interesting. It's interesting.

146
00:07:44,500 --> 00:07:47,220
I use it as a creative tool. Walk me through it.

147
00:07:47,260 --> 00:07:48,860
I use it to help me figure out.

148
00:07:48,860 --> 00:07:52,300
Well, walk me through a scenario if you can. Sure.

149
00:07:52,300 --> 00:07:54,060
Let me just look at my history real quick.

150
00:07:54,060 --> 00:07:58,140
Well, I tried to figure I didn't help me directly.

151
00:07:58,140 --> 00:08:00,340
Well, one, I was just trying to think of a new name for my org.

152
00:08:01,470 --> 00:08:03,230
I described what it did. It gave me some ideas.

153
00:08:03,230 --> 00:08:04,470
I said, those are horrible.

154
00:08:04,470 --> 00:08:05,670
How about something more like this?

155
00:08:05,670 --> 00:08:09,470
We kind of go back and forth as a can you make sure you use this word,

156
00:08:09,470 --> 00:08:13,430
but don't use any words like this as kind of a brainstorming partner.

157
00:08:14,590 --> 00:08:21,070
I use it for I use it for summarizing articles for for having

158
00:08:21,070 --> 00:08:23,030
having to explain complex things to me.

159
00:08:23,790 --> 00:08:26,430
There's a medical article I read once I had it explained.

160
00:08:26,670 --> 00:08:28,710
Oh, OK, I get it. I understand it now.

161
00:08:28,710 --> 00:08:33,540
Well, of course, of course, on that one, there is a risk.

162
00:08:34,140 --> 00:08:38,380
You're aware that there's a risk when it summarizes things that you don't understand.

163
00:08:38,900 --> 00:08:40,860
It could it could summarize wrong.

164
00:08:40,860 --> 00:08:45,630
Right. Of course, it could it could present itself as truth,

165
00:08:46,310 --> 00:08:48,590
something that is false, and then you walk away.

166
00:08:49,070 --> 00:08:52,030
Yeah. And usually what I do, because the way I work,

167
00:08:52,070 --> 00:08:55,870
I work around that is I don't I paste in a chunk of an article

168
00:08:56,720 --> 00:08:59,560
and the odds of it giving me something and then maybe another chunk later

169
00:08:59,560 --> 00:09:02,080
and the odds of it giving me the wrong thing twice or two.

170
00:09:02,440 --> 00:09:05,000
If those things don't jive, I have to go look up more.

171
00:09:05,000 --> 00:09:08,520
But again, I do know to take it with a grain of salt because it's not

172
00:09:09,380 --> 00:09:12,060
it's it's it's not a language processing engine.

173
00:09:12,060 --> 00:09:14,980
It's not it's not self-aware.

174
00:09:15,500 --> 00:09:18,100
It is not even a language processing engine.

175
00:09:18,140 --> 00:09:19,740
Yeah, you would know you would know the right words.

176
00:09:19,740 --> 00:09:22,820
But but yeah, I use it for OK.

177
00:09:22,820 --> 00:09:25,260
I'm going to cut this one out because I don't want the secret to get out.

178
00:09:25,380 --> 00:09:30,260
So somebody will figure it out someday.

179
00:09:30,580 --> 00:09:34,740
So anyway, I use it. You have a you have a I mean, you have a clear hint.

180
00:09:35,300 --> 00:09:36,020
All right.

181
00:09:36,020 --> 00:09:38,300
I might leave little bits of that in there, but I'm just

182
00:09:38,580 --> 00:09:40,380
because somebody, if they thought about it, would figure it out.

183
00:09:40,380 --> 00:09:43,060
They just they ask chat GPT what these names have in common.

184
00:09:43,460 --> 00:09:45,420
Breadcrumbs. All right.

185
00:09:45,420 --> 00:09:49,820
So but you and I want to mark it today is

186
00:09:50,020 --> 00:09:55,060
we're recording this on Friday, February 10th, episode 175, A.B.

187
00:09:55,060 --> 00:09:59,220
testing, and this is the first time in three, four, maybe five years.

188
00:09:59,220 --> 00:10:03,580
Brent has topics for discussion.

189
00:10:04,580 --> 00:10:07,620
And I probably stole some of this thunder because he wanted to talk about

190
00:10:07,620 --> 00:10:09,980
chat GPT and I'm going to join in the conversation.

191
00:10:09,980 --> 00:10:14,860
But I think he's going to lay down some some insights.

192
00:10:15,860 --> 00:10:18,660
Is that true or or or did I read the wrong agenda?

193
00:10:18,900 --> 00:10:22,700
I don't know if it's going to be insights like I am.

194
00:10:22,940 --> 00:10:28,460
Part of my brain is is trying to convince another part of my brain

195
00:10:28,860 --> 00:10:34,060
that I am just in conspiracy theory mode, and therefore I should ignore that.

196
00:10:34,460 --> 00:10:37,380
The part of my brain that's generating all of these conclusions.

197
00:10:37,580 --> 00:10:41,940
Well, I want to pause and remind people that who hopefully this isn't

198
00:10:41,940 --> 00:10:45,340
their first episode, but back in our, you know, well, I don't know

199
00:10:45,340 --> 00:10:50,060
what number it was, 170 something probably our prediction episode.

200
00:10:50,420 --> 00:10:54,580
Really, a month and a half ago, Brent predicted that chat GPT

201
00:10:54,580 --> 00:10:57,620
was going to be a multi billion dollar business and grow super fast.

202
00:10:57,620 --> 00:10:59,820
And here it is already.

203
00:11:00,670 --> 00:11:04,070
So I think anything you want to say about chat GPT

204
00:11:04,070 --> 00:11:05,470
probably has a good chance of happening.

205
00:11:05,470 --> 00:11:07,630
So how fucked are we?

206
00:11:08,990 --> 00:11:15,860
We're doomed the but before I go into that story, I don't know if we are,

207
00:11:15,860 --> 00:11:18,580
but I absolutely believe our grandchildren are.

208
00:11:18,980 --> 00:11:24,100
And in terms of short term stuff like you remember when we were at micro

209
00:11:24,100 --> 00:11:30,060
when when you were at Microsoft's do or as well, and we knew way in advance

210
00:11:30,580 --> 00:11:32,940
that test was disappearing.

211
00:11:33,500 --> 00:11:35,380
Yes. OK.

212
00:11:35,380 --> 00:11:38,020
And I don't know if you had these arguments, but I did.

213
00:11:38,420 --> 00:11:41,140
Right. People saying, what are you talking about?

214
00:11:41,140 --> 00:11:43,820
You're so full of crap and blah, blah, blah, blah, blah, blah, blah, blah.

215
00:11:44,500 --> 00:11:50,780
The way in in particular today, because I got even more information on on this topic

216
00:11:51,980 --> 00:11:56,140
because I run a data science team that specializes in NLP.

217
00:11:56,460 --> 00:12:00,460
And it has a lot of it has a high reputation

218
00:12:00,460 --> 00:12:02,460
and a lot of people throughout Microsoft know it.

219
00:12:03,360 --> 00:12:08,800
So I keep everybody in their mother is coming to me around chat GPT.

220
00:12:09,000 --> 00:12:14,320
OK, and I will say if another goddamn PM comes to me

221
00:12:14,800 --> 00:12:19,360
talking about chat GPT and cannot distinguish the difference between magic

222
00:12:19,360 --> 00:12:22,880
and data science, I'm going to explode.

223
00:12:24,360 --> 00:12:29,080
Any sufficiently advanced technology is indistinguishable from magic.

224
00:12:29,080 --> 00:12:30,960
We know this. Yeah.

225
00:12:31,520 --> 00:12:34,120
But they're like, well, can we go like you helps also?

226
00:12:34,960 --> 00:12:37,120
Can we just do? No, we can't.

227
00:12:37,440 --> 00:12:39,760
Well, well, couldn't we just no, we can't.

228
00:12:39,760 --> 00:12:43,960
Oh, my God. Why do you why did you just set up a daily one on one with me?

229
00:12:43,960 --> 00:12:46,760
Oh, my God. No, stop, stop.

230
00:12:46,760 --> 00:12:50,000
That's kind of my life the last two weeks.

231
00:12:50,600 --> 00:12:51,760
So what do they what?

232
00:12:51,760 --> 00:12:52,920
Give me an example.

233
00:12:52,920 --> 00:12:54,120
What do they want to do?

234
00:12:54,120 --> 00:12:59,040
What is their what is the odd Brent them chat GPT threesome they want to set up?

235
00:12:59,740 --> 00:13:05,620
Oh, in this particular case, they they want to use.

236
00:13:05,940 --> 00:13:10,300
I'll just say, can't we just use chat GPT to just eliminate our entire

237
00:13:10,300 --> 00:13:12,860
support organization, right? Things like that.

238
00:13:13,580 --> 00:13:14,980
I'm using hyperbole.

239
00:13:14,980 --> 00:13:16,900
That doesn't even fall into.

240
00:13:16,900 --> 00:13:20,300
Well, actually, I mean, I was going to say the answer to any sufficiently

241
00:13:20,300 --> 00:13:22,260
complex question is it depends.

242
00:13:23,420 --> 00:13:25,820
And there I would basically say, you know what?

243
00:13:26,100 --> 00:13:28,980
Bing team is probably already ahead of us on that one.

244
00:13:28,980 --> 00:13:32,300
We probably don't need to do anything because customers,

245
00:13:32,300 --> 00:13:34,620
when that's once they realize that

246
00:13:35,180 --> 00:13:40,540
and Bing is very focused on these data science, particularly on the issues,

247
00:13:41,180 --> 00:13:45,740
they're going to get they're going to get their well ahead of

248
00:13:46,340 --> 00:13:51,700
certainly my freaking team of 10 people of which for focus on NLP.

249
00:13:52,460 --> 00:13:54,820
Why are you talking to me anyway?

250
00:13:54,820 --> 00:13:56,620
I got I got to ask, because is this

251
00:13:57,220 --> 00:13:59,900
is this testers going away all over again in a way?

252
00:13:59,900 --> 00:14:02,340
I mean, not this is what it feels like to me.

253
00:14:02,340 --> 00:14:07,220
My my emotion right now around how I feel

254
00:14:07,860 --> 00:14:09,940
about a prediction I'm going to.

255
00:14:09,940 --> 00:14:11,380
Well, I've already made.

256
00:14:11,380 --> 00:14:16,700
I feel it as strong as you and I did then.

257
00:14:17,340 --> 00:14:19,340
And it's not because I'm predicting.

258
00:14:19,340 --> 00:14:22,740
It's because I see the direction the momentum is going.

259
00:14:23,500 --> 00:14:25,740
OK, you follow. You follow.

260
00:14:25,740 --> 00:14:28,140
I have I have a thread in my head.

261
00:14:28,140 --> 00:14:30,140
I want to have been all you. OK.

262
00:14:30,140 --> 00:14:32,460
So today I woke up in a bad mood.

263
00:14:33,100 --> 00:14:34,900
I wake up in a bad mood every day these days.

264
00:14:34,900 --> 00:14:37,660
And it was it was fully thinking of this because I'm very

265
00:14:38,020 --> 00:14:40,460
I feel very responsible to my team.

266
00:14:40,500 --> 00:14:42,860
And as you did when you were a test manager.

267
00:14:43,140 --> 00:14:45,980
Correct. Before you went and joined Bing as a dev manager.

268
00:14:46,660 --> 00:14:48,660
And what did I do?

269
00:14:48,660 --> 00:14:51,060
I went to go and learn.

270
00:14:51,060 --> 00:14:54,380
I had a very clear sense of what's coming next.

271
00:14:54,700 --> 00:14:57,060
I went to go learn it so I could broadcast it.

272
00:14:57,620 --> 00:15:01,740
OK. And and help those who don't

273
00:15:02,180 --> 00:15:05,780
who who weren't believers, who didn't understand, be prepared.

274
00:15:05,780 --> 00:15:11,940
Right. That whole process led to ultimately things like modern testing principles.

275
00:15:12,140 --> 00:15:17,940
Right. It's OK. Over the years, we see more and more evidence of this and et cetera.

276
00:15:18,380 --> 00:15:22,260
Right now, though, I'm at a point.

277
00:15:22,820 --> 00:15:24,380
I don't know.

278
00:15:24,380 --> 00:15:27,660
I run a team with data scientists and NLP.

279
00:15:27,660 --> 00:15:31,580
And I'm not certain what direction to invest them in.

280
00:15:31,580 --> 00:15:33,420
Where they should train.

281
00:15:33,420 --> 00:15:38,830
OK. And and it certainly doesn't help

282
00:15:39,030 --> 00:15:43,350
where when we're in an environment where we've been told, yeah,

283
00:15:43,550 --> 00:15:46,910
layoffs going to keep coming until March 31st. Right.

284
00:15:48,110 --> 00:15:51,830
But there's that's a shorter term problem that will get resolved.

285
00:15:51,830 --> 00:15:55,230
I'm pretty certain that like just before our podcast,

286
00:15:55,230 --> 00:15:59,350
I was having a similar conversation with one of my key partners on this.

287
00:16:00,230 --> 00:16:04,630
And I basically told him, look, I'm seriously thinking about getting out of NLP.

288
00:16:05,590 --> 00:16:07,350
And he's like, what?

289
00:16:07,350 --> 00:16:08,950
Oh, my God, Britt, I need you.

290
00:16:08,950 --> 00:16:09,830
What's going to happen?

291
00:16:09,830 --> 00:16:10,990
Bubba, right?

292
00:16:10,990 --> 00:16:13,630
So things will converge and we'll work out the business thing.

293
00:16:13,910 --> 00:16:16,510
What I wanted to talk about today.

294
00:16:16,710 --> 00:16:20,630
And hopefully I don't take too long to do it because I'm hoping we can have

295
00:16:20,630 --> 00:16:24,430
a discussion around what are options to deal with it.

296
00:16:25,200 --> 00:16:27,880
I mentioned on on our Slack channel

297
00:16:27,960 --> 00:16:31,560
that there was a new invention that came out.

298
00:16:31,920 --> 00:16:34,400
Apple glasses. Are you familiar with Apple glasses?

299
00:16:34,400 --> 00:16:36,920
Have you done any research into this thing?

300
00:16:37,200 --> 00:16:39,200
I wouldn't say I've done research, no.

301
00:16:39,600 --> 00:16:41,400
OK, there is a feature.

302
00:16:41,680 --> 00:16:45,960
You go look at the marketing thing and there is a feature that's being discussed.

303
00:16:46,280 --> 00:16:51,130
OK, and it says there is a camera

304
00:16:51,770 --> 00:16:56,060
on each lens to track your eyes

305
00:16:56,660 --> 00:17:02,140
so that the glasses can be more confident in what the user is looking at.

306
00:17:02,660 --> 00:17:05,660
How do you feel about how do you feel about that sentence?

307
00:17:06,420 --> 00:17:08,660
Well, doesn't bug you yet.

308
00:17:09,380 --> 00:17:11,340
It bugs me a little.

309
00:17:11,340 --> 00:17:14,420
I just remember to take my glasses off when I don't want it to know where I'm looking.

310
00:17:15,020 --> 00:17:17,650
OK, what if I remind you

311
00:17:18,450 --> 00:17:22,450
that roughly 80 percent of your eye movements are subconscious?

312
00:17:23,430 --> 00:17:25,670
Yeah, I would never wear them.

313
00:17:26,510 --> 00:17:31,600
OK, the only thing about Apple glasses here

314
00:17:32,120 --> 00:17:35,360
that I can come up with as a positive

315
00:17:36,220 --> 00:17:41,380
is that they are not their competitor Facebook, who's also building a similar thing.

316
00:17:41,780 --> 00:17:45,380
Yeah, Apple has has has shown very publicly

317
00:17:45,380 --> 00:17:49,860
a very willingness to tell the government to F off if they ask for private data.

318
00:17:49,940 --> 00:17:53,500
But I will tell you the wares of these glasses,

319
00:17:54,660 --> 00:17:57,980
that company will have the data.

320
00:17:58,820 --> 00:18:02,660
They will have the data to know everything about you,

321
00:18:03,420 --> 00:18:07,500
including things that you don't know in about a month.

322
00:18:08,380 --> 00:18:10,660
All of it. They will know.

323
00:18:11,780 --> 00:18:13,660
Hey, are you gay?

324
00:18:13,660 --> 00:18:18,100
They will know what religion you're in or believe in.

325
00:18:18,540 --> 00:18:21,260
They will know the perfect

326
00:18:21,980 --> 00:18:25,660
the the image of the perfect mate for you.

327
00:18:25,700 --> 00:18:30,220
They will know everything or that data will contain

328
00:18:30,700 --> 00:18:33,540
all they need to know to know everything about you.

329
00:18:34,470 --> 00:18:38,500
Now, there is one thing about human behavior that I

330
00:18:39,140 --> 00:18:42,160
I have seen over and over again.

331
00:18:42,160 --> 00:18:46,080
And it's this old phrase, the road to hell is paid with good intentions.

332
00:18:46,560 --> 00:18:49,840
And so now let's imagine Apple glasses

333
00:18:50,480 --> 00:18:53,480
and someone comes up with the idea of, hey,

334
00:18:54,400 --> 00:18:59,200
since it is so effective at knowing what they're looking at,

335
00:18:59,520 --> 00:19:01,520
could we not put them on infants?

336
00:19:01,520 --> 00:19:05,520
Could we not make like Apple infant goggles or something

337
00:19:06,000 --> 00:19:10,800
where where we can start getting additional insight into

338
00:19:10,960 --> 00:19:12,720
what their thought process is?

339
00:19:12,720 --> 00:19:14,120
They can't communicate yet.

340
00:19:14,120 --> 00:19:15,920
So that's harder. This could help.

341
00:19:15,920 --> 00:19:17,160
Oh, and even better.

342
00:19:17,160 --> 00:19:20,800
Could we use that to sort of understand the difference between babies

343
00:19:20,800 --> 00:19:25,240
with autism and normal babies?

344
00:19:25,960 --> 00:19:29,560
Right. This would then start to build a process.

345
00:19:29,880 --> 00:19:32,920
By the way, just just to interrupt the the

346
00:19:33,520 --> 00:19:36,480
for those listening air quotes around normal.

347
00:19:36,720 --> 00:19:38,720
Brent is not saying anyway.

348
00:19:38,720 --> 00:19:42,160
Just just want to clarify that for for that statement. Yeah.

349
00:19:43,000 --> 00:19:44,520
No, actually.

350
00:19:44,520 --> 00:19:46,480
So I understand what you're saying there.

351
00:19:46,480 --> 00:19:50,440
Okay. But that pushback is exactly what I'm afraid of.

352
00:19:51,200 --> 00:19:53,660
Who gets to define normal?

353
00:19:53,660 --> 00:19:55,300
Right. Right.

354
00:19:55,300 --> 00:19:57,300
No, we're on the we're on the same pager. Right.

355
00:19:57,300 --> 00:19:58,900
Because yeah, I don't.

356
00:19:58,980 --> 00:20:00,900
That's why the air quotes are there because you don't.

357
00:20:01,540 --> 00:20:03,460
This is very possible.

358
00:20:05,260 --> 00:20:07,860
I would say almost inevitable.

359
00:20:08,620 --> 00:20:10,500
Oh, it's absolutely inevitable.

360
00:20:10,500 --> 00:20:11,900
This is where I'm going.

361
00:20:11,900 --> 00:20:15,740
But even if you don't wear glasses, there are going to be ways to.

362
00:20:16,460 --> 00:20:19,380
But we've seen glimpses of the future in films

363
00:20:19,980 --> 00:20:24,380
where the ads you see, not just in your browser,

364
00:20:24,380 --> 00:20:28,500
but the ads you see walking around are tailored to you.

365
00:20:28,900 --> 00:20:33,380
Your world tailors around because there is no so much information about you.

366
00:20:33,860 --> 00:20:37,260
Right. If some today, like 20 years ago,

367
00:20:38,260 --> 00:20:42,180
if someone were to ask you some fact about something,

368
00:20:42,180 --> 00:20:45,260
you know, how deep is the deepest part of the ocean?

369
00:20:45,820 --> 00:20:49,580
Maybe you have a guess, but you you wouldn't know and you'd be fine with it.

370
00:20:49,620 --> 00:20:51,780
You're OK to estimate it today.

371
00:20:52,060 --> 00:20:54,540
We look up those things immediately on our phones,

372
00:20:54,540 --> 00:20:56,820
on Wikipedia or the Internet somewhere.

373
00:20:56,820 --> 00:20:59,540
But that's a lot of wasted effort to get that phone out

374
00:20:59,540 --> 00:21:02,620
and either type it in or voice recognition that query.

375
00:21:03,100 --> 00:21:08,600
Mm hmm. Why don't our why doesn't something connected to our bodies?

376
00:21:08,600 --> 00:21:10,760
And eventually maybe that we're going way off the deep end here.

377
00:21:10,760 --> 00:21:13,320
But why maybe even something implanted?

378
00:21:14,250 --> 00:21:15,210
Recognize that.

379
00:21:15,210 --> 00:21:18,170
And then with a thought or an action,

380
00:21:18,170 --> 00:21:21,490
I can hear a conversation and ask for more clarification.

381
00:21:22,090 --> 00:21:24,090
Like this is to keep it to glasses for now.

382
00:21:24,090 --> 00:21:26,330
My glasses obviously have microphones, too.

383
00:21:26,850 --> 00:21:29,690
And someone asked me a question and I can say, I don't know.

384
00:21:29,690 --> 00:21:31,610
And I give a special blink blink.

385
00:21:31,610 --> 00:21:36,410
And I get a little little thing in my ear telling me the answer to this thing.

386
00:21:36,810 --> 00:21:39,970
Oh, my God. Schools are irrelevant because I can cheat so easily.

387
00:21:40,170 --> 00:21:42,090
Everybody has to take the glasses off for their test.

388
00:21:42,090 --> 00:21:43,090
Oh, my God.

389
00:21:43,090 --> 00:21:44,450
Yeah, right.

390
00:21:45,490 --> 00:21:48,250
So I when I was in high school,

391
00:21:48,890 --> 00:21:52,090
I guess it was a period where there was a transition where some

392
00:21:52,450 --> 00:21:55,850
some math teachers allowed calculator, some didn't.

393
00:21:56,290 --> 00:21:57,290
Now it's common.

394
00:21:57,290 --> 00:22:00,090
Every every class has a high school. Right.

395
00:22:00,450 --> 00:22:04,090
Going back to the baby example, the other thing that that would do

396
00:22:04,690 --> 00:22:06,690
is it then normalizes the glasses.

397
00:22:07,410 --> 00:22:09,530
These are new human beings that don't.

398
00:22:10,210 --> 00:22:12,650
Yeah. They don't know a different world.

399
00:22:12,650 --> 00:22:14,010
Yeah. It may be glass.

400
00:22:14,010 --> 00:22:15,010
It'd be something else.

401
00:22:15,010 --> 00:22:16,770
I think there are.

402
00:22:16,770 --> 00:22:20,450
I remember this from I used to show this video was made by Microsoft

403
00:22:20,450 --> 00:22:23,490
when I used to give a talk at our new employee orientation.

404
00:22:24,090 --> 00:22:27,090
And it was a view into the future of health care

405
00:22:27,930 --> 00:22:33,290
and how through a wristwatch or a smart ring or some bit of jewelry,

406
00:22:33,850 --> 00:22:37,290
you could get a lot more constant feedback on your health,

407
00:22:37,930 --> 00:22:40,130
which I think, you know, there's some good benefit there, too.

408
00:22:40,130 --> 00:22:44,350
But just the same case, these glasses, there is potential for

409
00:22:44,990 --> 00:22:47,190
all of this data to be used in different ways.

410
00:22:47,590 --> 00:22:48,750
Raise your insurance.

411
00:22:48,750 --> 00:22:51,110
Understand when you're drinking too much.

412
00:22:51,110 --> 00:22:54,350
Understand if you've what you're if and what you're smoking.

413
00:22:54,910 --> 00:22:57,670
I think there again, all of this is inevitable.

414
00:22:57,990 --> 00:22:59,510
It's how the data is used.

415
00:22:59,510 --> 00:23:04,220
There's some ethical discussion, massive, massive ethical things.

416
00:23:04,220 --> 00:23:06,900
And here's and here is actually the crux of it.

417
00:23:07,140 --> 00:23:08,380
OK, and then I want to go back.

418
00:23:08,380 --> 00:23:13,540
Crux me. The dangerous part of this is at what point in time

419
00:23:14,260 --> 00:23:17,820
is this data being used to make decisions on your part,

420
00:23:18,100 --> 00:23:20,180
whether it be good or bad.

421
00:23:20,860 --> 00:23:24,060
Right. For example, I drive an F-150

422
00:23:24,730 --> 00:23:26,610
big gas guzzler right now.

423
00:23:26,610 --> 00:23:28,970
It's telling me I get 11 miles per gallon.

424
00:23:29,700 --> 00:23:32,860
You drive a Tesla purely electrical

425
00:23:33,750 --> 00:23:36,830
and you know this is going to be true.

426
00:23:36,830 --> 00:23:40,430
I don't know if it's in your opinion, but in terms of the people

427
00:23:40,430 --> 00:23:45,370
who are concerned about the climate, hey, wouldn't it be cool

428
00:23:45,370 --> 00:23:49,370
if we could get access to this data and force people

429
00:23:49,370 --> 00:23:51,210
to make eco friendly decisions?

430
00:23:52,540 --> 00:23:56,220
It's where this data becomes dangerous

431
00:23:56,780 --> 00:24:01,900
is when decisions are made for you and you're not involved.

432
00:24:02,500 --> 00:24:04,380
I think that's very Philip K.

433
00:24:04,380 --> 00:24:09,700
Dick yet, which is sort of in the adjacent possible of future possibilities.

434
00:24:10,020 --> 00:24:13,140
Well, no, that's not only a future possibility.

435
00:24:13,500 --> 00:24:17,940
People are going to willingly let the system make those decisions for it.

436
00:24:18,500 --> 00:24:20,420
I want to go into that, but I want to back up.

437
00:24:20,420 --> 00:24:23,980
This is do some magic editing put this way back, but I'm not going to

438
00:24:23,980 --> 00:24:25,260
because I'm far too lazy for that.

439
00:24:25,260 --> 00:24:29,140
But very early in the conversation, you were talking about

440
00:24:29,900 --> 00:24:31,100
grow way off in the future now.

441
00:24:31,100 --> 00:24:35,980
Talk about the now you're said you're feeling much now with this data science

442
00:24:36,460 --> 00:24:41,500
like you were with test, however many years ago, was 15 years ago,

443
00:24:41,500 --> 00:24:45,740
10 years ago, actually 10 years ago when we started realizing that,

444
00:24:45,740 --> 00:24:48,340
you know what, in the way we're shipping software, we're not going to

445
00:24:48,340 --> 00:24:49,820
test as much anymore.

446
00:24:49,820 --> 00:24:52,740
And now you're saying with the way with the tools available,

447
00:24:53,060 --> 00:24:55,140
we don't need as much data science anymore.

448
00:24:55,140 --> 00:24:58,340
Is that a clear is that a fair summary?

449
00:24:59,060 --> 00:25:02,180
We're not only not going to need as much data science, it's going to go

450
00:25:02,180 --> 00:25:03,500
into other places as well.

451
00:25:03,780 --> 00:25:04,340
Yeah. Okay.

452
00:25:04,340 --> 00:25:07,500
So let me let me talk about let me set the stage for people that

453
00:25:07,500 --> 00:25:12,740
remember and remember also, as I discussed when I posted my made my

454
00:25:12,740 --> 00:25:17,580
post on why most teams don't need dedicated software testers, the

455
00:25:17,580 --> 00:25:22,460
uproar of illogical really rebuttals.

456
00:25:23,100 --> 00:25:26,420
There are lots of teams out there still employing testers who are

457
00:25:26,420 --> 00:25:31,980
convinced, who are convinced they are absolutely essential to

458
00:25:31,980 --> 00:25:35,900
delivering software even on fast moving stuff.

459
00:25:36,770 --> 00:25:37,410
Can't change that.

460
00:25:37,410 --> 00:25:38,130
They're still around.

461
00:25:38,410 --> 00:25:38,770
Yep.

462
00:25:38,970 --> 00:25:40,930
Thus data science still be around.

463
00:25:41,210 --> 00:25:45,290
I think when we first saw this, we didn't know that we didn't

464
00:25:45,290 --> 00:25:47,210
know about the rise of data science yet.

465
00:25:47,210 --> 00:25:49,450
I think partway through we were podcasting with that.

466
00:25:49,450 --> 00:25:51,930
I can't find the, I don't know when the date was.

467
00:25:51,930 --> 00:25:56,420
I can't find the presentation, but there was a Microsoft

468
00:25:57,180 --> 00:25:58,580
internal event.

469
00:25:59,460 --> 00:26:02,660
God, I wish I could remember when it was, but Seth, Elliot and I,

470
00:26:02,700 --> 00:26:04,860
Seth, Elliot and I sat there.

471
00:26:04,860 --> 00:26:07,300
If you're listening, she does sometimes not very often.

472
00:26:07,300 --> 00:26:07,860
Hello.

473
00:26:08,730 --> 00:26:09,290
Hey, Seth.

474
00:26:09,850 --> 00:26:15,250
We were, we give a presentation on data science and I told the, the

475
00:26:15,250 --> 00:26:23,090
famous target story of how, of how target knew this girl was pregnant

476
00:26:23,090 --> 00:26:25,650
before her parents did and very famous.

477
00:26:25,650 --> 00:26:26,210
Go look it up.

478
00:26:26,210 --> 00:26:30,050
I can tell the whole story again through data science and analysis of data.

479
00:26:30,050 --> 00:26:34,410
What you're talking about seeing for the future is this turn to 11 or 11,000.

480
00:26:34,970 --> 00:26:35,250
Yeah.

481
00:26:35,370 --> 00:26:41,810
But what are the graphs I showed was a Google trends graph showing the data

482
00:26:41,810 --> 00:26:45,890
science, the phrase as it was showing up.

483
00:26:45,930 --> 00:26:46,930
It didn't exist.

484
00:26:48,020 --> 00:26:50,100
It really didn't exist at all.

485
00:26:50,620 --> 00:26:53,540
Other than, you know, insignificantly it existed 12 years ago,

486
00:26:53,540 --> 00:26:54,540
whatever the timeline was.

487
00:26:54,820 --> 00:26:56,740
It certainly wasn't really a title.

488
00:26:57,020 --> 00:26:59,740
And now it's now it's a huge thing.

489
00:27:01,000 --> 00:27:07,160
So I'm wondering if the thing that's next and whether it's a new role,

490
00:27:07,160 --> 00:27:08,920
you know, data, what's the, what's the joke?

491
00:27:09,280 --> 00:27:10,440
You know, what's the data scientists?

492
00:27:10,440 --> 00:27:14,360
It's a, it's a, uh, statistician that works in Silicon Valley.

493
00:27:14,440 --> 00:27:14,960
Ha ha.

494
00:27:15,480 --> 00:27:18,160
But I think the creative parts missing.

495
00:27:19,120 --> 00:27:21,360
So let me, let me, let me just finish my thought here.

496
00:27:21,360 --> 00:27:22,360
I'll ask you a question and go on.

497
00:27:22,640 --> 00:27:28,200
So you still have to figure out, you can't just give GPT three,

498
00:27:28,200 --> 00:27:29,160
here's a bunch of stuff.

499
00:27:29,160 --> 00:27:30,240
Tell me what I should do.

500
00:27:30,960 --> 00:27:36,330
Someone has to figure out what questions to ask, how to dig in there.

501
00:27:36,330 --> 00:27:38,810
But one thing, here's an observation I want you to comment on, and then

502
00:27:38,810 --> 00:27:39,770
I'll show you for a while.

503
00:27:39,970 --> 00:27:40,290
All right.

504
00:27:40,690 --> 00:27:44,650
One of the things that data scientists have told me for as long as I've

505
00:27:44,650 --> 00:27:49,650
known what a data scientist is, is how much of their time they spend cleaning

506
00:27:49,650 --> 00:27:54,150
up the data so that they can actually do something meaningful with it.

507
00:27:54,900 --> 00:27:55,260
Yep.

508
00:27:55,780 --> 00:27:59,940
And you can tell me, does GPT through technology, it seems to me,

509
00:27:59,980 --> 00:28:02,940
it works fine on dirty data.

510
00:28:03,620 --> 00:28:04,180
It is.

511
00:28:04,300 --> 00:28:09,100
That is not a limit, a limitation to, to the tech behind GPD.

512
00:28:09,460 --> 00:28:09,660
Yeah.

513
00:28:09,660 --> 00:28:10,580
I just realized this.

514
00:28:10,580 --> 00:28:14,340
So, uh, anyway, just a real little relevation while we talk, but, uh,

515
00:28:14,380 --> 00:28:15,140
I've talked long enough.

516
00:28:15,140 --> 00:28:18,220
So anyway, I made my point to continue on your stories.

517
00:28:18,340 --> 00:28:19,340
Tell me all kinds of things.

518
00:28:19,660 --> 00:28:21,980
Tell me how doomed we are and we'll make a movie.

519
00:28:22,340 --> 00:28:24,180
I don't know how doomed we are.

520
00:28:24,420 --> 00:28:24,740
Right.

521
00:28:24,980 --> 00:28:31,110
I just know we, I know our grandchildren from the, the context in which we

522
00:28:31,110 --> 00:28:36,070
judge the world today, our grandchildren are doomed, but it'll be normal.

523
00:28:36,270 --> 00:28:39,470
I guess the positive thing is that it'll be normalized for them.

524
00:28:40,190 --> 00:28:41,150
What that means.

525
00:28:41,150 --> 00:28:41,790
I don't know.

526
00:28:42,230 --> 00:28:47,590
People will go and say, but chat GPT can't do this and chat GPT can't do that.

527
00:28:47,590 --> 00:28:48,110
Chat GPT.

528
00:28:48,590 --> 00:28:48,830
Yeah.

529
00:28:48,830 --> 00:28:50,550
They, that's all true.

530
00:28:50,550 --> 00:28:51,430
So the hell what?

531
00:28:52,390 --> 00:28:52,630
Right.

532
00:28:52,630 --> 00:28:58,430
It's it's, to me, it reminds me it even further, it further solidifies my point

533
00:28:58,430 --> 00:29:04,350
of view on this prediction is so happening because the immediate thing is people to

534
00:29:04,350 --> 00:29:14,060
go and, and nitpick on what it can't do as sort of a self-defense mechanism.

535
00:29:14,220 --> 00:29:15,420
We saw this in tests.

536
00:29:15,460 --> 00:29:18,220
Oh, but who's going to find bugs?

537
00:29:18,260 --> 00:29:21,980
Devs don't want to, oh, that's a stupid argument.

538
00:29:22,620 --> 00:29:23,860
But your ship it to cut.

539
00:29:24,020 --> 00:29:25,980
Oh, that's a dumb argument too.

540
00:29:26,100 --> 00:29:30,100
Uh, it's eventually they lose the grip on the white knuckles.

541
00:29:30,500 --> 00:29:30,700
Right.

542
00:29:30,700 --> 00:29:32,540
Cause progress keeps moving forward.

543
00:29:33,060 --> 00:29:34,780
I'll give you a great example right now.

544
00:29:34,780 --> 00:29:38,770
Chat GPT does facts like ass.

545
00:29:39,370 --> 00:29:39,570
Okay.

546
00:29:39,570 --> 00:29:40,090
You ask.

547
00:29:40,690 --> 00:29:41,010
Okay.

548
00:29:41,570 --> 00:29:45,210
But people are like, Oh, chat GPT will never do facts.

549
00:29:45,650 --> 00:29:46,250
Okay.

550
00:29:46,770 --> 00:29:51,930
Well, we will wear the, the, the news article that came out this week where

551
00:29:51,970 --> 00:30:00,010
Steven Wolfram, the head of Wolfram alpha is offering to, to work with chat GPT

552
00:30:00,010 --> 00:30:03,300
to fix that problem, like stop.

553
00:30:03,660 --> 00:30:08,660
Google, Google has come out with an algorithm that you say, Hey, generate

554
00:30:08,660 --> 00:30:11,300
a song for me that feels like this.

555
00:30:11,860 --> 00:30:12,700
And guess what?

556
00:30:13,020 --> 00:30:14,460
It's pretty damn good.

557
00:30:15,020 --> 00:30:15,260
Right.

558
00:30:15,260 --> 00:30:15,900
Here's the thing.

559
00:30:16,860 --> 00:30:22,100
When you say creativity, like I'm going to be nothing but doom for the next five

560
00:30:22,100 --> 00:30:27,390
minutes, I'm just telling you, when you bring up creativity, what we view as

561
00:30:27,390 --> 00:30:32,790
creativity is the generation of new ideas and where do new ideas come from?

562
00:30:32,790 --> 00:30:33,190
Allen.

563
00:30:33,950 --> 00:30:34,110
My.

564
00:30:35,430 --> 00:30:36,030
Okay.

565
00:30:37,830 --> 00:30:45,470
Well, all of those ideas generally are documented someplace and this

566
00:30:45,470 --> 00:30:49,230
system is really good at pulling those in.

567
00:30:49,750 --> 00:30:52,870
Now, will it come up with random directions initially?

568
00:30:53,150 --> 00:30:53,950
Probably.

569
00:30:54,390 --> 00:30:59,110
But when you say, okay, generate a new idea for me in this direction, it'll get

570
00:30:59,110 --> 00:31:00,830
better and better and better and better.

571
00:31:01,310 --> 00:31:07,060
The last one that I, because I've been sharing this with, let me pause right

572
00:31:07,060 --> 00:31:10,740
there, cause this is the creativity brainstorming I was talking about.

573
00:31:10,740 --> 00:31:11,740
I can be writing a song.

574
00:31:11,740 --> 00:31:15,540
I could say, help me find a, help me find a word that rounds with orange.

575
00:31:15,820 --> 00:31:19,300
I could just say, what would the next line, what should the next line of the song be?

576
00:31:19,460 --> 00:31:24,580
And it may not be what I want or use, but it gives me an idea that I can work from.

577
00:31:24,620 --> 00:31:27,860
Ideas come from other ideas, which is what you wanted me to say earlier.

578
00:31:28,140 --> 00:31:28,500
Right.

579
00:31:28,700 --> 00:31:30,540
And that's what I use chat GPT for.

580
00:31:30,540 --> 00:31:32,500
I use it to get ideas.

581
00:31:32,820 --> 00:31:36,380
I might not use its ideas, but I may use, I may use its ideas come up with

582
00:31:36,380 --> 00:31:38,460
a new idea that I couldn't have come up with on my own.

583
00:31:38,820 --> 00:31:40,340
It's great because I don't like people.

584
00:31:40,660 --> 00:31:42,260
And now I have someone to brainstorm with.

585
00:31:43,260 --> 00:31:49,720
One of my employees today, we brought up some data and one of the things I realized.

586
00:31:50,160 --> 00:31:53,400
So my team documents all of its work in one note.

587
00:31:54,000 --> 00:31:57,320
And I'm like, huh, if I was a word I haven't heard in a long time,

588
00:31:57,320 --> 00:31:58,120
I forgot about that app.

589
00:31:58,520 --> 00:31:58,840
Right.

590
00:31:58,840 --> 00:32:03,720
And we do very tactical work, like two, two weeks at a time.

591
00:32:03,720 --> 00:32:03,920
Right.

592
00:32:04,120 --> 00:32:09,400
And I'm like, huh, if I were to copy paste that into the chat, GPT, every

593
00:32:09,720 --> 00:32:15,960
six months for each employee, and then ask chest GPT, Hey, um, given that these

594
00:32:15,960 --> 00:32:22,120
are the principles for me as a manager that are important, write a professional

595
00:32:22,120 --> 00:32:25,240
performance review based off of this work.

596
00:32:25,680 --> 00:32:26,000
Right.

597
00:32:26,000 --> 00:32:32,880
And I'm like, Oh, I, I now have just gone down the path of, of automating

598
00:32:33,320 --> 00:32:39,400
management and then my employee said, Oh, but I could do the same

599
00:32:39,400 --> 00:32:43,560
and create essentially an employee bot.

600
00:32:44,640 --> 00:32:46,720
And, and, and that just set me off.

601
00:32:46,720 --> 00:32:52,580
I'm like, Oh my God, we could Alan could create a, uh, an Alan.

602
00:32:53,300 --> 00:32:58,260
But we could, we could go and put these glasses on for a week so that our

603
00:32:58,260 --> 00:33:03,900
bots could really understand us and create a remarkably train it with all the,

604
00:33:04,060 --> 00:33:09,340
the, the code we've ever written, the bugs we filed and create a very

605
00:33:09,340 --> 00:33:14,180
realistic simulation of us in that particular persona.

606
00:33:14,460 --> 00:33:17,820
And then let's say you and I, there's something we disagree with.

607
00:33:18,340 --> 00:33:21,900
We get just instead of you, you and I could go get beers and just have the

608
00:33:21,900 --> 00:33:25,260
stupid bots argue it out until they come to the conclusion.

609
00:33:25,460 --> 00:33:31,540
And, and I'm like, uh, I even think I know how I could write that.

610
00:33:32,390 --> 00:33:36,430
And if I can do it, these experts building chat GPT absolutely can't.

611
00:33:37,230 --> 00:33:38,110
I'm not entirely sure.

612
00:33:38,150 --> 00:33:39,710
I could do that one.

613
00:33:39,950 --> 00:33:44,190
The only defense, by the way, there's only two things I've seen in terms of sort of

614
00:33:45,170 --> 00:33:51,390
defending against decisions being made on my behalf without my permission.

615
00:33:51,950 --> 00:33:56,470
There's only two things I've been able to is number one, come up with a new service

616
00:33:56,990 --> 00:34:00,310
and, and change regulations that makes it very clear.

617
00:34:00,390 --> 00:34:02,790
My data is mine.

618
00:34:03,630 --> 00:34:09,830
And I want the ability, a first ability is I, I want to know any time, even

619
00:34:09,830 --> 00:34:17,930
a single bit of my data, as in there are eight bits in a bite, even, even bit of

620
00:34:17,930 --> 00:34:21,410
my data was used in some decision-making process.

621
00:34:21,690 --> 00:34:26,010
I want to know the decisions being made off of my data or the other ways.

622
00:34:26,010 --> 00:34:30,130
So the whole new service that, that basically like the data gets moved

623
00:34:30,130 --> 00:34:31,770
to one of my vaults.

624
00:34:31,970 --> 00:34:33,250
I can delete it anytime.

625
00:34:33,250 --> 00:34:35,970
It's not, I have to ask Google to delete the data.

626
00:34:35,970 --> 00:34:37,890
No, it's stored in my location.

627
00:34:37,890 --> 00:34:38,730
It's mine.

628
00:34:38,890 --> 00:34:41,890
The fine line here is I get exactly what you're saying.

629
00:34:42,130 --> 00:34:42,450
Yep.

630
00:34:42,690 --> 00:34:47,610
And it should give, AI should give us suggestions, not decisions.

631
00:34:48,530 --> 00:34:50,920
But here's the deal.

632
00:34:51,740 --> 00:34:53,220
Let me go back to Apple glasses.

633
00:34:53,220 --> 00:34:56,980
Let's say could do some things like just something simple.

634
00:34:56,980 --> 00:35:03,270
Like today, you know, a lot of people use apps like my fitness pal or similar

635
00:35:03,270 --> 00:35:05,870
things to track their calories and exercise throughout the day.

636
00:35:06,270 --> 00:35:09,870
They're trying to get the right number of not only calories, but macros

637
00:35:09,870 --> 00:35:11,630
between fats, carbs, and protein.

638
00:35:12,270 --> 00:35:16,510
And they're painstakingly looking up things to get the breakdown and guessing

639
00:35:16,510 --> 00:35:19,310
here and there and trying to find the best diet for them in order to

640
00:35:19,310 --> 00:35:22,870
optimize the kind of body they want to have with their, with their

641
00:35:22,870 --> 00:35:24,150
exercise routine and everything.

642
00:35:24,470 --> 00:35:28,340
Now, theoretically, Apple glasses, they can automate all that.

643
00:35:28,340 --> 00:35:32,100
They can look at the food and look at a database in the background AI,

644
00:35:32,100 --> 00:35:32,820
figure it all out.

645
00:35:33,260 --> 00:35:34,340
It could just be automatic.

646
00:35:34,340 --> 00:35:38,500
It could tell me what I should and shouldn't eat from a plate or from a buffet

647
00:35:38,500 --> 00:35:42,780
or even from a menu I'm reading, which I think a lot of people, a lot of

648
00:35:42,780 --> 00:35:48,580
fitness and health conscious people would actually look at that as a plus,

649
00:35:48,620 --> 00:35:49,420
a positive.

650
00:35:49,700 --> 00:35:50,060
Yeah.

651
00:35:50,620 --> 00:35:52,100
And they be right.

652
00:35:53,060 --> 00:35:59,180
But also, you know, it's on the edge of when does, when does good for you

653
00:35:59,420 --> 00:36:02,340
become creepy and then bad?

654
00:36:02,580 --> 00:36:04,060
The road, that's what you're worried about.

655
00:36:04,100 --> 00:36:07,620
The road to hell is paved with good intentions.

656
00:36:07,620 --> 00:36:13,900
So there are some good intentions in how our lives will change.

657
00:36:13,900 --> 00:36:16,780
And you can even think, I mean, I can, we can tell the old fart story

658
00:36:16,780 --> 00:36:18,140
about how we didn't have the internet.

659
00:36:18,860 --> 00:36:21,580
By the way, in your 2000, you and I both had internet.

660
00:36:21,700 --> 00:36:22,660
I used to dial up.

661
00:36:22,660 --> 00:36:23,660
I have an ISDN.

662
00:36:23,740 --> 00:36:26,900
No, by 2000, I might have even had, I can't remember what I got cable

663
00:36:26,900 --> 00:36:28,500
for the first time, but I had dial up.

664
00:36:28,860 --> 00:36:30,460
I saw this interesting stat.

665
00:36:31,460 --> 00:36:36,620
400,000 people worldwide had access to the internet estimated in 2000.

666
00:36:37,370 --> 00:36:42,130
And today it's again, think the whole world, even third world countries.

667
00:36:42,850 --> 00:36:45,850
I don't, I just had a percentage, it's like 65%.

668
00:36:46,570 --> 00:36:49,210
But 65% of 7 billion, 7 million or eight.

669
00:36:49,730 --> 00:36:53,250
It's a big number, a big number of people have access to the internet

670
00:36:53,250 --> 00:36:54,370
to get their questions answered.

671
00:36:55,220 --> 00:37:00,140
What is that we used to talk about, I'm going to say Murfrees law, Moore's law

672
00:37:00,180 --> 00:37:05,460
in that computers were, computer power was doubling every, like three years,

673
00:37:05,460 --> 00:37:07,100
every number of years and getting all the numbers wrong.

674
00:37:07,880 --> 00:37:11,560
But it's this ease of access to information.

675
00:37:12,400 --> 00:37:15,280
It's going through the same sort of exponential growth.

676
00:37:15,880 --> 00:37:20,560
And the way that information and data is acting on is now going

677
00:37:20,560 --> 00:37:22,640
through exponential growth.

678
00:37:23,240 --> 00:37:23,760
Yes.

679
00:37:24,710 --> 00:37:28,630
And it's going to enable a lot of cool things.

680
00:37:28,710 --> 00:37:29,790
The good intentions.

681
00:37:30,680 --> 00:37:35,760
It's also going to enable, as we've seen already with Facebook and other places,

682
00:37:36,040 --> 00:37:37,320
a whole bunch of bad shit.

683
00:37:37,320 --> 00:37:39,040
We do not want to happen.

684
00:37:41,230 --> 00:37:41,590
Yep.

685
00:37:43,720 --> 00:37:46,840
And I'm wondering just really twist this thing in a different direction.

686
00:37:46,840 --> 00:37:53,920
Here's a run at a time is we have in the US at least we have a government made up

687
00:37:53,960 --> 00:37:59,400
of 99% of people who do not understand any of this because they were born

688
00:37:59,560 --> 00:38:03,120
a hundred years before Brent and I also, I'm worried.

689
00:38:03,120 --> 00:38:06,520
I like this need, this needs some regulation and some people

690
00:38:06,520 --> 00:38:07,880
thinking about how to solve it.

691
00:38:08,160 --> 00:38:10,080
But I don't know where those people come from.

692
00:38:10,480 --> 00:38:14,440
So now I'm on the Brent train and maybe we are all doomed because there's

693
00:38:14,440 --> 00:38:20,520
no way to even protect ourselves from the evil we're making even with good intentions.

694
00:38:20,800 --> 00:38:30,360
There's only one, one, it's either lock it away and make it such that lock it away in a way that even

695
00:38:31,080 --> 00:38:36,160
the slow assholes running the government don't have access to it.

696
00:38:36,980 --> 00:38:37,220
Right.

697
00:38:37,220 --> 00:38:40,820
But then right now with the laws, that's just a regulation away.

698
00:38:41,020 --> 00:38:43,340
I don't know if you're paying attention to crypto coin.

699
00:38:44,300 --> 00:38:50,100
That's kind of what I view the government's doing on crypto coin and doing it way too slow.

700
00:38:50,660 --> 00:38:50,860
Right.

701
00:38:50,860 --> 00:38:58,820
By the time they realize they need to get regulations in place, it's not only going to be too late,

702
00:38:58,820 --> 00:39:01,180
it's going to be way too late.

703
00:39:01,380 --> 00:39:03,300
So now I'm wondering.

704
00:39:03,300 --> 00:39:06,420
And here's the thing, I'm not even certain as something they can do.

705
00:39:07,230 --> 00:39:08,110
Potentially not.

706
00:39:08,230 --> 00:39:12,670
Because there's going to be other governments that will do the exact opposite.

707
00:39:13,230 --> 00:39:16,670
They're like, no, I am a dictator of my country.

708
00:39:17,310 --> 00:39:26,310
I want this because this allows me not only that, but I want the ability from my office to be able to

709
00:39:26,310 --> 00:39:28,670
control the decision making from my populace.

710
00:39:28,670 --> 00:39:31,270
Free apple glasses for everyone in North Korea.

711
00:39:31,980 --> 00:39:32,860
Right. Exactly.

712
00:39:33,100 --> 00:39:39,820
The only other thing I can think about is of how to prevent this is if a company comes up with

713
00:39:40,260 --> 00:39:43,260
something like it's basically fight fire with fire.

714
00:39:43,740 --> 00:39:51,940
There needs to be a technology where you can have a chat GPT type thing that that's whole role in

715
00:39:51,940 --> 00:39:54,980
life is to defend you.

716
00:39:55,500 --> 00:39:56,100
Yeah.

717
00:39:56,660 --> 00:39:58,940
If I was smart enough, I'd go work for that company.

718
00:39:59,220 --> 00:40:00,900
Hopefully that company exists.

719
00:40:00,940 --> 00:40:01,340
Maybe.

720
00:40:01,980 --> 00:40:03,260
Why can't it be Microsoft?

721
00:40:03,260 --> 00:40:06,540
How come you can't be the defenders of because Google do no evil.

722
00:40:06,540 --> 00:40:07,180
Google could do it.

723
00:40:07,580 --> 00:40:08,340
They're not going to do it.

724
00:40:09,020 --> 00:40:09,460
Yeah.

725
00:40:10,140 --> 00:40:16,780
I keep hearing that motto and I have concluded that that evil is an acronym, but I don't know what

726
00:40:16,780 --> 00:40:17,500
it stands for.

727
00:40:18,100 --> 00:40:21,740
So one other thing I thought of, we're going to go a little bit over time because I'm sure you

728
00:40:21,740 --> 00:40:22,820
have some closing thoughts here.

729
00:40:23,420 --> 00:40:25,460
Not again, you're wondering, we started late.

730
00:40:25,500 --> 00:40:25,780
All right.

731
00:40:25,820 --> 00:40:26,740
We started late.

732
00:40:26,860 --> 00:40:27,940
It's not an hour podcast.

733
00:40:27,940 --> 00:40:29,260
It's as long as we want it to be.

734
00:40:29,820 --> 00:40:30,460
That was for the list.

735
00:40:31,380 --> 00:40:32,260
I think you know this.

736
00:40:32,300 --> 00:40:37,620
I go backpacking two or three times as much as I could get out of the house during the

737
00:40:37,620 --> 00:40:37,980
summer.

738
00:40:38,810 --> 00:40:46,010
I like to be out in the middle of nowhere with no people around with no cell service

739
00:40:46,010 --> 00:40:50,610
because damn it, it's just too easy to glance at my phone all the time when I'm home or

740
00:40:50,610 --> 00:40:53,290
look something up or stream a video.

741
00:40:54,050 --> 00:41:01,510
And I think a lot of people have forgotten what it's like to be a little bored or to be

742
00:41:01,590 --> 00:41:03,550
a little undistracted.

743
00:41:04,190 --> 00:41:06,190
And every time I'm out there, this story is going somewhere.

744
00:41:06,230 --> 00:41:06,790
Don't worry about it.

745
00:41:07,030 --> 00:41:10,950
Every time I'm out there, I think, you know, this is why people live off the grid.

746
00:41:10,950 --> 00:41:16,950
There's a lot of intrinsic value you get from just being off the grid.

747
00:41:17,190 --> 00:41:21,790
And we've all known or at least known of people who know people that have gone off

748
00:41:21,790 --> 00:41:23,070
the grid, no social media.

749
00:41:23,470 --> 00:41:28,070
Maybe they use a flip phone and they just do not go online at all.

750
00:41:28,830 --> 00:41:30,630
They would never buy Apple glasses.

751
00:41:31,070 --> 00:41:39,310
I wonder if this, I mean, in the dystopic science fiction thriller I'm viewing as

752
00:41:39,310 --> 00:41:43,750
this conversation, I think it's a growing number of people and maybe even

753
00:41:43,750 --> 00:41:51,470
communities in our doomsday future who consciously stay off of every single

754
00:41:51,470 --> 00:41:52,910
thing that can track them.

755
00:41:53,510 --> 00:41:55,830
Oh, I know of multiple people.

756
00:41:55,830 --> 00:42:00,230
Like, but I think there will be more as this track, as not only this

757
00:42:00,230 --> 00:42:05,590
tracking becomes more ubiquitous, but suggestions are as you worry decisions

758
00:42:05,590 --> 00:42:10,150
are made on your behalf, it would encourage more people just to say no to it all.

759
00:42:10,430 --> 00:42:11,310
Uh, yeah.

760
00:42:11,510 --> 00:42:17,630
And in there, I'm like now I know I'm going into conspiracy theory, but, but

761
00:42:18,030 --> 00:42:21,660
there are some on the, on the crypto coin thing.

762
00:42:22,260 --> 00:42:26,140
There are some that believe that a boot, a push towards a pure digital

763
00:42:26,140 --> 00:42:30,910
currency is blocks that as a solution.

764
00:42:31,940 --> 00:42:32,180
Right.

765
00:42:32,180 --> 00:42:34,480
It's essentially, right.

766
00:42:34,480 --> 00:42:34,800
Yeah.

767
00:42:35,520 --> 00:42:41,280
If, if your option is to then go back to God, what is it?

768
00:42:41,280 --> 00:42:47,840
600 BC and, and, and do trading of goods and services, right?

769
00:42:47,840 --> 00:42:52,920
The whole point of money was to, to simplify you, you go off the grid,

770
00:42:52,920 --> 00:42:55,160
you start raising sheep, you want eggs.

771
00:42:55,160 --> 00:42:58,320
You're like, okay, I'll take a hundred eggs for my one sheep.

772
00:42:58,320 --> 00:42:59,120
Bartering work.

773
00:42:59,120 --> 00:42:59,520
Sure.

774
00:42:59,520 --> 00:42:59,800
Right.

775
00:43:00,160 --> 00:43:05,680
And so if we move to, to, to pure cryptocurrency, well, that can't happen.

776
00:43:06,280 --> 00:43:10,080
Then, then the other thing that's going to happen, of course, I think Burt, yeah,

777
00:43:10,080 --> 00:43:12,680
I don't happen to burning man becomes a year round thing.

778
00:43:12,920 --> 00:43:13,600
Exactly.

779
00:43:13,960 --> 00:43:18,200
It becomes, there will be, that's exactly right.

780
00:43:19,200 --> 00:43:19,800
And I'm nodding.

781
00:43:19,800 --> 00:43:21,440
I'm not against this feature.

782
00:43:21,640 --> 00:43:24,800
Bernie, man, I don't think has enough space to fit all.

783
00:43:25,160 --> 00:43:27,240
No, we need several of these, all these entities.

784
00:43:27,600 --> 00:43:28,080
All right.

785
00:43:28,400 --> 00:43:29,800
Uh, I got to go, man.

786
00:43:30,120 --> 00:43:30,440
Yeah.

787
00:43:30,640 --> 00:43:33,240
Um, and you're eating, so what's being the podcast is over.

788
00:43:33,600 --> 00:43:34,160
It's over.

789
00:43:36,100 --> 00:43:36,740
All right, everybody.

790
00:43:36,740 --> 00:43:37,620
Thanks for listening.

791
00:43:37,660 --> 00:43:40,340
Um, if we're, if the world still exists in a few weeks, we'll

792
00:43:40,340 --> 00:43:41,460
be back for another podcast.

793
00:43:41,700 --> 00:43:42,460
I'm Alan.

794
00:43:42,620 --> 00:43:43,300
I'm Brent.

795
00:43:43,660 --> 00:43:45,580
And we'll see you another time.

