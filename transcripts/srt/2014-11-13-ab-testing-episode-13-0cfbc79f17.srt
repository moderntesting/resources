1
00:00:16,340 --> 00:00:22,260
Hey everybody. Hello. Hey, I'm Alan. I'm Brent. And guess what? We're back again for episode lucky number 13

2
00:00:23,060 --> 00:00:28,820
of AB testing. So how you doing Brent? I'm doing swell. Good. What's new with you? Well, so we're

3
00:00:28,820 --> 00:00:33,620
running a little, we started late today. It was very exciting. Did you have a hard time getting out

4
00:00:33,620 --> 00:00:39,440
of bed this morning? No. So I ran in, got to the car, I was going to be right on top behind the

5
00:00:39,440 --> 00:00:45,060
school bus on 40th. Yeah, and apparently you can't just like zip around them. You can't. They put

6
00:00:45,060 --> 00:00:49,060
that little sign out that says don't go. And I was in the wrong lane. I couldn't even do a UE.

7
00:00:49,060 --> 00:00:53,380
That happened. Oh wow. Yeah, that happens. Um, even in my neighborhood, if I leave a little late,

8
00:00:53,940 --> 00:00:57,700
I may as well leave a lot late because I'm going to be driving behind a school bus for a while.

9
00:00:57,700 --> 00:01:02,580
Oh no, I, I, it's exactly true. If I can't get out of my house before seven,

10
00:01:03,140 --> 00:01:12,660
then stay there until eight. I'm usually in by five 36. Wow. I get in it. Yeah. I think

11
00:01:12,660 --> 00:01:15,300
I get in early because there's nobody here when I get here at eight usually.

12
00:01:16,880 --> 00:01:21,360
There's a couple of people here. There's one guy in my building that I compete with to see who,

13
00:01:21,360 --> 00:01:27,520
who gets there first. That's usually me. That's a really dorky competition. Yeah. And it's not an

14
00:01:27,520 --> 00:01:34,220
example. I would suggest others follow. All right. Hey, uh, what's new with me? Well,

15
00:01:34,220 --> 00:01:39,420
I'm glad you asked. Uh, last week, was it just last week? Yeah. I took a couple of days and

16
00:01:39,420 --> 00:01:45,340
went to the Google test automation conference up in Kirkland G tack as the cool kids call it.

17
00:01:46,400 --> 00:01:52,640
A couple hundred people there. It was fun. Met a lot of people reconnected with a lot of people.

18
00:01:52,640 --> 00:01:55,840
It was a good time, but a couple of things I want to talk about. I'm not going to go through

19
00:01:55,840 --> 00:02:02,050
a laundry list of the talks and what they were about. Just out of curiosity though.

20
00:02:02,930 --> 00:02:06,610
Have you gone to these events before? I assume this is my second G tech. I go to all the G

21
00:02:06,610 --> 00:02:13,300
techs that I can drive to and use in Seattle and I'm, I can't remember 2000. I can't remember

22
00:02:14,020 --> 00:02:19,060
was the size of the audience about the same as last year. Yeah, it is about, well,

23
00:02:19,060 --> 00:02:24,580
I didn't go last year about the same. The Seattle, when I went last time. Yeah. They keep it.

24
00:02:24,580 --> 00:02:31,060
They keep the attendance at a cap. I don't know what it is. Okay. All right. So anyway,

25
00:02:31,060 --> 00:02:36,020
a bunch of people giving talks about stuff they're doing a lot on tools, obviously for

26
00:02:36,020 --> 00:02:42,740
Android and web automation. Selenium is definitely, you know, becoming not becoming it's, it is the

27
00:02:42,740 --> 00:02:49,460
de facto for, for web automation. Selenium is sweet. Yes. Cool thing like Sel Android

28
00:02:49,460 --> 00:02:54,260
for Selenium on Android. Some cool little ideas and guy from American Express just talked about

29
00:02:54,260 --> 00:02:58,820
the different freely available tools he uses for testing. It was just kind of a cool,

30
00:02:58,820 --> 00:03:04,660
nice little overview, but a couple themes came up. I wanted to talk about, well, one theme.

31
00:03:05,940 --> 00:03:12,020
And then one observation. So a guy from Facebook was there. Okay. And, and Brett and I have talked

32
00:03:12,020 --> 00:03:16,660
about combined engineering and if, can you have one engineering team? I just wrote a couple blog

33
00:03:16,660 --> 00:03:23,220
posts about it. He gets up there and he up to give his talk, not up there, but up give his talk.

34
00:03:23,220 --> 00:03:27,140
And he rehashes, he restates the fact that, you know, Facebook has no testers, you know,

35
00:03:27,140 --> 00:03:32,020
and we get by. And then I thought it was interesting that his title, his role is he's

36
00:03:32,020 --> 00:03:37,300
the lead of a product reliability team. Okay. They don't have testers. They have

37
00:03:37,300 --> 00:03:41,540
product reliability. If I, and if I heard right, they also have engineering productivity teams.

38
00:03:42,290 --> 00:03:47,650
Okay. A lot of the things we expect those test minded generalizing specials to do,

39
00:03:47,650 --> 00:03:52,050
they don't have testers, but they have, you know, it just, it, it rehashes to me,

40
00:03:52,050 --> 00:04:00,160
it reiterates to me that the testing activity continues whether you have a test discipline

41
00:04:00,160 --> 00:04:06,240
or test team or not. The, the, do you, do you have data that confirms that? Like, so

42
00:04:06,880 --> 00:04:13,600
those in podcast land, I don't have data. I have, hold on, hold on. Those in podcast land holding

43
00:04:13,600 --> 00:04:19,040
can't see Alan's face, but the, what your body language just communicated was,

44
00:04:20,160 --> 00:04:22,640
yeah, dude, it's still a testing. You're calling it something else.

45
00:04:24,480 --> 00:04:30,240
Yes. And no, I think maybe a little bit I'm saying that, but I think more importantly is

46
00:04:30,240 --> 00:04:37,620
it doesn't fricking matter. I think, I think, I mean, you call it by a different name,

47
00:04:37,620 --> 00:04:45,740
but we've discussed this numerous times. And I think on the podcast that, those roles,

48
00:04:47,520 --> 00:04:55,840
productivity, engineering, reliability are really sort of non-functional requirements

49
00:04:55,840 --> 00:05:06,740
required specialists. And so I don't know. I mean, I would say we were to disambiguate the term

50
00:05:06,740 --> 00:05:11,780
test. Yeah. Those are the type of responsibilities that we funded in our organizations prior,

51
00:05:12,340 --> 00:05:17,620
but the key pivot is things around component tests and functional tests, right? That's going

52
00:05:17,620 --> 00:05:24,180
to the, the dev org. And I haven't heard yet you say that this guy owned those, either of those.

53
00:05:24,500 --> 00:05:29,120
And I'm actually, to be honest, and I hope he's not listening because I don't remember what he

54
00:05:29,120 --> 00:05:34,830
talked about. I guess that was the interesting point it took away. Generally at most dev companies,

55
00:05:34,830 --> 00:05:37,550
developers are going to own that unit and functional testing. And they definitely do it

56
00:05:37,550 --> 00:05:41,230
Facebook. But for some of the big picture things that people worry that when a lot of

57
00:05:41,230 --> 00:05:44,670
people go up in arms and they say, Oh, they don't have a test team or some company

58
00:05:44,670 --> 00:05:48,750
X doesn't have a test team, they go, well X and Y and Z aren't going to happen.

59
00:05:48,750 --> 00:05:56,930
And my point is that those things do happen. Yeah. It's a couple of podcasts ago, you brought up

60
00:05:56,930 --> 00:06:02,290
the one scenario, the one Twitter where a guy was like, if you don't have a test team, you suck in

61
00:06:02,290 --> 00:06:09,330
essence. Right. And, and it's just, it's just not true. Facebook definitely has a continuous

62
00:06:09,330 --> 00:06:16,050
deployment model. They definitely have flighting rings. They use exposure control to minimize risk.

63
00:06:16,770 --> 00:06:23,730
Right. Yeah. Anyway, what was your takeaway from, I mean, did you think they were doing something?

64
00:06:25,040 --> 00:06:29,040
As I mentioned before, and I now have to say it again, because you don't freaking listen to me.

65
00:06:29,040 --> 00:06:33,440
No, you said that testing activity. I don't remember what he, what his talk was about.

66
00:06:33,440 --> 00:06:36,400
I just thought that statement was interesting. So I'm going to move on

67
00:06:37,120 --> 00:06:41,620
before you asked me again and I get angry, like the angry weasel,

68
00:06:42,100 --> 00:06:47,950
um, people that read my, the, the about angry weasel part of my blog will know the whole story

69
00:06:47,950 --> 00:06:52,590
behind what angry weasel means. Nothing to do with me has to do with the tooth of the weasel and

70
00:06:52,590 --> 00:07:00,910
a German automotive issue. So yeah. Anyway, moving on. Thanks for sharing. There was an

71
00:07:00,910 --> 00:07:06,430
interesting theme. Probably I'm not going to say half. It was more than half. Three quarters of the

72
00:07:06,430 --> 00:07:14,270
talks mentioned a two word phrase, some more often. And it became like the joke, like, Oh,

73
00:07:14,270 --> 00:07:19,150
they said it. Everybody drink. It's like the G tech drinking game. Um, which was the term

74
00:07:19,150 --> 00:07:24,540
flaky tests. Brent, have you ever heard of this word before? Flaky tests?

75
00:07:24,540 --> 00:07:29,580
Yes. Over and over and over and over. What the hell is wrong with us? The flaky tests

76
00:07:29,580 --> 00:07:34,060
are so prominent. Uh, there's an interesting vanity metric people put up on the slides.

77
00:07:34,620 --> 00:07:41,010
Uh, how many automated tests they run? I don't care how many automated tests you run.

78
00:07:41,010 --> 00:07:47,170
I guess it does give you, if you're trying to show scale, but even then it's such a, you know,

79
00:07:47,170 --> 00:07:52,290
tests aren't created equal. We know this idea of flaky tests came up blatant vanity metric.

80
00:07:52,290 --> 00:07:58,450
And, uh, guess what? It's come up at Microsoft about a half a zillion, no quad zillion,

81
00:07:59,200 --> 00:08:05,520
I'm quite drill zillion. I don't know a number, a big number, Googleplex. Yeah.

82
00:08:05,520 --> 00:08:12,720
Maybe it's come up with Google times. Yeah. Uh, with, I run this test. Sometimes it's flaky.

83
00:08:12,720 --> 00:08:17,360
Sometimes it's not. Let me give you a statement and I can argue both sides of this. Uh, someone

84
00:08:17,360 --> 00:08:25,500
said at the conference, a flaky test is worse than no test. What do you think about that?

85
00:08:26,220 --> 00:08:35,680
Yeah. I, so when we talk about flaky tests, it reminds me of a white paper comment, uh,

86
00:08:35,680 --> 00:08:40,640
Bill Gates did years ago where he talks, talks about the automation paradox. Do you remember

87
00:08:40,640 --> 00:08:46,000
the automation paradox? Uh, vaguely. Yeah. Automation paradox. Was it called the automation

88
00:08:46,000 --> 00:08:50,000
paradox? I remember the comment. I don't remember the paper, but anyway, go on. Yeah. The, the,

89
00:08:50,000 --> 00:08:56,720
it might've been test paradox. I don't know. In essence, the paradox is not a podcast.

90
00:08:56,800 --> 00:09:06,720
On long-term memory and recall. No, but it is one on ADHD for sure. Um, the, the test paradox in essence

91
00:09:07,520 --> 00:09:15,950
is if you can make a world's perfect test that validates the code, then you have to actually

92
00:09:15,950 --> 00:09:23,980
create the test that's better than the code. So there's a, then so that there's a implication

93
00:09:23,980 --> 00:09:28,940
that perhaps you should be using the code inside the test and not the code inside the product.

94
00:09:30,270 --> 00:09:34,590
That's what makes it a paradox. I hope that was sort of a philosophical point and not,

95
00:09:35,920 --> 00:09:41,840
uh, this was back in, in, in the day Bill Gates still ran the show. All right. And we had

96
00:09:42,400 --> 00:09:50,660
what a quadrillion dollars in payroll going to test. And, uh, I remember one small percent

97
00:09:50,660 --> 00:09:54,420
going to engineering. It's probably 15 years ago, the test architect group, we had a presentation

98
00:09:54,420 --> 00:09:58,820
for Bill Gates and talking about what's going on. And he walked into the room saying,

99
00:09:58,820 --> 00:10:04,340
I spend a lot of money on tests. I have no idea what you guys do. We test. Yeah, exactly.

100
00:10:04,980 --> 00:10:13,060
The remainder off the track. Yeah. So I think that was a really long answer. Actually,

101
00:10:13,700 --> 00:10:16,980
a lot of words that didn't actually answer the question. So let me answer the question.

102
00:10:17,780 --> 00:10:23,700
Are flaky tests better than or worse than no test. And it depends because not only are not

103
00:10:23,700 --> 00:10:29,660
all tests created equal, not all flaky tests are created equal. And by definition,

104
00:10:29,660 --> 00:10:35,580
flaky is a test that sometimes passes, sometimes fails, and you have to spend your time looking

105
00:10:35,580 --> 00:10:41,890
at it to figure out whether it's a product bug or a test bug. So if you, if you evaluate it across,

106
00:10:41,890 --> 00:10:50,030
uh, payroll cost, yeah, a flaky test is going to cost you more than no test in the short term.

107
00:10:50,750 --> 00:10:58,670
In the long term, you don't know it having no test, right? No tests on something that no one uses

108
00:10:58,670 --> 00:11:03,710
probably not going to cost you much. But there seems to be an unwritten rule in test automation is

109
00:11:04,430 --> 00:11:09,550
I've written an automated test. I've created a test that does something, you know,

110
00:11:09,550 --> 00:11:13,470
I could not have made a test. And now that I've written it, because it's automated,

111
00:11:13,470 --> 00:11:19,950
it now must run forever and ever and never and never to be retired. It must run on all

112
00:11:19,950 --> 00:11:25,950
platforms available because it's automated. It's free. I would say, I mean, I don't think I would

113
00:11:25,950 --> 00:11:33,710
agree with particularly with where we are in the modern world with flaky tests, uh, better or

114
00:11:33,710 --> 00:11:42,060
worse than no test. Um, but if you're going to have tests, they need as best as possible. They

115
00:11:42,060 --> 00:11:48,860
need to not be flaky. The point of putting that test into play is you're trying to create an

116
00:11:48,860 --> 00:11:54,780
automated system that you can trust. And that's where I've gotten to is trustworthy tests. You

117
00:11:54,780 --> 00:12:01,620
want tests that you can trust the result of. Uh, I forget who it was, but I almost screamed from

118
00:12:01,620 --> 00:12:10,510
my back row little observatory when another speaker said, uh, he was asked about, you know,

119
00:12:10,590 --> 00:12:14,510
he's talking about flaky tests and they had a bunch of flaky tests and they do things like rerun them

120
00:12:14,510 --> 00:12:19,390
until they pass and so things like that, um, you know, investigate them. And he mentioned the

121
00:12:19,390 --> 00:12:22,670
fact that we've run through in the past at Microsoft where testers read a bunch of automation

122
00:12:22,670 --> 00:12:28,750
and their new job became to investigate failure failed tests all day. But someone asked him and

123
00:12:28,750 --> 00:12:35,920
it wasn't even me. Uh, he said, so do you investigate? Do you believe you have flaky tests

124
00:12:36,800 --> 00:12:41,120
in your tests that are passing? Do you have tests that are passing that maybe should be failing?

125
00:12:41,760 --> 00:12:46,400
Of course. And his answer was, and I'm going to recreate, you know, that scene in the movie

126
00:12:46,400 --> 00:12:49,280
where you yell something out, but it doesn't really happen. You're still sitting there.

127
00:12:49,280 --> 00:12:54,960
That happened with me. He said, I haven't seen that. So when asked, have you seen tests that

128
00:12:54,960 --> 00:12:59,440
are passing? It should be failing. He goes, I haven't seen that. And the little voice in my

129
00:12:59,440 --> 00:13:05,730
head stood up on my chair and screamed and yelled, I'm going to back up here. Have you looked?

130
00:13:07,660 --> 00:13:11,180
Of course, which the answer was no. And we fight and I punch him and tear his head off and throw

131
00:13:11,180 --> 00:13:16,620
it out of the hallway. You are angry today. No, you know, it's the things that happen in

132
00:13:16,620 --> 00:13:21,660
your head. They're safe there. You bottle them up and squish them in. Then that anger never

133
00:13:21,660 --> 00:13:26,620
gets out into the world. Oh, I've been repressing feelings for decades. Oh, and I get that. That's

134
00:13:26,620 --> 00:13:35,260
a whole different story. The other thing, like I want to tie this to the modern world, I guess.

135
00:13:36,450 --> 00:13:42,050
This reminds me of a presentation we saw from a colleague of ours, Michael.

136
00:13:43,010 --> 00:13:49,650
Oh, hey, Michael, speaking at Sasquog next month, sasqag.org on the third Thursday of November,

137
00:13:49,650 --> 00:13:53,890
whatever day that is for Seattle listeners. Come out and listen to what Michael has to say. Michael

138
00:13:53,890 --> 00:13:58,850
recently left Microsoft. He's hanging out trying to figure out what to do next. He's

139
00:13:59,890 --> 00:14:09,180
the kicker on there. If you go to sasquag.org and check it out. Check out the after first talk. I

140
00:14:09,180 --> 00:14:13,500
can't I'll paraphrase it here, but he figured out that he didn't like testing after being a tester

141
00:14:13,500 --> 00:14:18,260
for 15 years. So he's going to talk about that kind of how he came to that realization and kind

142
00:14:18,260 --> 00:14:22,020
of where he's going from here. But I don't know any more details. I wonder if are we talking about

143
00:14:22,020 --> 00:14:32,180
the same Michael? Michael H. C. Oh, hey, peer peer disconnect here. But still, if your local

144
00:14:32,180 --> 00:14:43,070
go to Sasquog on Thursday. Tell me about Michael C. So Michael Michael H. Yeah. He was a mentee of

145
00:14:43,070 --> 00:14:50,030
mine until very recently, although I guess I guess he might still be. But yeah, I didn't know he was

146
00:14:50,030 --> 00:14:56,270
doing the Sasquog tech. I should go show up. Right. But the Michael C. No. So one of the

147
00:14:56,270 --> 00:15:06,660
things that he did is he ran tests or test results through a Bayesian network in essence.

148
00:15:08,580 --> 00:15:13,060
And one of the things that he was able to find by by applying data science to the tests

149
00:15:14,320 --> 00:15:22,160
is that you're able to determine which tests when they report pass, you can trust that they're

150
00:15:22,160 --> 00:15:28,690
actually passing and which tests when they report fail, you can trust that they're actually failing.

151
00:15:28,690 --> 00:15:34,370
And some tests are very good negative indicators and some are very good positive indicators.

152
00:15:34,370 --> 00:15:38,930
And I found that was fascinating because I've been a test manager for

153
00:15:39,330 --> 00:15:46,980
a good portion of my career. And flaky tests. Yes, when you're in that role, it's just an

154
00:15:46,980 --> 00:15:54,580
irritant because you can see it stealing your payroll day over day. Yeah. So but if I had a

155
00:15:54,580 --> 00:16:02,340
signal that could tell me, hey, this sweet, when it passes, I can trust it. But this this other

156
00:16:02,340 --> 00:16:08,450
suite, I can only trust it when it fails, then it would help me reduce the cost of flaky.

157
00:16:08,450 --> 00:16:12,260
I think that's a start. I'm gonna tie that to something that came out of the conference

158
00:16:12,260 --> 00:16:16,420
in my takeaway, and I'm not going to go deeply into my diatribe on test selection.

159
00:16:16,980 --> 00:16:21,380
I'll save it for I'll save it for another podcast or a blog. But what was interesting that came out,

160
00:16:21,380 --> 00:16:24,820
there was some email after the conference among the attendees. And I think someone from

161
00:16:24,820 --> 00:16:30,100
Google mentioned that, well, we use code coverage to figure out which tests, which tests hit which

162
00:16:30,100 --> 00:16:34,020
lines of code. So that's great. That's a good start. And people latched on that, like, oh,

163
00:16:34,100 --> 00:16:38,260
my gosh, I know how to do test selection. Now I can do this. And let me tell you,

164
00:16:38,260 --> 00:16:46,930
it is just the start. And it was so interesting to me that, yes, you can use that. Yes, you can use

165
00:16:49,360 --> 00:16:54,560
engines based on, you know, trying to evaluate the trustworthiness of the test.

166
00:16:54,560 --> 00:17:01,680
And building that engine where Michael, Michael's work started and needs to keep on going is there

167
00:17:01,680 --> 00:17:11,250
are dozens of factors that can go into what makes up a trustworthy test or makes up a valuable test

168
00:17:11,250 --> 00:17:16,610
even better. Yeah, whether a test passes or fails, and I can trust the pass and fail result is good.

169
00:17:16,610 --> 00:17:20,850
That's great. But I want to know whether that test is going to be valuable for me at that moment in

170
00:17:20,850 --> 00:17:27,810
time, which coverage provides part of the speed the test runs, if it's in there, if I can get the

171
00:17:27,810 --> 00:17:33,170
same value of a test that runs in one second from a test that runs in 10 minutes, that one second test

172
00:17:33,170 --> 00:17:39,250
is a lot more valuable. And I can go down a list and I need to enumerate these, not speaking,

173
00:17:39,250 --> 00:17:44,050
so I forget some important ones. But the idea is, what I've done in the past is build sort of a

174
00:17:44,050 --> 00:17:50,370
heuristic engine that takes a dozen or 20 or 30 factors of what makes up a valuable test. You

175
00:17:50,370 --> 00:17:55,710
know, how long has it been since that test has been run is when I use a lot. Going back to the

176
00:17:55,710 --> 00:18:00,990
comment about running every test on every build forever and ever, that's great. But if that test

177
00:18:00,990 --> 00:18:08,270
has passed every single time for the last 20 years, and I don't run it today, is that okay?

178
00:18:08,270 --> 00:18:12,670
Some testers actually freak out. No, because the day you stop running it is the day that there'll

179
00:18:12,670 --> 00:18:17,310
be a bug there, it'll find. So I would say there's less value in a test that has passed

180
00:18:17,310 --> 00:18:22,830
every single time. But the moment I don't run it for a day or for a build, it gets a little

181
00:18:22,910 --> 00:18:29,780
bit more value. So there's value in how long has it been since this test has run. So has

182
00:18:29,780 --> 00:18:35,900
this test found a bug before? Has this test found multiple bugs? I can find multiple ways to wait

183
00:18:35,900 --> 00:18:43,100
bugs and then pick, say I have 100 million kazillion tests and they take 30, you know,

184
00:18:43,980 --> 00:18:50,300
because I'm web scale, I can run, you know, I can run them in 24 hours. Well, I don't have 24

185
00:18:50,300 --> 00:18:57,970
hours. I want to run tests in an hour. And the data farm junkies will go, well, more machines! But

186
00:18:57,970 --> 00:19:02,130
no, there's probably less than an hour of those tests that are actually valuable at that moment

187
00:19:02,130 --> 00:19:07,410
in time and run an algorithm to figure out which are the most hour long worth of valuable tests.

188
00:19:08,050 --> 00:19:12,210
There was a lot of science that was occurring in the last 10 years.

189
00:19:12,690 --> 00:19:16,450
Science occurred? That's awesome. Yeah, no, there was a lot of studies around like,

190
00:19:17,410 --> 00:19:21,650
in terms of some of the variables you're talking about, like you didn't mention co-coverage.

191
00:19:22,850 --> 00:19:27,970
I mentioned using the cover. Yeah, I did. I mentioned using... You said coverage.

192
00:19:29,330 --> 00:19:32,530
That could have been feature coverage, which is how I interpreted it. But that's fine.

193
00:19:33,170 --> 00:19:37,570
Use the tools. Use the tools, Luke. Yeah, like a lot of hypotheses around

194
00:19:37,570 --> 00:19:45,250
what's valuable. Like my experience is, okay, I didn't find a lot of value in example. Hey,

195
00:19:45,250 --> 00:19:53,410
this test found a bug before versus this one didn't. Because it turns out that the bug that

196
00:19:53,410 --> 00:19:59,650
it found was just a stupid dev error. And they fixed it. And now it's unlikely to do that.

197
00:20:01,090 --> 00:20:07,680
The one reason why we had to do all these tests, because there was so much uncertainty and so

198
00:20:08,320 --> 00:20:17,920
little ability to get tests correlated to the code that they're covering and the customer scenario

199
00:20:17,920 --> 00:20:23,920
that they're covering. There was also a big push over quantity over quality. A lot of automated

200
00:20:23,920 --> 00:20:28,960
tests was better than having some really good well crafted ones. And a lot of times flaky tests,

201
00:20:28,960 --> 00:20:33,920
whether they're false positives or false negatives, happened because someone was just cranking out

202
00:20:33,920 --> 00:20:39,280
some code that did crappy junk and they weren't thinking about writing a trustworthy test.

203
00:20:39,280 --> 00:20:47,820
I remember my second to last TM gig. One of the PMs came to me and just just like I don't

204
00:20:47,820 --> 00:20:55,100
think we have enough tests. How many is enough? And I basically said, okay, let's talk about this

205
00:20:55,100 --> 00:21:00,210
because I want to be very crisp on this discussion. What if I were to tell you

206
00:21:01,470 --> 00:21:12,110
that today we have a thousand tests for this area and that I can take one IC and create another

207
00:21:12,110 --> 00:21:18,750
thousand, double those tests in one week. She's like, that'd be fantastic. Let's get that going.

208
00:21:18,750 --> 00:21:25,700
All right, a batch smell to make that happen. Yeah. And I said, hold on. What if I tell you

209
00:21:25,700 --> 00:21:36,620
that my thousand tests currently cover the product 5% and that new thousand tests will get me to six

210
00:21:36,620 --> 00:21:46,000
total? She's like, what's cover the product mean? And I said, co-coverage. So yeah, you're very much

211
00:21:46,000 --> 00:21:54,340
right because just like Bill Gates didn't know what test did. If Bill Gates doesn't know,

212
00:21:54,820 --> 00:22:02,820
it shouldn't expect a standard PM to know. Right. This goes back into the whole, we want to count

213
00:22:02,820 --> 00:22:07,780
things. Unfortunately, some actually, I don't want to, some people want to count things and use that

214
00:22:07,780 --> 00:22:14,860
as a measure of progress. I want to measure things and counts may be important to that,

215
00:22:14,860 --> 00:22:22,140
but counts by themselves are vanity metrics. I agree. Interesting. I, my big takeaway at

216
00:22:22,140 --> 00:22:28,930
the end was I think, you know, I've thought a lot about this and have, I think pretty good

217
00:22:28,930 --> 00:22:34,610
ideas about test selection. And I've kind of, I know I'm comfortable retiring tests. I'm comfortable

218
00:22:34,610 --> 00:22:40,720
not running tests. I'm comfortable trying to come up with, you know, I'm comfortable that I have

219
00:22:40,720 --> 00:22:45,520
good algorithms for figuring out which are the most important tests to run. But it was interesting

220
00:22:45,520 --> 00:22:52,610
that I didn't feel like anyone else was really not only were they there or close or whether they

221
00:22:52,610 --> 00:22:59,170
cared. They were happy just to run lots of tests and ignore failures or, or have logic in the test

222
00:22:59,170 --> 00:23:05,330
to rerun the test if it failed or silly things like that. So your, your story reminds me of another

223
00:23:05,330 --> 00:23:13,650
colleague of ours, Harry, who, um, when I worked with him was over, when I was a part of the Bing

224
00:23:13,650 --> 00:23:19,250
team and Harry had come from a Google and was working there as well. And he had taught the

225
00:23:19,250 --> 00:23:23,970
Bing team this concept called heuristic based testing, which is something he had learned from

226
00:23:23,970 --> 00:23:34,300
Google and a heuristic based test is one that's good enough. It has a benefit that it is super fast,

227
00:23:35,500 --> 00:23:43,650
right? You don't, um, as an example, if you're trying to test a map function instead of, uh,

228
00:23:44,610 --> 00:23:50,370
testing the directions, uh, accurately and, and going through, is this right? Is this right? And

229
00:23:50,370 --> 00:23:54,290
coming up with your own algorithm, we go, can I find a better algorithm that it should have picked?

230
00:23:55,440 --> 00:24:02,370
What you do is you load, load up another shipping map software and you compare side by side,

231
00:24:02,370 --> 00:24:08,460
which ends up being a ton easier, uh, than trying to map through the algorithm.

232
00:24:08,460 --> 00:24:11,740
Well, isn't that, you know, I'm very familiar with Harry's work on, you know,

233
00:24:11,740 --> 00:24:17,580
using models for this. It was, um, this was more than this was relatively, but what's

234
00:24:17,580 --> 00:24:23,100
interesting is, um, and I'm a big fan of Harry. Uh, what we used to do, one of the mistakes

235
00:24:23,100 --> 00:24:27,980
testers make a long time ago is, and I've done this myself early in my career is I ended up

236
00:24:28,780 --> 00:24:35,020
re-implementing the functionality of the program as the Oracle. So I did this, I was writing, um,

237
00:24:35,020 --> 00:24:41,340
some graphic. I want to make sure some grip. Could I use the function get pixel in windows

238
00:24:41,340 --> 00:24:47,010
to test set pixel? You know, could I, could I, could I set a pixel to gray and then get a

239
00:24:47,010 --> 00:24:52,290
pixel and see if it was gray. If it was a test past and that's where test paradox comes in. And

240
00:24:52,290 --> 00:24:56,130
what's interesting is, yeah, cause I, I went through this big thing where, okay, actually I

241
00:24:56,130 --> 00:25:00,450
can query the graphic drive and get the actual color there rather than go through the API,

242
00:25:00,450 --> 00:25:07,730
that, which is actually what set get pixel does. Um, and what Harry's solution was rather than

243
00:25:07,730 --> 00:25:13,810
rewrite that Oracle yourself, use the competition as an Oracle. Yeah. Nothing wrong with that,

244
00:25:13,810 --> 00:25:19,650
but it's, it's, it's a hell of a lot easier. It is a hell of a lot of easier, a ton faster,

245
00:25:20,290 --> 00:25:25,680
but it does create flaky tests because every so often you'll end up with a scenario

246
00:25:26,320 --> 00:25:34,510
where actually what your product is doing is better than the competitor. Yeah. And what I've

247
00:25:34,510 --> 00:25:39,870
been on products that I won't name where we had a competitor and our goal was to be bug for bug

248
00:25:40,670 --> 00:25:52,020
compatible with our competitor. We had, okay. All right. So anyway, I think, uh, probably

249
00:25:52,020 --> 00:25:59,010
more on G tech. We're going to move on. Um, let's talk a little about, uh, so you want to talk about

250
00:25:59,010 --> 00:26:04,690
the culture of meeting. No, I talk about meeting. So two, okay. Now, uh, where was I going to start?

251
00:26:05,330 --> 00:26:12,480
I had a thread with some peers last night and yesterday afternoon. And I said, we're talking

252
00:26:12,480 --> 00:26:16,000
about, I asked some questions and I said, don't really know. And then somebody said,

253
00:26:16,000 --> 00:26:21,200
we should hash this out in a room because email is tough. And I'm all about getting out of email.

254
00:26:22,240 --> 00:26:28,370
And I did something that I rarely do. I rarely do. I'm, because trying to get in sync with this team

255
00:26:28,370 --> 00:26:35,730
and just a lot of unknowns. And I, you will rarely hear this phrase from me out loud or an

256
00:26:35,730 --> 00:26:44,000
email. I said, how about we set up a weekly recurring meeting to hash some of these things out?

257
00:26:45,120 --> 00:26:51,280
If I were next to you at that time, I would have stared at you like, what did you just say?

258
00:26:52,080 --> 00:26:57,440
So, um, a lot of people think meetings are bad. You're, you're, you're starting to grow up.

259
00:26:57,440 --> 00:27:01,040
It's meeting meetings can be bad. You'll be suggesting V teams next.

260
00:27:01,900 --> 00:27:03,740
I'm going to suggest punching you in the throat.

261
00:27:05,680 --> 00:27:10,220
All right. So what problem did you solve with this suggestion?

262
00:27:11,170 --> 00:27:17,790
Let me, let me finish. Let me finish. So, um, uh, the reason is,

263
00:27:19,150 --> 00:27:24,190
the reason I, it doesn't, here is coming from me is I think many recurring meetings are,

264
00:27:24,190 --> 00:27:29,390
end up turning into the, let's sit around the room where everyone does email and someone will

265
00:27:29,390 --> 00:27:35,150
basically, I've actually been in an org where people got in the room and, um, everybody got their

266
00:27:35,150 --> 00:27:39,470
laptops out there working along and they didn't know what the meeting was about. So someone said,

267
00:27:39,470 --> 00:27:43,870
did you read the email I sent? And they said, no. So he read the email out loud to the room.

268
00:27:43,870 --> 00:27:47,470
It's like the opinion is like, Oh my God, this is surreal. It should be in like,

269
00:27:47,470 --> 00:27:51,070
like an office space kind of movie. It's crazy. I think a good example of

270
00:27:52,530 --> 00:27:57,410
recurring meetings at work are daily standups. Yep. They have a civic agenda. You,

271
00:27:57,490 --> 00:28:03,520
stay on track. It's like these things happen. You're done. Uh, I think brainstorming that not

272
00:28:03,520 --> 00:28:08,480
really for a recurring meeting, but brainstorming meetings work well for meetings. Uh, status

273
00:28:08,480 --> 00:28:16,240
meetings. Nuh-uh. That Brad's shaking his head too. So now I'm like, no. So I think when you

274
00:28:16,240 --> 00:28:23,230
need to get alignment, um, and this meeting will probably work a lot like a standup meeting.

275
00:28:23,230 --> 00:28:27,870
It's weekly. It's weird, but where it's, I will gather agenda items off here. Here's what

276
00:28:27,870 --> 00:28:33,150
we want to hash out. Make sure we're on the same page on, um, and then try and end as quickly as

277
00:28:33,150 --> 00:28:37,710
possible. Cancel and we don't need it. So it's more of a synchronization meeting. It really is.

278
00:28:37,710 --> 00:28:43,070
It is not, not a status meeting at all. I refuse to even attend someone else's status meeting.

279
00:28:43,710 --> 00:28:50,720
Uh, unless I shouldn't say refuse, I avoid cause some cases I will go if I want to get

280
00:28:50,720 --> 00:28:55,600
information, but generally my experiences status meetings are a big waste of time.

281
00:28:57,280 --> 00:29:03,840
They do not make Alan happy. I know this from, uh, even if it's a call in status meeting,

282
00:29:03,840 --> 00:29:08,000
I does not make Alan happy. Here's the point. And you know, I don't want to come across as old

283
00:29:08,000 --> 00:29:13,600
meetings are bad. Don't want to go to a meeting. What I want to come across is if I'm going to

284
00:29:13,600 --> 00:29:17,120
show up at a meeting and everyone's on their laptop and you're telling me a bunch of stuff

285
00:29:17,120 --> 00:29:25,280
that I, I could learn just as well. If you sent them to me via email, uh, I, uh, I don't care.

286
00:29:26,320 --> 00:29:31,600
The, the fact that everyone's on their laptop is a, is a, is a death note sign.

287
00:29:31,600 --> 00:29:34,720
So let me tell it, let me put another spin on this. So recurring me, I mean, I was meeting,

288
00:29:34,720 --> 00:29:40,160
it's going to be great. Um, and, but you know, I, I will schedule these things when the necessary.

289
00:29:40,160 --> 00:29:44,480
So let me flip this around. Um, I've been trying to meet with, um, a manager and other

290
00:29:44,480 --> 00:29:49,860
groups to talk about some stuff and calendar, his calendar, my calendar is pretty flexible.

291
00:29:49,860 --> 00:29:53,940
I have some meetings on there, but they're all, almost all of them are movable. Uh,

292
00:29:53,940 --> 00:29:58,340
calendar is totally packed for two weeks. So I said a meeting for two weeks out,

293
00:29:58,340 --> 00:30:05,570
find a slot, come to his office, be great day of the meeting. Two weeks later, uh, his admin

294
00:30:06,900 --> 00:30:12,260
emails me. He's not like a, it's not a exec or anything, but his admin emails me and says,

295
00:30:12,900 --> 00:30:18,820
uh, so-and-so is really busy. Need to push this out two more weeks. And to me,

296
00:30:19,970 --> 00:30:25,970
I don't know this person or their org, but when you have, even if you're a manager,

297
00:30:25,970 --> 00:30:30,770
especially if you're a manager, I know managers do have more meetings, but when you are that,

298
00:30:30,770 --> 00:30:37,650
when you're packed in meetings all day long and for two weeks at a time and your next opening,

299
00:30:37,650 --> 00:30:41,730
it's a, it's kind of a red flag for me. I don't know if you have an experience similar. I'm just

300
00:30:41,730 --> 00:30:45,330
kind of curious if I'm the only one that, that that sort of sparks the, the bristly,

301
00:30:45,410 --> 00:30:52,240
spidey meeting sense. Oh, no, I absolutely do. And again, um, for those on, on podcast land,

302
00:30:52,240 --> 00:30:59,970
one difference between Alan and I is I am a manager and that is my life, right? How many

303
00:30:59,970 --> 00:31:04,850
of those meetings you think you are, you're there because you're moving the product forward

304
00:31:04,850 --> 00:31:10,450
versus you're there because you're expected to be there. Uh, I don't go to the ones that

305
00:31:10,530 --> 00:31:18,370
I'm expected to be there. I go to the ones where either I actually take a, an open space sort of

306
00:31:18,370 --> 00:31:25,330
principle, uh, where either I will learn something or I believe I can contribute. But even then,

307
00:31:26,290 --> 00:31:32,770
uh, because of my unique role right now in the organization, there's a lot of high demand for

308
00:31:32,770 --> 00:31:38,960
my attention. I actually think it's because I'm wearing too many hats and that I should

309
00:31:38,960 --> 00:31:45,840
conscientiously begin a process of delegating some of these hats to people who I'm trying to

310
00:31:45,840 --> 00:31:50,800
grow into leadership. And that's what I'm going to look into next. And it's thankful that most of

311
00:31:50,800 --> 00:31:55,040
my team doesn't listen to this podcast. Well, this is another reason why I think

312
00:31:56,160 --> 00:32:00,480
flattening orgs is a good idea. I think if you're going to have managers that need to be,

313
00:32:00,480 --> 00:32:04,560
and they're actually good meetings, but managing managers are going to be in meetings most of the

314
00:32:04,560 --> 00:32:09,680
day. Let's have fewer managers. Let's have more people getting work done. The not that

315
00:32:09,680 --> 00:32:13,600
manager work isn't work, but you know what I mean? No. So the, the challenge will be once

316
00:32:14,930 --> 00:32:20,130
in order to make that succeed, we have to flatten and get rid of command and control. Yeah. Right.

317
00:32:20,130 --> 00:32:24,690
The reason why I mean, shameless plug in my last post, talk exactly about that. You need to have

318
00:32:25,330 --> 00:32:30,530
flatter orgs help in a lot of ways, but you have to have the right managers leading those

319
00:32:30,530 --> 00:32:36,850
orgs, or they're going to fail. If I have to be in every decision meeting with that, that my team

320
00:32:36,850 --> 00:32:42,610
of 30 don't have a decision meeting, right? Then it's never going to scale. Right. Well,

321
00:32:42,610 --> 00:32:47,970
that's kind of what happens. The reason the rationale for all of these sort of high level

322
00:32:47,970 --> 00:32:53,410
status meetings is there, of course, the decision meetings. One of the things, and we have a

323
00:32:53,410 --> 00:32:57,010
mailbag item and we're almost out of time, but one of the things that reminds me of is a

324
00:32:57,010 --> 00:33:05,170
conversation between a friend of yours and mine, Mr. James Whitaker. Everyone listening to this

325
00:33:05,170 --> 00:33:12,690
podcast knows who he is, so I won't obfuscate it, but he wrote a blog post about a year ago,

326
00:33:13,250 --> 00:33:18,850
basically saying on the culture of meetings and just banned them all. And I actually-

327
00:33:18,850 --> 00:33:21,970
What? James being an extremist? Weird.

328
00:33:21,970 --> 00:33:24,450
I know wacky. It's totally out of his character.

329
00:33:25,410 --> 00:33:30,290
We love you, James, but we know you're too cool to listen to our podcast.

330
00:33:30,290 --> 00:33:36,990
So he's not one of three, for sure. And it was interesting because I pushed back on that and I

331
00:33:36,990 --> 00:33:45,070
said, you're going too far, man. And I walked him through a meeting. If it adds ROI to the system,

332
00:33:45,070 --> 00:33:51,230
it needs to be done. And I walked him through the lean principle around how you decide. So lean,

333
00:33:51,230 --> 00:33:56,340
the number one principle of lean is get rid of waste out of the system. And if we have a meeting

334
00:33:56,340 --> 00:34:01,780
that is waste, yeah, get rid of it. And I walked him through. He was like, well, how do you know?

335
00:34:02,340 --> 00:34:07,060
And I said, well, it's very easy. If you would do more of it if you could,

336
00:34:09,120 --> 00:34:14,930
then it's not waste. That's value you're adding to the system. If you wouldn't do more of it

337
00:34:14,930 --> 00:34:21,490
if you could, but would rather instead would do less, or it could do less, then it's blatant

338
00:34:21,490 --> 00:34:25,730
waste. But if you can't do less, then it's actually just the cost of doing business.

339
00:34:26,620 --> 00:34:34,210
Now, when I bring this up, we talk about stand up, right? Most times you could do more stand up,

340
00:34:34,960 --> 00:34:43,440
but you wouldn't, right? And you could also do less, but you wouldn't, right? Stand up

341
00:34:44,080 --> 00:34:53,550
is basically a cost for execution, right? It adds some amount of value. You don't want to get rid of

342
00:34:53,550 --> 00:35:02,580
it, but again, you can overdo it. So you don't need to have three standups a day. Whereas status

343
00:35:02,580 --> 00:35:12,530
meetings, I would argue for the most part, they're waste. Yes, yeah. Agreed. So what's interesting

344
00:35:12,530 --> 00:35:17,650
is you can have, it's not all meetings are labeled so clearly, but I like this model because you may

345
00:35:17,650 --> 00:35:25,900
walk out of a meeting and go, wow, there was some value there, but I don't feel like it was a great

346
00:35:25,900 --> 00:35:30,780
use of my time. And you can ask the question, could we do less of it? And you go, and here's a message

347
00:35:31,820 --> 00:35:35,740
that I hit the default meeting length seems to be an hour long. It's like,

348
00:35:36,460 --> 00:35:39,820
could we do this less of this? Yeah, let's try and do this meeting in a half hour now.

349
00:35:40,540 --> 00:35:47,260
One of the games, because I'm all about data driven now. So one of the games I do to keep myself sane

350
00:35:47,260 --> 00:35:59,410
when I'm in these meetings is I count the number of open laptops and I try to estimate how much that

351
00:35:59,410 --> 00:36:07,200
meeting costs Microsoft. That's a dangerous game. Oh, it's fun though. And if you're running a meeting

352
00:36:07,200 --> 00:36:13,920
and I'm gonna close on this, but generally, if I'm running a meeting, I try and engage people.

353
00:36:13,920 --> 00:36:19,730
Some people just are there to listen and veg out sometimes. Like if it's a, some meetings,

354
00:36:19,730 --> 00:36:23,090
some people have like something that comes up, they need to attend to over their laptop. And I'm

355
00:36:23,090 --> 00:36:29,010
not going to demand laptops be closed, but I try and make sure the meeting makes them want to close

356
00:36:29,010 --> 00:36:36,050
them or makes them need to close them. I went to a seminar by Eric Reese. Oh, yeah.

357
00:36:36,050 --> 00:36:40,290
It was pretty cool. And one of the things that he started off before he started talking,

358
00:36:40,290 --> 00:36:50,220
he's like, if you want to have your laptop open or your mobile phone, go ahead. Because I interpret

359
00:36:50,220 --> 00:36:58,080
that as feedback. Very good. Yeah. And my laptop was open the whole time at GTAC because I was,

360
00:36:58,080 --> 00:37:04,880
I was tweeting things and they even gave me, I got, I got a special Seattle GTAC hoodie because

361
00:37:04,960 --> 00:37:09,360
I was one of their top tweeters about GTAC. Okay. So yeah. So I had lots of up for that,

362
00:37:09,360 --> 00:37:13,120
like till I was listening and then taking care of some work stuff at the same time.

363
00:37:13,120 --> 00:37:19,650
So do you want to, is it time? It is a while. It's time. Hey kids, you know, it's time for

364
00:37:20,830 --> 00:37:28,800
mail. All right. Two things from the mail bag, one short one, one long one.

365
00:37:29,520 --> 00:37:35,710
The short one is, uh, someone asked in a while back and I'm sorry, I didn't find the comment,

366
00:37:35,790 --> 00:37:42,830
but someone asked, Hey, what software do you use to record your podcast? And I think as far as I know,

367
00:37:44,110 --> 00:37:52,270
90% of Lisa, the, the amateur podcast or what's below amateur, I don't know, uh, use, uh, uh,

368
00:37:52,270 --> 00:37:58,910
freeware software, um, or, uh, called audacity. It's great. It works for, um, it scales really

369
00:37:58,910 --> 00:38:02,830
well. You can record whole albums on this software. It's, it's really great. And is that

370
00:38:02,910 --> 00:38:08,510
what we're using? That's what we're using. Yeah. Called audacity to search a UDA, C I T Y.

371
00:38:09,460 --> 00:38:14,820
So that's, that's taken care of. The other one is, uh, last time we talked about, uh,

372
00:38:14,820 --> 00:38:20,580
feedback, your evaluation and feedback. And Brent gave me this, uh, exercise he uses when he says,

373
00:38:21,140 --> 00:38:26,260
um, you know, how would you rate this, uh, what I did on a scale of one to 10 and what would make

374
00:38:26,260 --> 00:38:32,100
it a 10. And then Neil stud wrote in and he says, thanks for a great episode guys. You're

375
00:38:32,100 --> 00:38:37,940
welcome. Yes. Go Neil, et cetera, et cetera, et cetera. Good information. You read the comments on

376
00:38:37,940 --> 00:38:43,700
angry weasel.com, whack AB testing. But he says my only concern with Brent's suggestion,

377
00:38:44,340 --> 00:38:50,180
what would they need to do to make it a 10 is that when scaled up amongst many reviewers,

378
00:38:50,180 --> 00:38:56,820
it could result in a laundry list of people's tiny flaws, which could be, would be depressing

379
00:38:56,820 --> 00:39:00,900
to receive, though I would trust the manager to filter and collate the list down. But

380
00:39:00,900 --> 00:39:11,100
how do you deal with that potential, uh, outcome? So I have never actually encountered that,

381
00:39:12,540 --> 00:39:22,350
the feedback mechanism. Um, so Neil does point out something that's, that could be very interesting

382
00:39:22,350 --> 00:39:27,980
because usually when I do this feedback, uh, paradigm, I'm doing it face to face with the person

383
00:39:28,620 --> 00:39:36,480
and they're not thinking about the, well, other than Alan, uh, they're not thinking about the,

384
00:39:36,480 --> 00:39:42,590
the little minutia, um, aggregated up. Yeah. That absolutely could be an issue. And,

385
00:39:42,590 --> 00:39:48,430
and I would rely on the manager to do exactly as Neil suggests to, to collate that and to

386
00:39:48,430 --> 00:39:56,960
discover the patterns out of those individual things. The, um, the other issue is it's very

387
00:39:57,040 --> 00:40:02,960
rare to encounter someone giving, giving, from my experience, giving a number less than four.

388
00:40:04,000 --> 00:40:11,300
If you remember the, the process, they come up with their own number and then they have to, um,

389
00:40:11,300 --> 00:40:17,300
communicate what could have been done differently in some sort of time range, uh, to move that

390
00:40:17,300 --> 00:40:24,510
number to a 10. That first number isn't so much important except for that it, it, it communicates

391
00:40:24,510 --> 00:40:29,710
where they're coming from. And a lot of the times a four really means, yeah, this guy really

392
00:40:29,710 --> 00:40:35,970
irritates me, but we're talking and I don't want to come across as this guy really irritates me.

393
00:40:37,040 --> 00:40:44,160
A five is almost always average and a seven is, you know, just slightly above average.

394
00:40:44,160 --> 00:40:48,000
And people have their different interpretations on that as well. You know, we get these, uh,

395
00:40:48,720 --> 00:40:52,960
surveys that say, you know, did you, you know, blah, blah, blah, dissatisfied,

396
00:40:53,040 --> 00:40:58,800
very dissatisfied, dissatisfied, neither satisfied or dissatisfied satisfied, very satisfied,

397
00:40:58,800 --> 00:41:06,160
typical five point scale. And there's some people, um, who will never put very satisfied because it

398
00:41:06,160 --> 00:41:10,400
always could have been better. Yeah. So, but, but some people will go, it was, it was pretty

399
00:41:10,400 --> 00:41:16,000
good. I'll say very satisfied. So a lot of variation. And so that number one, that one to

400
00:41:16,000 --> 00:41:22,400
10 scale, uh, without any, since it's purely subjective, there's tons of variation. So yeah,

401
00:41:22,400 --> 00:41:27,120
it's about the discussion and the points, not about the score. The points afterwards,

402
00:41:27,120 --> 00:41:32,560
the second bit, it's about taking the feedback and deriving it into something that's actionable.

403
00:41:32,560 --> 00:41:37,440
If you're, if you guys remember the phrase, it's basically what could I have done differently in

404
00:41:37,440 --> 00:41:44,000
order to score that 10? Whereas, um, so one of the positive thing is that that kind of filters out

405
00:41:44,000 --> 00:41:52,020
the subjective, right? Hey, Alan needs to stop being so angry, right? Well, how much non angry

406
00:41:52,420 --> 00:41:59,330
does he need to be? It, it about three beers worth. It translates the feedback into something a bit

407
00:41:59,330 --> 00:42:05,250
more actionable. The other thing that I would suggest though, as well, is if, if, if you use

408
00:42:05,250 --> 00:42:11,090
this technique or, or let's say we operationalize it in some pure feedback form someday, um,

409
00:42:12,500 --> 00:42:16,910
just because the person gave you their opinion of what you could have done

410
00:42:18,100 --> 00:42:25,150
doesn't mean it's valuable, right? It's, everybody has their opinion and it's priceless that they

411
00:42:25,150 --> 00:42:32,270
gave you that feedback, but it's still up to you, uh, the person, the feedback receiver to decide what

412
00:42:32,270 --> 00:42:38,020
they want to do with that feedback. And you may decide, you know what? Yeah. If, if I were to

413
00:42:38,020 --> 00:42:43,460
satisfy that guy's criteria, I'm going to have to give up something that I truly value. So I'm just

414
00:42:43,460 --> 00:42:51,460
going to have to live with, um, that consequence and it's all good. Of course. It's always all good.

415
00:42:51,460 --> 00:42:58,500
Right. Guess what? Are we out of time? We are. That is the, what I was having you guess. Nice

416
00:42:58,500 --> 00:43:02,660
guess. Congratulations. Thanks for the mailbag items. Keep them coming. We love those things.

417
00:43:02,660 --> 00:43:06,740
Yeah. We'll talk about anything, right? Yeah. As you can tell. All right. Thanks everybody.

418
00:43:06,740 --> 00:43:09,460
I'm Alan. I'm Brent. We'll see you next time. Bye.

