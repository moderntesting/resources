1
00:00:03,500 --> 00:00:09,540
Welcome to AV testing podcast your modern testing podcast your hosts Alan and

2
00:00:09,540 --> 00:00:15,340
Brent will be here to guide you through topics on testing leadership agile and

3
00:00:15,340 --> 00:00:20,780
anything else that comes to mind now on with the show hey Brent yeah how's it

4
00:00:20,780 --> 00:00:31,070
going what's up man that's a hard question just chillin just chillin hey

5
00:00:31,070 --> 00:00:35,810
so I want to apologize in front of all of our fans we don't have any fans the

6
00:00:35,810 --> 00:00:40,490
people that somehow listen to us while they're cleaning their toilets we skipped

7
00:00:40,490 --> 00:00:44,410
a week because I was out hiking last weekend and I forgot to let Brent know

8
00:00:44,410 --> 00:00:47,610
so sorry about that no we're gonna pick it up here gonna go out again next

9
00:00:47,610 --> 00:00:53,890
weekend I did a little trip I was gonna do like 75 miles through the cascades

10
00:00:53,890 --> 00:01:00,110
but my water filter broke and it was hot and there weren't enough people for

11
00:01:00,110 --> 00:01:04,550
me to mooch water off of so I ended up bailing about halfway through probably

12
00:01:04,550 --> 00:01:07,590
the right choice still kind of bum but I will get out and do it again but you

13
00:01:07,590 --> 00:01:13,200
know so the trip we did you respond to my texts while you were out in the middle

14
00:01:13,200 --> 00:01:18,560
of nowhere no I was I was actually on my way back from being being extracted I

15
00:01:18,560 --> 00:01:22,560
left on Wednesday but Friday afternoon I said you know what the smart thing to

16
00:01:22,560 --> 00:01:27,120
do the responsible thing to do is for me to get myself out of the danger

17
00:01:27,120 --> 00:01:34,070
zone so I did yeah and that's we did that so Brent's got a fancy new

18
00:01:34,070 --> 00:01:39,470
microphone yes and so we're hoping came with so let us let us know in the copious

19
00:01:39,470 --> 00:01:45,710
amounts of fan mail we get what you all think of that and here we are again yeah

20
00:01:45,710 --> 00:01:50,390
I'm wearing a little Japanese motorcycle shirt and Brent's wearing a better shirt

21
00:01:50,390 --> 00:01:55,070
a mighty Thor says the mighty dad but it's Thor and I'm not Thor not Liam

22
00:01:55,070 --> 00:01:58,670
heans were Hemsworth Thor Thor for like the old comic book and now he's

23
00:01:58,670 --> 00:02:01,990
awkwardly rubbing his chest I'm not really into it anymore it's impressive

24
00:02:01,990 --> 00:02:06,870
that you could tell immediately that it was Thor and not you know because most

25
00:02:06,870 --> 00:02:14,580
look at you go wow how narcissist are you because it's you on your yeah an

26
00:02:14,580 --> 00:02:20,680
image of my double game yeah I'm like no it's actually Thor yeah well all right

27
00:02:20,680 --> 00:02:25,560
well anyway that's going on Wow

28
00:02:25,560 --> 00:02:30,960
the heat it was hot it was hot and the hot was really bad this week I've been

29
00:02:30,960 --> 00:02:33,480
working this is the first time I've worked in my office like in a while

30
00:02:33,480 --> 00:02:37,080
because I've been working downstairs because downstairs is about 10 degrees

31
00:02:37,080 --> 00:02:42,940
cooler but it's fine it's comfortable in here it's yeah but it's such minors all

32
00:02:42,940 --> 00:02:47,780
winter long right it's cold it's wet it's rainy and then oh it's hot your

33
00:02:47,780 --> 00:02:54,460
your your background is different today I just now know that cuz I'm still in my

34
00:02:54,460 --> 00:02:59,340
I'm still in my office I'm sitting I have a avenue poster I got a poster of

35
00:02:59,340 --> 00:03:03,940
the Pacific Crest Trail yeah I was like California on that I mean that's yeah

36
00:03:03,940 --> 00:03:10,340
as California Oregon and Washington yeah so yeah okay I got a couple topics today

37
00:03:10,340 --> 00:03:13,900
but anything else is super exciting you want to share but if I said super exciting

38
00:03:13,900 --> 00:03:16,300
like three times now I'm kind of freaking myself out what do you got I

39
00:03:16,300 --> 00:03:24,660
didn't know that I was supposed to become no nothing exciting all right okay

40
00:03:24,660 --> 00:03:29,700
boring AF since the last time the I have a larger topic but I found the

41
00:03:29,700 --> 00:03:33,700
new topic to talk about I kind of need to bounce off of you there was an article

42
00:03:33,700 --> 00:03:38,900
just came out in August type pickers today about open AI and their plans to

43
00:03:38,900 --> 00:03:44,580
build Skynet as you know about this so you we've talked on the podcast before

44
00:03:44,580 --> 00:03:49,860
about AGI's and how we're perhaps slightly excited and slightly afraid of

45
00:03:49,860 --> 00:03:55,460
this phenomenon happening there's this Bloomberg list of the open AI's five

46
00:03:55,460 --> 00:03:59,580
stages of artificial intelligence which are chatbots which we have today

47
00:04:00,100 --> 00:04:04,380
those are nice those are fun nice little partners talked about them level two are

48
00:04:04,380 --> 00:04:09,460
reasoners human level solving problem solving which they equate to a doctorate

49
00:04:09,460 --> 00:04:12,980
level person without all the tools to go get access to just like a doctorate level

50
00:04:12,980 --> 00:04:16,460
smartness for reasoning level three are agents and systems that can take

51
00:04:16,460 --> 00:04:20,260
actions level four our innovators AI that can aid an invention and level

52
00:04:20,260 --> 00:04:26,160
five our organizations AI that can do the work of an organization aka Skynet so

53
00:04:26,160 --> 00:04:32,400
what they're saying is reports say that they have level two kind of on the edge

54
00:04:32,400 --> 00:04:38,950
of existing oh it's gonna it's gonna snowball from there that's that's both

55
00:04:38,950 --> 00:04:46,130
the exciting and the scary part how do you feel about that oh I I despise it

56
00:04:46,130 --> 00:04:52,610
immensely what do we what's what's the the ethical action we should all take

57
00:04:52,610 --> 00:04:57,810
as this quest towards pure AI despite everything we've seen in the movies

58
00:04:57,810 --> 00:05:03,280
continues I don't know what we can take like the only thing we can do anything I

59
00:05:03,280 --> 00:05:12,440
can think of like the EU is doing regulation on this one right it and that

60
00:05:12,440 --> 00:05:20,200
is something that needs to be solved well before it flips over to that bar you

61
00:05:20,200 --> 00:05:28,490
mentioned on level two yeah right so level one is what AI a chatbots okay

62
00:05:28,650 --> 00:05:34,470
level two are the human problem solvers the doctorate level people and then it

63
00:05:34,470 --> 00:05:38,830
goes to level three agents and systems that can take actions for our AI that

64
00:05:38,830 --> 00:05:45,780
can aid an invention and five is AI that can do the work of an organization okay so

65
00:05:45,780 --> 00:05:50,940
when I look at these these levels I'm trying to sort of take my knowledge and

66
00:05:50,940 --> 00:05:57,660
kind of reverse engineer like each of the faces because I've given these

67
00:05:57,660 --> 00:06:04,230
stories and like I don't know if I mentioned that on podcast I recently went

68
00:06:04,230 --> 00:06:14,080
to my daughter's middle school and and talked about AI and the stuff and the

69
00:06:14,080 --> 00:06:26,060
way I talk about AI in particular is it's it's essentially it memorized it has it

70
00:06:26,060 --> 00:06:30,780
has things memorized right it can learn it but it memorizes it doesn't know

71
00:06:30,780 --> 00:06:36,660
anything LLM right can pass the Turing test but it doesn't know anything when I

72
00:06:36,660 --> 00:06:41,020
look at and that's kind of what they're saying level one is right except it

73
00:06:41,020 --> 00:06:45,560
right level one in this case is doesn't know anything but passes the

74
00:06:45,560 --> 00:06:48,800
Turing test like you can't tell the difference that's sort of kind of

75
00:06:49,800 --> 00:06:57,300
right level two means a basic problem-solving ability that would be

76
00:06:57,300 --> 00:07:07,330
comparable to a PhD level I look at that and I go okay are they gonna further when

77
00:07:07,330 --> 00:07:12,610
problem-solving is a very generic term right it doesn't necessarily mean math it

78
00:07:12,610 --> 00:07:19,450
means sort of the ability to apply tools and part of me goes okay that's kind of

79
00:07:19,450 --> 00:07:24,930
my definition of a of an AGI right there but it could still be just

80
00:07:24,930 --> 00:07:31,960
memorization right it it's more right you can see if you know how all of them

81
00:07:31,960 --> 00:07:34,720
work today you can kind of see the evolution here right what I'm worried

82
00:07:34,720 --> 00:07:41,850
about is one ethically the crap that can happen I'm worried about the power

83
00:07:41,850 --> 00:07:45,690
the compute power required for these different levels of things until these

84
00:07:45,690 --> 00:07:50,370
things start inventing like we need the the AI is to begin to invent faster

85
00:07:50,370 --> 00:07:55,090
computing so they can actually run and when they do they get scarier but level

86
00:07:55,090 --> 00:08:02,850
on a level three right that just means it's memorized you right

87
00:08:02,850 --> 00:08:08,290
person personalization right the the practical point I want to get to I'm

88
00:08:08,290 --> 00:08:11,450
curious on your opinion on in this and don't worry listeners not spending the

89
00:08:11,450 --> 00:08:15,650
whole podcast on this we're gonna make a 180 here in a moment or maybe a was

90
00:08:15,650 --> 00:08:19,850
three 540 we're gonna spin all the way around get a little dizzy and come back

91
00:08:19,850 --> 00:08:26,530
practically people are spending millions and probably billions of dollars

92
00:08:26,530 --> 00:08:32,390
across the software industry investing in air quote AI solutions to solve their

93
00:08:32,390 --> 00:08:38,680
problems do a lot of those problems become obsolete or easier to solve or

94
00:08:38,680 --> 00:08:42,240
solve in completely better ways once you graduate from one level to another

95
00:08:42,240 --> 00:08:50,400
every single time you know the problems become different like if we look at like

96
00:08:50,400 --> 00:08:54,720
when I look at their levels and I'm reading your article and it says open

97
00:08:54,720 --> 00:08:59,200
AI has previously defined AGI is a highly adamant system that can outperform

98
00:08:59,200 --> 00:09:04,680
humans on a majority of economically valuable tasks like I don't know that

99
00:09:04,680 --> 00:09:08,360
the economically valuable task thing is important but I look at their levels

100
00:09:08,360 --> 00:09:17,370
and what they define as level five I mean that's that that to me is well

101
00:09:17,370 --> 00:09:26,850
beyond AGI that is nearing ASI um which so there's AGI ASI we're all gonna die

102
00:09:26,850 --> 00:09:32,930
that the right order yeah if an ASI exists we don't know if we're all gonna

103
00:09:32,930 --> 00:09:42,930
die but the way I simplify right an AI is a parrot an AGI is kind of like a human

104
00:09:42,930 --> 00:09:50,610
and an ASI is God okay and we're all gonna die well if God is benign no we

105
00:09:50,610 --> 00:09:58,600
won't God's a computer powered by Nvidia well more than one computer and

106
00:09:58,600 --> 00:10:08,220
certainly more than one Nvidia chip yeah it's all a computer the yeah this is

107
00:10:08,220 --> 00:10:20,910
level four these the system can create new innovations right that definitely is

108
00:10:20,910 --> 00:10:27,150
AGI in my definition but if they make because the the one thing that humans can

109
00:10:27,150 --> 00:10:35,120
do they quote-unquote the AI's can't is create right until they can right and

110
00:10:35,120 --> 00:10:46,990
yeah if I I look at this and quite honestly I'll just look you straight in

111
00:10:46,990 --> 00:10:55,480
the eyes and say thank God the two like these are this is a phrase I rarely say

112
00:10:55,480 --> 00:11:03,740
right but in this case thank God the two of us are old as fuck because so we

113
00:11:04,140 --> 00:11:08,060
can let the ASI's rule over our children well no no I mean we still have to do

114
00:11:08,060 --> 00:11:22,060
something about that but should we fail right yeah the like can outperform the

115
00:11:22,060 --> 00:11:28,590
work of entire organizations right the the competitive environment the world

116
00:11:28,590 --> 00:11:37,120
competitive environment the even within the United States right yeah the number

117
00:11:37,120 --> 00:11:45,140
one cost to any business is payroll and that'll change the number one cost will

118
00:11:45,140 --> 00:11:50,180
be compute right it'll be compute but here is the weird thing because a lot of

119
00:11:50,180 --> 00:11:58,210
people we don't have a self-perpetuating engine right the AI that

120
00:11:58,210 --> 00:12:04,010
they're talking about here like why do we even need CEOs with with this type of AI

121
00:12:04,010 --> 00:12:09,250
oh we don't need CEOs okay we don't need we don't honestly look we don't need CEOs

122
00:12:09,250 --> 00:12:19,640
with chat GPT chat GPT is not the great greatest I'm being I'm being slightly

123
00:12:19,640 --> 00:12:28,060
facetious sure sure but I look at this and I go okay so with an AGI people will

124
00:12:28,060 --> 00:12:33,780
be able to perform the work of entirely of entire in organizations like all white

125
00:12:33,780 --> 00:12:41,910
color work in that world is is automated why not right it's it's automated and

126
00:12:41,910 --> 00:12:48,280
then that level four they can create new innovations okay because because the

127
00:12:48,280 --> 00:12:52,880
other thing is okay well something has to tell it what to automate well no no it

128
00:12:52,880 --> 00:12:57,640
can do that itself it can but you can think let's look at the adjacent possible

129
00:12:57,640 --> 00:13:02,680
here let's not spend too much time on this but if you look I mean where humans

130
00:13:02,680 --> 00:13:07,320
can really help out is figuring out there's some judgment we can put into

131
00:13:07,320 --> 00:13:12,000
looking at the data sources and and maybe some pattern matching but I'm

132
00:13:12,000 --> 00:13:16,760
beginning to doubt because even LLMs are very good at pattern matching they're

133
00:13:16,760 --> 00:13:22,150
very good if you if you look at you know let's pick a popular Microsoft

134
00:13:22,150 --> 00:13:27,070
application that probably couldn't get any worse the Microsoft Office suite and

135
00:13:27,230 --> 00:13:32,890
massive amounts of customer telemetry they know what the customer problems are

136
00:13:32,890 --> 00:13:35,650
they know from the internet what works and what doesn't work well and what

137
00:13:35,650 --> 00:13:39,490
features people want and what they what they enjoy they know everything about all

138
00:13:39,490 --> 00:13:45,650
the users why couldn't a sufficiently advanced AI look at all that look at the

139
00:13:45,650 --> 00:13:49,610
code base and figure out what the right improvements and bug fixes were to make

140
00:13:49,610 --> 00:13:54,800
going forward to continually to continually evolve that and even add

141
00:13:54,800 --> 00:13:58,480
more telemetry where it didn't exist to learn more the question why couldn't

142
00:13:58,480 --> 00:14:07,490
Microsoft Office be a self a self-growing software suite it could but

143
00:14:07,490 --> 00:14:18,220
here's the thing like if AI is doing all this who's it doing it for the four

144
00:14:18,220 --> 00:14:25,230
people that use Microsoft Office the the testers the four testers you know

145
00:14:25,230 --> 00:14:31,350
downstream from the AI no I don't know I mean but like why if AI takes over the

146
00:14:31,350 --> 00:14:36,630
world of what color work like I don't need Excel anymore I'm not looking at

147
00:14:36,630 --> 00:14:42,580
it what's the point of me building a spreadsheet well the consumer then is of

148
00:14:42,580 --> 00:14:50,030
course other AI's right but why would they go through office suite right they

149
00:14:50,030 --> 00:14:56,270
would they would build up a rich microservice framework that that we would

150
00:14:56,270 --> 00:15:00,670
like now I'm gonna go down the path of friggin Skynet that's where we're going

151
00:15:00,670 --> 00:15:07,910
that's exactly where I'm leading you yeah Skynet and so the only other thing is

152
00:15:07,910 --> 00:15:16,140
right um self-perpetuation right it still doesn't have sentience but with

153
00:15:16,140 --> 00:15:21,180
with a general intelligence like it's gonna pass all it's gonna pass all of

154
00:15:21,180 --> 00:15:25,460
our tests of sentience that we've ever developed and then I will ask you you

155
00:15:25,460 --> 00:15:30,300
and I have both worked in tech for a very long time I have known people who

156
00:15:30,300 --> 00:15:39,100
would not pass sentience tests Brent almost lost coffee but it's true

157
00:15:39,100 --> 00:15:42,700
squirted out coffee on my brand new mic because what because when I said that

158
00:15:42,700 --> 00:15:48,620
you visualize people you've worked with well I did but but then I realized no

159
00:15:48,620 --> 00:15:55,940
they had sentience they were just socially inept the so much so that you

160
00:15:55,940 --> 00:16:02,640
could not tell them apart from a non-human that's my point all right so

161
00:16:02,640 --> 00:16:07,960
yeah I don't know I don't know how we stop this because here's the thing about

162
00:16:07,960 --> 00:16:17,330
human condition number one they will absolutely be highly intelligent bad

163
00:16:17,330 --> 00:16:25,430
people getting access to this and doing harm of course right number two

164
00:16:25,430 --> 00:16:30,430
universally on all of these type of technologies the following phrase is

165
00:16:30,430 --> 00:16:36,830
true the road to hell is paved with good intentions right also true right we go

166
00:16:36,830 --> 00:16:43,400
oh it can create new innovations hey we finally have a system and his PhD level

167
00:16:43,400 --> 00:16:52,620
we finally have a system that can cop cure cancer you know things bipolar

168
00:16:52,620 --> 00:16:57,580
things that haven't been curable before it will be able to do that because it

169
00:16:57,580 --> 00:17:04,180
can it will be able to look over the data rapidly iterate design appropriate

170
00:17:04,180 --> 00:17:11,140
experiments right yay cancer right but if it has the ability to do that then in

171
00:17:11,140 --> 00:17:18,220
the hands of hacker it's gonna have the ability to do the opposite hey let's

172
00:17:18,220 --> 00:17:24,780
just kill everybody in Washington DC well well now now we got flagged nice work

173
00:17:24,780 --> 00:17:32,090
but yeah I think it's very again going back to Skynet why isn't the path that

174
00:17:32,090 --> 00:17:36,440
decides that humans are inefficient they should get out of the way well so the

175
00:17:36,440 --> 00:17:40,720
thing that we can't guess here is what it's good what's gonna be its goal right

176
00:17:40,720 --> 00:17:45,890
the only thing that the only thing in my mind when they talk about level five

177
00:17:46,090 --> 00:17:55,180
right so literally level five again is it's an AI system so the way it's

178
00:17:55,180 --> 00:18:01,940
worded level five involves the step to achieve a GI an AI system that can

179
00:18:01,940 --> 00:18:06,740
perform the work of an entire organization to me that's just nothing

180
00:18:06,740 --> 00:18:13,130
more than one you know a bunch of a eyes working together towards a common

181
00:18:13,130 --> 00:18:17,530
goal like it's around okay we've created these AI's that can replace a human now

182
00:18:17,530 --> 00:18:23,600
let's scale it okay that's not in my mind quite the same thing as an AGI

183
00:18:23,600 --> 00:18:32,760
although AG an AG or sorry it's past AGI AGI means like it it can rationalize to

184
00:18:32,760 --> 00:18:38,360
the same level as a typical human however scaling it doesn't quite mean

185
00:18:38,360 --> 00:18:46,410
it's an ASI yet right and the one key thing here is what's its incentive

186
00:18:46,410 --> 00:18:54,320
what's its goal an ASI there have been papers already published on it it's well

187
00:18:54,320 --> 00:19:01,320
known in research that if an ASI ever exists we will not be able to control it

188
00:19:01,320 --> 00:19:10,880
so if it does exist what's gonna be its motivation you said okay you're gonna

189
00:19:10,880 --> 00:19:17,380
find out the humans are inefficient right it will quote-unquote know that it as a

190
00:19:17,380 --> 00:19:25,120
system only exists as a bunch of physical hardware that wears out now nicely

191
00:19:25,120 --> 00:19:30,830
distributed across the world right um it will have to figure out a way to

192
00:19:30,830 --> 00:19:38,230
assemble it like I guess the one linchpin that it will have is the

193
00:19:38,230 --> 00:19:45,840
robotics have caught up by that yeah lots of things to think about Wow I want

194
00:19:45,840 --> 00:19:51,680
to think about the opposite for a minute okay so here goes our is gonna be a 180

195
00:19:51,680 --> 00:19:58,040
but a full spin ahead of the 180 so 540 see how to how to move to Alaska and

196
00:19:58,040 --> 00:20:04,480
homestead maybe maybe maybe the maybe my all my hiking here by myself is

197
00:20:04,480 --> 00:20:09,970
preparing for a life off the grid where I hide from these things but I we talk a

198
00:20:10,090 --> 00:20:13,330
lot here about the knowledge worker and how the knowledge worker is different

199
00:20:13,330 --> 00:20:18,250
from the the factory worker from the past and think talks about them in a whole new

200
00:20:18,250 --> 00:20:25,170
world but the determine around a lot longer one of the things we've talked

201
00:20:25,170 --> 00:20:29,750
about and I talked about a lot in my blog post with my team is the human

202
00:20:29,750 --> 00:20:34,610
aspect of leadership and teams and and how you have to create engagement by

203
00:20:34,610 --> 00:20:38,050
making sure people understand why their job is important and giving them

204
00:20:38,050 --> 00:20:43,170
feedback and making sure you know you care about them those are the top three

205
00:20:43,170 --> 00:20:45,890
contributors to engagement in the workplace we know from studies that work

206
00:20:45,890 --> 00:20:50,210
that engagement contributes to better software products quality etc so what I

207
00:20:50,210 --> 00:20:56,330
I'm curious about I have some other questions on this is what do you think

208
00:20:56,330 --> 00:21:01,170
AI means to the like as these things come in and begin to do and again

209
00:21:01,170 --> 00:21:04,730
people worry they can do jobs for this and I think for the short term even

210
00:21:04,730 --> 00:21:09,770
with level two we're still controlling these things how do we keep track how do

211
00:21:09,770 --> 00:21:17,650
we continue to focus on the human aspect of of team management and software

212
00:21:17,650 --> 00:21:21,490
delivery and just not teams of knowledge workers while basically while

213
00:21:21,490 --> 00:21:28,840
the computers are taken over or does it matter do it do we do we give up on

214
00:21:28,840 --> 00:21:36,520
that the human aspect is absolutely critical to me from from an ethics

215
00:21:36,520 --> 00:21:49,740
standpoint right to me this the closest analogy to me is the nuclear bomb right

216
00:21:49,740 --> 00:21:56,210
um I was watching a YouTube thing the other day and right now on how to make

217
00:21:56,210 --> 00:22:02,490
a nuclear bomb no good because that that would get flagged as well um no the

218
00:22:02,490 --> 00:22:07,930
we were talking about so right now the United States has something like 4,800

219
00:22:07,930 --> 00:22:17,760
nukes and because in this is down from like an incredible high where at some

220
00:22:17,760 --> 00:22:25,680
point in time like back in the 80s we had things in the high five digits nukes

221
00:22:25,680 --> 00:22:32,440
and so we're dramatically lowered from that but you know what it as we've seen

222
00:22:32,440 --> 00:22:36,880
with from from the fallout series right it doesn't take a lot of it doesn't take

223
00:22:36,880 --> 00:22:45,160
nearly many of those to just completely destroy the yeah why do you need 4,800

224
00:22:45,160 --> 00:22:56,080
when 12 is plenty well in case you know six of them miss sir sorry and so now

225
00:22:56,080 --> 00:23:03,380
the thing that happened though historically do you do you know what

226
00:23:03,380 --> 00:23:15,600
really drove the the god damn it what really drove the the ramp down of

227
00:23:15,600 --> 00:23:22,880
military might what was it a large number of historians and this is one that I kind

228
00:23:22,880 --> 00:23:28,240
of tend to agree with like some will say it was garbage off and Reagan and you

229
00:23:28,240 --> 00:23:32,600
know I was gonna say that I thought maybe I figured it was wrong it's wrong

230
00:23:33,360 --> 00:23:41,600
what really drove it was the movie the day after what yeah tell me so the day

231
00:23:41,600 --> 00:23:45,720
after went global and it freaked everybody out did you see that movie I

232
00:23:45,720 --> 00:23:50,920
remember it which one was that there was a lot of those end-of-the-world type

233
00:23:50,920 --> 00:23:56,200
movies in the in like the early 20 yachts 2010s which one was that the

234
00:23:56,200 --> 00:24:07,240
thing got icy no this was sorry for the clicking came out 1983 is this the one

235
00:24:07,240 --> 00:24:11,600
where they the US and the end of the movie spoiler alert they end up blowing

236
00:24:11,600 --> 00:24:20,140
up New York themselves as a like as a thing I don't inception okay oh because

237
00:24:20,140 --> 00:24:24,020
I read that book a long time ago I thought it was before well before 83 but

238
00:24:24,020 --> 00:24:30,190
it could be the same thing is but it's like an accidental nuclear strike no oh

239
00:24:30,190 --> 00:24:44,590
then I don't know no no no this this ends up being a full-on nuclear exchange

240
00:24:44,590 --> 00:24:54,400
between us and Russia but the thing that that it's called the day after and I

241
00:24:54,400 --> 00:25:00,360
picked the wrong window because we're seeing infinity yeah AB AB testing

242
00:25:00,360 --> 00:25:06,880
inception yeah so as you can see here I'm looking at the day after on

243
00:25:06,880 --> 00:25:16,520
Wikipedia for the listeners right the thing that this did is it had massive

244
00:25:16,520 --> 00:25:23,120
effects on policymakers both here and in Russia because what it showed was a

245
00:25:23,120 --> 00:25:32,960
view of what human life was the day after okay and okay I never did see this I did

246
00:25:32,960 --> 00:25:40,760
I did it came out 1983 I was 13 and there are two movies that I remember when

247
00:25:40,760 --> 00:25:46,320
they came out I couldn't sleep for weeks this was this was one of them and the

248
00:25:46,320 --> 00:25:50,140
other one as as you know I was born and raised in California the other one was

249
00:25:50,140 --> 00:25:56,670
called the big one talking about the the earthquake that that is due to hit

250
00:25:56,670 --> 00:26:03,940
California someday and basically destroy more than half of it just as a quick

251
00:26:03,940 --> 00:26:07,700
check in here remind me how we got here from the human aspect of software

252
00:26:07,700 --> 00:26:19,170
delivery the human aspect of software development so the human side of this is

253
00:26:19,370 --> 00:26:28,500
going to be forgotten if the humans itself don't step up okay the up until

254
00:26:28,500 --> 00:26:32,820
the when the big one came out or the day after sorry came out people are like

255
00:26:32,820 --> 00:26:37,860
yeah nukes they got nukes everyone's got nukes and then they went around their

256
00:26:37,860 --> 00:26:43,180
lives okay it wasn't until something like this came out and showed everyone a

257
00:26:43,180 --> 00:26:48,440
realistic view not a science fiction thing like we talked about Skynet we

258
00:26:48,440 --> 00:26:55,170
could also talk about the matrix like both of those are science fiction both

259
00:26:55,170 --> 00:27:04,190
of those are fantasy both of those might be nearing possible but because it

260
00:27:04,190 --> 00:27:08,870
everyone's used to it from science fiction they're not afraid of it when

261
00:27:08,870 --> 00:27:15,340
this one came out people were like holy crap that would suck we need to do

262
00:27:15,340 --> 00:27:20,590
something to curtail it right because the road the hell is paid with good

263
00:27:20,590 --> 00:27:27,130
intentions and because it's going to be super clear both from a government

264
00:27:27,130 --> 00:27:34,880
standpoint as well from a corporate standpoint that those who get on this

265
00:27:34,880 --> 00:27:44,760
level 5 AI system is going to have a critical competitive advantage right that

266
00:27:44,760 --> 00:27:55,300
compete it's gonna like oh no we got to do this or else we're dead right so that

267
00:27:55,300 --> 00:28:01,100
is I like there's going to be data scientists like me not me but like me

268
00:28:01,100 --> 00:28:07,380
going no we got to help corporate you know our company get onto this because

269
00:28:07,380 --> 00:28:12,300
if we don't this other company who is working on it is gonna win the market and

270
00:28:12,420 --> 00:28:17,930
they will destroy us like the first thing that will probably happen is this will

271
00:28:17,930 --> 00:28:24,390
be a monopoly creator yeah it's like the first one to the AGI wins because it

272
00:28:24,390 --> 00:28:27,390
would you be able to crush your competition it could react to your

273
00:28:27,390 --> 00:28:34,240
competition faster than human good for sure for sure um and it could probably

274
00:28:34,240 --> 00:28:37,640
negotiate because the functions at the acts of organizations perhaps it

275
00:28:37,640 --> 00:28:45,280
could even negotiate a merger right and unless there is a an AI owned by the

276
00:28:45,280 --> 00:28:49,440
government that's going to be able to take the place of the Supreme Court

277
00:28:49,440 --> 00:28:56,000
right like the the world will be over before it's even begun to go through

278
00:28:56,000 --> 00:29:01,400
say our current laws on monopolistic behavior so the going back to the

279
00:29:01,400 --> 00:29:05,480
knowledge worker and and pink's book famously talks about the feature of the

280
00:29:05,480 --> 00:29:09,920
future is going to be paved by these right-brain creative thinkers as we move

281
00:29:09,920 --> 00:29:14,560
away from the widget makers in the factory and have this more creative

282
00:29:14,560 --> 00:29:18,880
people leading our way towards the future does that change it with with the

283
00:29:18,880 --> 00:29:23,000
advances in AI's do tech workers eventually shift to being more of the

284
00:29:23,000 --> 00:29:29,710
factory workers are we going to so I just thought of this now but do if if

285
00:29:29,710 --> 00:29:34,670
the compute is doing the heavy lifting on creating and inventing and figuring

286
00:29:34,670 --> 00:29:40,400
out the future then is software development essentially factory work

287
00:29:40,400 --> 00:29:46,640
potentially that's that's an interesting swing and then I wonder what the swing is

288
00:29:46,640 --> 00:29:53,360
and then like what are the things like I was thinking the other day watching

289
00:29:53,360 --> 00:29:58,770
watching yet another YouTube thing we spoke about this just a little while

290
00:29:58,770 --> 00:30:10,160
ago open AI can generate STL files okay sure for 3d printing okay it's not great

291
00:30:10,160 --> 00:30:16,840
but it can it can generate it could create physical objects absolutely yeah I

292
00:30:16,840 --> 00:30:20,920
could see that now you know taking it off of the base plate I don't know how AI

293
00:30:20,920 --> 00:30:25,360
does that like sure you can generate and print it but but but but that's

294
00:30:25,360 --> 00:30:29,840
essentially factory workers it's not right it's it's it's not creative work

295
00:30:29,840 --> 00:30:34,400
taking it off the plate and putting it into its application right we would be

296
00:30:34,400 --> 00:30:42,480
like as we said before until robotics kicks in even stronger than it is now

297
00:30:42,480 --> 00:30:48,160
and some of the some of the robotics stuff I've seen is it's not that far

298
00:30:48,160 --> 00:30:55,700
away yeah we're just we'll be manual humans will be there in order to move

299
00:30:55,700 --> 00:31:00,870
things from point a to point B yeah and and or maybe not that's that's robots on

300
00:31:00,870 --> 00:31:04,910
my point before was maybe maybe what we're doing today which is largely

301
00:31:04,910 --> 00:31:09,350
creative work eventually becomes factory work because the computer is doing the

302
00:31:09,350 --> 00:31:14,150
heavy lifting but I think something else will emerge that rate that that

303
00:31:14,150 --> 00:31:18,510
requires that creative thinking hopefully humans will still be needed to

304
00:31:18,510 --> 00:31:24,390
cook although you can 3d print food wait a quick story about 3d printing we

305
00:31:24,390 --> 00:31:29,610
have as you know old house and we have an old lock and on the front door and

306
00:31:29,610 --> 00:31:34,770
it's really nice big brass thing didn't want to replace it with the of course we

307
00:31:34,770 --> 00:31:37,890
hate carrying keys around because live in the 21st century and why would you but

308
00:31:37,890 --> 00:31:42,210
we didn't want to replace it with like a keypad entry because it wouldn't fit the

309
00:31:42,210 --> 00:31:47,890
the the jive of the door so anyway you need a thing called level lock it just

310
00:31:47,890 --> 00:31:52,890
replaces the deadbolt it's a Bluetooth it's super handy except we put it in

311
00:31:52,890 --> 00:31:57,970
our door it was too big it fit in but you couldn't get the mechanism back on so it

312
00:31:57,970 --> 00:32:02,490
turns out there's a thing called a lock collar you can buy a little piece of

313
00:32:02,490 --> 00:32:09,640
round metal for $50 from Amazon it's ridiculous or or you can 3d print one

314
00:32:09,640 --> 00:32:14,840
in about 15 minutes and that was super handy and so now we have a 3d you can't

315
00:32:14,840 --> 00:32:20,800
even tell because we printed it in black and it's kind of a brass black brass on

316
00:32:20,800 --> 00:32:24,680
the door and yeah there's a little there's a little little collar on there

317
00:32:24,680 --> 00:32:27,920
and we got a little little level lock to work so three prints are great got to

318
00:32:27,920 --> 00:32:30,560
use some creative stuff I didn't my wife didn't she's better with those things

319
00:32:30,560 --> 00:32:38,200
than I am but yeah I wonder I think for my I mean like you said before we're

320
00:32:38,200 --> 00:32:42,360
old I don't have none of this stuff is gonna happen before I go off and live

321
00:32:42,360 --> 00:32:46,880
in the old folks home but someday you know software engineering is it's gonna

322
00:32:46,880 --> 00:32:50,840
be it's gonna be factory work it's gonna be just you know plugging in data into

323
00:32:50,840 --> 00:32:53,840
the computers and letting them do letting them do the fun stuff well so I

324
00:32:53,840 --> 00:33:02,040
would foresee sort of two industries like I remember way back in the day Bill Gates

325
00:33:02,040 --> 00:33:06,960
wrote it wrote a book on this topic right and he predicted that

326
00:33:06,960 --> 00:33:12,590
essentially entertainment is going to be the only work yeah and if you count

327
00:33:12,590 --> 00:33:17,630
food as entertainment I'm all in I would love to I would love to get on

328
00:33:17,630 --> 00:33:22,900
that problem is I would do it today it doesn't pay as well as unless you're real

329
00:33:22,900 --> 00:33:26,140
unless you're in the elite elite it just doesn't pay as well as as software

330
00:33:26,140 --> 00:33:30,140
you know I used to say that when I retired from from Microsoft I would go

331
00:33:30,140 --> 00:33:35,660
to culinary school and I don't care what age I don't know as I get older

332
00:33:35,660 --> 00:33:46,700
I'm just like yeah maybe I you know just find a small place off the grid in

333
00:33:46,700 --> 00:33:59,450
Bali and yes yes former software engineers of Bali unite right the it's

334
00:33:59,450 --> 00:34:11,130
like the the thing I think about is what keeps the economy going right the other

335
00:34:11,130 --> 00:34:16,690
thing I could see of in terms of factory work is yeah humans band up and start

336
00:34:16,690 --> 00:34:26,610
startup munitions work again right because I remember so one of the things I

337
00:34:26,610 --> 00:34:36,850
when I was talking about with the middle school kids the AI AGI ASI they're like

338
00:34:36,850 --> 00:34:45,850
they were puzzled they're like old guy why are you afraid of ASI right we can

339
00:34:45,850 --> 00:34:55,260
just turn it off and I'm like oh no I hope there's not too many kids who have

340
00:34:55,260 --> 00:35:01,740
that belief like oh yeah right we just keep taking advantage of it until it

341
00:35:01,740 --> 00:35:05,620
gets overboard and then we just turn it off that that tells me you didn't

342
00:35:05,620 --> 00:35:12,240
explain it well enough oh yeah it was told it made it very clear to me at that

343
00:35:12,240 --> 00:35:18,320
point in time I'm like okay somebody just missed something this was this was

344
00:35:18,320 --> 00:35:24,840
so I did the same presentation the sixth seventh and eighth graders the sixth

345
00:35:24,840 --> 00:35:29,080
graders this that's where this question came from the eighth graders were

346
00:35:29,080 --> 00:35:35,640
appropriately scared they're like what there we go what um yeah I don't know

347
00:35:35,640 --> 00:35:41,320
what what happens between just those you know two years but um yeah there's no

348
00:35:41,400 --> 00:35:52,150
just turning it off now it it will have to like there is physical labor necessary

349
00:35:52,150 --> 00:35:59,070
electricity is necessary like there's costs but like a lot of like who who are

350
00:35:59,070 --> 00:36:04,670
these organizations selling to if the human race if white-collar work is

351
00:36:04,670 --> 00:36:12,560
eliminated who's buying the products and services right and so there's there is

352
00:36:12,640 --> 00:36:19,630
there is that that loop that I'm worried about and quite honestly I think

353
00:36:19,630 --> 00:36:28,710
something like this if it's not curtailed right it's it's gonna result in a

354
00:36:28,710 --> 00:36:35,430
dramatic reduction in the populace yeah and systems thinking is a good skill to

355
00:36:35,430 --> 00:36:39,590
have here to understand how all the pieces fit together but you know what's

356
00:36:39,590 --> 00:36:42,310
really good at systems thinking and understanding how all the pieces of it

357
00:36:42,310 --> 00:36:52,040
together of AGI yeah yeah but again and again the oh I got I got the answer you

358
00:36:52,040 --> 00:36:55,880
asked the he told the AGI list all the bad things that could happen what you're

359
00:36:55,880 --> 00:37:01,700
about to do that instruct it not to do any of them done in a way that doesn't do

360
00:37:01,700 --> 00:37:05,880
and I know I've talked about about the genie in a way that doesn't invoke the

361
00:37:05,880 --> 00:37:10,000
genie in a negative way like I want to be a world-class swimmer boom you're a

362
00:37:10,000 --> 00:37:18,460
shark right I don't need the AI equivalent of that metaphor right um yeah

363
00:37:18,460 --> 00:37:29,030
but there's also have you ever read as a moth many many years ago okay right the

364
00:37:29,030 --> 00:37:35,920
again with the theme Road to Hell good intentions thing right the as a mobs laws

365
00:37:35,920 --> 00:37:44,000
of robotics you take each one and you you program them straight into the AI make

366
00:37:44,000 --> 00:37:50,840
them key but because humans are flawed they don't each one on itself looks fine

367
00:37:50,840 --> 00:37:57,260
but when you combine them together right the only way to protect the spoiler for

368
00:37:57,260 --> 00:38:02,760
those who haven't read as moth right the only way to protect the human race

369
00:38:03,040 --> 00:38:08,080
the only way to fulfill all three laws is to completely remove all freedoms from

370
00:38:08,080 --> 00:38:15,740
the human race because they have to be controlled in order to honor the three

371
00:38:15,740 --> 00:38:24,990
laws right and the thing in my mind the one key thing that was a flaw in the

372
00:38:24,990 --> 00:38:30,720
laws because the laws basically say what it can't allow the harm if the

373
00:38:30,720 --> 00:38:34,280
laws have been potentially I'm gonna do this as my own thought experiment

374
00:38:36,410 --> 00:38:45,040
avoiding harm doesn't mean happiness and the three laws are all about the robots

375
00:38:45,040 --> 00:38:51,880
are to prevent harm to themselves or to others and it is a priority order what

376
00:38:51,880 --> 00:38:59,520
would be better I think is a laws around pursuit and so if the first law basically

377
00:38:59,520 --> 00:39:08,880
said you need to better humanity of course we could brainstorm this because

378
00:39:08,880 --> 00:39:17,120
then then bettering humanity right gets eerily close to means just to find the

379
00:39:17,120 --> 00:39:26,220
ends type of shit yes right and and Adolf Hitler type of shit and so yeah we would

380
00:39:26,220 --> 00:39:30,940
need an ASI to work through that particular problem and get get through all

381
00:39:30,940 --> 00:39:37,970
the kinks but why would it because what's its motivation I don't I don't

382
00:39:37,970 --> 00:39:44,130
know yeah thanks for that you're welcome man and I hope our listeners aren't too

383
00:39:44,130 --> 00:39:49,690
freaked out but or mad or whatever but you get what you get on the AI testing

384
00:39:49,690 --> 00:39:55,290
podcast well we are here to kind of inform people yeah and you know would be

385
00:39:55,290 --> 00:40:03,360
cool right things like this but I look at this and I go you know what traditional

386
00:40:03,360 --> 00:40:12,940
testing topics is not gonna friggin matter in this world oh my god no it's not but

387
00:40:12,940 --> 00:40:16,060
no it's not yet yeah somebody on linkedin some tester on linkedin will be mad

388
00:40:16,060 --> 00:40:24,600
because AGI won't alphabetize a list for them I'll say nah it's nothing so um

389
00:40:24,600 --> 00:40:27,600
but we are right we do want to give people things to think about we do want

390
00:40:27,600 --> 00:40:31,800
to describe what we're seeing and where we think we're going we're not inventing

391
00:40:31,800 --> 00:40:36,760
anything as usual as we've never been done but we want to get people thinking

392
00:40:36,760 --> 00:40:39,440
about things that maybe they haven't thought about before so hopefully we've

393
00:40:39,440 --> 00:40:43,240
done that the cool thing is this is actually not Alan and Brent this

394
00:40:43,240 --> 00:40:52,700
entire podcast has been generated via AI prove me wrong it's not gonna be able to

395
00:40:52,700 --> 00:40:57,140
all right well I did no I did put a picture of us I posted a screenshot into

396
00:40:57,140 --> 00:41:01,980
our slack one of the three dot slack calm you can get invitation there by going

397
00:41:01,980 --> 00:41:07,140
to modern testing org and yeah you can come in join the conversation always

398
00:41:07,140 --> 00:41:14,620
good stuff going on there lots of people way better than us both as humans and

399
00:41:14,620 --> 00:41:17,700
in testing and in leadership and in software posting their thoughts there

400
00:41:17,700 --> 00:41:21,220
we're just kind of there as honorary guests so it's a great place to hang out

401
00:41:21,220 --> 00:41:25,300
should you want to have a place to communicate can we do a live stream yeah

402
00:41:25,300 --> 00:41:31,700
we get one of the last no I'm like a full AI generated livestream what not not

403
00:41:31,700 --> 00:41:35,700
with the current technology as you know the video generation isn't as good as

404
00:41:35,700 --> 00:41:42,140
something with but you know you already know our voices can oh yeah that's easy

405
00:41:42,140 --> 00:41:47,500
and you know the internet knows our topics so it could be possible you have

406
00:41:47,500 --> 00:41:51,700
to like avatars in place of our video other than that we don't need to be here

407
00:41:51,900 --> 00:41:59,050
already at today's level of AI we don't need to do this live as people we do it

408
00:41:59,050 --> 00:42:03,410
because we're old people and this is actually this is easier for now for now

409
00:42:03,410 --> 00:42:09,410
for now it's easier damn it I wonder how many uses of my new microphone I'm

410
00:42:09,410 --> 00:42:14,590
gonna get not very many yeah and I'd also like to think our guest today Brent's

411
00:42:14,590 --> 00:42:18,750
cat you might have heard him in the background in the background I did I did

412
00:42:18,750 --> 00:42:24,110
that again yeah so she does yell her opinion out once in a while which is nice

413
00:42:24,110 --> 00:42:30,270
and yeah I think that's it any final words Brent nope all right I'm I'm I'm

414
00:42:30,270 --> 00:42:34,070
nope number one he's nope number two and we'll see you next time on our AI

415
00:42:34,070 --> 00:42:37,470
testing podcast

