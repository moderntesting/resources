1
00:00:00,000 --> 00:00:11,620
Welcome to AV testing podcast, your modern testing podcast, your host, Alan and Brent.

2
00:00:11,620 --> 00:00:12,620
That's me.

3
00:00:12,620 --> 00:00:17,380
He here to guide you through topics on testing, leadership, agile, and anything else that

4
00:00:17,380 --> 00:00:18,380
comes to mind.

5
00:00:18,380 --> 00:00:19,380
Including AI.

6
00:00:19,380 --> 00:00:20,380
On with the show.

7
00:00:20,380 --> 00:00:21,380
Hey, Britt.

8
00:00:21,380 --> 00:00:22,380
Hey, Alan.

9
00:00:22,380 --> 00:00:24,220
We're now putting annotations into the intro.

10
00:00:24,220 --> 00:00:25,220
People know our secrets.

11
00:00:25,220 --> 00:00:26,220
Yeah.

12
00:00:26,220 --> 00:00:27,890
All right.

13
00:00:27,890 --> 00:00:28,890
Well, here we are.

14
00:00:28,890 --> 00:00:29,890
Our dreams.

15
00:00:29,890 --> 00:00:31,690
A water.

16
00:00:31,690 --> 00:00:32,690
No, your name.

17
00:00:32,690 --> 00:00:33,690
All right.

18
00:00:33,690 --> 00:00:34,690
Here we are.

19
00:00:34,690 --> 00:00:35,690
Thank you.

20
00:00:35,690 --> 00:00:36,690
Episode something.

21
00:00:36,690 --> 00:00:37,690
219.

22
00:00:37,690 --> 00:00:38,690
Is that right?

23
00:00:38,690 --> 00:00:39,690
Do we know?

24
00:00:39,690 --> 00:00:40,690
I actually.

25
00:00:40,690 --> 00:00:41,690
I'm going to call it 219.

26
00:00:41,690 --> 00:00:45,690
I had it up and then I closed it.

27
00:00:45,690 --> 00:00:46,690
AB testing.

28
00:00:46,690 --> 00:00:49,530
Oh, what if we were the AB testing?

29
00:00:49,530 --> 00:00:51,130
Like for getting a six pack.

30
00:00:51,130 --> 00:00:52,130
Yeah.

31
00:00:52,130 --> 00:00:53,130
What if we're all about like.

32
00:00:53,130 --> 00:00:54,130
Whatever it is.

33
00:00:54,130 --> 00:00:55,130
Okay.

34
00:00:55,130 --> 00:00:56,130
So this is episode 2018.

35
00:00:56,130 --> 00:00:57,130
Oh, hold on a second.

36
00:00:57,130 --> 00:00:58,130
Brad's taking his shirt off.

37
00:00:58,130 --> 00:00:59,130
Hold on a second.

38
00:00:59,130 --> 00:01:01,610
No, those don't count as a six pack.

39
00:01:01,610 --> 00:01:02,610
That's that's.

40
00:01:02,610 --> 00:01:04,810
But it's a damn solid one pack.

41
00:01:04,810 --> 00:01:05,810
It's a keg.

42
00:01:05,810 --> 00:01:06,810
Yeah.

43
00:01:06,810 --> 00:01:11,790
If we were about AB testing.

44
00:01:11,790 --> 00:01:15,870
I am definitely the wrong co-host for this show.

45
00:01:15,870 --> 00:01:16,870
Oh yeah.

46
00:01:16,870 --> 00:01:17,870
Same.

47
00:01:17,870 --> 00:01:19,830
Same, same, same.

48
00:01:19,830 --> 00:01:20,830
How are things.

49
00:01:20,830 --> 00:01:23,030
Again, episode 218.

50
00:01:23,030 --> 00:01:24,030
Episode 218.

51
00:01:24,030 --> 00:01:25,030
Don't get it right.

52
00:01:25,030 --> 00:01:26,030
Don't skip over.

53
00:01:26,030 --> 00:01:27,030
Yeah.

54
00:01:27,030 --> 00:01:31,100
How are things going?

55
00:01:31,540 --> 00:01:38,380
Hey, I will tell you like sometimes it happens.

56
00:01:38,380 --> 00:01:44,480
Like we've been together now forever.

57
00:01:44,480 --> 00:01:46,960
I know half as long as my marriage.

58
00:01:46,960 --> 00:01:49,900
Yeah.

59
00:01:49,900 --> 00:01:58,280
And I have to say like the advice you gave for me on 217.

60
00:01:58,280 --> 00:02:02,260
Man, is that working out?

61
00:02:02,260 --> 00:02:05,780
One is, is it skills?

62
00:02:05,780 --> 00:02:08,660
It's adding joy to my life.

63
00:02:08,660 --> 00:02:12,260
Tell us more about that so I can stroke my ego and you people know what you're talking

64
00:02:12,260 --> 00:02:13,260
about.

65
00:02:13,260 --> 00:02:14,260
Yeah.

66
00:02:14,260 --> 00:02:20,140
So last, last time, uh, Ellen, I asked, Hey, Ellen, how I'm paraphrasing.

67
00:02:20,140 --> 00:02:23,420
I don't know what the hell I asked.

68
00:02:23,420 --> 00:02:31,430
But Hey, uh, how do you shrink the distance between teams in the remote world?

69
00:02:31,430 --> 00:02:39,550
And I brought up like the water cooler chat was really important because it helped construct

70
00:02:39,550 --> 00:02:44,700
interpersonal relationships, which is just a pain in the ass to do.

71
00:02:44,700 --> 00:02:53,360
Um, in the remote, particularly the remote async world, I am in Allen, amongst other things

72
00:02:53,360 --> 00:02:58,200
suggested, Hey, I did this thing called question of the day.

73
00:02:59,200 --> 00:03:07,090
Right after we recorded that episode on Monday, I created a channel, um, on teams.

74
00:03:07,090 --> 00:03:17,130
Uh, and the channel's name was simply, uh, my, my, my team's alias, a dash in QOTD.

75
00:03:17,130 --> 00:03:20,490
And I didn't even explain QOTD.

76
00:03:20,490 --> 00:03:26,750
I just started putting a date and a question and that's it.

77
00:03:26,750 --> 00:03:30,950
I didn't explain what I was doing, why I was doing it, what's going on.

78
00:03:30,950 --> 00:03:34,150
Why isn't this an evil trap for management?

79
00:03:34,150 --> 00:03:35,150
Just it.

80
00:03:35,150 --> 00:03:40,870
And then I always answer the question and, uh, first, because I don't know, you, you didn't

81
00:03:40,870 --> 00:03:43,190
describe when you asked the question.

82
00:03:43,190 --> 00:03:44,910
When did you answer yourself?

83
00:03:44,910 --> 00:03:46,350
Um, good question.

84
00:03:46,350 --> 00:03:49,950
Uh, sometimes right away, sometimes later, sometimes not at all.

85
00:03:49,950 --> 00:03:50,950
I see.

86
00:03:50,950 --> 00:03:51,950
Okay.

87
00:03:51,950 --> 00:03:52,950
Eventually not at all.

88
00:03:52,950 --> 00:03:53,950
Um, yeah.

89
00:03:54,150 --> 00:03:55,150
Yeah.

90
00:03:55,150 --> 00:04:03,060
And I've been doing it and the team engagement on it is, is just super active.

91
00:04:03,060 --> 00:04:09,440
Um, so let me ask, are there, when people answer the question, people follow up in threads

92
00:04:09,440 --> 00:04:13,920
or whatever with, with, uh, a clarifying questions or me to use or things like that.

93
00:04:13,920 --> 00:04:14,920
Oh yeah.

94
00:04:14,920 --> 00:04:16,640
Like we, that's the best part.

95
00:04:16,640 --> 00:04:17,640
That's the best part.

96
00:04:17,640 --> 00:04:24,200
And when people do follow up questions and engage, like I'll say, for example, today,

97
00:04:25,060 --> 00:04:31,780
my question of the day is if you could invent a holiday, what would it celebrate?

98
00:04:31,780 --> 00:04:33,300
That's a great question.

99
00:04:33,300 --> 00:04:39,890
And Alan on the spot, if you could invent a holiday, what would it celebrate?

100
00:04:39,890 --> 00:04:40,890
April 4th.

101
00:04:40,890 --> 00:04:44,150
It's the anniversary of the AB testing podcast.

102
00:04:44,150 --> 00:04:45,390
Okay.

103
00:04:45,390 --> 00:04:48,780
That is an entirely perfect response.

104
00:04:48,780 --> 00:04:50,230
Yes.

105
00:04:50,230 --> 00:04:55,980
I answered, um, benign chaos.

106
00:04:55,980 --> 00:04:56,980
That was my first thing.

107
00:04:56,980 --> 00:05:03,940
I'm like folks would celebrate it by performing random acts of randomness spelled R a O R, but

108
00:05:03,940 --> 00:05:09,720
pronounced roar being non harmful would be required.

109
00:05:09,720 --> 00:05:15,900
But within that constraint, do your part to maximize entropy for a day.

110
00:05:15,900 --> 00:05:17,420
And um,

111
00:05:17,420 --> 00:05:18,420
What a weirdo.

112
00:05:18,420 --> 00:05:19,420
Yeah.

113
00:05:19,420 --> 00:05:24,780
Uh, I am definitely, I don't know if you ever played D and D, but I would 100% as a

114
00:05:24,780 --> 00:05:29,820
character, I would 100% be a chaotic good alignment.

115
00:05:29,820 --> 00:05:36,800
Um, and one of my, one of my employees, they wrote, I have always loved the appearance

116
00:05:36,800 --> 00:05:40,560
and an emergence of randomness when you least expect it.

117
00:05:40,560 --> 00:05:44,840
The most harmful kind of course, it plays a key role in humor.

118
00:05:44,840 --> 00:05:50,520
And they said on a related note, a person once selling a lamp on a college buy and

119
00:05:50,520 --> 00:05:51,640
sell Facebook group.

120
00:05:51,640 --> 00:05:55,960
I wondered what if the lamp could reply to the owner's post?

121
00:05:55,960 --> 00:06:02,360
What if it was surprised and taken aback that was being sold like this without its knowledge.

122
00:06:02,360 --> 00:06:08,440
So a few account creation hops later, wallah, the lamp had its own Facebook account managed

123
00:06:08,440 --> 00:06:15,040
to join the page and reply to its owner's posts, expressing shock.

124
00:06:15,040 --> 00:06:16,040
That's really funny.

125
00:06:16,040 --> 00:06:17,040
That's really fun.

126
00:06:17,040 --> 00:06:21,000
And I'm like, yes, that's freaking benign chaos day.

127
00:06:21,920 --> 00:06:23,300
Right?

128
00:06:23,300 --> 00:06:27,700
And he was saying, yeah, he would support it as being a world holiday.

129
00:06:27,700 --> 00:06:31,460
If he could, that is good.

130
00:06:31,460 --> 00:06:31,860
Right.

131
00:06:31,860 --> 00:06:33,940
It serves no purpose.

132
00:06:34,850 --> 00:06:36,690
It is not positive or negative.

133
00:06:36,690 --> 00:06:38,690
It is just random.

134
00:06:40,230 --> 00:06:40,550
Right.

135
00:06:40,550 --> 00:06:41,630
Am I like it?

136
00:06:41,750 --> 00:06:42,270
Yeah.

137
00:06:43,540 --> 00:06:45,580
I play it for many years.

138
00:06:45,580 --> 00:06:47,420
I played this chaotic, good rogue.

139
00:06:48,100 --> 00:06:51,900
Uh, and he was, yeah, I, I played him.

140
00:06:52,100 --> 00:06:53,100
I played him to the alignment.

141
00:06:53,100 --> 00:06:54,100
It was fun.

142
00:06:54,100 --> 00:06:58,500
He was, you know, yeah, you know, D&D.

143
00:06:58,500 --> 00:07:01,190
I like playing.

144
00:07:01,190 --> 00:07:05,150
I like the alignment aspect, not just that go kill everything.

145
00:07:05,150 --> 00:07:06,790
Like, let's make this kind of fun.

146
00:07:06,790 --> 00:07:12,710
So anyway, the number of times the rest of my party went, oh, no, not again.

147
00:07:12,710 --> 00:07:23,170
Yeah, see, that's the fun thing about chaotic good.

148
00:07:23,170 --> 00:07:32,830
Everyone knows, okay, that you're going to be good, but yeah, you may, you know, rules

149
00:07:32,830 --> 00:07:37,260
are guidance, really, not rules.

150
00:07:37,260 --> 00:07:38,740
Anyway.

151
00:07:38,740 --> 00:07:39,740
Yeah.

152
00:07:39,740 --> 00:07:40,740
You know, yeah.

153
00:07:40,740 --> 00:07:41,870
Yeah.

154
00:07:41,870 --> 00:07:44,610
What would Hans Solo do?

155
00:07:44,610 --> 00:07:45,610
All right.

156
00:07:45,610 --> 00:07:46,610
Shoot first.

157
00:07:46,610 --> 00:07:48,740
Anything else from your end?

158
00:07:48,740 --> 00:07:51,180
He obviously shot first.

159
00:07:51,180 --> 00:07:52,180
Right.

160
00:07:52,180 --> 00:07:54,170
Anyway.

161
00:07:54,170 --> 00:07:55,170
No.

162
00:07:55,170 --> 00:07:56,170
All right.

163
00:07:56,170 --> 00:08:03,330
So as we discussed on the last time on the AB testing last time on AB testing, I recorded

164
00:08:03,330 --> 00:08:07,900
a couple of days before I told my team, but I am leaving my job.

165
00:08:07,900 --> 00:08:09,180
I do not have another job.

166
00:08:09,180 --> 00:08:11,220
I'm going to be professionally homeless.

167
00:08:11,220 --> 00:08:12,220
Nope.

168
00:08:12,220 --> 00:08:13,220
Professionally, I get paid.

169
00:08:13,220 --> 00:08:18,140
I'm going to be broken homeless and walk from Mexico to Canada on some stupid trail with

170
00:08:18,140 --> 00:08:20,620
about a thousand other people, but it's going to be fun.

171
00:08:20,620 --> 00:08:21,620
I'm all prepared.

172
00:08:21,620 --> 00:08:23,060
I've weighed everything.

173
00:08:23,060 --> 00:08:24,180
It's all ready to go.

174
00:08:24,260 --> 00:08:27,940
I let my team know last week.

175
00:08:27,940 --> 00:08:33,380
And what's interesting, what happened is as I let people know there was surprise.

176
00:08:33,380 --> 00:08:38,380
I had some really, really good feedback from my team coming out.

177
00:08:38,380 --> 00:08:43,540
I have tried to set them up in a way where they can just, I told my manager to give

178
00:08:43,540 --> 00:08:49,660
my head count away to another org or to not backfill because the team can kind of run

179
00:08:49,660 --> 00:08:50,660
on their own.

180
00:08:50,660 --> 00:08:54,460
I need somebody with a fancy title to yell at somebody with another fancy title.

181
00:08:54,460 --> 00:08:57,460
But that's really what my role has been reduced to.

182
00:08:57,460 --> 00:09:02,780
But anyway, going well, got a couple of great pieces of feedback.

183
00:09:02,780 --> 00:09:08,020
Somebody told me today, they appreciated me trying to challenge the system, which is

184
00:09:08,020 --> 00:09:09,660
like not surprised you do.

185
00:09:09,660 --> 00:09:11,300
That's what I do.

186
00:09:11,300 --> 00:09:16,100
You don't hire a change agent to stick with the system.

187
00:09:16,100 --> 00:09:17,100
I know.

188
00:09:17,100 --> 00:09:18,100
I know.

189
00:09:18,100 --> 00:09:27,870
Actually, the best feedback I got is somebody said, well, there was two parts of it.

190
00:09:27,870 --> 00:09:31,150
Oh, said thank you for your unconventional approach.

191
00:09:31,150 --> 00:09:33,190
It was a breath of fresh air.

192
00:09:33,190 --> 00:09:34,430
And what happened is...

193
00:09:34,430 --> 00:09:36,350
Okay, I'm good, baby.

194
00:09:36,350 --> 00:09:41,510
My breath of fresh air was a big fat fart to some people, I think.

195
00:09:41,510 --> 00:09:42,150
But it was air.

196
00:09:42,150 --> 00:09:44,910
It was air coming from somewhere.

197
00:09:44,910 --> 00:09:46,270
So yeah, wrapping things up.

198
00:09:46,510 --> 00:09:49,750
What's happened is my calendar has taken a massive nose dive.

199
00:09:49,750 --> 00:09:56,810
People have already started scheduling meetings, not with me with my directs or other people.

200
00:09:56,810 --> 00:10:00,530
So I have my one on ones left for the next...

201
00:10:00,530 --> 00:10:04,130
I guess through next week, April 4th is my last day.

202
00:10:04,130 --> 00:10:10,690
It always makes sense for everyone to do this.

203
00:10:10,690 --> 00:10:17,190
But I got to say, it's the saddest part of leaving a team.

204
00:10:17,670 --> 00:10:25,350
Suddenly, you yourself, all this energy and then the last two weeks, it's just like crickets.

205
00:10:25,350 --> 00:10:26,790
It's like quiet quitting.

206
00:10:26,790 --> 00:10:31,310
But instead of me stopping, slowing my work, the team has quit on me.

207
00:10:31,310 --> 00:10:33,310
The organization has quit on me.

208
00:10:33,310 --> 00:10:34,830
And I'm not mad about it.

209
00:10:34,830 --> 00:10:38,550
I've talked to people about it and they said, use the time to relax.

210
00:10:38,550 --> 00:10:42,230
So I'm doing a lot of steps at the Stairmaster.

211
00:10:42,230 --> 00:10:44,110
I may just do one...

212
00:10:44,190 --> 00:10:47,830
May just work from there, work from the gym next week and just do Stairs in between meetings.

213
00:10:47,830 --> 00:10:49,630
I'm not quite sure.

214
00:10:49,630 --> 00:10:50,670
We will figure it out.

215
00:10:50,670 --> 00:10:55,630
So anyway, good feedback from the folks, but they know they're in good shape.

216
00:10:55,630 --> 00:10:57,790
Transition stuff is done.

217
00:11:00,460 --> 00:11:01,140
Yeah.

218
00:11:01,140 --> 00:11:04,980
So I want one last thing and one last...

219
00:11:04,980 --> 00:11:07,500
I'm trying to push some promotions through before I'm gone.

220
00:11:07,500 --> 00:11:08,420
So we'll see how that goes.

221
00:11:08,420 --> 00:11:10,020
See what I can do in five days.

222
00:11:10,020 --> 00:11:11,620
I'm semi-confident.

223
00:11:11,620 --> 00:11:13,060
I believe in you.

224
00:11:13,060 --> 00:11:14,300
And I believe in me too.

225
00:11:14,740 --> 00:11:16,820
That's my challenge and problem.

226
00:11:16,820 --> 00:11:17,900
Well, all right.

227
00:11:17,900 --> 00:11:20,900
I think that's it for the...

228
00:11:20,900 --> 00:11:24,180
That's the work stuff, the trail stuff.

229
00:11:24,180 --> 00:11:25,740
Do we want to talk about the...

230
00:11:25,740 --> 00:11:27,620
This is the penultimate episode of A.B.

231
00:11:27,620 --> 00:11:28,580
Testing for a while.

232
00:11:28,580 --> 00:11:29,790
Right.

233
00:11:29,790 --> 00:11:35,030
And we're going to do a recording next week that will come out on April 4th,

234
00:11:35,030 --> 00:11:37,830
which is now a national, I think a world holiday.

235
00:11:37,830 --> 00:11:38,390
Right.

236
00:11:38,390 --> 00:11:40,230
The A.B.

237
00:11:40,230 --> 00:11:42,790
Testing anniversary, 11th anniversary special.

238
00:11:42,790 --> 00:11:43,750
There won't be any guests.

239
00:11:43,750 --> 00:11:46,630
It will be us reflecting on our time together and talking about what's next.

240
00:11:46,630 --> 00:11:47,190
I can...

241
00:11:47,190 --> 00:11:50,430
If you want to ask me questions about my trail gear,

242
00:11:50,430 --> 00:11:53,110
you can throw them in one of the three.slack.com.

243
00:11:53,110 --> 00:11:58,220
You can go to moderntosny.org to get a link to that invitation.

244
00:11:58,220 --> 00:12:02,940
I've been clicking the renew button every 30 days on this, on that link forever.

245
00:12:02,940 --> 00:12:05,820
But Brent and Perzi will take care of that going forward.

246
00:12:05,820 --> 00:12:11,060
And hopefully I hope to return to the slack in mid-September after that.

247
00:12:11,060 --> 00:12:17,160
So to segue into a topic that Brent and I talked about briefly before,

248
00:12:17,160 --> 00:12:20,600
I looked at my LinkedIn notifications as I do once in a while.

249
00:12:20,600 --> 00:12:21,880
Not very often.

250
00:12:21,880 --> 00:12:23,040
Sometimes I missed...

251
00:12:23,040 --> 00:12:23,560
I talked to...

252
00:12:23,560 --> 00:12:24,320
Remember Lena?

253
00:12:24,320 --> 00:12:29,820
Lena Zubit, a guest on the podcast?

254
00:12:29,820 --> 00:12:30,900
I talked to Lena about...

255
00:12:30,900 --> 00:12:33,900
This is actually my kind of tangent here.

256
00:12:33,900 --> 00:12:34,620
We talked about...

257
00:12:34,620 --> 00:12:38,340
She had some questions about like the history of the bug bash.

258
00:12:38,340 --> 00:12:42,340
And the internet says that the bug bash was invented at Microsoft.

259
00:12:42,340 --> 00:12:44,780
And it was there when I joined in 95.

260
00:12:44,780 --> 00:12:48,140
But I couldn't really find any documentation on how it came about.

261
00:12:48,140 --> 00:12:53,220
There may be somebody like Tierney or someone who was there in the late 80s and 90s

262
00:12:53,220 --> 00:12:55,940
and testing who may remember how that came about.

263
00:12:55,940 --> 00:12:59,570
If anyone knows, it would be Tierney.

264
00:12:59,570 --> 00:13:00,770
But maybe nobody ever knows.

265
00:13:00,770 --> 00:13:03,330
So we talked about that for a while, some of the bug bashes we did.

266
00:13:03,330 --> 00:13:04,330
It was an interesting conversation.

267
00:13:04,330 --> 00:13:06,090
But I messed up...

268
00:13:06,090 --> 00:13:09,330
I missed a meeting with her because I never checked LinkedIn

269
00:13:09,330 --> 00:13:11,130
and she sent me a message and I said I'd be there.

270
00:13:11,130 --> 00:13:13,210
But I didn't put it on my calendar.

271
00:13:13,210 --> 00:13:14,210
And then she went to check in.

272
00:13:14,210 --> 00:13:16,410
Didn't hear back from me for like five days.

273
00:13:16,410 --> 00:13:17,890
So anyway, that happened.

274
00:13:17,890 --> 00:13:19,530
Sorry, Lena.

275
00:13:19,530 --> 00:13:21,970
I went to LinkedIn and checked my messages,

276
00:13:21,970 --> 00:13:23,530
took my notifications today.

277
00:13:23,530 --> 00:13:29,180
And we are number two on some list of software testing podcasts.

278
00:13:29,180 --> 00:13:29,580
OK.

279
00:13:29,580 --> 00:13:32,900
What's interesting is I don't remember the last time we talked very much about

280
00:13:32,900 --> 00:13:34,060
software testing.

281
00:13:34,060 --> 00:13:35,860
But who's number one?

282
00:13:35,860 --> 00:13:36,860
What the hell?

283
00:13:36,860 --> 00:13:40,660
Oh, dude, dude, I was on this podcast once, the performance guy, Joe Colantoneo.

284
00:13:40,700 --> 00:13:41,940
Oh, OK.

285
00:13:41,940 --> 00:13:43,620
So somebody better than us, that's why.

286
00:13:43,620 --> 00:13:44,260
We shouldn't even...

287
00:13:44,260 --> 00:13:47,430
Honestly, we shouldn't even be number two.

288
00:13:47,430 --> 00:13:48,030
Double honest.

289
00:13:48,030 --> 00:13:48,630
It was on...

290
00:13:48,630 --> 00:13:51,790
We probably shouldn't be on a testing podcast.

291
00:13:51,790 --> 00:13:54,990
It was on a list of software testing podcasts.

292
00:13:54,990 --> 00:13:58,030
Dude, I'll bring up my link.

293
00:13:58,030 --> 00:14:01,850
Trying to cue you up here before you get distracted.

294
00:14:01,850 --> 00:14:04,170
The idea is, I'm going to go ahead and cue this up.

295
00:14:04,170 --> 00:14:07,850
We started A-B testing because we were coming from the testing world.

296
00:14:07,850 --> 00:14:09,210
I think I was still in it.

297
00:14:09,210 --> 00:14:11,010
And we were seeing changes happening.

298
00:14:11,010 --> 00:14:15,050
And we wanted to talk to people about what those changes meant

299
00:14:15,050 --> 00:14:17,970
and how to navigate those.

300
00:14:17,970 --> 00:14:23,870
By and large, despite the deniers, those changes that we predicted have happened.

301
00:14:23,870 --> 00:14:25,070
They're common.

302
00:14:25,070 --> 00:14:27,550
There are people that don't believe in them still, but they're...

303
00:14:27,550 --> 00:14:30,990
I mean, I can deny that the sky is blue also.

304
00:14:30,990 --> 00:14:32,870
But that's how we started the podcast.

305
00:14:32,870 --> 00:14:38,030
But then Brent had some ideas on sort of building on that.

306
00:14:38,030 --> 00:14:38,830
Brent's still reading.

307
00:14:38,830 --> 00:14:40,390
I'm trying to like get...

308
00:14:40,390 --> 00:14:41,110
You ready to go here?

309
00:14:41,230 --> 00:14:41,830
Ready to take over?

310
00:14:41,830 --> 00:14:42,310
Yeah.

311
00:14:42,310 --> 00:14:42,670
All right.

312
00:14:42,670 --> 00:14:44,990
I'm passing the speaking baton to Brent.

313
00:14:44,990 --> 00:14:46,070
I've cued you up.

314
00:14:46,070 --> 00:14:48,750
Talk about what you talked about before and we'll go on from there.

315
00:14:48,750 --> 00:14:50,710
Okay.

316
00:14:50,710 --> 00:14:53,220
And I found the list.

317
00:14:53,220 --> 00:14:57,420
And actually, I will re-cue even though you cued.

318
00:14:57,420 --> 00:14:58,740
I found the list.

319
00:14:58,740 --> 00:15:01,580
It was published two days ago.

320
00:15:01,580 --> 00:15:02,380
Yeah.

321
00:15:02,380 --> 00:15:03,140
Just came.

322
00:15:03,140 --> 00:15:05,540
I linked in notifications.

323
00:15:05,540 --> 00:15:07,210
Yeah.

324
00:15:07,210 --> 00:15:10,150
I wonder how they do.

325
00:15:10,150 --> 00:15:11,030
Okay.

326
00:15:11,030 --> 00:15:16,600
So as Alan cued up, why did we start this podcast?

327
00:15:16,600 --> 00:15:25,350
Well, if you guys remember, did we do that in 199 where we went back and listened to episode

328
00:15:25,350 --> 00:15:26,270
one?

329
00:15:26,270 --> 00:15:27,670
Yeah, 198 to 199.

330
00:15:27,670 --> 00:15:28,870
We went back and listened to episode one.

331
00:15:28,870 --> 00:15:29,390
Right.

332
00:15:29,390 --> 00:15:32,710
And we did it so you don't have to.

333
00:15:36,780 --> 00:15:41,160
It was a reminder that why did we do this?

334
00:15:41,160 --> 00:15:48,640
At that point in time, Agile was threatening to change everything.

335
00:15:48,640 --> 00:15:58,680
And Alan and I had observed through our own practice and through first observing other

336
00:15:58,680 --> 00:16:08,280
companies and then observing and leading that type of change internally that said, oh,

337
00:16:08,280 --> 00:16:12,120
well, if you care about testing, like we learned.

338
00:16:12,120 --> 00:16:21,240
If you care about testing this new world that we're clearly heading into is not going to

339
00:16:21,240 --> 00:16:23,420
be for you.

340
00:16:23,420 --> 00:16:28,900
But if you care about quality, this new world is fantastic.

341
00:16:28,900 --> 00:16:36,140
And we said, okay, there's going to be some set of people who are interested in knowing

342
00:16:36,140 --> 00:16:41,100
whether or not the grass is greener on the other side of the hill, how to get there

343
00:16:41,100 --> 00:16:46,470
and how to thrive.

344
00:16:46,470 --> 00:16:53,430
So we started this up and said, okay, let's just share our experience and see what happens.

345
00:16:53,430 --> 00:17:00,350
And avid listeners along the way, we ended up developing what we call the modern testing

346
00:17:00,350 --> 00:17:03,820
principles.

347
00:17:03,820 --> 00:17:17,630
And I thought that maybe today what we talk about is not AI, but the AI phenomenon.

348
00:17:17,630 --> 00:17:31,800
I don't, unlike back then where there were numerous examples of people making the shift

349
00:17:31,800 --> 00:17:35,270
and how to succeed and how to fail.

350
00:17:35,270 --> 00:17:42,280
And in this particular time, Alan and I also began to execute this shift.

351
00:17:42,280 --> 00:17:51,820
I got to the point where I was directly coaching people and teaching people how to shift.

352
00:17:51,820 --> 00:18:06,300
We're in a phase in the software industry where AI is beginning to take a tangible hold.

353
00:18:06,300 --> 00:18:13,070
And I see a lot of parallels to back then as to now, right?

354
00:18:13,070 --> 00:18:18,550
A lot of people holding on with white knuckles, oh, AI is going to take my job.

355
00:18:18,550 --> 00:18:20,310
Oh, AI.

356
00:18:20,310 --> 00:18:28,700
Like who's going to watch the AI?

357
00:18:28,700 --> 00:18:36,850
No one has yet said what would be the equivalent in the AI world?

358
00:18:36,850 --> 00:18:42,150
What would be the equivalent of, oh no, we got to keep tests around because dev doesn't

359
00:18:42,150 --> 00:18:46,020
want to do testing.

360
00:18:46,020 --> 00:18:50,580
Like we got to keep dev around because who doesn't want to do development?

361
00:18:50,580 --> 00:18:52,780
I don't know.

362
00:18:52,780 --> 00:19:02,500
But I thought maybe today we talk about what we think, not what we think, because again,

363
00:19:02,500 --> 00:19:06,950
what I'm stating here is we're still too early.

364
00:19:06,950 --> 00:19:14,020
Alan and I are still too early to really go and say, oh, this is the path for success.

365
00:19:14,020 --> 00:19:21,340
This is how you begin to migrate from the skill set that you had to the skill set that

366
00:19:21,340 --> 00:19:22,340
you need.

367
00:19:22,700 --> 00:19:27,980
We've talked about many times similar things.

368
00:19:27,980 --> 00:19:33,620
I am absolutely convinced that the jobs of the future are going to be human and AI working

369
00:19:33,620 --> 00:19:34,620
together.

370
00:19:34,620 --> 00:19:38,570
But what does that tangibly mean?

371
00:19:38,570 --> 00:19:39,810
How do we produce product?

372
00:19:39,810 --> 00:19:40,810
How do we ship product?

373
00:19:40,810 --> 00:19:42,490
Do we do either of these?

374
00:19:42,490 --> 00:19:43,490
What is that?

375
00:19:43,490 --> 00:19:49,770
Like there's not enough, to my visibility, there aren't enough early adopters where

376
00:19:49,770 --> 00:19:53,620
it is clear the direction to go.

377
00:19:53,620 --> 00:19:59,620
So I thought even given that ambiguity, maybe it would be worthwhile based on what Alan and

378
00:19:59,620 --> 00:20:05,140
I know today to go and revisit the modern testing principles.

379
00:20:05,140 --> 00:20:09,260
Oh, I see where you're going.

380
00:20:09,260 --> 00:20:15,060
And go based on what we know today, what if this is still valid?

381
00:20:15,060 --> 00:20:18,140
What are these should be removed?

382
00:20:18,140 --> 00:20:20,040
What should be tweaked?

383
00:20:20,280 --> 00:20:21,280
Thoughts?

384
00:20:21,280 --> 00:20:22,280
God, yeah.

385
00:20:22,280 --> 00:20:25,600
MT principles 3.0, discussion begins.

386
00:20:25,600 --> 00:20:30,960
Well, I mean, I guess we could just call it a brand now.

387
00:20:30,960 --> 00:20:33,640
Again, yeah, it's a brand name.

388
00:20:33,640 --> 00:20:38,880
Just like AB testing isn't really about, not that modern and not that much about testing.

389
00:20:38,880 --> 00:20:39,880
Right.

390
00:20:39,880 --> 00:20:46,630
But you could argue it's becoming a lot more like a slash B testing.

391
00:20:46,630 --> 00:20:48,850
Right?

392
00:20:48,850 --> 00:20:50,730
It is.

393
00:20:51,490 --> 00:20:57,750
I'm going to, I know you have a thought here because I'm honestly glancing at the principles

394
00:20:57,750 --> 00:20:58,750
having a hard time.

395
00:20:58,750 --> 00:21:01,950
So I'm going to let you start and then I'll probably figure it out and we can jump in.

396
00:21:01,950 --> 00:21:02,950
So please continue.

397
00:21:02,950 --> 00:21:03,950
You're having a hard time.

398
00:21:03,950 --> 00:21:04,950
All right.

399
00:21:04,950 --> 00:21:09,870
Applying to figure out how you want to apply what you just said.

400
00:21:09,870 --> 00:21:13,870
I don't want to like, so do you remember?

401
00:21:13,870 --> 00:21:15,310
Maybe they just all go away.

402
00:21:15,310 --> 00:21:21,030
Maybe you remember centuries ago where you had the idea of, Hey, Brent, I just read

403
00:21:21,030 --> 00:21:23,310
this nifty book on round principles.

404
00:21:23,310 --> 00:21:26,110
How about we sit right down.

405
00:21:26,110 --> 00:21:27,110
That's what I'm proposing.

406
00:21:27,110 --> 00:21:28,110
Okay.

407
00:21:28,110 --> 00:21:29,110
All right.

408
00:21:29,110 --> 00:21:34,150
Given that we actually do not know enough about how, how this AI phenomenon is going

409
00:21:34,150 --> 00:21:36,190
to change things, right?

410
00:21:36,190 --> 00:21:40,990
That's the problem because we're putting our finger in the air and predicting.

411
00:21:40,990 --> 00:21:43,950
Like we already heard from Jason Arbonne, right?

412
00:21:43,950 --> 00:21:47,150
Everything we're all going to be testers, but that's, that's his prediction.

413
00:21:47,150 --> 00:21:48,150
Right.

414
00:21:48,150 --> 00:21:49,150
But you know, it's funny.

415
00:21:49,150 --> 00:21:50,150
Yeah.

416
00:21:50,150 --> 00:21:54,590
I come back from my trip and, and because I work in a country where I have to have a job

417
00:21:54,590 --> 00:21:59,270
to have healthcare and I pay out the nose for it while I'm gone.

418
00:21:59,270 --> 00:22:03,790
What if I, I mean, there's probably a non-zero chance I go back into a testing job.

419
00:22:03,790 --> 00:22:07,820
Just are there, are there any testing jobs?

420
00:22:07,820 --> 00:22:10,860
Maybe there aren't any, I was going to say, maybe I go back into a testing job and Jason's

421
00:22:10,860 --> 00:22:13,980
prediction comes true, but there may not be any left by the time I'm back.

422
00:22:13,980 --> 00:22:14,980
So don't worry about that.

423
00:22:14,980 --> 00:22:15,980
Go on.

424
00:22:15,980 --> 00:22:16,980
All right.

425
00:22:16,980 --> 00:22:17,980
No.

426
00:22:18,890 --> 00:22:24,510
AGI, despite AGI still has a ways to go.

427
00:22:24,510 --> 00:22:25,510
Okay.

428
00:22:25,510 --> 00:22:26,950
That's not going to happen while I'm gone.

429
00:22:26,950 --> 00:22:27,950
I'm not worried.

430
00:22:27,950 --> 00:22:35,140
It's not going to happen while you're gone, but I do think multiple, I don't know if it

431
00:22:35,140 --> 00:22:40,250
would be globally or just within my company.

432
00:22:40,250 --> 00:22:44,410
But I do think things are going to change at least two times over by the time you

433
00:22:44,410 --> 00:22:45,820
get back.

434
00:22:45,820 --> 00:22:46,820
Right.

435
00:22:46,820 --> 00:22:48,400
Can we?

436
00:22:48,640 --> 00:22:53,500
The, let me just make sure the sky is not falling based on.

437
00:22:53,500 --> 00:22:58,740
It's a text saying AGIs are out the world is ending.

438
00:22:58,740 --> 00:22:59,740
No.

439
00:22:59,740 --> 00:23:00,810
Okay.

440
00:23:00,810 --> 00:23:03,800
So let's do it this way.

441
00:23:03,800 --> 00:23:17,700
Let's go principle by principle and then say based on what we know, does the AI, does

442
00:23:17,700 --> 00:23:24,210
the new AI driven business, how does that impact the principle?

443
00:23:24,210 --> 00:23:25,210
How is that the AI driven?

444
00:23:25,210 --> 00:23:26,210
I don't know.

445
00:23:26,210 --> 00:23:35,240
Even the term to say here, imagine the company, the new early adopting company, and

446
00:23:35,240 --> 00:23:42,510
they are all in on, they're not providing AI.

447
00:23:42,510 --> 00:23:47,310
They are using AI to provide their goods and or services.

448
00:23:47,310 --> 00:23:48,310
Okay.

449
00:23:48,310 --> 00:23:49,310
And they're successful.

450
00:23:49,310 --> 00:23:52,440
How do we do it?

451
00:23:52,440 --> 00:23:55,040
Now, number one.

452
00:23:55,080 --> 00:23:57,460
I think that's an easy one.

453
00:23:57,460 --> 00:24:01,980
Does principle number one change in that world?

454
00:24:01,980 --> 00:24:07,980
Who, let me just for context, when we started these our who said we and our and

455
00:24:07,980 --> 00:24:13,260
those pronouns are referred to testers originally and then the people of the

456
00:24:13,260 --> 00:24:16,340
develop people of people's people, the development team.

457
00:24:16,340 --> 00:24:21,830
So our is the people delivering the software, right?

458
00:24:21,830 --> 00:24:23,590
It's still the people delivering the software.

459
00:24:23,590 --> 00:24:24,590
I think so.

460
00:24:24,590 --> 00:24:25,590
Yeah.

461
00:24:25,590 --> 00:24:26,590
Who is we?

462
00:24:26,590 --> 00:24:27,590
Okay.

463
00:24:27,590 --> 00:24:34,820
People who just like back then we were we presented this like our were like the

464
00:24:34,820 --> 00:24:35,820
people under threat.

465
00:24:35,820 --> 00:24:36,820
Okay.

466
00:24:36,820 --> 00:24:37,820
Right.

467
00:24:37,820 --> 00:24:40,740
So I'm going to say it doesn't change the priority should be the same on any

468
00:24:40,740 --> 00:24:42,060
software development team.

469
00:24:42,060 --> 00:24:43,620
The priority is improving the business.

470
00:24:43,620 --> 00:24:44,620
Okay.

471
00:24:44,620 --> 00:24:48,060
Actually of any member of a company, your priority is improving the company.

472
00:24:48,060 --> 00:24:49,260
Okay.

473
00:24:49,260 --> 00:24:50,260
Done.

474
00:24:50,260 --> 00:24:53,190
One down, six to go.

475
00:24:53,190 --> 00:24:57,590
We use models like lean thinking theory of constraints to help identify

476
00:24:57,590 --> 00:25:02,050
prioritize and mitigate bottlenecks from the system.

477
00:25:02,050 --> 00:25:05,720
So I should probably not answer all of these first.

478
00:25:05,720 --> 00:25:09,680
I'm into this one because those things are still important for software, but

479
00:25:09,680 --> 00:25:14,760
I would change this one to apply system thinking and critical thinking in

480
00:25:14,760 --> 00:25:20,520
order to make sure that our phrase it differently than our AI overlords are

481
00:25:20,520 --> 00:25:23,200
contributing adequately appropriately to our business.

482
00:25:23,200 --> 00:25:25,820
What do you think?

483
00:25:25,820 --> 00:25:28,460
I thought we had one where we covered systems thinking.

484
00:25:28,460 --> 00:25:29,620
No, system thinking.

485
00:25:29,620 --> 00:25:32,500
I absolutely think is true.

486
00:25:32,500 --> 00:25:37,100
And then when I think about AI, right?

487
00:25:37,100 --> 00:25:45,270
I'm like, okay, if we have a, if we in a true AI agent world, we definitely

488
00:25:45,270 --> 00:25:47,360
will need system thinking.

489
00:25:47,360 --> 00:25:48,360
Okay.

490
00:25:48,360 --> 00:25:52,640
And we will probably still need lean thinking in some regards because lean

491
00:25:52,640 --> 00:25:57,760
thinking is all about producing the max number of widgets at the, at

492
00:25:57,760 --> 00:26:00,760
reducing waste, reducing waste and cost.

493
00:26:00,760 --> 00:26:01,760
Right.

494
00:26:01,760 --> 00:26:08,910
And, um, and the theory of constraints I still feel is like, if we have

495
00:26:08,910 --> 00:26:15,320
all these AI agents, who's identifying prioritizing mitigating bottlenecks?

496
00:26:15,320 --> 00:26:22,200
Like I don't see something I think still needs to do that unless these

497
00:26:22,200 --> 00:26:28,140
agents are so cheap, um, we don't need to prioritize.

498
00:26:28,140 --> 00:26:31,460
So what I see happen, again, I imagine this business where AI is helping us improve

499
00:26:31,460 --> 00:26:33,500
the business, which is great.

500
00:26:33,500 --> 00:26:41,350
I think that there are, like we need people, you know, overseers in a way to

501
00:26:41,350 --> 00:26:48,570
make sure just, just to help people not willy nilly accept code or to examine

502
00:26:48,570 --> 00:26:49,570
the code.

503
00:26:49,570 --> 00:26:53,610
It reminds me, you know, 15 months ago in our prediction episode,

504
00:26:53,610 --> 00:26:57,770
whatever year that was, I remember predicting and I'm going to pat myself

505
00:26:57,770 --> 00:27:00,650
on the back for this because the longer the time goes by, the more strongly

506
00:27:00,650 --> 00:27:02,170
I feel this is true.

507
00:27:02,170 --> 00:27:07,170
A key skill of an engineering team with the feature is the ability to read

508
00:27:07,170 --> 00:27:09,860
and evaluate code.

509
00:27:09,860 --> 00:27:15,340
And I'm not sure how we model that here, but you like, I think reading

510
00:27:15,340 --> 00:27:20,940
and comprehending and critiquing code is becoming, and it has already become

511
00:27:20,940 --> 00:27:23,420
more important than the ability to write it.

512
00:27:23,420 --> 00:27:30,280
I actually now disagree.

513
00:27:30,280 --> 00:27:32,620
Oh, Tommy Moore.

514
00:27:32,620 --> 00:27:47,270
Um, where I see the AI agent, um, movement going, uh, if I take it to the

515
00:27:47,270 --> 00:27:58,060
ultimate degree, uh, I see a world where except for two important exceptions,

516
00:27:58,060 --> 00:28:03,350
um, all code is throw away.

517
00:28:03,350 --> 00:28:04,710
Oh, tell me more.

518
00:28:04,710 --> 00:28:05,940
Okay.

519
00:28:05,940 --> 00:28:12,410
So I'll try to connect the dots.

520
00:28:12,410 --> 00:28:19,520
Um, so as we know, uh, I will tell you more.

521
00:28:19,520 --> 00:28:22,060
Where do ideas come from?

522
00:28:22,060 --> 00:28:23,710
Other ideas.

523
00:28:23,710 --> 00:28:24,750
Everybody knows this.

524
00:28:24,750 --> 00:28:27,640
Great.

525
00:28:27,640 --> 00:28:34,220
Um, now quickly summarize for our audience.

526
00:28:34,220 --> 00:28:39,300
This term called the adjacent possible.

527
00:28:39,300 --> 00:28:44,100
Oh, to me, I mean, this is relevant here because the adjacent possible is

528
00:28:44,100 --> 00:28:46,090
that's what AI has done.

529
00:28:46,090 --> 00:28:49,490
Adjacent possible is when you, there's a technology that exists.

530
00:28:49,490 --> 00:28:54,750
It's maybe it's new, likely it's new, but what it enables the things that are

531
00:28:54,750 --> 00:28:58,230
now possible to it cause they're adjacent to this become where the true

532
00:28:58,230 --> 00:28:59,230
innovations come from.

533
00:28:59,230 --> 00:29:00,430
That's what I've always believed.

534
00:29:00,430 --> 00:29:01,670
I believed.

535
00:29:01,670 --> 00:29:06,390
Gen AI was always the enabler of the adjacent possible and you know, 90% of

536
00:29:06,390 --> 00:29:10,230
the companies made bullshit applications with it, but we're starting to see

537
00:29:10,230 --> 00:29:15,150
companies actually solve real problems using gen AI and that's the adjacent

538
00:29:15,150 --> 00:29:16,230
possible, right?

539
00:29:16,830 --> 00:29:21,950
Uh, the way I would actually summer, that's actually not the adjacent

540
00:29:21,950 --> 00:29:23,750
possible because it's now solved.

541
00:29:23,910 --> 00:29:29,430
The adjacent possible is the, the thing, the ideas that haven't been created

542
00:29:29,430 --> 00:29:35,110
yet, but now can be because we have advanced the tech tree to go back to

543
00:29:35,110 --> 00:29:39,480
like geeky video games, right?

544
00:29:39,760 --> 00:29:45,840
We have gen AI and so that there's a now a whole new set of things that are now

545
00:29:45,920 --> 00:29:50,100
possible, uh, that weren't possible before gen AI.

546
00:29:50,340 --> 00:29:50,660
Okay.

547
00:29:51,260 --> 00:30:01,750
Now what Jenny, I does enable today is near instant disruption.

548
00:30:02,190 --> 00:30:06,230
Cause the other problem with the other phenomenon around adjacent possible that

549
00:30:06,230 --> 00:30:12,150
is really important here is, and I love Steven Johnson's explanation here.

550
00:30:13,180 --> 00:30:23,010
When new things become adjacent possible, it's terribly common for the same idea

551
00:30:23,010 --> 00:30:29,680
to be invented near, um, the same timeline, but entirely different times of

552
00:30:29,680 --> 00:30:30,200
the world.

553
00:30:30,560 --> 00:30:30,840
Yep.

554
00:30:30,920 --> 00:30:31,360
Absolutely.

555
00:30:31,360 --> 00:30:32,440
We're seeing, and we see that.

556
00:30:32,680 --> 00:30:33,400
And we see that.

557
00:30:34,000 --> 00:30:41,540
And what AI is doing is actually encouraging that adjacent possible.

558
00:30:41,980 --> 00:30:44,700
Like there's a piece of tech that my team owns.

559
00:30:45,180 --> 00:30:45,380
Okay.

560
00:30:45,380 --> 00:30:51,080
And just internally within Microsoft, I am knocking down a competitor like every month.

561
00:30:52,530 --> 00:30:52,810
Right.

562
00:30:52,810 --> 00:30:55,410
And now I'm not actually knocking down competitors.

563
00:30:55,410 --> 00:30:59,450
Whereas originally I was like inviting collaboration and all that, but I'm

564
00:30:59,450 --> 00:31:04,210
finding, Oh, some new team has this idea and now they're two years behind me.

565
00:31:04,290 --> 00:31:06,530
I'm like, look, I need to keep advancing.

566
00:31:06,730 --> 00:31:08,810
I don't have time to pause and ramp you up.

567
00:31:09,090 --> 00:31:09,530
Sorry.

568
00:31:10,510 --> 00:31:19,100
Um, and that is mostly because I have no idea how to quickly bring in these people.

569
00:31:20,150 --> 00:31:23,190
Um, and get them up to speed.

570
00:31:24,030 --> 00:31:29,720
Now, what that leads me to believe is that eventually we're going to get to a point

571
00:31:30,620 --> 00:31:38,040
where an all new dis disruption is going to happen monthly, weekly, daily.

572
00:31:38,400 --> 00:31:38,760
Okay.

573
00:31:39,360 --> 00:31:42,840
Now, the other thing I did is the other day is I went to open AI.

574
00:31:42,840 --> 00:31:47,620
I have a subscription and I typed in the following prompt.

575
00:31:48,340 --> 00:31:52,540
What is the sign of three, four, five, six, seven, eight, nine.

576
00:31:53,140 --> 00:31:56,660
And it wasn't actually those cause I specifically did random numbers.

577
00:31:56,660 --> 00:31:58,220
I kind of pounded my number key.

578
00:31:58,780 --> 00:31:59,140
Okay.

579
00:32:00,350 --> 00:32:09,980
And not only did it get it right, which surprised me, um, because I'm like,

580
00:32:11,330 --> 00:32:14,710
LLM is nothing more than a parrot.

581
00:32:14,710 --> 00:32:17,630
It is outputting mem, memorize things.

582
00:32:18,110 --> 00:32:23,470
But now open AI has sort of opened up the black box a little bit.

583
00:32:24,740 --> 00:32:28,660
And you can open up a dropdown and go, okay, how did you figure this out?

584
00:32:29,880 --> 00:32:30,960
And then I've seen those.

585
00:32:30,960 --> 00:32:31,280
Yeah.

586
00:32:31,520 --> 00:32:38,680
And then I looked at it and I'm like, Oh, that's fricking brilliant.

587
00:32:39,600 --> 00:32:49,210
And what they did is the, this part of it is still the black box to me, but they had

588
00:32:49,210 --> 00:32:55,940
something in there that figured out that leveraging the model directly, answering

589
00:32:55,940 --> 00:32:58,850
the question is not what it should do.

590
00:32:59,780 --> 00:33:07,620
What it should do is generate how to answer the question.

591
00:33:09,330 --> 00:33:15,840
And in this particular case, it generated a Python script and immediately executed

592
00:33:15,840 --> 00:33:18,180
it and that's what it outputted.

593
00:33:22,140 --> 00:33:22,620
Interesting.

594
00:33:23,060 --> 00:33:24,860
Are you doing the example right now?

595
00:33:24,860 --> 00:33:27,140
I am, but mine didn't give me the code.

596
00:33:27,140 --> 00:33:28,260
But anyway, I had to look.

597
00:33:28,340 --> 00:33:28,580
Okay.

598
00:33:29,100 --> 00:33:31,140
It asked if I wanted the answer in radians or degrees.

599
00:33:32,100 --> 00:33:32,460
Okay.

600
00:33:32,980 --> 00:33:41,850
Um, the, and I'm like, Oh yeah, that's exactly right.

601
00:33:41,970 --> 00:33:48,100
Because like I think about if I were asking you interview questions, if you came

602
00:33:48,100 --> 00:33:52,020
to me and my team and I asked you a bunch of interview questions to some

603
00:33:52,020 --> 00:33:53,660
degree, I care about what you know.

604
00:33:54,180 --> 00:33:54,420
Okay.

605
00:33:54,660 --> 00:34:00,130
But when it comes to specifics, I care more about, do you know, do you

606
00:34:00,130 --> 00:34:02,330
know how to figure it out?

607
00:34:03,910 --> 00:34:04,510
Yes.

608
00:34:05,980 --> 00:34:09,900
That is what the current model on open AI can do.

609
00:34:10,660 --> 00:34:16,500
It can switch between answering versus no, answering is a bad result.

610
00:34:16,580 --> 00:34:22,100
I should instead generate a Python script and even Python script is fantastic

611
00:34:22,100 --> 00:34:24,020
because it's an interpretive language.

612
00:34:24,020 --> 00:34:25,540
It doesn't have to be compiled.

613
00:34:25,540 --> 00:34:27,380
It can be immediately executed.

614
00:34:28,580 --> 00:34:28,940
Okay.

615
00:34:30,500 --> 00:34:33,500
As that continues, right.

616
00:34:33,700 --> 00:34:35,820
Disruption is going to continue to happen.

617
00:34:36,260 --> 00:34:40,220
There's no point in me starting a project that's going to take two months.

618
00:34:40,980 --> 00:34:47,600
If the technology is advancing such that, um, some other random team, because

619
00:34:47,600 --> 00:34:51,000
of the adjacent, the possible, there's no way for me to know that this team

620
00:34:51,000 --> 00:34:56,060
was coming, but if the, if the technology advances, let's say I have

621
00:34:56,060 --> 00:34:59,540
a project that takes two months, but the technology advances in one month,

622
00:34:59,540 --> 00:35:04,950
such that that project can be done in a week, I'm still three weeks

623
00:35:05,030 --> 00:35:07,670
away from finishing my solution.

624
00:35:08,780 --> 00:35:10,380
They now disrupted me.

625
00:35:10,500 --> 00:35:12,820
They have a solution before me.

626
00:35:13,960 --> 00:35:17,960
Um, which of course I will look into and then I will be the, the

627
00:35:17,960 --> 00:35:19,280
disrupt them tomorrow.

628
00:35:19,480 --> 00:35:20,680
I have a product idea.

629
00:35:20,760 --> 00:35:21,080
Yeah.

630
00:35:22,320 --> 00:35:25,560
Uh, it comes from, uh, I don't think, I don't think they do this anymore,

631
00:35:25,560 --> 00:35:31,000
but years and years ago, decades ago, Google, uh, would at the end of every

632
00:35:31,000 --> 00:35:33,800
week, employees could optionally submit a status report.

633
00:35:34,080 --> 00:35:34,560
Yes.

634
00:35:34,640 --> 00:35:37,520
And, but that status report was searchable and it could tell you like,

635
00:35:37,640 --> 00:35:39,320
Hey, somebody else working on the same stuff.

636
00:35:39,320 --> 00:35:40,080
Do you want to talk to them?

637
00:35:40,280 --> 00:35:49,780
Now my proposal is, uh, uh, open AI enterprise where if you choose privacy

638
00:35:49,780 --> 00:35:54,620
aside, uh, it would keep track of what sorts of questions everyone in the,

639
00:35:54,980 --> 00:35:58,740
everyone in the company is asking and pair people who seem to be working

640
00:35:58,740 --> 00:35:59,940
on solving the same problem.

641
00:36:02,740 --> 00:36:04,500
That's a fantastic idea.

642
00:36:06,900 --> 00:36:07,500
It's not bad, huh?

643
00:36:07,540 --> 00:36:08,700
Yeah, that's a good idea.

644
00:36:08,740 --> 00:36:09,620
I have been trying.

645
00:36:09,700 --> 00:36:16,560
I mean, I don't know how my story fired those synapses, but, um, that is

646
00:36:16,560 --> 00:36:20,880
a good idea because I, I too had learned around how Google, Google.

647
00:36:20,920 --> 00:36:21,400
And the, yeah.

648
00:36:21,480 --> 00:36:24,120
And the adjacent possible doesn't confine to a company, but if you want

649
00:36:24,120 --> 00:36:28,200
to optimize a company and make sure, at least across a corporation, you

650
00:36:28,200 --> 00:36:29,600
aren't solving the same problem.

651
00:36:30,080 --> 00:36:35,630
Why not, why not have the, have the AI figure out if you are doing that.

652
00:36:38,620 --> 00:36:38,980
Yeah.

653
00:36:39,700 --> 00:36:39,940
Yeah.

654
00:36:40,340 --> 00:36:40,580
All right.

655
00:36:40,580 --> 00:36:41,620
Just, just thinking about that.

656
00:36:41,980 --> 00:36:42,700
No, no, no, no.

657
00:36:42,700 --> 00:36:44,020
I mean, you're not convincing me.

658
00:36:44,020 --> 00:36:47,660
I'm wrong on the fact that reading code is critically important.

659
00:36:48,140 --> 00:36:48,700
No, no, no.

660
00:36:48,700 --> 00:36:56,880
So here's the thing as so in here, I have also thought through.

661
00:36:58,650 --> 00:37:02,150
So if we take what I just said, Yep.

662
00:37:03,060 --> 00:37:08,740
Um, to the nth degree, then in, so I'll, I'll, I'll circle back to

663
00:37:08,740 --> 00:37:14,090
my three things, all code, except two things, two types of code

664
00:37:15,570 --> 00:37:17,210
becomes throwaway code.

665
00:37:18,020 --> 00:37:18,300
Okay.

666
00:37:18,420 --> 00:37:18,820
Okay.

667
00:37:19,380 --> 00:37:24,650
So you need to do a query against the database.

668
00:37:25,210 --> 00:37:27,570
Actually, you don't need a query against the database.

669
00:37:27,610 --> 00:37:29,530
You need an answer to something.

670
00:37:29,770 --> 00:37:30,010
Yeah.

671
00:37:31,090 --> 00:37:32,530
Database is an input to my question.

672
00:37:32,610 --> 00:37:32,810
Go ahead.

673
00:37:32,810 --> 00:37:33,130
Great.

674
00:37:33,530 --> 00:37:40,660
It's Hey, how, how, how many widgets did Alan Page by last month?

675
00:37:41,140 --> 00:37:41,460
Right.

676
00:37:42,410 --> 00:37:51,780
Um, it realizes it doesn't have that answer memorized, but it knows how to

677
00:37:51,780 --> 00:37:59,840
derive the answer generates, um, Python code to authenticate, to connect,

678
00:38:00,920 --> 00:38:05,960
writes out the query, executes the query, summarize it, returns an answer.

679
00:38:07,500 --> 00:38:07,820
Okay.

680
00:38:07,980 --> 00:38:08,900
Today.

681
00:38:09,260 --> 00:38:13,260
Well, even a year ago, that would have sounded like fiction today.

682
00:38:13,260 --> 00:38:16,500
It would still be work, but that's entirely viable.

683
00:38:17,730 --> 00:38:19,290
Now the two types of code.

684
00:38:19,690 --> 00:38:21,130
So that code would be throwaway.

685
00:38:21,330 --> 00:38:24,170
It's generate execute, generate execute.

686
00:38:24,490 --> 00:38:31,460
Like, so in that world, Jason's 100% right.

687
00:38:32,700 --> 00:38:34,820
We're not doing QA testing for that code.

688
00:38:35,620 --> 00:38:37,500
That code lives three seconds.

689
00:38:38,720 --> 00:38:39,040
Right.

690
00:38:39,080 --> 00:38:39,480
Oh yeah.

691
00:38:39,840 --> 00:38:44,840
In that world, we're like, okay, let's make sure these agents are, um,

692
00:38:45,400 --> 00:38:46,400
doing the right thing.

693
00:38:47,880 --> 00:38:54,280
Uh, but the code that doesn't become throwaway would be the language platform.

694
00:38:54,320 --> 00:38:57,320
Like I've been talking about Python, right?

695
00:38:57,360 --> 00:39:01,600
Because Python's critical here again, cause it's an interpretive language.

696
00:39:02,000 --> 00:39:04,960
You don't, you don't want to waste time on going through a build step.

697
00:39:06,180 --> 00:39:18,510
Um, as well as the atomic classes, objects, API, like the atomic functions

698
00:39:18,510 --> 00:39:25,700
that you can call to do something useful, like authenticate, those things,

699
00:39:26,780 --> 00:39:29,620
those things become the, the base action.

700
00:39:30,570 --> 00:39:36,770
Um, and the AI just generates the right thing to execute against it.

701
00:39:37,400 --> 00:39:42,480
Um, I absolutely believe that's where things are, are, are heading.

702
00:39:43,520 --> 00:39:49,560
Um, because as more and more people do to the fact that the adjacent possible

703
00:39:50,500 --> 00:39:56,660
is going to cause people to get frustrated and abandon their projects because

704
00:39:56,660 --> 00:39:58,540
something came out of the blue and disrupted it.

705
00:39:59,740 --> 00:40:03,140
And the only way to deal with that, if you eventually realize there's no

706
00:40:03,140 --> 00:40:06,660
point in me starting anything, because I won't be able to finish it in time.

707
00:40:07,300 --> 00:40:10,940
To me, the, the only solution is, is just skip the head.

708
00:40:11,820 --> 00:40:15,460
Like you have to design your projects as if the entire code base is throwaway

709
00:40:16,220 --> 00:40:18,820
and it still has to succeed.

710
00:40:20,220 --> 00:40:25,810
You know what you're, I think we can both be right.

711
00:40:26,370 --> 00:40:26,930
I agree.

712
00:40:26,930 --> 00:40:29,610
That's a good way to approach, but you still have to know if the code base

713
00:40:29,610 --> 00:40:35,280
that you're going to throw away, you have to be, you still have to know.

714
00:40:35,360 --> 00:40:37,320
I mean, you know what I mean?

715
00:40:37,320 --> 00:40:40,920
It's like, maybe it's just a trust, but verify thing with AI generated code.

716
00:40:41,200 --> 00:40:43,080
You and I both know it's getting way better.

717
00:40:43,080 --> 00:40:46,200
It was good before it's getting, it's scary.

718
00:40:46,800 --> 00:40:47,320
It's scary.

719
00:40:47,320 --> 00:40:48,920
Good, but it's not perfect.

720
00:40:49,040 --> 00:40:49,440
No.

721
00:40:50,340 --> 00:40:55,420
And we need, we still need people who can look at it and go, yes, this is

722
00:40:55,420 --> 00:40:59,380
doing what I asked it to versus close enough, enter submit.

723
00:41:00,020 --> 00:41:00,260
Yeah.

724
00:41:00,260 --> 00:41:00,540
No.

725
00:41:00,540 --> 00:41:02,020
And here's the thing that's fun.

726
00:41:02,180 --> 00:41:06,020
Like when you, when you talk about code, like the, the most common

727
00:41:06,140 --> 00:41:13,740
hallucination I get from, from AI when I'm coding is it makes up things that don't

728
00:41:13,740 --> 00:41:18,540
exist, it makes up these atomic functions that don't exist, but then when I look

729
00:41:18,540 --> 00:41:22,620
at it, I'm like, wait, why doesn't that thing exist?

730
00:41:23,180 --> 00:41:23,420
Yeah.

731
00:41:23,860 --> 00:41:28,940
And, and actually that could be one of the fixes for your AI generated code.

732
00:41:28,940 --> 00:41:30,380
You should like that doesn't exist.

733
00:41:30,420 --> 00:41:31,340
Please go write it.

734
00:41:32,410 --> 00:41:38,850
The only way throwaway code is viable is if it can be generated and run and

735
00:41:38,850 --> 00:41:43,290
discarded in a couple of weeks, there's a step in there of making sure it does

736
00:41:43,290 --> 00:41:44,250
what it's expected to do.

737
00:41:44,410 --> 00:41:47,930
And, and I hate the metric of lines of code, but I'm going to use it for a

738
00:41:47,930 --> 00:41:49,050
second, bear with me.

739
00:41:49,490 --> 00:41:54,010
If like, if I could write a couple hundred codes, like codes, a couple

740
00:41:54,010 --> 00:41:56,770
hundred lines of code in a day and debug it and test it, make sure it's

741
00:41:56,770 --> 00:41:59,250
working, um, in the day, I felt pretty good.

742
00:41:59,250 --> 00:42:00,530
I felt like I cranked out a bunch of stuff.

743
00:42:01,050 --> 00:42:04,130
I should be able to do a couple thousand reviewing and making sure,

744
00:42:04,130 --> 00:42:08,410
especially with today's languages, um, that each line does as much as

745
00:42:08,410 --> 00:42:10,810
10 lines of my old code did, um, or 20.

746
00:42:11,210 --> 00:42:17,570
But, uh, you still have to just give it a once over because of lots of reasons.

747
00:42:18,330 --> 00:42:21,570
And we need people that can do that very good and very quickly,

748
00:42:22,390 --> 00:42:23,390
even for throwaway code.

749
00:42:25,700 --> 00:42:30,140
And maybe by the time I will, that's no longer true because it just,

750
00:42:30,220 --> 00:42:33,380
it's just because the AI does that for us, but we're not there.

751
00:42:33,380 --> 00:42:36,820
If we have time at the end of this podcast, we don't, we're on

752
00:42:36,820 --> 00:42:38,060
principle number two, we do.

753
00:42:38,060 --> 00:42:39,180
I will.

754
00:42:40,550 --> 00:42:43,830
So there's intellectual, uh, there's IP there.

755
00:42:44,430 --> 00:42:50,850
Um, but if we have time, I'll talk to you off error and I think it will

756
00:42:50,850 --> 00:42:51,810
fulfill your goal.

757
00:42:51,850 --> 00:42:53,810
It might happen by the end of the podcast.

758
00:42:54,090 --> 00:42:55,920
Um, all right.

759
00:42:56,320 --> 00:42:56,760
All right.

760
00:42:57,570 --> 00:43:01,710
So yeah, I don't know how we handle theory of constraints.

761
00:43:01,830 --> 00:43:02,550
We don't.

762
00:43:02,550 --> 00:43:05,350
I think this one is, I can only get to number, at least get to number

763
00:43:05,350 --> 00:43:06,310
three or four before we're done.

764
00:43:06,990 --> 00:43:12,230
But number two is we have to use some different models to prioritize.

765
00:43:12,350 --> 00:43:14,030
And this goes into throwaway code.

766
00:43:14,950 --> 00:43:16,590
We want to prioritize.

767
00:43:17,550 --> 00:43:18,110
We duck.

768
00:43:18,510 --> 00:43:21,430
I don't know if the bottlenecks, they're still going to exist, but

769
00:43:21,430 --> 00:43:26,660
we're trying to optimize is, is delivery.

770
00:43:27,020 --> 00:43:30,220
bottlenecks are still, are you still going to exist in the system?

771
00:43:30,700 --> 00:43:35,380
But when we wrote this, um, testing was the bottleneck.

772
00:43:35,860 --> 00:43:39,530
We were thinking more well.

773
00:43:39,570 --> 00:43:39,850
Okay.

774
00:43:39,850 --> 00:43:47,360
So if we generalize this and we say bottlenecks are resources being consumed

775
00:43:47,480 --> 00:43:52,500
and generally like resources, we, I was, when we were doing this, I was

776
00:43:52,500 --> 00:43:55,780
thinking about people, but now we generalize it resources.

777
00:43:55,820 --> 00:43:56,380
Okay.

778
00:43:56,540 --> 00:43:57,340
Well, you know what?

779
00:43:57,460 --> 00:43:59,860
In this future world, I'm going to need compute.

780
00:43:59,900 --> 00:44:00,860
I'm going to need storage.

781
00:44:00,860 --> 00:44:02,020
I'm going to need networking.

782
00:44:02,540 --> 00:44:04,500
I'm going to need databases, right?

783
00:44:04,860 --> 00:44:08,580
Maybe, maybe that's how it extends.

784
00:44:08,620 --> 00:44:11,660
Let me, let me throw a thinking cap on here.

785
00:44:12,100 --> 00:44:15,340
Brian Finster said on our podcast, and I agree with him so much.

786
00:44:15,340 --> 00:44:19,500
I want to keep on repeating this is the best way to optimize your

787
00:44:19,500 --> 00:44:21,580
system is to try and do CD.

788
00:44:22,710 --> 00:44:23,030
Yeah.

789
00:44:23,070 --> 00:44:25,350
Cause it shows all the things like, well, we can't because we

790
00:44:25,350 --> 00:44:26,510
block, we'll go fix block.

791
00:44:26,990 --> 00:44:34,410
So I think you're thinking with the, with AI helping a mature organization

792
00:44:34,410 --> 00:44:40,140
to does it correctly, uh, I mean, CD of course you're doing CD, but, but is

793
00:44:40,140 --> 00:44:41,380
there anything else they need to do?

794
00:44:41,420 --> 00:44:43,020
Are there other bottlenecks to do the AI?

795
00:44:43,740 --> 00:44:45,060
Can the AI CD?

796
00:44:45,100 --> 00:44:45,460
Yeah.

797
00:44:45,820 --> 00:44:49,660
So I almost wish I hadn't even brought up the last topic because

798
00:44:49,660 --> 00:44:56,370
I'm just now going, yeah, Alan automatically on the fly generated

799
00:44:56,370 --> 00:44:57,810
code that's throw away.

800
00:44:57,930 --> 00:45:00,130
Is there a faster CD than that?

801
00:45:00,210 --> 00:45:03,730
There is not, it can't be.

802
00:45:04,450 --> 00:45:07,210
It's like, yeah, we, we, we don't compile it till it's production.

803
00:45:07,650 --> 00:45:08,250
It's interpretive.

804
00:45:08,290 --> 00:45:08,690
You know what I mean?

805
00:45:08,690 --> 00:45:08,810
Right.

806
00:45:09,770 --> 00:45:13,250
And so, all right, let's try to move on to number three.

807
00:45:13,250 --> 00:45:13,730
How about that?

808
00:45:13,730 --> 00:45:14,010
Okay.

809
00:45:14,890 --> 00:45:17,050
We are a forest for continuous improvement in the doubt that

810
00:45:17,050 --> 00:45:19,850
optimize our practices in order to succeed rather than using safety

811
00:45:19,850 --> 00:45:20,850
nets, sketch our failures.

812
00:45:20,890 --> 00:45:22,290
Obviously you're very testing there.

813
00:45:22,690 --> 00:45:24,010
Um, I'm gonna let you go first on this one.

814
00:45:24,010 --> 00:45:24,930
Cause I did the last one.

815
00:45:26,210 --> 00:45:32,380
Um, that obviously the language would change.

816
00:45:32,380 --> 00:45:36,250
I would keep that one, but I think it would need to be

817
00:45:36,250 --> 00:45:37,930
significantly tweaked.

818
00:45:38,570 --> 00:45:39,010
Yeah.

819
00:45:39,050 --> 00:45:39,370
Right.

820
00:45:39,370 --> 00:45:46,140
This is, we do still optimize our practices and actually that by the

821
00:45:46,140 --> 00:45:50,420
way is where I interpret systems thinking in here.

822
00:45:50,940 --> 00:45:52,660
Uh, but I think, I think you're right.

823
00:45:53,020 --> 00:45:55,860
System thinking needs to be more highlighted.

824
00:45:56,460 --> 00:45:56,820
Right.

825
00:45:57,060 --> 00:46:01,140
That's the only, if we have a whole bunch of AI agents doing

826
00:46:01,140 --> 00:46:07,980
suboptimal, um, optimization, Oh, our world's gonna suck.

827
00:46:08,930 --> 00:46:14,250
Um, because it's not the case that everything's independent, right?

828
00:46:14,290 --> 00:46:17,930
We're going to create a world where, where we can visibly see the

829
00:46:17,930 --> 00:46:20,210
butterfly effect within hours.

830
00:46:20,890 --> 00:46:24,800
Um, but continuous improvement.

831
00:46:24,800 --> 00:46:26,680
I think that's always going to be the case.

832
00:46:27,480 --> 00:46:30,640
Uh, adaptation, agile adaptation.

833
00:46:31,360 --> 00:46:37,580
Um, I think that's even more critical today than it was when we wrote this.

834
00:46:39,060 --> 00:46:40,220
Yeah, I think so.

835
00:46:40,260 --> 00:46:45,700
Both, both, both internally as a human being working on, uh, in this world,

836
00:46:46,020 --> 00:46:50,740
as well as externally, because again, in my worldview, disruption

837
00:46:50,740 --> 00:46:53,140
is going to happen all the time.

838
00:46:56,140 --> 00:46:56,420
Okay.

839
00:46:56,740 --> 00:46:59,250
And so, right.

840
00:46:59,970 --> 00:47:06,210
I think we're heading to a world where, you know, six kids and the, in a,

841
00:47:06,450 --> 00:47:12,180
working out of a garage, um, could scale something up.

842
00:47:12,220 --> 00:47:15,460
That could be a threat to, you know, something as big as Amazon.

843
00:47:15,460 --> 00:47:18,300
I don't know how they would do it, but I think that's something that

844
00:47:18,740 --> 00:47:20,340
is becoming more and more possible.

845
00:47:21,540 --> 00:47:24,150
Um, all right.

846
00:47:24,670 --> 00:47:25,310
How about that?

847
00:47:25,390 --> 00:47:26,030
Any comments?

848
00:47:26,470 --> 00:47:30,340
I, yeah.

849
00:47:30,340 --> 00:47:31,780
I got a whole bunch of reach.

850
00:47:31,820 --> 00:47:35,100
I read you reach you in a second, but no, I think that's kind of it.

851
00:47:35,100 --> 00:47:37,380
I do want to get through the rest and then I have one special bonus

852
00:47:37,380 --> 00:47:38,380
thing to read at the end here.

853
00:47:38,500 --> 00:47:38,860
Okay.

854
00:47:38,940 --> 00:47:41,340
Um, what about the quality culture of our team?

855
00:47:41,660 --> 00:47:43,620
I mean, is that, is that still a thing?

856
00:47:43,820 --> 00:47:47,920
I mean, we talked about, we talked about friends and I stopped caring

857
00:47:47,920 --> 00:47:52,280
about testing, I realized we care about quality far more than an activity.

858
00:47:53,580 --> 00:47:57,700
Uh, but how does AI help with the building that quality culture?

859
00:47:58,180 --> 00:48:01,660
And remember care and quality are two sides of the same coin.

860
00:48:02,140 --> 00:48:09,090
So yeah, the, um, I think that becomes a lot more critical.

861
00:48:10,700 --> 00:48:11,140
Tell me more.

862
00:48:11,580 --> 00:48:20,890
The, the, in terms of how to produce quality, because the, we enter from a

863
00:48:20,890 --> 00:48:25,760
world of determinism, right?

864
00:48:25,800 --> 00:48:27,040
What is checked in code?

865
00:48:27,320 --> 00:48:29,960
That's deterministic code, right?

866
00:48:29,960 --> 00:48:35,070
And again, going back to what might've ended up being the poison pill for

867
00:48:35,070 --> 00:48:40,110
the whole soul exercise, going back to that, I'm like, okay, well, if the

868
00:48:40,110 --> 00:48:46,690
code is throwaway, then what's the definition of quality and I still

869
00:48:46,690 --> 00:48:53,840
stand by it's the one we've always used, which is, um, the customer's

870
00:48:53,840 --> 00:48:54,440
point of view.

871
00:48:54,760 --> 00:48:54,960
Yeah.

872
00:48:55,400 --> 00:48:56,560
But that's principle number five.

873
00:48:56,880 --> 00:48:57,160
Right.

874
00:48:57,400 --> 00:49:02,820
But now this becomes more the quality culture here goes in a different way.

875
00:49:03,300 --> 00:49:09,370
Cause previously we were talking about how do we build quality in a, in

876
00:49:09,370 --> 00:49:10,730
a deterministic world?

877
00:49:11,860 --> 00:49:18,950
Hey, Alan, your, your if condition is, is doing greater than when it should

878
00:49:18,950 --> 00:49:21,190
be doing greater than equals, right?

879
00:49:21,510 --> 00:49:23,350
You go check it in, right?

880
00:49:23,670 --> 00:49:23,910
Right.

881
00:49:23,910 --> 00:49:25,350
It's a deterministic world.

882
00:49:25,430 --> 00:49:27,070
It's a lot easier to determine.

883
00:49:27,150 --> 00:49:31,910
I will tell you, even if developer, even if most of the code, all of the

884
00:49:31,910 --> 00:49:38,150
code is AI generated, uh, the team who is interacting with the AI and

885
00:49:38,150 --> 00:49:41,470
doing some work there, it isn't, it isn't all automated by AI.

886
00:49:41,790 --> 00:49:45,390
That team needs to have a quality first mindset.

887
00:49:46,720 --> 00:49:47,240
Yes.

888
00:49:48,060 --> 00:49:48,300
Yes.

889
00:49:48,300 --> 00:49:48,860
Absolutely.

890
00:49:49,060 --> 00:49:50,100
There's something still there.

891
00:49:50,100 --> 00:49:50,260
No.

892
00:49:50,260 --> 00:49:54,220
And actually this reminds me, this takes me back to my time in bank.

893
00:49:55,120 --> 00:49:55,360
Right.

894
00:49:55,360 --> 00:49:58,080
And I talked about it on the podcast.

895
00:49:58,080 --> 00:49:58,960
Years ago, right?

896
00:49:59,000 --> 00:50:04,750
One of the things that was tragic, the interesting is that bugs did not

897
00:50:04,750 --> 00:50:07,100
matter in bank, right?

898
00:50:07,100 --> 00:50:11,540
Because they have multiple different data feeds changing every single day.

899
00:50:11,780 --> 00:50:17,780
You have a model that's changing similarly constantly all the time.

900
00:50:18,140 --> 00:50:19,180
So it was a throwaway code.

901
00:50:20,090 --> 00:50:21,290
The experiences.

902
00:50:21,330 --> 00:50:27,350
So the code wasn't throwaway, but the experiences were okay.

903
00:50:27,430 --> 00:50:32,270
So you, you would go to Bing and you would type a query like price of Xbox.

904
00:50:32,790 --> 00:50:33,070
Right.

905
00:50:33,070 --> 00:50:38,150
And then you would, you might, you might one time notice that it average

906
00:50:38,190 --> 00:50:44,080
it saying, Hey, Walmart, but, but maybe one out of a hundred times you hit

907
00:50:44,080 --> 00:50:48,120
refresh and it says a phone number instead of Walmart, right?

908
00:50:48,680 --> 00:50:54,240
Um, you wouldn't report that phone number bug, even though it was a bug.

909
00:50:55,230 --> 00:51:00,670
What you have to report is the pattern, the observation, Hey, something

910
00:51:00,670 --> 00:51:05,790
is wrong in your system because the experience is non-deterministic.

911
00:51:07,700 --> 00:51:12,620
And so all I'm saying here in terms of the throwaway and actually given,

912
00:51:12,980 --> 00:51:16,220
given HTML five is so much goddamn code.

913
00:51:16,300 --> 00:51:17,140
Yeah, you're right.

914
00:51:17,500 --> 00:51:23,760
Or rather, yeah, it might be that, that we're already kind of in

915
00:51:23,760 --> 00:51:25,040
the throwaway code world.

916
00:51:26,780 --> 00:51:27,140
Yeah.

917
00:51:27,820 --> 00:51:28,060
Anyway.

918
00:51:28,060 --> 00:51:28,340
All right.

919
00:51:29,180 --> 00:51:30,820
Um, okay.

920
00:51:30,860 --> 00:51:34,620
I promise for the next of these, I am not going to use the term throwaway code.

921
00:51:34,980 --> 00:51:36,620
I am trying to battle the poison.

922
00:51:36,620 --> 00:51:39,180
We kind of already talked about five, five doesn't change.

923
00:51:39,260 --> 00:51:42,220
The customer is only one capable to judge and evaluate the quality of our product.

924
00:51:42,900 --> 00:51:43,220
Done.

925
00:51:43,540 --> 00:51:46,020
Number six, we use data extensively.

926
00:51:46,020 --> 00:51:46,220
Yeah.

927
00:51:46,220 --> 00:51:48,700
They have one problem with number five is who the hell is the customer.

928
00:51:53,110 --> 00:51:53,710
But I don't know.

929
00:51:53,990 --> 00:51:54,630
Keep on going.

930
00:51:55,230 --> 00:51:55,590
All right.

931
00:51:55,710 --> 00:51:56,230
Data.

932
00:51:56,830 --> 00:52:01,750
I think the way we use data or how data is consumed, the customer data is different.

933
00:52:02,760 --> 00:52:06,600
But how we have written there, I don't think changes.

934
00:52:07,640 --> 00:52:07,840
Yeah.

935
00:52:07,840 --> 00:52:08,760
I don't think it does either.

936
00:52:09,200 --> 00:52:15,950
The it's in matter of fact, like the second part of that principle, close the gaps

937
00:52:15,950 --> 00:52:17,670
between product hypothesis and business impact.

938
00:52:17,670 --> 00:52:19,870
I think, I think that has to go faster.

939
00:52:19,910 --> 00:52:20,590
I think that, yeah.

940
00:52:20,630 --> 00:52:20,790
Yeah.

941
00:52:21,270 --> 00:52:22,590
We didn't say decrease.

942
00:52:22,590 --> 00:52:23,350
We said close.

943
00:52:23,390 --> 00:52:26,150
So they're going to, that's the thing that's going to have to

944
00:52:26,150 --> 00:52:28,910
happen is that that has to happen faster.

945
00:52:29,430 --> 00:52:33,950
How about we expand abilities and know how across the team, getting the fact,

946
00:52:33,950 --> 00:52:36,270
this is the one that says, Hey, if you get really good at this,

947
00:52:36,270 --> 00:52:37,150
testers may go away.

948
00:52:37,510 --> 00:52:40,670
Do we now say if you get really good at this developers may go away?

949
00:52:40,830 --> 00:52:41,270
I do.

950
00:52:42,060 --> 00:52:43,100
I think I do too.

951
00:52:44,140 --> 00:52:51,830
Because I think, no, I think, I think the developers who hold on to white

952
00:52:51,830 --> 00:52:59,110
knuckles around, I'm a, I think we're going to see a world where developers

953
00:52:59,150 --> 00:53:01,350
begin to use the same language testers did.

954
00:53:02,390 --> 00:53:03,430
I'm a craftsman.

955
00:53:05,430 --> 00:53:05,910
They will.

956
00:53:06,670 --> 00:53:06,910
Yeah.

957
00:53:07,030 --> 00:53:11,470
And, and the people it's, and again, I'm going to go back to, I need to write.

958
00:53:11,470 --> 00:53:13,670
I with me leaving.

959
00:53:13,710 --> 00:53:16,390
Um, I haven't read any blog posts and all this one I need to write.

960
00:53:16,390 --> 00:53:21,310
And this topic is this Dan pink's lesser known book before drive was

961
00:53:21,310 --> 00:53:22,830
called the a whole new world.

962
00:53:23,940 --> 00:53:27,260
And it's about how right brainer we talked about last time.

963
00:53:27,260 --> 00:53:28,700
I'm sorry, stop it.

964
00:53:29,060 --> 00:53:30,780
A whole new mind, maybe whole new mind.

965
00:53:31,220 --> 00:53:35,860
Um, how right brainers will like people with critical thinking are going to rule

966
00:53:35,860 --> 00:53:38,300
the world and he's more right than he ever was.

967
00:53:38,300 --> 00:53:39,900
And he wrote the book almost 20 years ago.

968
00:53:40,180 --> 00:53:43,940
And those are the people that are going to be interacting with the

969
00:53:43,940 --> 00:53:45,380
Gen AI to develop to you.

970
00:53:45,420 --> 00:53:47,020
Was it critical or creative?

971
00:53:48,130 --> 00:53:50,650
Creative, creative, creative people.

972
00:53:51,050 --> 00:53:58,400
So let me, uh, in my spare time over here, um, I asked our friend chat GPT.

973
00:53:59,360 --> 00:54:01,840
I said, these are the empty principles from Alan Brent.

974
00:54:02,440 --> 00:54:06,320
How would we change these to reflect AI driven software release rather than the

975
00:54:06,320 --> 00:54:08,960
empty approach, which is mostly about continuous delivery.

976
00:54:09,560 --> 00:54:10,720
You can argue with that part.

977
00:54:10,720 --> 00:54:11,720
I want to give us some context.

978
00:54:12,320 --> 00:54:13,800
Um, it said, great question.

979
00:54:13,880 --> 00:54:18,990
Oh, by the way, uh, what is up with chat GPT?

980
00:54:20,050 --> 00:54:21,050
I asked it.

981
00:54:21,090 --> 00:54:24,970
Um, I have, I'm not going to mention anything here, but I'm going to

982
00:54:25,170 --> 00:54:25,890
hold on a second here.

983
00:54:26,690 --> 00:54:29,770
I have some rare albums and I'm not going to mention which ones they are.

984
00:54:29,770 --> 00:54:34,050
I have one that has a very provocative cover and I, I got, I want to sell it.

985
00:54:34,090 --> 00:54:35,690
I'm uncomfortable having it on my collection.

986
00:54:35,890 --> 00:54:36,370
Okay.

987
00:54:37,010 --> 00:54:44,420
But I asked it about it and he said, and chat GPT, I almost give it a, he says,

988
00:54:45,060 --> 00:54:47,060
it gave me a TLDR at the end.

989
00:54:47,660 --> 00:54:49,100
I said, yes, it's rare.

990
00:54:49,220 --> 00:54:49,620
Yes.

991
00:54:49,620 --> 00:54:50,420
It's collectible.

992
00:54:50,460 --> 00:54:50,900
Yes.

993
00:54:50,900 --> 00:54:52,340
It's controversial AF.

994
00:54:53,740 --> 00:54:54,220
Yeah.

995
00:54:54,260 --> 00:54:55,940
No, that happened to me too.

996
00:54:56,020 --> 00:54:56,540
Today.

997
00:54:56,580 --> 00:54:58,500
I don't remember the exact example.

998
00:54:58,540 --> 00:55:03,740
I'm like, no, no, do not turn it down a little bit.

999
00:55:03,740 --> 00:55:04,700
Chat GPT.

1000
00:55:05,420 --> 00:55:12,650
No, I don't, God, I don't want these damn bots to have personalities.

1001
00:55:12,690 --> 00:55:12,970
Okay.

1002
00:55:13,010 --> 00:55:15,410
Let me, let me whip through this answer to this question.

1003
00:55:15,410 --> 00:55:16,570
So, um, it's a great question.

1004
00:55:16,570 --> 00:55:17,290
Blah, blah, blah.

1005
00:55:17,610 --> 00:55:21,010
Our priority is delivering measurable customer and business value guided

1006
00:55:21,010 --> 00:55:22,130
by intelligent systems.

1007
00:55:23,550 --> 00:55:25,150
I wouldn't use those words, but it's not wrong.

1008
00:55:25,270 --> 00:55:26,510
I like ours better.

1009
00:55:26,750 --> 00:55:30,910
We use AI augmented models to identify, prioritize, and eliminate friction

1010
00:55:30,910 --> 00:55:32,510
in our development delivery pipelines.

1011
00:55:32,830 --> 00:55:34,990
I don't, I think our discussion was better, but it's not bad.

1012
00:55:35,110 --> 00:55:35,470
Yep.

1013
00:55:35,990 --> 00:55:38,350
We are a force for adaptive automation, continual.

1014
00:55:38,350 --> 00:55:40,710
So you're finding our systems with feedback and loops rather than

1015
00:55:40,710 --> 00:55:42,230
relying on manual safety nets.

1016
00:55:42,950 --> 00:55:43,910
It's not wrong.

1017
00:55:43,950 --> 00:55:44,390
Yeah.

1018
00:55:44,950 --> 00:55:46,270
I didn't think about that angle.

1019
00:55:46,310 --> 00:55:47,430
I didn't think about that angle.

1020
00:55:47,430 --> 00:55:48,230
It's right on.

1021
00:55:49,210 --> 00:55:52,890
We lead the organization in developing a quality mindset with AI as a

1022
00:55:52,890 --> 00:55:56,090
co-pilot and promoting shared accountability for product excellence.

1023
00:55:56,810 --> 00:55:57,810
Again, it's not whole.

1024
00:55:57,810 --> 00:55:58,970
Wait, wait, wait, wait.

1025
00:56:00,940 --> 00:56:02,900
It brings in shared accountability.

1026
00:56:03,490 --> 00:56:03,810
Yeah.

1027
00:56:05,210 --> 00:56:06,810
Who has shared accountability?

1028
00:56:06,810 --> 00:56:11,530
Like last, I know LLM doesn't give a crap if you hold it.

1029
00:56:11,530 --> 00:56:14,410
No, but no, it's the team, the peep, the humans.

1030
00:56:14,490 --> 00:56:14,970
Okay.

1031
00:56:15,570 --> 00:56:15,810
Okay.

1032
00:56:15,810 --> 00:56:18,610
We coach teams on, here's a subtitle subtext.

1033
00:56:18,850 --> 00:56:22,450
We coach teams on interpreting AI insights and building trust in

1034
00:56:22,450 --> 00:56:25,770
autonomous or semi-autonomous release systems, which is kind of what.

1035
00:56:26,530 --> 00:56:27,690
MT 3.0 should do.

1036
00:56:27,810 --> 00:56:28,130
Yeah.

1037
00:56:28,810 --> 00:56:32,050
Help people get the trust so they don't pull out of their white knuckles.

1038
00:56:32,050 --> 00:56:32,810
They ride the wave.

1039
00:56:32,890 --> 00:56:33,090
Right.

1040
00:56:33,450 --> 00:56:36,570
Number five, we believe that customers and AI informed representations

1041
00:56:36,570 --> 00:56:38,330
of them are the ultimate judges of quality.

1042
00:56:38,610 --> 00:56:39,730
Now here's an interesting take.

1043
00:56:39,730 --> 00:56:43,050
We believe in the customer we haven't talked about because now we

1044
00:56:43,050 --> 00:56:45,210
know customers, we have data from them.

1045
00:56:45,410 --> 00:56:50,290
Can we use AI to build a representation of them that can evaluate quality?

1046
00:56:51,940 --> 00:56:52,740
That's a whole new one.

1047
00:56:52,940 --> 00:56:54,140
That's, that's the case.

1048
00:56:54,300 --> 00:56:54,500
Yep.

1049
00:56:55,140 --> 00:56:58,820
Um, we use telemetry usage data and AI driven analysis to

1050
00:56:58,820 --> 00:57:00,940
continuously close the gap between what we built and what the

1051
00:57:00,940 --> 00:57:02,180
business needs, that's fine.

1052
00:57:02,340 --> 00:57:06,340
Oh, it was said, we democratize AI knowledge and tools across the team,

1053
00:57:06,340 --> 00:57:09,300
enabling everyone to participate in building smarter products and

1054
00:57:09,300 --> 00:57:10,180
smarter systems.

1055
00:57:11,490 --> 00:57:12,290
I get by that.

1056
00:57:12,530 --> 00:57:13,930
These are horrible.

1057
00:57:15,800 --> 00:57:18,920
From a, from a, a, a, I probably could have done a better prompt,

1058
00:57:18,960 --> 00:57:21,920
but, um, not bad, right?

1059
00:57:21,960 --> 00:57:22,360
Bad.

1060
00:57:22,360 --> 00:57:27,000
And what we just spent the last 40 minutes on, we could have just asked GPT.

1061
00:57:27,040 --> 00:57:28,080
I know we could have started.

1062
00:57:28,680 --> 00:57:31,720
This is actually the way I do a lot of, um, I'm trying to form words.

1063
00:57:31,720 --> 00:57:34,440
I'll, I'll, I'll have it give me a start.

1064
00:57:34,480 --> 00:57:38,640
I wouldn't use these words and not quite right, but refining what's here.

1065
00:57:38,680 --> 00:57:40,160
This is the same thing for code.

1066
00:57:40,360 --> 00:57:41,320
I want to read this.

1067
00:57:41,320 --> 00:57:46,360
Like again, I use my writing skills to read this, understand where it does and

1068
00:57:46,360 --> 00:57:50,440
does not match my intent and tweak it until I like it as a reputation of me.

1069
00:57:50,680 --> 00:57:56,440
I, we have to do the same thing with code, but what I really like, uh, is.

1070
00:57:57,190 --> 00:57:59,750
This is kind of, it's really close to spot on.

1071
00:57:59,750 --> 00:58:02,970
So your idea, we took your idea.

1072
00:58:03,170 --> 00:58:07,090
We took something that could have taken weeks and months of vetting and we did

1073
00:58:07,090 --> 00:58:08,810
some discussion, accelerated it.

1074
00:58:08,810 --> 00:58:14,810
And we have a new place to kind of start whatever the XX principles are for.

1075
00:58:15,170 --> 00:58:16,650
Uh, when I get back from the PCT.

1076
00:58:17,370 --> 00:58:17,730
Right.

1077
00:58:18,010 --> 00:58:19,410
And the world may have changed by them.

1078
00:58:19,410 --> 00:58:22,450
That might be a great place to kick off the way, way, way then.

1079
00:58:22,530 --> 00:58:26,770
The world will have like, again, I think the world likely

1080
00:58:26,770 --> 00:58:28,210
will have changed multiple times.

1081
00:58:28,250 --> 00:58:35,710
But one last thing, no matter how many times the world has changed by

1082
00:58:35,710 --> 00:58:43,660
the time you get back, do you see any reason to believe why the modern

1083
00:58:43,660 --> 00:58:45,620
testing motto would change?

1084
00:58:46,950 --> 00:58:48,670
That's not modern, not about testing.

1085
00:58:48,750 --> 00:58:49,110
Nope.

1086
00:58:50,860 --> 00:58:53,260
Oh, accelerate the achievement of shippable quality.

1087
00:58:53,580 --> 00:58:54,060
Yes.

1088
00:58:54,260 --> 00:58:54,500
Yeah.

1089
00:58:54,500 --> 00:58:55,340
That's what AI does.

1090
00:58:55,500 --> 00:58:55,780
You're right.

1091
00:58:55,780 --> 00:58:56,300
You're out of the money.

1092
00:58:56,300 --> 00:58:57,700
Brit, you're a fucking profit.

1093
00:58:58,180 --> 00:58:59,060
I mean, a freaking profit.

1094
00:58:59,060 --> 00:58:59,420
Sorry.

1095
00:58:59,740 --> 00:59:00,020
Sorry.

1096
00:59:00,020 --> 00:59:02,500
Apple, if you come censor my podcast and kick us off.

1097
00:59:04,860 --> 00:59:05,380
Yeah.

1098
00:59:05,660 --> 00:59:07,580
I am not going to check the explicit box.

1099
00:59:07,580 --> 00:59:12,980
Please don't listen with your kids in the car if you have a time machine.

1100
00:59:14,800 --> 00:59:19,520
But if you do have a time machine, please contact us on the DM.

1101
00:59:19,960 --> 00:59:23,840
Could you please come back before the 2016 election and help us with

1102
00:59:23,840 --> 00:59:26,720
some little bit more, little bit more information.

1103
00:59:26,760 --> 00:59:28,480
Oh, you were thinking way too small.

1104
00:59:29,160 --> 00:59:32,480
I guess if someone's got a time machine, I have much better purposes for it.

1105
00:59:33,120 --> 00:59:33,480
Okay.

1106
00:59:33,920 --> 00:59:34,440
All right.

1107
00:59:34,480 --> 00:59:34,920
All right.

1108
00:59:34,960 --> 00:59:36,360
Biff from back to the future.

1109
00:59:36,680 --> 00:59:41,440
We are done with episode two, 18 of the AB testing podcast, the AB testing podcast.

1110
00:59:41,440 --> 00:59:42,960
I am Alan Abb.

1111
00:59:43,320 --> 00:59:44,520
I am Brit Abb.

1112
00:59:44,920 --> 00:59:46,080
See you later.

