1
00:00:00,000 --> 00:00:08,560
I'll do a bit. I'll bet that both of you are in a QA titled role in 24 months.

2
00:00:12,260 --> 00:00:19,060
Welcome to AB testing podcast, your modern testing podcast. Your hosts, Alan and Brent

3
00:00:19,060 --> 00:00:24,580
will be here to guide you through topics on testing, leadership, agile, and anything else

4
00:00:24,580 --> 00:00:30,420
that comes to mind. Now on with the show. Guess what? It's me again. It's Alan and we're here for

5
00:00:30,420 --> 00:00:37,360
episode 193 of the AB testing podcast. We have Brent and we have the return of Jason Arvin.

6
00:00:38,420 --> 00:00:46,300
Hey, the return of Jason Arvin. Because last time, last time, and I'll quote myself,

7
00:00:46,300 --> 00:00:51,740
Zen Caster shit the bed. And that was the last time then Castro was ever used on this podcast.

8
00:00:52,780 --> 00:01:00,380
So far been using Riverside FM so far. Very, very good. Very, very happy. Please don't let me down.

9
00:01:00,380 --> 00:01:04,580
That's where we are. Zen Caster saved me though, because I said some things I'm glad they weren't.

10
00:01:07,220 --> 00:01:13,140
You know what, Jason, you are clever enough. You could go in, like immediately go hack

11
00:01:13,140 --> 00:01:16,500
the Zen Caster server and just remove all that stuff.

12
00:01:18,500 --> 00:01:20,980
Maybe maybe I act, maybe I accent castor.

13
00:01:23,380 --> 00:01:29,540
Well, it's, it's good to have you here. We're going to do, I think I put it in our prediction

14
00:01:29,540 --> 00:01:35,380
show. We're going to do more guests here because one, the guests are more interesting than us. And

15
00:01:35,380 --> 00:01:45,170
two, more importantly, I am so tired of Brent. Totally. It's, it's, I mean, it's good to have

16
00:01:45,170 --> 00:01:50,130
guests because they bring in a fresh perspective until we figure out the next thing that's

17
00:01:50,130 --> 00:01:55,010
important to us to talk about. No, I always have important things to talk about. It's in fact,

18
00:01:55,570 --> 00:02:00,370
do you? I am practically incapable of talking about things that aren't important.

19
00:02:01,090 --> 00:02:07,410
Okay. I'm happy to be filler. I can't think of the last time I said something that wasn't important.

20
00:02:08,290 --> 00:02:14,450
Sure. Yeah. So, so Jason, your team's listening to this one. Is that what's going on?

21
00:02:14,450 --> 00:02:19,490
You know what? It's interesting. When I was at previous company, a lot of people listened to my

22
00:02:19,490 --> 00:02:24,690
podcast or read my blog. I think, I don't know if anybody listens to my podcast at the

23
00:02:24,690 --> 00:02:28,690
new place. They do read because I post that on LinkedIn all the time connected there. And

24
00:02:29,650 --> 00:02:36,290
they don't know how much I'm drawing from my daily life to inspire some of the topics.

25
00:02:36,290 --> 00:02:41,250
But we'll get more into that later. This is not about me. This is about Jason. And Jason is very

26
00:02:41,250 --> 00:02:49,250
interested in fine art and the variance of meatballs in the Italian market. So we're going

27
00:02:49,330 --> 00:02:55,970
to go into all of those subjects deeply. Jason, just as a reminder, kind of tell people what you're

28
00:02:55,970 --> 00:03:00,130
up to. And then I'll ask on my list of questions that will get us kind of diving into this thing.

29
00:03:00,130 --> 00:03:07,710
What I'm up to now, I'm actually taken a year off and focused on, focused on applying AI

30
00:03:07,710 --> 00:03:13,550
to software testing and like exclusively and voraciously. And it's been wonderful because

31
00:03:14,190 --> 00:03:17,870
the AI robots do what you tell them to do mostly. Yeah. And trying to figure out how to get

32
00:03:18,430 --> 00:03:23,390
experimenting and trying to see what works and what doesn't and as quickly as possible.

33
00:03:23,390 --> 00:03:27,230
And I think I've narrowed down a few kind of themes, but I've been experimenting for

34
00:03:27,790 --> 00:03:32,270
AI for human manual testers, exploratory testers to figure out how I could help them.

35
00:03:32,990 --> 00:03:37,630
Also kind of a full automation, like everyone's dream, which is how do you get an AI robot to

36
00:03:37,630 --> 00:03:43,470
just, my testing bot to just test the application and APIs without a human involved. So focusing

37
00:03:43,470 --> 00:03:47,790
on kind of researching them. Super cool stuff. And I have lots of questions to go in there

38
00:03:47,870 --> 00:03:52,510
but I had a conversation. Was it yesterday? I was talking to one of my employees. I was in Los

39
00:03:52,510 --> 00:03:56,990
Angeles and we're talking about AI. It's always a weird question to bring up with somebody like,

40
00:03:56,990 --> 00:04:00,590
what do you think of AI? Because you never know what people are going to say, right?

41
00:04:00,590 --> 00:04:04,510
He didn't know me well enough to know. And I said the usual stuff you've heard me say,

42
00:04:04,510 --> 00:04:09,790
but something I've maybe said on the podcast is referring to something Stephen Johnson calls

43
00:04:09,790 --> 00:04:16,780
the adjacent possible. Meaning all of a sudden we've made an innovation that enables us to

44
00:04:16,780 --> 00:04:22,860
build a whole new world of innovation. And I think generative AI is that. And I don't think

45
00:04:22,860 --> 00:04:30,540
many, if any folks, despite all the cool things people have done, have really made that leap that

46
00:04:30,540 --> 00:04:35,100
I know is inevitably going to happen. So I'm glad to hear, you know, I think you have a chance to be

47
00:04:35,100 --> 00:04:40,140
like, like there's people like this trying to figure out what, what is this adjacent

48
00:04:40,140 --> 00:04:45,180
possible innovation we can make. So I want to talk more about that, but I also want to talk

49
00:04:45,180 --> 00:04:53,740
more about testers and how AI replaces, enhances, helps, fixes testers. You had an interesting

50
00:04:53,740 --> 00:05:01,340
LinkedIn post. Was that today or yesterday? I just saw it today on Jason's looking at me like,

51
00:05:01,340 --> 00:05:03,020
I didn't post something like testing.

52
00:05:03,020 --> 00:05:06,140
I had about the 737 MAX. I don't think that's what you're talking about.

53
00:05:06,140 --> 00:05:09,500
No, no, no. Which one was, hold on, hold on, hold on.

54
00:05:10,290 --> 00:05:16,130
Oh, was it the quote that my prediction that people have left the testing field will come back to it?

55
00:05:16,770 --> 00:05:21,570
Yes. Yes. I want you to talk more about that. That's interesting.

56
00:05:21,570 --> 00:05:22,530
That was just click bait.

57
00:05:22,530 --> 00:05:25,090
I don't believe you yet, but convince me.

58
00:05:26,620 --> 00:05:27,100
It worked.

59
00:05:30,780 --> 00:05:32,720
Next question, Alan.

60
00:05:32,720 --> 00:05:32,960
Next.

61
00:05:32,960 --> 00:05:41,120
No, I just figured like testers were getting tired of me and they're scared of AI. So I just appealed

62
00:05:41,120 --> 00:05:47,710
to them. Yeah.

63
00:05:47,710 --> 00:05:49,470
Yeah. That's the comments.

64
00:05:50,910 --> 00:05:55,310
This is what I get for skimming articles. I actually think they're not sarcastic.

65
00:05:55,310 --> 00:05:59,390
Actually very genuine. I think it's for probably kind of not the reasons that people would expect.

66
00:06:00,190 --> 00:06:04,030
I didn't even really outline those. One is, first of all, we were saying a second ago,

67
00:06:04,030 --> 00:06:10,190
I think is very right. I think people are like the adjacent stuff, but I think people are not

68
00:06:10,190 --> 00:06:16,190
thinking AI first still. They're thinking about how to modify existing processes and software or

69
00:06:16,190 --> 00:06:21,470
to create, like use AI to generate Java code instead of actually just having AI run.

70
00:06:22,590 --> 00:06:27,870
So I think there's people still haven't kind of gone through that bias yet,

71
00:06:27,870 --> 00:06:33,070
which is normal when there's technology transitions. But so when it comes to what I

72
00:06:33,070 --> 00:06:37,790
said was I think a predict that there'll be that a lot of testers have left the field

73
00:06:37,790 --> 00:06:41,790
as they get further and longer careers. There's a lot of you guys have talked about this before,

74
00:06:41,790 --> 00:06:45,870
too. There's also like a salary cap and testing to be frank. And so people left it

75
00:06:45,870 --> 00:06:51,390
just for more money, let alone or like we were talking before that we started this, Alan,

76
00:06:51,390 --> 00:06:57,230
sometimes people just get sick of testing because it can become mundane or boring or

77
00:06:57,310 --> 00:07:02,190
exhausting in its own way. I think one is that there'll be one is there's just the general

78
00:07:02,190 --> 00:07:07,630
pressures are coming on. AI is genuinely helping in the software engineering world.

79
00:07:07,630 --> 00:07:11,630
People can and Brent's been talking about this too, like where you can just write better code,

80
00:07:11,630 --> 00:07:16,110
not just better code, but more code. So it'll be more stuff to be tested. But I also think

81
00:07:16,110 --> 00:07:23,150
that it's a little story where I'm a story. Brent's going out great. But when I walked into

82
00:07:23,310 --> 00:07:28,430
when I went from Microsoft to Google, I skipped my first week of nuclear training because I thought I

83
00:07:28,430 --> 00:07:32,430
was too cool for that. And so I went, but I went to the search building and I just walked around

84
00:07:32,430 --> 00:07:37,790
with my new badge because I wanted to see the Holy Land and everyone's title there and this

85
00:07:37,790 --> 00:07:42,750
one building that worked on search, everyone's title was a search quality engineer. And so I

86
00:07:42,750 --> 00:07:46,910
think, and the reason is, is that they weren't building traditional software. They're building AI

87
00:07:46,910 --> 00:07:52,830
and very high scale algorithmic type software. And so really what it is, it's very easy to write

88
00:07:52,830 --> 00:07:57,630
some code and then you let 10 or a hundred thousand machines or a million machines process it.

89
00:07:57,630 --> 00:08:01,470
The hard problem is testing it. How do you know that it's of high quality? Especially when you

90
00:08:01,470 --> 00:08:07,390
think about a search engine, and we'll come back around is that the hard problem with search box

91
00:08:07,390 --> 00:08:10,990
is people can type anything they want into it. And it sounds familiar. It sounds a lot like

92
00:08:10,990 --> 00:08:17,310
chat GPT. Google and the search story though is still constrained because it only returns things

93
00:08:17,310 --> 00:08:22,430
that are on the internet that exist already. The problem with chat GPT is it can return anything,

94
00:08:22,510 --> 00:08:29,070
things that either don't exist or things that are made up. And so quality is the hardest

95
00:08:29,070 --> 00:08:35,230
problem in software engineering, particularly in AI. So I think that it's not just that the people

96
00:08:35,230 --> 00:08:42,110
will come back to testing to do traditional testing. I think it's that a lot of those engineers

97
00:08:42,110 --> 00:08:48,270
that left testing to PM or dev will be far more valued because they have that testing context with

98
00:08:48,270 --> 00:08:54,110
that engineering or PM background. And they can be monsters and incredibly important when software

99
00:08:54,110 --> 00:09:00,270
goes through this transition where it's AI first, because you need a search quality like testing

100
00:09:00,270 --> 00:09:07,550
approach for that software. I see where you're coming from. I spent time in Bing.

101
00:09:08,270 --> 00:09:14,910
Right. And I need to enter up with the editor. Bing is like Google, you can use it to Google

102
00:09:14,910 --> 00:09:22,720
things. It's from Microsoft. Anyway, go on, Brent. I'm just currently pausing and wondering

103
00:09:23,360 --> 00:09:32,800
what episode is anyone going to view that horse that is already dead, no longer worth, no longer

104
00:09:32,800 --> 00:09:39,440
worth kicking. When Bing is known as a search engine, that's when I'll do it.

105
00:09:40,160 --> 00:09:48,640
Okay, hopefully you're paying attention to news because anyway, moving on, not doing tangent. So

106
00:09:48,640 --> 00:09:55,600
one of the things that that I learned when going to Bing, it was interesting because this

107
00:09:55,600 --> 00:10:06,530
was the same thing. Buds did not matter in Bing, because they were not helpful. Because we have a

108
00:10:07,410 --> 00:10:13,970
machine learning algorithm that's constantly learning the best way given the context, how to present

109
00:10:13,970 --> 00:10:21,330
answers. And not only that, but we have thousands of streams underneath it that are changing sort of

110
00:10:21,330 --> 00:10:26,850
the input to that same model. So that's one of the reasons why you might go to Bing and type in

111
00:10:26,850 --> 00:10:31,810
a search query and then hit refresh and get an entirely different experience. And hit refresh

112
00:10:31,890 --> 00:10:37,410
again and get an entirely different experience. What ended up being valuable there? Testers that

113
00:10:37,410 --> 00:10:43,570
went over and thought, oh, I've just my job is to find bugs. They got frustrated because no one,

114
00:10:43,570 --> 00:10:52,370
they all got one fixed. What was valuable is help us diagnose and find the pattern.

115
00:10:53,120 --> 00:10:59,120
So to Jason's point of view, I don't know that I agree with specifically testing,

116
00:10:59,840 --> 00:11:05,120
but that one important skill that I think underlines testing, which is the ability

117
00:11:05,120 --> 00:11:15,150
to structurally diagnose, that will be critical. Pausing for Jason to say I'm full of shit.

118
00:11:15,150 --> 00:11:16,510
Alan will always do that.

119
00:11:19,120 --> 00:11:25,200
I worked at Bing too, like on the search engine. I don't know if you knew that before I went to,

120
00:11:25,200 --> 00:11:30,000
I was there during the MSN search and the transition to Bing, which nobody

121
00:11:30,480 --> 00:11:33,760
wanted the logo logo where for some reason.

122
00:11:34,480 --> 00:11:35,280
That was before me.

123
00:11:36,000 --> 00:11:40,240
Yeah. So I totally get that. I hit that personally went through that transition

124
00:11:40,240 --> 00:11:46,720
from a functional kind of testing perspective to dealing with, yeah, bugs were uninteresting.

125
00:11:48,800 --> 00:11:52,960
There was a page that we shared internally, I think it was still around, where people could

126
00:11:52,960 --> 00:11:57,040
file bugs and you could actually do a search inside of Microsoft to do a search and it would

127
00:11:57,040 --> 00:12:00,240
do the search on Bing and Google. And then you could say A or B, which was better. And then you

128
00:12:00,240 --> 00:12:05,980
could type in why I don't know if I remember that. I remember that.

129
00:12:05,980 --> 00:12:08,380
You know, that was whole.

130
00:12:08,380 --> 00:12:13,180
That was still there. That was still there when I left in at least a couple of years

131
00:12:13,180 --> 00:12:16,300
afterwards. I haven't, I haven't tried the URL in forever.

132
00:12:16,300 --> 00:12:21,500
Yeah. So guess why it was built. It was because all these functional engineers at Microsoft wanted

133
00:12:21,500 --> 00:12:26,780
to file a bug. Here's the search. Here's the link. Fix the code. Right. It was just a honey

134
00:12:26,780 --> 00:12:30,940
pot so that people would type those things in and we never looked at the data because it

135
00:12:30,940 --> 00:12:37,420
was not useful. Like, like to what Brent saying, it was actually just a, a DDoS against people

136
00:12:37,420 --> 00:12:43,600
filing bugs. But what Brent said made me think of something. I'm going to add more fuel to the

137
00:12:43,600 --> 00:12:48,640
fire here before I forget because I'm old and I forget things is when you talk about how AI is

138
00:12:48,640 --> 00:12:55,040
going to help testers and help testing. And then Brent's comment around, I forget what he said,

139
00:12:55,040 --> 00:13:01,970
but there's a wide definition of what testing is. And there's a weird thing that Brent and I

140
00:13:01,970 --> 00:13:09,810
have talked about here where Brent and I care about quality and the testing we do is in the

141
00:13:09,810 --> 00:13:19,760
search of quality versus there's a school of folks who are focused on the craft of testing as a,

142
00:13:19,760 --> 00:13:28,080
as an end result almost. And do you think AI helps which school is AI can be more advantageous

143
00:13:28,080 --> 00:13:33,840
for the people who just want to just do better testing and for the sake of testing or the people

144
00:13:33,840 --> 00:13:38,800
who want to help use testing to find higher product quality? That's a leading question. You know,

145
00:13:38,800 --> 00:13:45,920
the answer is the people that search for quality in the day, but not to the world. And why is

146
00:13:45,920 --> 00:13:52,080
that? Because the, Michael Brent was saying, like you can't actually just go in and fix,

147
00:13:52,080 --> 00:13:57,760
rewire the search engine, right? And then it'll be good tomorrow. It's dynamic, it's data driven.

148
00:13:58,480 --> 00:14:02,800
And we can't possibly understand the nets, right? They're doing all that ranking.

149
00:14:03,360 --> 00:14:09,040
So the end purpose has to be quality and particularly how do you quantify quality? Like

150
00:14:09,040 --> 00:14:14,720
you might remember Brent, like there's NDCG, like normalize, just kind of communicate. But

151
00:14:14,720 --> 00:14:19,520
it's a way to measure, you actually have to quantify quality so that you can feedback the

152
00:14:19,520 --> 00:14:25,840
AI training systems. This is a better or worse building than yesterday's, right? And so it's,

153
00:14:25,840 --> 00:14:31,200
you actually instead of saying I have got, like the old world is I've written a hundred test cases,

154
00:14:31,200 --> 00:14:37,680
right? 94% of them happen to have passed. And these are the ones that I've happened to have

155
00:14:37,680 --> 00:14:42,560
written today. But that quantification doesn't work in AI. You have to actually measure the

156
00:14:42,560 --> 00:14:48,400
actual statistically significant quantification of quality. You have to measure it, actually get

157
00:14:48,400 --> 00:14:53,760
down to like a floating point, right? So yeah, that shift in software, how we build software

158
00:14:53,760 --> 00:15:00,240
from traditional like Java, C-sharp kind of functional coding to AI means that you focus

159
00:15:00,240 --> 00:15:04,160
on the end game, which is quality and to figure out how to quantify it. Then you have to figure

160
00:15:04,160 --> 00:15:09,120
out how to measure it. And you have to figure out how to feed that back into the system.

161
00:15:09,200 --> 00:15:14,800
So it's very much ends versus the means where traditional software is focused on the means

162
00:15:14,800 --> 00:15:18,000
to an end. I totally agree with what you're saying.

163
00:15:18,720 --> 00:15:23,920
So I want to share a little story here is there's nothing to do with AI, maybe AI helps here. So

164
00:15:24,640 --> 00:15:28,800
I did the thing that you do from time to time, Jason, you don't do because you have

165
00:15:28,800 --> 00:15:35,120
better things to do in life, where I responded to a threat on where I responded to a threat

166
00:15:35,120 --> 00:15:39,040
on LinkedIn just because I was bored, not because I felt like they were going to listen to me.

167
00:15:39,440 --> 00:15:45,040
And it was a thread around someone made some comment about asking customers to test. I see,

168
00:15:45,040 --> 00:15:48,560
actually, I brought it up because it seemed like three or four different posts where they think that

169
00:15:49,600 --> 00:15:55,600
continuous delivery or, or no dedicated testers means you're asking customers to test. And by that

170
00:15:55,600 --> 00:16:00,560
you're meaning you're asking customers to enter bug reports. I just added a point of clarity

171
00:16:00,560 --> 00:16:07,520
around that because generally, we're not we're getting feedback and data from customers to that

172
00:16:07,520 --> 00:16:12,240
allows us to understand how they're using the product if they're seeing failures, then adjusting

173
00:16:12,240 --> 00:16:17,440
either backing out making changes, we can automate a whole bunch of that. But a pause there because I

174
00:16:17,440 --> 00:16:23,420
think that's an area where I think AI can actually help us out a lot. It can help you

175
00:16:23,420 --> 00:16:28,780
can we got to define quality, I think we use AI to help define that quality criteria. Here's my

176
00:16:28,780 --> 00:16:33,180
application, here's what it does help me understand what are some metrics I can monitor

177
00:16:33,180 --> 00:16:38,780
in production to let us know if this if customers are finding value in this. It can help us zero in

178
00:16:38,780 --> 00:16:43,500
on the right set of engagement metrics, the right set of error metrics to look at to make sure we're

179
00:16:43,500 --> 00:16:49,660
building the right product from customer customer value point of view. That said, I jumped in and

180
00:16:49,660 --> 00:16:55,360
said that stuff and if somebody had the most interesting answer, he said, when we get into

181
00:16:55,360 --> 00:16:59,920
using engagement metrics to evaluate a feature, we're talking about software from a product

182
00:16:59,920 --> 00:17:06,910
standpoint. True. What I what I'm missing there is, and I'm way off in the weeds here before I get

183
00:17:06,910 --> 00:17:12,830
the actual question here. Because the product standpoint, customers don't want well tested

184
00:17:12,830 --> 00:17:18,030
software. They want I gave a talk of this like 20 years ago, I had this like fake box of software

185
00:17:18,030 --> 00:17:23,390
saying 90% code coverage over 97% of tests passed. Those aren't the bullet points you put

186
00:17:23,390 --> 00:17:28,670
on the box. The bullet points on the box are the value you give people. So yeah, from core for

187
00:17:28,670 --> 00:17:33,820
quality perspective, we need to think about things from a product perspective. And then

188
00:17:33,820 --> 00:17:39,260
Brent, you'll love this from principle number one, they said testing is about more than evaluating

189
00:17:39,260 --> 00:17:43,900
the profitability of a feature. It's about finding all the ways the software could be used

190
00:17:43,900 --> 00:17:47,580
that result in the company losing money due to user attrition or reputational damage.

191
00:17:48,220 --> 00:17:52,860
It's the last line of defense before the company does something stupid they could have prevented.

192
00:17:53,740 --> 00:17:57,180
So going back to something Brett and I had said, I don't think this is a test

193
00:17:57,260 --> 00:18:01,180
responsibility functional correctness is a responsibility of the developer.

194
00:18:01,860 --> 00:18:07,300
So what I understand is there's a shift here. And I don't care if the shift happens goes back to

195
00:18:07,300 --> 00:18:11,620
your also your post about the late adopters. There's gonna be people testing last. Hey,

196
00:18:11,620 --> 00:18:17,540
Brent, you get a cat? I've had a cat. She has decided that it is time for

197
00:18:17,540 --> 00:18:23,700
patents. What I want to get an idea because I don't know how I have a good idea how we'd

198
00:18:23,700 --> 00:18:29,220
use AI to help define those those metrics at the very beginning. But how do you think we can use

199
00:18:29,220 --> 00:18:33,380
AI to make sure that we're testing the right things? That's the point of question when I get

200
00:18:33,380 --> 00:18:38,580
to I took 20 minutes to ask this question. We in test I've been doing this long enough time to

201
00:18:38,580 --> 00:18:43,940
know that a lot of the time, and I read between the lines in this reply, it's there to we over

202
00:18:43,940 --> 00:18:48,900
test, we test too much, we test things that customers will actually never do. We spend a

203
00:18:48,980 --> 00:18:54,500
lot of time trying to hold products up because we find some edge case that's actually never going

204
00:18:54,500 --> 00:18:59,060
to be seen. And these are smart testers doing this stuff. They just don't know when to stop.

205
00:19:00,260 --> 00:19:05,460
How can we use AI to optimize like you talked about making manual testers better? How can we

206
00:19:05,460 --> 00:19:12,020
use AI to optimize that test selection? So we're testing just enough to make sure that we're

207
00:19:12,020 --> 00:19:17,700
delivering value to customers? I think it goes back to incentives, like management incentives,

208
00:19:18,420 --> 00:19:24,020
personal goals and incentives like so testers are rewarded for creating test cases,

209
00:19:24,740 --> 00:19:30,580
and having dashboards. So they got to help more. They're not as closely tied in with

210
00:19:31,380 --> 00:19:36,100
the business or product or even user satisfaction or happiness, even though they claim to be

211
00:19:36,100 --> 00:19:42,260
emulating it. There's no real tie to that in money or really rewards or recognition. So I think

212
00:19:42,980 --> 00:19:46,980
fundamentally, if the changes incentives for testers to agree with your premise,

213
00:19:46,980 --> 00:19:50,580
and I think the only way to change it is to change their incentives. But I don't think that's

214
00:19:50,580 --> 00:19:56,100
possible. Adding all that, I think so it goes, what do you say it's back to developers and product

215
00:19:56,100 --> 00:20:04,500
managers that really need to own that ultimately. Because I don't think testers can or will adapt.

216
00:20:05,060 --> 00:20:11,300
And specifically, like you're saying, I think it's all going toward, I forget the Microsoft

217
00:20:11,300 --> 00:20:15,700
E, Microsoft speak term for it, but it's analytics, right? What is it, Brent? What's

218
00:20:15,700 --> 00:20:19,140
the catchphrase right now? What do you use at Microsoft? It's not instrumentation. It's

219
00:20:19,780 --> 00:20:24,900
telemetry. So I think it's all about telemetry. And I think literally, I don't know if you guys

220
00:20:24,900 --> 00:20:28,980
have followed this, but there's open telemetry. So specifically how I can help that there's new

221
00:20:28,980 --> 00:20:34,500
open telemetry protocol. And that means that all these different, just so people are listening,

222
00:20:34,500 --> 00:20:40,260
though, like it's used to be like you purchase Splunk or you purchase some specific vendors'

223
00:20:40,260 --> 00:20:45,620
software and you log to it, and then they render it back to you and do analytics and alerting

224
00:20:45,620 --> 00:20:49,300
and warning and like New Relic and these things. But they've standardized that layer. So I think

225
00:20:49,300 --> 00:20:53,220
what's finally now that there's a standardization in that layer, I think the vendors didn't really

226
00:20:53,220 --> 00:20:57,860
want to do it. They're kind of being dragged into it. But now there's a standardization that layer,

227
00:20:57,860 --> 00:21:04,020
there's a standard scheme or schema for all that telemetry data. And now that means that you can

228
00:21:04,020 --> 00:21:12,420
do not just can you write a single kind of AI analytics thing against everyone's data,

229
00:21:12,420 --> 00:21:18,260
but that you can now compare data. So you can compare one coffee shops websites with another

230
00:21:18,260 --> 00:21:23,140
coffee shops websites data. Because the problem is baselining. So you can have alerts, but like you're

231
00:21:23,140 --> 00:21:26,980
saying, is there a bug? Is it important to the business? Does it cause drama? Is it ever going

232
00:21:26,980 --> 00:21:30,900
to be encountered again? Is it a one-off? And that's where I think AI can do the analytics,

233
00:21:30,900 --> 00:21:37,220
but particularly if it has access to the data from other similar sites and historical data.

234
00:21:37,220 --> 00:21:40,900
So I think open telemetry will help open that up a bit because it will have common tools to be

235
00:21:40,900 --> 00:21:45,780
more incentive to build common tools for analytics. And then you'll be able to share

236
00:21:45,780 --> 00:21:51,140
kind of that summarize data across these different verticals to get a baseline of quality.

237
00:21:51,140 --> 00:21:57,300
Open telemetry does a fantastic job in terms of if nothing else, data democratization.

238
00:21:58,080 --> 00:22:04,400
There is a couple of projects that I am working on within Microsoft today that leverage open

239
00:22:04,400 --> 00:22:11,840
telemetry. And it's just, I can't wait for that to finish landing, because a big part of my

240
00:22:11,840 --> 00:22:19,440
effort is often just data cleaning or conforming to schema. And I'm like, Oh, it will not at all

241
00:22:19,440 --> 00:22:23,120
make me upset if that part of my team's job is just eliminated.

242
00:22:23,120 --> 00:22:26,240
Right. And even the smart stuff, right and more value. Yep.

243
00:22:26,240 --> 00:22:26,480
Right.

244
00:22:27,460 --> 00:22:30,740
I want to double down on my comment from a few minutes ago listening to your talk Jason and

245
00:22:30,820 --> 00:22:36,740
YouTube rant is that I think at Microsoft we had written so much telemetry and there was so much

246
00:22:36,740 --> 00:22:42,900
internal knowledge that just writing good telemetry or reasonably good telemetry was common. I'm sure

247
00:22:42,900 --> 00:22:47,860
it was the same at Google. What I've discovered since then it's difficult for companies to learn

248
00:22:47,860 --> 00:22:52,260
like there's no guide. I guess there are guidebooks. They don't read them. They're not very

249
00:22:52,260 --> 00:22:56,900
good at figuring out what to measure and how to look at it. I think that's a great place for it.

250
00:22:56,900 --> 00:23:05,840
I think just that definition if it's hard, but it's teachable, I think AI can help bridge that gap.

251
00:23:05,840 --> 00:23:09,040
I totally agree. And this one, but I think it feels in the category still of like,

252
00:23:09,040 --> 00:23:13,200
we hand wave and AI goes here and then something magic comes out. But I think

253
00:23:14,080 --> 00:23:18,320
that's the feeling on a lot of AI tools. It's just magic. It's hard. Let's use AI.

254
00:23:18,320 --> 00:23:18,880
It'll get done.

255
00:23:18,880 --> 00:23:22,640
It just be magically exactly. And Gartner will write some charts about it. Actually,

256
00:23:23,280 --> 00:23:28,720
add double down real quick. There's real data. I did some biz dev work with one of those analytics

257
00:23:28,720 --> 00:23:33,600
companies. I guess you should be anonymous. But I went there and I talked to them about their

258
00:23:33,600 --> 00:23:39,680
dashboards for their telemetry analytics. And they said, and they make their worth billions of

259
00:23:39,680 --> 00:23:44,960
dollars, but they said, no one looks at their dashboards. And so they work because it's not

260
00:23:44,960 --> 00:23:49,680
actionable and they don't know if it's good or bad. So you have a chart wiggling around

261
00:23:50,480 --> 00:23:53,920
over time. But you don't know what the right thresholds are, like you're saying,

262
00:23:53,920 --> 00:23:59,440
Alan. No one knows how to set that. No one knows what the high water marks, low water marks should

263
00:23:59,440 --> 00:24:04,400
be. And if it's good enough. But I think that that's kind of what back to Brent saying,

264
00:24:04,400 --> 00:24:09,520
if everyone has this common scheme for representing that data, then you can start benchmarking,

265
00:24:09,520 --> 00:24:14,640
not just against historical timeline series, time series of your own data, but against other

266
00:24:15,280 --> 00:24:18,960
vendors and other verticals that can be anonymized based on just general purpose

267
00:24:18,960 --> 00:24:24,880
metrics to see, are you the worst coffee coffee shop website on the planet in terms of

268
00:24:25,600 --> 00:24:29,760
latency, for example, right? Then you know you should make it better. But I think it's a

269
00:24:30,960 --> 00:24:36,160
long way to say, I think it also will enable a relative evaluation of whether it's good or bad.

270
00:24:36,160 --> 00:24:40,720
So you just throw that data in. And it looks at all and says, ah, you're really sucking in this

271
00:24:40,720 --> 00:24:43,680
area, but you're really doing good in this. Congratulations. Like you don't have a lot of

272
00:24:43,680 --> 00:24:48,320
crashes in production in the JavaScript, but you have a lot of latency compared to everyone's

273
00:24:48,320 --> 00:24:52,960
slow loading pitch. So then people can know what to do with those. Those charts are finally

274
00:24:52,960 --> 00:24:57,120
actionable. I think that might come with open telemetry with a combination of some

275
00:24:57,120 --> 00:25:04,000
some analytics and the IML. It will. And one of the things like years ago, Alan and I

276
00:25:04,640 --> 00:25:11,760
had a long series of discussions around dashboards and KPIs. And it was right when we were kind of

277
00:25:11,760 --> 00:25:18,080
in our honeymoon phase of our love over Eric Reese. And I'll just say like, if you're

278
00:25:18,080 --> 00:25:22,400
listening to this podcast and you don't know the difference between actionable and vanity metrics,

279
00:25:22,960 --> 00:25:29,680
pick up Eric Reese's book. Understand that because that's stage one before you even begin to kind

280
00:25:29,680 --> 00:25:34,080
of go down the path that Jason saw. Also, if you read the book, you'll see where Brett and I

281
00:25:34,080 --> 00:25:39,680
stole most of our ideas from. A good portion. I mean, I stole from the Pop and Dykes. I stole from

282
00:25:40,560 --> 00:25:52,960
Leffingwell. And I have shared on the screen, right, to the point of I'm using Jason's

283
00:25:54,160 --> 00:25:57,520
listeners will need to imagine there's a screen in front of them.

284
00:25:57,520 --> 00:26:07,440
Yeah, so I went to Jason's expert tester spin off of chat GPT. I don't actually know what that

285
00:26:07,520 --> 00:26:11,360
should actually be called, but we've talked about this before.

286
00:26:11,360 --> 00:26:13,200
And it's super cool.

287
00:26:13,200 --> 00:26:21,120
Yeah. And I'm like, okay, if so not only did I ask it to sort of generate a KPI to evaluate the

288
00:26:21,120 --> 00:26:27,280
output of LOM, I didn't even tell it the context. And I said, do it as if you are Alan Page.

289
00:26:27,920 --> 00:26:32,160
And I just scan through it. I'm not going to board our audience on this, but I scan through it. And

290
00:26:32,240 --> 00:26:38,160
I'm like, all right, some of them, I think you're reasonable. Like it gave a long list that I do

291
00:26:38,160 --> 00:26:45,120
think Alan, like the categories, I think are definitely Alan. All right. Even includes

292
00:26:45,120 --> 00:26:51,520
diversity and inclusivity. Now, whether or not the actual measures, right, it, I think it did a

293
00:26:51,520 --> 00:26:59,200
reasonable job of not only saying what to measure, but the implementation. Now, I only scan through

294
00:26:59,200 --> 00:27:05,680
it. And this is a problem I face on a daily basis. So I'll go back and read there and go,

295
00:27:05,680 --> 00:27:12,000
okay, well, well, Alan LLM actually helped me unlock a solution to a problem I'm chasing.

296
00:27:12,670 --> 00:27:19,950
But yeah, I think that direction is, is, is kind of heading that way. The thing is,

297
00:27:21,780 --> 00:27:27,700
I'm trying to pair this to what we talked about at the very beginning, Jason's hypothesis that

298
00:27:27,700 --> 00:27:36,910
these experts will come back. And Jason, does that not potentially contradict? Or how do you see this

299
00:27:36,910 --> 00:27:44,560
happening at the same time of sort of the AI first? Because the way I see those people coming back,

300
00:27:44,560 --> 00:27:51,040
in my mind, it would be what they call the centaurs. It's basically those people working

301
00:27:51,040 --> 00:27:58,240
alongside LLM are going to nail this problem for us. Whereas you're saying that AI first.

302
00:27:59,120 --> 00:28:02,960
So what is the mechanics of what you're doing right now with that GPT is,

303
00:28:03,920 --> 00:28:10,720
imagine you had the standard open telemetry was more, you know, tactics, but this part is so

304
00:28:10,720 --> 00:28:15,840
imagine you have your JSON blob of your high level open telemetry metrics, right? And you just,

305
00:28:15,840 --> 00:28:19,200
you pass it to this bot or similar bot and the bot can come back and say,

306
00:28:19,360 --> 00:28:23,600
you know, oh, for a coffee shop, you tell it, this is a coffee shop. These are my core metrics.

307
00:28:23,600 --> 00:28:27,360
And it might come back with a reasonable assessment and tell you like what you should

308
00:28:27,360 --> 00:28:32,880
probably be focusing on. That's kind of an early step of in this process. But guess, guess who

309
00:28:32,880 --> 00:28:42,000
would be the best person or AI or, or person to analyze that those, like we did with this GPT

310
00:28:42,960 --> 00:28:51,380
with an emulated Allen, Allen, like, like, this is a very loose approximation of Allen.

311
00:28:51,380 --> 00:28:56,340
Allen, like if Allen want to spend a few hours, or I'm happy to work with him on it, we could

312
00:28:56,340 --> 00:29:02,820
build an Allen bot that deeply encodes a lot of his thoughts. And I'll make this bet you're

313
00:29:02,820 --> 00:29:09,700
ready for this wager. I asked you guys, asked you what kind of mood you wanted me to be in.

314
00:29:09,700 --> 00:29:18,140
But you had your chance. I will, I will make a bet that not not all my cash, but

315
00:29:20,780 --> 00:29:26,620
I'll do $1,000, whatever, if you want. But anyway, I'll do a bit. I'll bet that both of you

316
00:29:27,180 --> 00:29:36,700
are in a QA titled role in 24 months. Because of this broad because, because guess who's best

317
00:29:36,700 --> 00:29:42,620
at testing that system. It's not the engineer working on Brent's all worked up now, because it's

318
00:29:42,620 --> 00:29:47,740
a, it's a, it's a ladder thing, but it, but you'll be paid. And I also have fault with

319
00:29:47,740 --> 00:29:52,380
you'd be paid more than you are today. You'd be caught tomorrow because, because these systems

320
00:29:52,380 --> 00:29:57,260
are complex, the developers don't know how to test it. But the best thing is the best thing

321
00:29:57,260 --> 00:30:03,260
to analyze that is, is Allen and Allen bot or a Brent and a Brent bot, not the engineers,

322
00:30:03,260 --> 00:30:08,800
not the PMs that are out there today and not definitely not the tester. So just, just before

323
00:30:08,800 --> 00:30:10,880
you, I know you're like your antibodies are already out. Yeah.

324
00:30:12,960 --> 00:30:18,880
Brent's head's about to shoot off of his body. Yeah. Yeah. I've seen this progression over years,

325
00:30:18,880 --> 00:30:22,160
by the way, because, because what is the progression of every Microsoft tester,

326
00:30:22,160 --> 00:30:28,560
like to go into management and then somehow escape, get an escape pod to PM or dev. But

327
00:30:28,560 --> 00:30:35,360
I'm telling you that just building at Google with the smartest PhDs in search and the best

328
00:30:35,360 --> 00:30:40,000
engineers in the planet working on the most profitable, insanely profitable software on the

329
00:30:40,000 --> 00:30:46,160
planet, we're all called software quality engineers. When Microsoft acquired Softimage

330
00:30:46,720 --> 00:30:50,000
and then ignored it for like 10 years and sold it. Anyway, when they acquired it,

331
00:30:50,000 --> 00:30:54,880
one of the things I learned about them, this is 25 years ago, that was their leveling system was

332
00:30:55,600 --> 00:31:04,420
dev, senior dev, principal dev, fellow QA, because they were the systems thinkers at the top who

333
00:31:04,420 --> 00:31:09,140
knew so much about the system. They were in charge of quality. Now it's a different world

334
00:31:09,140 --> 00:31:16,210
because I almost, I don't know if QA would be in the title, but I could see a path where it's in

335
00:31:16,210 --> 00:31:28,000
the role because the for whatever QA test, S debt, quality, whatever, has been so bastardized and beat

336
00:31:28,000 --> 00:31:33,360
down across the industry. It's not even the right title for that. I, I fully believe there will be

337
00:31:33,360 --> 00:31:40,080
people doing exactly as you describe and Brenton and or I could very well be part of that. Sort of

338
00:31:40,080 --> 00:31:45,940
is like the AI is my pet and I'm teaching it how to do this thing for everybody. But I don't

339
00:31:45,940 --> 00:31:53,620
think we'll be able to call the role a QA role because of how badly it's been just smeared across

340
00:31:53,620 --> 00:32:01,490
the industry. But I think that's the problem is that that role is emergent. And so if you,

341
00:32:01,490 --> 00:32:08,000
if you look at the long, this long tail, you look, you integrate over like four, five, six, seven

342
00:32:08,000 --> 00:32:14,880
years and you assume things don't aren't static. What's going on is that the AI starting to write

343
00:32:15,360 --> 00:32:20,080
the AI, right? That's starting happening now, whether you want to admit it or not. And so

344
00:32:20,880 --> 00:32:28,640
guess what, guess what isn't automated? The evaluation, the testing of it, right? So what

345
00:32:28,640 --> 00:32:34,000
will happen is there'll be Brenton go like, no, I'm still going to be using SQL Server.

346
00:32:34,160 --> 00:32:43,440
And I don't know, but no, I'm not. I'm going to, I'm going to, I'm going to push back and say,

347
00:32:43,440 --> 00:32:49,040
no, the AI is also evaluating the AI. Like, I don't think that's a whole.

348
00:32:51,680 --> 00:32:54,080
How does, how does GPT for evaluate or test it today?

349
00:32:55,020 --> 00:33:02,780
Oh, by a small army of metrics by, by human beings. But not the, but so you know,

350
00:33:02,780 --> 00:33:09,900
the language around an AGI, you know, a language around an AGI. And I believe AGI's already exist,

351
00:33:09,900 --> 00:33:14,220
that this, this stuff is already happening. What happened to you since we talked last,

352
00:33:14,220 --> 00:33:18,540
Brent? What happened to me? You were telling me that stuff will never be thinking or sent

353
00:33:18,540 --> 00:33:22,700
the intent. It's a stochastic word. Oh, I'm still on the page that yeah, we need to do

354
00:33:22,700 --> 00:33:27,340
whatever the hell we can do to prevent this from happening. That part's not changed. But

355
00:33:28,300 --> 00:33:35,180
just because I'm fighting a, what is the old Greek myth, the poor dude that had to push the boulder

356
00:33:35,180 --> 00:33:41,100
up the hill for it, just because I'm that guy, yeah, it doesn't mean my opinion on that's changed.

357
00:33:41,900 --> 00:33:46,940
Right. It's, but so back to the, specifically the title thing, which I think is what the

358
00:33:46,940 --> 00:33:51,260
revulsion comes from in the, the cathartic. Go ahead and Brent, stop playing with your desk.

359
00:33:51,260 --> 00:33:57,100
I walk, I thought you told him that already. We know testing was a dirty word in quality.

360
00:33:57,180 --> 00:34:02,460
It was a dirty word. And this debate's been going back like in 2010, right? But I'm telling you,

361
00:34:02,460 --> 00:34:06,940
I walked into the Google search building and guess what the titles of the engineers were

362
00:34:07,890 --> 00:34:12,850
search quality engineers. And we're going to stop there for now. We'll pick it up again

363
00:34:12,850 --> 00:34:19,330
next week with episode one 94. This has been episode one 93 of the AB testing podcast.

