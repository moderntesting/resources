1
00:00:00,110 --> 00:00:03,160
Thank you. It is a little my daughter

2
00:00:06,470 --> 00:00:10,510
Welcome to a be testing podcast your modern testing podcast

3
00:00:10,710 --> 00:00:16,270
Your hosts Alan and Brent will be here to guide you through topics on testing

4
00:00:16,750 --> 00:00:21,410
Leadership agile and anything else that comes to mind now on with the show

5
00:00:23,030 --> 00:00:24,670
Could we be?

6
00:00:24,670 --> 00:00:26,860
any more

7
00:00:26,860 --> 00:00:31,240
Pathetic old men is it possible? Are we the most pathetic old men that exist on the internet?

8
00:00:32,420 --> 00:00:33,780
No

9
00:00:33,780 --> 00:00:36,610
Yeah, there's something we can strive for for sure

10
00:00:36,650 --> 00:00:43,930
So as many of our readers know I gave up on grooming part way into the pandemic and grew my hair super long

11
00:00:43,930 --> 00:00:47,010
And and my beard super long. I have since recently

12
00:00:48,760 --> 00:00:49,920
partially

13
00:00:49,920 --> 00:00:52,600
Groomed to myself. I have a beard looks

14
00:00:54,120 --> 00:00:55,800
Defuzzed

15
00:00:55,800 --> 00:01:00,480
It's shorter than Brent's but one thing I didn't realize still Brent was looking at his cat is that

16
00:01:00,880 --> 00:01:05,920
We both are holding on like I I couldn't cut all my hair off

17
00:01:05,920 --> 00:01:09,480
I can still do the little man bun ponytail thing and Brent has one too

18
00:01:09,480 --> 00:01:14,080
And I realized looking at his how much I need to cut more of my hair off because I realized

19
00:01:14,920 --> 00:01:18,990
How bad I must look no you look great actually

20
00:01:20,040 --> 00:01:22,320
like thank you, but you don't have

21
00:01:23,200 --> 00:01:27,400
You don't have the little hair island up here that looks stupid

22
00:01:27,600 --> 00:01:31,160
More out so that the whole damn thing

23
00:01:31,880 --> 00:01:33,880
Like now I have a little

24
00:01:35,240 --> 00:01:39,440
You're not the target demographic. I'm trying to make my hair look good for it before just you know

25
00:01:39,440 --> 00:01:42,400
Oh, just so you know sure you know

26
00:01:43,360 --> 00:01:50,440
Just just know now you got the whole sexy salt and pepper look going on. That's that's that's that's what my t-shirt says

27
00:01:53,650 --> 00:01:56,990
We are back for episode this is 212 I believe that's on right to you

28
00:01:57,390 --> 00:02:02,820
Sure, right 212 212 is an even number not a prime number to even number

29
00:02:03,780 --> 00:02:10,900
Also the area code of Manhattan in New York City. That's a fun fact. I didn't also also a palindrome to undo

30
00:02:11,380 --> 00:02:16,060
Many good things. I can't think anything else about about 212, but that's that's up to five

31
00:02:16,060 --> 00:02:18,740
All right, and that's the end of our podcast today. Thanks everyone for listening

32
00:02:19,580 --> 00:02:23,320
How's it going? What's new tell me something exciting about what's happening in your world?

33
00:02:24,930 --> 00:02:30,670
Uh, well, it's been a while like we haven't talked the election at all. Oh

34
00:02:31,850 --> 00:02:34,090
Wait, there was an election I missed out. I

35
00:02:36,950 --> 00:02:39,230
Understand why you might be repressing it

36
00:02:41,180 --> 00:02:45,060
I mean, I'm embarrassed but anyway go on

37
00:02:49,460 --> 00:02:51,300
I'm embarrassed

38
00:02:51,300 --> 00:02:52,700
you know

39
00:02:52,700 --> 00:02:55,220
49% of the people who voted voted for a

40
00:02:55,820 --> 00:02:58,980
Convicted rapist and felon and all-around fraud

41
00:02:59,860 --> 00:03:03,700
Not good human being that's who we are as a country. I'm embarrassed

42
00:03:07,180 --> 00:03:12,180
Okay, so I've said on the podcast before I I did not like

43
00:03:14,380 --> 00:03:16,380
Biden or Trump

44
00:03:17,910 --> 00:03:24,190
Kamala the we haven't talked about her at all and that is actually part of the reason why

45
00:03:24,790 --> 00:03:29,710
She lost I don't think Trump won. I think she lost

46
00:03:29,710 --> 00:03:33,670
I think I I call that correct. I am mad at the Democrats

47
00:03:33,750 --> 00:03:41,270
They did not call out the misinformation around the border of the economy, etc. Etc. They thought they could just yeah

48
00:03:41,270 --> 00:03:48,270
They didn't wait again their strategy and we can just simplify it because we're not a political podcast

49
00:03:48,270 --> 00:03:53,870
Not their strategy was a complete and total utter clusterfuck. Yes

50
00:03:54,830 --> 00:03:58,950
Yeah, yeah, and we will deeply pay the price for that

51
00:04:00,090 --> 00:04:04,410
That's where we're at. So anyway other than that moving on from the election. What else has been going on?

52
00:04:06,400 --> 00:04:10,940
We had a huge storm here in Seattle

53
00:04:11,540 --> 00:04:15,900
Area we did we had a bomb cycle and did you have power you're on the east side you have power

54
00:04:16,520 --> 00:04:18,520
Yeah, no, I live in

55
00:04:19,560 --> 00:04:21,560
neck of the woods that

56
00:04:22,240 --> 00:04:25,600
all of our wires are are underground so

57
00:04:26,590 --> 00:04:33,410
We do get power outages, but the tree has to fall directly on the transformer like the probability

58
00:04:33,410 --> 00:04:36,670
So yeah, we've been fine. Yeah ours flickered a few times. We have been solid

59
00:04:36,670 --> 00:04:40,870
We have a couple more storms coming in over the weekend. So good times. Yeah, there's supposed to be something today

60
00:04:42,230 --> 00:04:46,150
Yeah, so anyway, I've been traveling a lot. I was in New York last week

61
00:04:46,150 --> 00:04:50,470
I was home for about a week in between I was in Montana taking care of

62
00:04:51,390 --> 00:04:52,870
Just

63
00:04:52,870 --> 00:04:58,990
Family stuff before that I was in New York for 10 days again before that it's been a lot you have family stuff related to Montana

64
00:04:58,990 --> 00:05:00,990
I didn't know this. Yeah. Yeah

65
00:05:01,810 --> 00:05:03,330
Okay

66
00:05:03,330 --> 00:05:08,290
So I haven't been moving on I haven't been home much. It's good to be home

67
00:05:09,250 --> 00:05:13,960
And I kind of just trying to catch up on shit. Yeah

68
00:05:14,420 --> 00:05:17,930
So anyway, yeah, I never let you answer your question. What else is new?

69
00:05:17,930 --> 00:05:22,610
What's new other than the power outage that you didn't have storm bomb cyclone?

70
00:05:23,480 --> 00:05:28,240
You missed the whole connect season. We talked about remember last time we talked about

71
00:05:28,760 --> 00:05:34,680
The the how performance reviews are done horribly like 99% of the company has talked about how I try and come back

72
00:05:35,640 --> 00:05:39,680
Episode 211 that's one back in your history. You can hear a lot more about that

73
00:05:40,480 --> 00:05:41,840
there is

74
00:05:41,840 --> 00:05:43,840
there is

75
00:05:44,040 --> 00:05:48,780
Something sort of new popping up at Microsoft that I think is an interesting

76
00:05:50,560 --> 00:05:52,980
topic

77
00:05:53,380 --> 00:05:58,460
Maybe I'll throw it out. How about that? Can you tell me? All right. I'm ready the

78
00:05:59,300 --> 00:06:02,500
Nothing could surprise me. No, so it goes back to

79
00:06:03,500 --> 00:06:05,500
to AI again

80
00:06:05,920 --> 00:06:08,840
However, you remember remember way back

81
00:06:09,800 --> 00:06:12,120
MS. Poll I do remember MS. Poll

82
00:06:12,920 --> 00:06:20,760
And the engagement survey and this poll is gone. It's replaced by a new system a new engagement survey. Okay. Yep

83
00:06:21,380 --> 00:06:23,400
and

84
00:06:23,400 --> 00:06:25,560
one of the current questions is

85
00:06:27,740 --> 00:06:29,450
I

86
00:06:29,450 --> 00:06:31,450
know how

87
00:06:31,450 --> 00:06:37,170
It's I'm screwed it up and I know how to use AI and my day-to-day life

88
00:06:37,170 --> 00:06:43,400
Oh interesting. I bet that number is pretty low. It is certainly

89
00:06:44,080 --> 00:06:48,240
Well in my organization, it's not shock

90
00:06:49,640 --> 00:06:51,000
But

91
00:06:51,000 --> 00:06:54,480
Overall, it's it's it's generally viewed as lower than

92
00:06:55,000 --> 00:07:01,400
Than people would like I'm not certain I've not had any conversations with him. Like what's the right number?

93
00:07:02,000 --> 00:07:05,440
But I do think it's interesting that that showed up

94
00:07:06,160 --> 00:07:10,520
Well kudos to the HR team for adding that question

95
00:07:10,920 --> 00:07:13,880
Right. Yeah, why?

96
00:07:14,240 --> 00:07:21,240
Because I agree but why because it's not normally they're focused on engagement and how they look on the Gallup polls blah blah blah

97
00:07:21,600 --> 00:07:25,680
Here's a actual question you can ask you kind of know the answer

98
00:07:25,680 --> 00:07:30,400
But you want to confirm it because you can do something about it. You can actually do something

99
00:07:30,400 --> 00:07:36,380
This is common. I would bet every company that asked this whether you're Microsoft or you know

100
00:07:37,360 --> 00:07:39,360
Walmart

101
00:07:39,860 --> 00:07:42,100
Nobody answers that question as positive

102
00:07:42,100 --> 00:07:47,860
No group of employees answered that question as positively as you'd like because you and I know there are many

103
00:07:48,300 --> 00:07:54,220
advantages to productivity to efficiency to decision-making that can happen if you use AI effectively and

104
00:07:54,700 --> 00:08:00,460
It's just it's just good data to have to point out that we're not taking advantage of what's there and I'm going the whole

105
00:08:00,460 --> 00:08:05,620
Gen AI isn't really AI but whatever it's helpful no matter what and I don't think most people

106
00:08:06,020 --> 00:08:08,020
Know how to make it helpful

107
00:08:09,840 --> 00:08:12,400
Yeah, yeah, no, I think number one

108
00:08:14,200 --> 00:08:19,080
The fact that they are asking that question

109
00:08:20,560 --> 00:08:28,360
Right. What does it do? It asks that question every manager in the company gets a score against that question and

110
00:08:29,880 --> 00:08:34,440
Then they immediately take action like this is this is

111
00:08:34,440 --> 00:08:36,200
just

112
00:08:36,200 --> 00:08:43,000
a fantastic way to motivate a behavior change in a direction and just

113
00:08:43,880 --> 00:08:45,880
like ridiculously

114
00:08:45,880 --> 00:08:51,840
Yes. Yes, but also because there you can correlate that question against things like

115
00:08:52,280 --> 00:08:55,720
I have ample time to learn new topics helping with my job

116
00:08:55,760 --> 00:09:01,840
So I imagine that's low that could be low too or maybe it isn't and it's high but they're spending their time learning other things

117
00:09:01,920 --> 00:09:03,960
There are other questions in engagements

118
00:09:03,960 --> 00:09:08,520
It's like the whole advantage these things is not the way people sorry on soapbox

119
00:09:08,600 --> 00:09:15,920
The way people screw up surveys is they just treat every question individually. You have to do analysis to find

120
00:09:16,440 --> 00:09:23,880
Correlations and find out to helps answer questions on why things are the way they are and once you do that with that question

121
00:09:23,880 --> 00:09:25,880
You're gonna find some interesting insights

122
00:09:27,780 --> 00:09:33,060
Yeah, unfortunately, I mean you can and if you're if you're the team doing

123
00:09:33,380 --> 00:09:39,140
Analysis that has everything in the clear. You probably can do a lot more. Obviously. I only have

124
00:09:39,820 --> 00:09:41,820
my team scores and

125
00:09:42,820 --> 00:09:45,700
only in aggregation and without commentary

126
00:09:46,380 --> 00:09:53,930
So I just look at him go neat vanity number. I have to get for me that sucks

127
00:09:53,930 --> 00:09:58,610
I don't I don't know who runs HR at Microsoft anymore. It's a revolving door there. That's fine

128
00:09:58,610 --> 00:10:01,810
It's probably somebody better given you know, it's probably somebody good

129
00:10:02,210 --> 00:10:06,730
That's not the point. The point is I would be willing to bet you know

130
00:10:06,730 --> 00:10:10,450
I pick on Microsoft a lot, but I'd be willing to bet that a company like Microsoft are

131
00:10:11,090 --> 00:10:17,090
Taking a data analysis approach to the survey stuff and looking for correlations and trying to find not just oh

132
00:10:17,090 --> 00:10:20,330
Let's give people some AI training. They're gonna find some

133
00:10:20,890 --> 00:10:25,410
behavioral things and management and team structure that are going to help influence that I

134
00:10:28,260 --> 00:10:30,220
will

135
00:10:31,220 --> 00:10:33,220
Join you in your optimism. I

136
00:10:34,020 --> 00:10:38,340
Like in my little neck of the woods like I run a data science team. Yeah, that's what we do

137
00:10:39,500 --> 00:10:44,820
But again, the debt is not sufficient. I have no visibility to

138
00:10:46,220 --> 00:10:47,620
to

139
00:10:47,620 --> 00:10:52,000
Matter of fact other teams have come in and said hey, can you data science our scores?

140
00:10:52,660 --> 00:10:55,860
Data science your scores. I kind of believe they'd say that

141
00:10:58,280 --> 00:11:00,640
Some do

142
00:11:00,720 --> 00:11:03,280
Some do that the the ones I like working with do

143
00:11:04,360 --> 00:11:06,320
the

144
00:11:06,320 --> 00:11:10,960
Right gives a figure they go. Okay, I don't I don't have the skill the thing

145
00:11:10,960 --> 00:11:15,840
I wanted to drive with this that I think is potentially interesting because this one question

146
00:11:16,760 --> 00:11:18,540
has

147
00:11:18,540 --> 00:11:20,760
like I as I mentioned has

148
00:11:21,440 --> 00:11:26,000
encouraged a behavior amongst in particular Dev and PM and

149
00:11:27,530 --> 00:11:30,130
and the Dev and PM managers are

150
00:11:30,930 --> 00:11:32,620
basically going

151
00:11:32,620 --> 00:11:34,480
late

152
00:11:34,480 --> 00:11:40,520
WTF what the hell am I supposed to do here? Right and and

153
00:11:43,740 --> 00:11:48,120
Meaning even those managers don't have enough understanding of

154
00:11:48,740 --> 00:11:50,740
of AI and

155
00:11:51,660 --> 00:11:53,980
There's now it just event

156
00:11:56,760 --> 00:12:02,840
Carefully because I love my company and that is honest. That's me just saying it but

157
00:12:03,720 --> 00:12:05,990
There is a

158
00:12:05,990 --> 00:12:10,710
There are two things that one thing that's bad

159
00:12:10,710 --> 00:12:16,030
That's happened here is is there's a lot of discussions I'm having with people. Oh

160
00:12:17,290 --> 00:12:22,600
And I call it AI and magic same thing

161
00:12:22,600 --> 00:12:30,550
And then the another discussion is AI and LLM same thing and as we both know

162
00:12:30,950 --> 00:12:32,950
These are both false

163
00:12:33,930 --> 00:12:40,970
But that's how they are approaching it with me and so I go, okay, how do I engage here? How do I help them?

164
00:12:41,570 --> 00:12:44,010
move forward like p.m.

165
00:12:45,130 --> 00:12:50,690
Hit me this week and I spent a lot of time with death because that's easier for me to understand

166
00:12:50,690 --> 00:12:57,330
I'm like look look at any place in your architecture where you need to automate decision-making where your devs have made up numbers

167
00:12:57,930 --> 00:13:02,990
Like if made up number equals another made up number then do thing

168
00:13:04,130 --> 00:13:07,350
Timeouts as a very simple example

169
00:13:09,720 --> 00:13:15,910
PM is a new one and they're like, well, what am I supposed to do here? I don't like well two things

170
00:13:16,310 --> 00:13:18,430
that were very obvious to me is

171
00:13:20,410 --> 00:13:23,010
Look at the part of the job that's that's

172
00:13:25,440 --> 00:13:26,870
tiresome and

173
00:13:26,870 --> 00:13:34,510
See if if AI can can help you like I find good success in terms of I don't like

174
00:13:34,830 --> 00:13:36,590
using LLM

175
00:13:36,590 --> 00:13:38,750
to write my slide decks

176
00:13:39,430 --> 00:13:42,670
But I do like it in terms of writing

177
00:13:43,470 --> 00:13:45,820
the the outline of the deck

178
00:13:46,260 --> 00:13:53,230
Do you have you used LLM to do death work? I have it doesn't mean it hasn't been great

179
00:13:53,230 --> 00:13:58,230
But it's been helped this bit. It's again. It's a collaboration partner. It gives me some ideas and I go well that sucks

180
00:13:58,230 --> 00:14:11,180
But I can build on that say something. Hello. Oh you sound maybe

181
00:14:11,580 --> 00:14:13,580
Welcome back Brent to the AB testing podcast

182
00:14:14,180 --> 00:14:16,180
Thank you 12 even number

183
00:14:17,020 --> 00:14:21,700
Blah blah blah question on the engagement survey, although it has a different name on on

184
00:14:23,630 --> 00:14:27,710
Using AI appropriately people come to you to talk about whether AI is magic or

185
00:14:28,550 --> 00:14:34,760
Whether LLMs and AI are the same thing. I think pretty common across the industry. I hear those same problems all the time

186
00:14:35,480 --> 00:14:37,480
where I am, so

187
00:14:37,720 --> 00:14:42,000
Do you remember where you were on that journey or should I just take off in a different direction?

188
00:14:42,200 --> 00:14:44,200
You can go wherever you want. No

189
00:14:45,340 --> 00:14:49,100
Fully agree and we've talked about it a lot the fact that people don't know how to use AI effectively

190
00:14:49,140 --> 00:14:52,300
They don't know how to use and it's it's fine. They're going to learn it. I

191
00:14:53,140 --> 00:14:59,180
Think there's extremes where they think it's like we get the folks on LinkedIn saying oh, it can't count ours in strawberry

192
00:14:59,180 --> 00:15:03,300
It's stupid, but you're an idiot because it actually is very powerful to help and I think

193
00:15:03,820 --> 00:15:07,980
Blah blah blah so if I said before I want to talk about you're gonna get

194
00:15:08,060 --> 00:15:10,060
back in the AI stuff, but

195
00:15:11,350 --> 00:15:13,340
one thing

196
00:15:13,340 --> 00:15:19,940
No company as a collective has ever done really well that I've worked out in 30 years is actually

197
00:15:20,540 --> 00:15:25,500
Do I've never seen a company do the right things with those engagement service?

198
00:15:25,860 --> 00:15:30,660
It's almost like they do them. Okay. We have to do an engagement survey. It'll tell us whatever

199
00:15:31,380 --> 00:15:32,580
but

200
00:15:32,580 --> 00:15:38,740
It's actually used right. It's a really powerful tool. I know Microsoft used to do like let's do a morale event

201
00:15:38,740 --> 00:15:42,980
Let's all go to a movie or a bowling the day before we fill the survey out to make people feel a little bit better

202
00:15:43,460 --> 00:15:45,340
Absolutely idiotic and stupid

203
00:15:45,340 --> 00:15:52,220
there is a little bit of a day of a snapshot of the day that comes into fact comes into play with those surveys, but

204
00:15:53,540 --> 00:15:59,540
I think I'm taking this different direction because actually something I've thought about a lot and I went to a

205
00:16:00,060 --> 00:16:04,980
leadership session when I was in New York last week and I was talking to the guy that runs it I think my

206
00:16:05,140 --> 00:16:07,060
retirement job may be to run

207
00:16:07,060 --> 00:16:12,460
Like leadership programs or talent and leadership things like that's I think I'm at some point

208
00:16:12,460 --> 00:16:17,460
I'm gonna get out of software completely because it's killing me and I'm gonna do something to help people become better leaders

209
00:16:17,460 --> 00:16:23,900
So anyway, a lot of folks look at their survey results and they'll either pat themselves on the back or not

210
00:16:23,900 --> 00:16:25,760
They'll look at the low stuff

211
00:16:25,760 --> 00:16:28,920
Often what happens you look at like what sucked you? Are you?

212
00:16:29,520 --> 00:16:33,660
Okay sharing what what like sucked in your your team on your thing

213
00:16:34,080 --> 00:16:36,080
You can

214
00:16:39,010 --> 00:16:45,960
Holy crap, did you know this is going

215
00:16:53,940 --> 00:16:58,340
You know one thing about record we need to cover recording studio so don't have all this bullshit, okay

216
00:17:01,500 --> 00:17:07,900
Dropped your tablet and blah blah blah and slightly crippled. Okay fair enough fair enough

217
00:17:15,240 --> 00:17:18,120
This is gonna show up on my podcast

218
00:17:21,260 --> 00:17:28,500
Alan does things like that. All right, let's see what happens here. Okay. Now. Anyway, we're we were you're told

219
00:17:28,500 --> 00:17:31,620
Oh my god, where the hell were we what sucked on your engagement survey?

220
00:17:32,090 --> 00:17:34,090
Super you for you on mine

221
00:17:35,240 --> 00:17:37,440
One of them that I remember is

222
00:17:39,360 --> 00:17:43,960
Compensation, okay, and what can you Brent personally do about that?

223
00:17:45,430 --> 00:17:49,530
Jack all you can't do anything. No, that's false. Oh, tell me okay

224
00:17:49,530 --> 00:17:52,450
I was I was actually hoping you'd agree with me so I could disagree with you. Tell me more. Oh

225
00:17:53,740 --> 00:17:56,740
Sorry. Yeah, you're right. What am I gonna do? I'm not

226
00:17:59,110 --> 00:18:01,110
Authority on that

227
00:18:01,270 --> 00:18:09,270
No, actually, I'm one of the rare people at Microsoft that believe that if you if you approach it the right way

228
00:18:10,090 --> 00:18:13,690
Every one of these things the frontline manager can take action

229
00:18:14,650 --> 00:18:15,290
Yes, right

230
00:18:15,290 --> 00:18:21,310
So there's and that gets into two big mistakes people make in looking at their engagement survey results one is

231
00:18:21,710 --> 00:18:25,950
They look at things that are negative and say I can't do anything about that employees see they need more comp

232
00:18:26,030 --> 00:18:31,230
They need more money. Sorry the same thing with any more education whatever employees go. Well, it's not my job

233
00:18:31,230 --> 00:18:36,090
I can't do that which is a big chunk of BS. You can affect that go back to Dan pink

234
00:18:36,190 --> 00:18:39,510
maybe it's not money maybe it's autonomy and purpose and mastery or

235
00:18:40,190 --> 00:18:43,470
Maybe there are things you can do about money. You can make a difference there

236
00:18:44,760 --> 00:18:50,760
Other thing is no and actually even further and again, it's Dan pink. But the other thing further

237
00:18:51,560 --> 00:18:54,600
Yeah, I I have my scores up my lowest is

238
00:18:55,680 --> 00:18:58,240
I have a good deal at my company

239
00:18:58,880 --> 00:19:00,880
right, that's the lowest one and

240
00:19:01,840 --> 00:19:03,120
and

241
00:19:03,120 --> 00:19:07,040
There's a reasonable balance between what I contribute and what I get in return

242
00:19:08,060 --> 00:19:10,590
and

243
00:19:10,590 --> 00:19:14,350
That one most managers will interpret it as money

244
00:19:15,080 --> 00:19:19,240
But that's not what it says. It says I have a good deal

245
00:19:19,880 --> 00:19:21,050
Yep

246
00:19:21,050 --> 00:19:23,820
right and

247
00:19:23,820 --> 00:19:27,260
People go after as Dan pink points out

248
00:19:28,640 --> 00:19:30,640
people go after

249
00:19:30,880 --> 00:19:32,990
money

250
00:19:32,990 --> 00:19:34,030
as

251
00:19:34,030 --> 00:19:36,350
the plan not even be but

252
00:19:37,370 --> 00:19:39,050
CDef

253
00:19:39,050 --> 00:19:41,630
Right people are more engaged

254
00:19:42,960 --> 00:19:49,200
By by being in the environment where they're energized by it. They feel like they have a purpose

255
00:19:49,920 --> 00:19:51,920
They have some sort of autonomy

256
00:19:52,240 --> 00:19:53,390
right

257
00:19:53,390 --> 00:19:54,350
those

258
00:19:54,350 --> 00:19:56,350
matter far

259
00:19:56,590 --> 00:19:58,670
more

260
00:19:58,670 --> 00:20:01,310
Than the money once the money is at

261
00:20:02,800 --> 00:20:03,760
a

262
00:20:03,760 --> 00:20:05,760
basic level right

263
00:20:06,270 --> 00:20:14,910
Right, that's for the majority people there's there's always the others who are like comparing themselves to their cousins or you know things like that

264
00:20:15,630 --> 00:20:16,750
right

265
00:20:16,750 --> 00:20:19,070
um

266
00:20:19,070 --> 00:20:23,230
So okay, so you're absolutely right. So the mistakes people make is

267
00:20:23,950 --> 00:20:27,390
They either throw stuff away say I can't do anything about that

268
00:20:27,790 --> 00:20:30,590
Or they only focus on the low thing say a low thing

269
00:20:31,150 --> 00:20:34,350
Say okay, I say give people more money more money and that it'll be great

270
00:20:36,110 --> 00:20:38,830
You also have to focus on like what's your highest thing?

271
00:20:40,160 --> 00:20:44,960
What are you best? What is your what is your team say as far as engagement goes your best at manager respect?

272
00:20:45,680 --> 00:20:47,440
okay, so

273
00:20:47,440 --> 00:20:51,600
Um, what's gonna happen if you focus on getting people more money?

274
00:20:52,240 --> 00:20:54,240
But you sacrifice manager respect

275
00:20:55,960 --> 00:21:01,160
You're gonna play engagement survey whack-a-mole the the the the point is that's a bad example

276
00:21:01,160 --> 00:21:04,280
That would be hard. But but yeah, you're right if you focus

277
00:21:05,000 --> 00:21:07,160
If you don't focus on the whole system

278
00:21:07,860 --> 00:21:12,200
Exactly, and that is the mistake people make they don't focus on the whole system

279
00:21:12,280 --> 00:21:15,960
So again in my in my future job where i'm doing this full time

280
00:21:16,520 --> 00:21:20,520
I would make sure every leader of a certain size team has a coach to help them

281
00:21:21,000 --> 00:21:27,320
Evaluate what it means help them figure out the actual behaviors. They are doing that they should continue and behaviors

282
00:21:27,320 --> 00:21:28,920
They could be doing to help improve things

283
00:21:28,920 --> 00:21:30,360
You're not trying to

284
00:21:30,360 --> 00:21:35,400
Play whack-a-mole with the small things and ignore the things the low things and ignore the things you're doing well at

285
00:21:35,640 --> 00:21:40,200
You have to figure out what you can adjust in the system to make these numbers are a rough reflection

286
00:21:40,600 --> 00:21:43,000
Of how the org feels about coming to work for you

287
00:21:43,560 --> 00:21:46,760
So you have to attack it as a system and you have to look at

288
00:21:47,160 --> 00:21:50,600
It's a reflection everything happening at once so you understand

289
00:21:51,080 --> 00:21:58,600
Uh what you what behaviors need to change? There's no leader out there who's perfect every single one including myself including you

290
00:21:58,840 --> 00:22:00,040
Can do some things better?

291
00:22:00,040 --> 00:22:04,200
We can do some things better that help our teams focus on their work and be more productive

292
00:22:04,600 --> 00:22:07,880
And I think a lot of managers are just freaking lazy and don't bother

293
00:22:08,750 --> 00:22:13,550
That pisses me off lazy managers piss me off lazy leaders piss me off

294
00:22:14,460 --> 00:22:16,860
There's a lot of these assets when I think about

295
00:22:17,840 --> 00:22:19,840
right the new system

296
00:22:20,080 --> 00:22:26,490
like they have

297
00:22:26,490 --> 00:22:28,170
There's a lot

298
00:22:28,170 --> 00:22:29,770
That they can do

299
00:22:29,770 --> 00:22:31,770
to make it more actionable

300
00:22:32,480 --> 00:22:39,520
Like even someone like me like I have spent an hour trying to reverse and successfully reverse engineer how they

301
00:22:40,160 --> 00:22:42,160
How all the data flows?

302
00:22:42,640 --> 00:22:46,240
Because I was trying to figure out how to drive action with this

303
00:22:47,260 --> 00:22:48,810
um

304
00:22:48,810 --> 00:22:56,010
They've dumped it down to a point where i'm like, yeah, I wouldn't be surprised if so many people

305
00:22:56,250 --> 00:22:57,930
Okay, what if?

306
00:22:57,930 --> 00:22:59,930
Can I walk you through a thought exercise?

307
00:23:00,090 --> 00:23:00,970
Yeah

308
00:23:00,970 --> 00:23:01,850
Okay

309
00:23:01,850 --> 00:23:06,650
We have the engagement service those can be different every company you can add questions like the ai question is great

310
00:23:07,290 --> 00:23:12,410
What if there was a separate survey for leaders you can say people managers or or people at a certain level?

311
00:23:12,650 --> 00:23:15,130
I would say people at a certain certain size organization

312
00:23:16,090 --> 00:23:22,410
That asked you maybe even in free form text about behaviors that you employ as a leader

313
00:23:23,050 --> 00:23:29,450
things like I make I focus on making sure every person on my team gets feedback at least once a week or

314
00:23:30,250 --> 00:23:33,370
What form or short answer what forms of feedback do you get?

315
00:23:33,930 --> 00:23:39,850
Um, how do you motivate your team or question? They can be they can be uh bullet points, you know multiple choice or they can be

316
00:23:40,010 --> 00:23:40,890
um

317
00:23:41,050 --> 00:23:48,330
Short answer so we're giving all the leaders of a say more than 20 people all these questions to kind capture their behaviors

318
00:23:49,370 --> 00:23:50,410
now

319
00:23:50,410 --> 00:23:55,130
Can I take the results? I haven't thought about this yet. Can I take the results of that?

320
00:23:56,520 --> 00:23:59,100
Correlate them with the results of the survey

321
00:24:00,200 --> 00:24:03,880
Put it into an llm and put a chat bot on it and ask it

322
00:24:04,680 --> 00:24:08,120
Here are my survey scores. What are some things I can do to?

323
00:24:08,840 --> 00:24:13,960
uh balance these numbers higher across my organization or whatever the question is because I should be able to see

324
00:24:14,920 --> 00:24:16,280
I would think

325
00:24:16,280 --> 00:24:22,840
uh those because behavior drives motivation and I would think from those behaviors from again at microsoft size from

326
00:24:23,380 --> 00:24:28,600
Hundreds or a thousand other managers and and they're correlated with their

327
00:24:29,320 --> 00:24:33,320
Engagement survey scores. I would hope I could find some common ground

328
00:24:33,400 --> 00:24:37,240
It's a it's a kind of a cheap way to learn from each other on how to

329
00:24:39,000 --> 00:24:41,880
How to treat those that survey like a system

330
00:24:44,560 --> 00:24:46,490
Yes

331
00:24:46,490 --> 00:24:48,490
I actually think

332
00:24:48,920 --> 00:24:52,440
What what you inspired for me when you as you're talking through that?

333
00:24:52,920 --> 00:24:54,590
um

334
00:24:54,590 --> 00:24:55,870
is

335
00:24:55,870 --> 00:24:57,390
strength finders

336
00:24:57,390 --> 00:25:03,630
We we've talked about this and I this isn't a a tangent to go deep into strength fighters, but

337
00:25:04,730 --> 00:25:08,730
if you read when you go take the strength finders test you you get a

338
00:25:09,820 --> 00:25:11,660
A pamphlet

339
00:25:11,660 --> 00:25:19,260
That talks about each of your strengths and it gives you a little paragraph. You you recall what i'm talking about. Yep. I do. Okay now

340
00:25:20,220 --> 00:25:26,220
If you read that paragraph versus going to the strength finders book and reading the paragraph there

341
00:25:26,940 --> 00:25:28,940
You'll notice that there's similarities

342
00:25:30,640 --> 00:25:36,560
That that the theme is similar like uh, for example me one of my strengths was restorative

343
00:25:37,520 --> 00:25:42,880
I go and read it in the book. It'll say a bunch of things and I go read it in my own

344
00:25:44,160 --> 00:25:50,400
PDF and it's it's it's in the same ballpark, but it's different and that is because

345
00:25:51,580 --> 00:25:53,580
Gallup has the ability

346
00:25:54,510 --> 00:25:55,770
to

347
00:25:55,770 --> 00:25:58,970
Because the strengths aren't independent they play off of each other

348
00:25:59,720 --> 00:26:01,720
so my other strengths

349
00:26:02,040 --> 00:26:05,880
pivot around how I use my restorative it's not the

350
00:26:07,040 --> 00:26:11,760
Not the pure restorative that you would find in the the book, but it's one that's

351
00:26:13,280 --> 00:26:19,520
Aligned by the other ones and for me, it would be absolutely that would be an llm tool worth

352
00:26:20,810 --> 00:26:22,330
building

353
00:26:22,330 --> 00:26:24,330
where it takes

354
00:26:26,200 --> 00:26:32,760
As long as this was done in a absolute psychologically safe. Yeah environment because

355
00:26:33,400 --> 00:26:35,400
What i'm about to propose

356
00:26:35,640 --> 00:26:37,800
Could absolutely be used for evil

357
00:26:38,440 --> 00:26:41,800
Right. It would be handy if I could go to a tool

358
00:26:43,000 --> 00:26:45,000
Give it access to my email

359
00:26:45,790 --> 00:26:48,190
Give it my scores and say

360
00:26:48,830 --> 00:26:50,760
Hey

361
00:26:50,760 --> 00:26:52,760
what are the

362
00:26:53,080 --> 00:26:55,080
Here's an example question. What?

363
00:26:56,170 --> 00:26:59,130
What are the

364
00:26:59,370 --> 00:27:02,250
top five high roi proposals

365
00:27:03,800 --> 00:27:05,160
You have for me

366
00:27:05,160 --> 00:27:06,040
right

367
00:27:06,040 --> 00:27:07,800
look at my patterns

368
00:27:07,800 --> 00:27:10,350
because

369
00:27:10,350 --> 00:27:13,230
If I try to take make a program out of each of these

370
00:27:13,790 --> 00:27:15,820
It's going to be a challenge

371
00:27:15,980 --> 00:27:17,260
and with

372
00:27:17,260 --> 00:27:20,220
Uh, I think there's like 30 40 questions

373
00:27:21,370 --> 00:27:23,370
and so trying to do

374
00:27:23,690 --> 00:27:26,250
Me sitting down and building a system out of that

375
00:27:27,050 --> 00:27:29,610
Is going to be very complex and time consuming

376
00:27:30,800 --> 00:27:32,240
Well, let's go

377
00:27:32,240 --> 00:27:37,200
And I have an ego so i'm gonna go right? Oh, no, this wasn't me

378
00:27:37,360 --> 00:27:42,080
This was the top down bullshit thing that I had no choice over right?

379
00:27:44,010 --> 00:27:46,010
I want to meta even more because

380
00:27:47,130 --> 00:27:50,970
Why don't I there's a challenge with a lot of leaders is they

381
00:27:51,850 --> 00:27:55,610
There's uh unconscious incompetence. They don't realize how much they don't know they don't know

382
00:27:56,330 --> 00:28:00,490
Uh, most leaders i'm not one of them would say they don't need a coach or a mentor

383
00:28:01,310 --> 00:28:07,150
Uh, and I think most leaders actually do I don't think most leaders are as good at doing their job as they think they are

384
00:28:07,470 --> 00:28:11,150
So that's that's a problem. I can't necessarily solve however

385
00:28:12,220 --> 00:28:16,940
Why doesn't ai just look at not just email but other communications I do

386
00:28:17,420 --> 00:28:23,900
Even even to my meetings since most meetings for me at least are online and can be if they're not recorded can be analyzed

387
00:28:24,540 --> 00:28:27,820
Why don't I get coaching tips weekly or daily on things?

388
00:28:27,820 --> 00:28:30,860
I could do differently or like when I talk about feedback

389
00:28:31,020 --> 00:28:34,780
I like to give continue feedback or improve feedback like you great

390
00:28:35,020 --> 00:28:38,780
I like I like the way you listen while i'm talking and makes you feel like i'm listening to thanks for doing that

391
00:28:38,780 --> 00:28:43,740
Please keep doing that. I don't have to think of a new thing you've done, but I want to reinforce the things you do well

392
00:28:44,140 --> 00:28:45,760
and then

393
00:28:45,760 --> 00:28:48,320
Uh and this ai can do this as well. Hey you you

394
00:28:48,880 --> 00:28:52,560
In the it's the meeting you were able to get everyone in the meetings to talk and that was great

395
00:28:52,800 --> 00:28:58,720
Keep doing that. I want the ai to tell me that the ai can be changed can be trained on not just the

396
00:28:59,280 --> 00:29:01,920
The literature out there, but the literature out there

397
00:29:02,320 --> 00:29:06,960
Like there's a lot of literature out there that is based on interpretation of facts accelerates the same thing

398
00:29:07,200 --> 00:29:11,120
It has an interpretation of the facts. It's pretty good. It's pretty objective in the way it does it

399
00:29:11,520 --> 00:29:14,960
a lot of leadership books in the same way what I wanted to do is take the

400
00:29:15,760 --> 00:29:17,680
the canon of

401
00:29:17,680 --> 00:29:23,280
of known solid leadership principles from gallup from strength spiners who's based on data

402
00:29:23,980 --> 00:29:30,240
Correlate that with how I behave at work in a way that can be observed by ai and then give me

403
00:29:30,800 --> 00:29:34,160
Coaching feedback in small chunks on a regular basis

404
00:29:35,290 --> 00:29:41,210
And I think the cat wants to talk apparently, but that's what that's what I want and that's doable today, right?

405
00:29:41,450 --> 00:29:46,570
That is doable today someone could build that now. Here's the question and it's

406
00:29:47,210 --> 00:29:50,490
I love it when she does is she's like i'm right here pet me. Um

407
00:29:51,610 --> 00:29:57,130
I've had multiple cats use the only one that's ever done this the cat was batting brent on the arm while I was talking

408
00:29:57,530 --> 00:30:01,930
Yeah, she was like hey, I want your attention pet me now

409
00:30:03,690 --> 00:30:06,490
what you just said

410
00:30:06,570 --> 00:30:08,080
mate

411
00:30:08,080 --> 00:30:09,920
Is interesting

412
00:30:09,920 --> 00:30:12,480
In let me let me do something with you

413
00:30:13,640 --> 00:30:19,080
Let's see how well it plays out. All right. Okay. I was like I have on my android phone

414
00:30:20,700 --> 00:30:22,490
the microsoft

415
00:30:22,490 --> 00:30:28,030
Co-pilot app. Okay. Okay. I don't know if you have android. It says brent is great to see you

416
00:30:28,030 --> 00:30:31,070
I do have an android phone with no sim in it that I use for

417
00:30:31,870 --> 00:30:34,350
Watching tick tock there is a neat button

418
00:30:35,180 --> 00:30:37,310
Right there. All right

419
00:30:37,390 --> 00:30:39,390
Okay, i'm going to turn up the volume

420
00:30:41,200 --> 00:30:45,740
And this what does the button do? Hey, brent. What can I do for you?

421
00:30:48,480 --> 00:30:51,040
Hey, can we go back to a previous conversation?

422
00:30:53,100 --> 00:30:54,300
For sure

423
00:30:54,300 --> 00:30:59,630
What were we chatting about? How do you feel?

424
00:31:00,670 --> 00:31:02,670
about voice driven

425
00:31:03,230 --> 00:31:05,310
ai bots with a personality

426
00:31:07,950 --> 00:31:12,030
Speaking strictly from experience. I think they're an interesting tool

427
00:31:12,590 --> 00:31:18,990
They can make interactions more engaging and personalized. It's like having a conversation with a knowledgeable friend

428
00:31:19,310 --> 00:31:24,110
Of course, it's all about finding the right balance between personality and popis

429
00:31:24,350 --> 00:31:28,750
Too much personality could be distracting but the right amount makes it more relatable

430
00:31:29,440 --> 00:31:31,180
What do you think?

431
00:31:31,180 --> 00:31:37,260
Well, I think it's interesting that you did not bring up the primary detractor

432
00:31:37,980 --> 00:31:40,880
What would you say that is?

433
00:31:41,040 --> 00:31:49,360
Probably the risk of over-reliance on these bots. They're a great aid, but shouldn't replace human interaction entirely

434
00:31:49,600 --> 00:31:55,280
They can also sometimes unintentionally reinforce biases or lead to privacy concerns

435
00:31:55,680 --> 00:32:02,000
It's kind of like wielding a double-edged sword you get the good stuff, but you need to watch out for the sharp edges

436
00:32:03,820 --> 00:32:09,580
I think you we should be stronger. We shouldn't just be having to watch out for the sharp edges

437
00:32:10,730 --> 00:32:12,330
um

438
00:32:12,330 --> 00:32:15,530
Like do you have any concern around?

439
00:32:17,020 --> 00:32:18,540
human beings

440
00:32:18,540 --> 00:32:22,300
Falling in love with ai and how that may impact society

441
00:32:23,930 --> 00:32:28,570
That's a complex topic for sure while it's understandable people may form

442
00:32:28,810 --> 00:32:32,250
Attachments or emotional bonds with their ai companions

443
00:32:32,730 --> 00:32:37,770
It raises questions about how those relationships might impact real-life connections

444
00:32:38,250 --> 00:32:46,110
It could blur the lines between genuine human interaction and artificial ones potentially affecting how people perceive relationships

445
00:32:46,410 --> 00:32:50,990
It's a nuanced conversation with ethical emotional and societal implications

446
00:32:51,610 --> 00:32:54,090
Any specific part of that intrigues you?

447
00:32:55,400 --> 00:33:01,160
Yeah, the ethical concerns and how we're gonna how we're going to prevent the ethical concerns

448
00:33:01,800 --> 00:33:08,680
Before they happen. It sounds like you feel you're sounds like you're okay with us taking a reactive approach

449
00:33:10,780 --> 00:33:12,140
Not really

450
00:33:12,140 --> 00:33:16,380
I think we should definitely be proactive about ethical concerns

451
00:33:16,700 --> 00:33:23,260
Especially when it comes to rapidly evolving technologies like ai setting strong ethical guidelines

452
00:33:23,500 --> 00:33:29,100
Fostering transparency and ensuring accountability are crucial steps to minimize risks

453
00:33:29,500 --> 00:33:37,500
It's about anticipating challenges and having robust frameworks in place before things get okay. Thank you. I gotta go back to a podcast. Bye

454
00:33:39,160 --> 00:33:42,360
Sure thing brent. Enjoy your podcast. Oh my god

455
00:33:43,000 --> 00:33:45,000
So that's weird. Um

456
00:33:45,240 --> 00:33:51,560
And yeah, but how did you feel about that? Like the first time I did that I was with my daughter in the car

457
00:33:52,520 --> 00:33:54,840
And I don't know how I let it do it

458
00:33:55,080 --> 00:33:59,480
But basically if I were to go to a different screen and I would show you

459
00:34:00,280 --> 00:34:01,320
that

460
00:34:01,320 --> 00:34:02,960
is all

461
00:34:02,960 --> 00:34:04,570
llm

462
00:34:04,570 --> 00:34:07,850
Like everything that they it was llm running

463
00:34:09,050 --> 00:34:11,210
And it was just the voice ai

464
00:34:11,830 --> 00:34:15,050
Outputting it there's a couple takeaways I have from that one is

465
00:34:15,610 --> 00:34:20,250
I no longer need you on the podcast. I can just as have an ai to talk to

466
00:34:21,610 --> 00:34:25,210
Let's see if I can figure out how to do that. No, don't do not do not stop that

467
00:34:26,570 --> 00:34:30,490
I think I think the key thing is is yeah, we we all saw her

468
00:34:31,850 --> 00:34:34,140
Was it her or she her?

469
00:34:34,140 --> 00:34:38,140
Who my ai the movie the movie with the guy fall in love with the ai. So

470
00:34:38,540 --> 00:34:42,060
Oh, I haven't seen that one. I'm not talking about you know

471
00:34:44,590 --> 00:34:51,070
Focusing on the ai, but the ai I think even the way I use chad gpd today. It's a coach. It's a collaborator

472
00:34:52,160 --> 00:35:00,480
So why don't given in general how bad leaders are across the industry and how much data we have on how?

473
00:35:01,440 --> 00:35:06,320
On on what it takes to be a good leader and what it takes to lead it to to make a team successful

474
00:35:06,720 --> 00:35:12,160
Why don't we why why can't we take those conversations from all the ways we interact with people?

475
00:35:12,640 --> 00:35:18,240
And have an ai become our coach to help help these leaders learn how to behave better and improve

476
00:35:18,560 --> 00:35:24,000
That's the question because so now we're going back like okay brent that neat tangent

477
00:35:24,000 --> 00:35:29,280
But why the hell did we go that way? Right? And so what I walked away with okay

478
00:35:29,840 --> 00:35:32,160
And I I don't think this is what you're saying

479
00:35:32,960 --> 00:35:39,600
But what I walked away with wouldn't it be great if we could have an ai watch what we do and give us little affirmations

480
00:35:40,720 --> 00:35:47,040
But affirmations corrections course corrections tell us to do things that we're not doing okay. It's of course corrections. I love that

481
00:35:49,470 --> 00:35:51,320
towards

482
00:35:51,320 --> 00:35:55,720
What goal what correction and who gets to define that?

483
00:35:56,200 --> 00:36:00,600
Okay, i'm gonna go off data here research shows that highly engaged teams deliver

484
00:36:01,720 --> 00:36:03,720
Uh faster

485
00:36:04,200 --> 00:36:08,600
With quality and there are facts on what creates highly engaged teams

486
00:36:08,760 --> 00:36:11,640
They need to know why their job is important. They need to learn manager cares about them

487
00:36:11,880 --> 00:36:14,520
They have to they have to get feedback are the biggest thing

488
00:36:15,320 --> 00:36:17,480
And and and then those those dive into

489
00:36:17,960 --> 00:36:23,240
I feel like you're missing my point because you're mentioning all the things that you know, I already agree with I I know but I think

490
00:36:23,720 --> 00:36:28,920
I think if you look a lot of managers, you'll ask them for example. Hey, do you give feedback team?

491
00:36:28,920 --> 00:36:34,760
Yeah, I give my team a lot of feedback and then because it's not observed because it happens behind closed doors and closed meetings

492
00:36:35,000 --> 00:36:37,000
We don't know if that's true or not

493
00:36:37,320 --> 00:36:42,840
What I want to have is the ai to tell me if i'm doing the things that create highly engaged teams

494
00:36:42,920 --> 00:36:45,560
Or maybe I tell the coach here is what I want to get better at

495
00:36:45,880 --> 00:36:50,840
I want to get better at making a closer connection with my employees and get so they'll so that

496
00:36:51,480 --> 00:36:55,560
I can give them strong feedback without feeling like without them feeling like i'm attacking them

497
00:36:56,520 --> 00:36:59,720
Now we're on the same page and and then having some

498
00:37:00,600 --> 00:37:05,480
So we're on the same page having something doing Pavlovian experiments with me

499
00:37:06,280 --> 00:37:07,560
without

500
00:37:07,560 --> 00:37:09,560
my conscious awareness

501
00:37:09,640 --> 00:37:11,640
I'm I'm just like

502
00:37:11,640 --> 00:37:16,520
In the beginning I have to have the ai help me figure out what the goal is because if i'm your typical

503
00:37:16,820 --> 00:37:19,240
mediocre to worst manager or leader

504
00:37:19,800 --> 00:37:25,080
I don't know what those things are I need to do because i've never done as good as I was as a software developer

505
00:37:25,400 --> 00:37:32,440
I've done jack squat on learning anything about leadership and growing people. I completely get that you are essentially

506
00:37:33,710 --> 00:37:36,190
What you are done is trying to create

507
00:37:37,070 --> 00:37:38,030
a

508
00:37:38,030 --> 00:37:40,190
fourth order of ignorant solution

509
00:37:41,240 --> 00:37:47,160
Yes, I want to use ai to help people discover what they don't know they don't know right you are correct and all i'm saying

510
00:37:47,800 --> 00:37:50,760
And i'm aboard on that one, but it's got to be

511
00:37:51,300 --> 00:37:53,320
ethical af

512
00:37:53,320 --> 00:37:58,680
It does. It does have to be ethical af but I think it can be I think it can be and and

513
00:37:59,400 --> 00:38:04,440
I would be surprised because i've never had an idea that wasn't already invented or already on the market

514
00:38:05,240 --> 00:38:09,000
I would not be surprised if one of these companies like fellow or whoever who focus on

515
00:38:09,560 --> 00:38:12,940
These managers, excuse me manager employee interactions

516
00:38:13,560 --> 00:38:18,920
Haven't already thought about this or building this in some way. This is I think it's critical. I think

517
00:38:19,640 --> 00:38:25,880
I don't like like you just heard what what bing is. Yeah, like it's not far off

518
00:38:27,290 --> 00:38:29,290
I don't I personally

519
00:38:29,610 --> 00:38:31,610
Like I think I told you

520
00:38:32,170 --> 00:38:35,530
The first time I did this I I did a different voice

521
00:38:36,560 --> 00:38:38,760
and

522
00:38:38,760 --> 00:38:40,920
My daughter was in the car and she's like

523
00:38:42,060 --> 00:38:43,770
dad

524
00:38:43,770 --> 00:38:47,610
Because the way it responded dad the ai hates you

525
00:38:48,410 --> 00:38:49,450
and

526
00:38:49,450 --> 00:38:53,370
Because of the way they responded i'm like, yeah, I totally get what she's saying

527
00:38:54,160 --> 00:38:59,920
And i'm like, this is why I don't like anyway, i'm not going to kick that horse any further

528
00:39:00,560 --> 00:39:04,720
I absolutely believe it's unethical to put personality into ai

529
00:39:05,440 --> 00:39:07,440
because of

530
00:39:07,600 --> 00:39:11,680
The yeah the problem because because people

531
00:39:13,960 --> 00:39:15,960
Number one, it's well known

532
00:39:16,440 --> 00:39:22,600
People are very easily manipulated and an ai could do that and a friendly ai could absolutely do that

533
00:39:23,160 --> 00:39:25,260
Super well

534
00:39:25,260 --> 00:39:26,540
Anyway, yeah

535
00:39:26,540 --> 00:39:29,420
No, okay. I agree. So I also don't like the personality

536
00:39:29,500 --> 00:39:34,780
I think it's a nice little parlor trick they've done to make it a little scary to to creep towards uncanny valley

537
00:39:35,180 --> 00:39:37,660
I do not like it either, but I think there's a huge

538
00:39:38,360 --> 00:39:43,260
opportunity there and I think we're yeah, you're right. I want to find a cure for fourth order of ignorance

539
00:39:43,820 --> 00:39:44,620
Yes

540
00:39:44,620 --> 00:39:46,860
And I I agree. There's a way to do it

541
00:39:47,500 --> 00:39:49,500
I just as i've mentioned before

542
00:39:50,460 --> 00:39:53,560
right, it's

543
00:39:53,560 --> 00:39:56,920
The fact that there's so few systems thinkers in this space

544
00:39:57,320 --> 00:39:58,600
Ah

545
00:39:58,600 --> 00:40:00,600
is problematic

546
00:40:00,680 --> 00:40:02,920
This is why like if you get down to it, we don't need

547
00:40:03,800 --> 00:40:06,520
And this is why people can't coming, you know looping back

548
00:40:06,600 --> 00:40:10,840
This is why people come to you and can't differentiate between ai and magic or llm's and ai

549
00:40:11,400 --> 00:40:16,040
I think the need for systems thinking in the new world we're growing into is becoming

550
00:40:16,920 --> 00:40:20,920
Uh more than critical, um, like like vital. Yeah vital

551
00:40:21,710 --> 00:40:23,710
like I like

552
00:40:23,950 --> 00:40:30,510
Near existential I don't use that word very often because it's so buzzwordy now, but near existential

553
00:40:30,830 --> 00:40:32,590
Yeah, I agree. I think

554
00:40:32,590 --> 00:40:34,590
it has to change or

555
00:40:34,830 --> 00:40:36,090
I mean

556
00:40:36,090 --> 00:40:41,130
It has to change because we'll never take full advantage of the tech the advances into technology until we do

557
00:40:43,240 --> 00:40:44,840
Yes, so yeah

558
00:40:44,840 --> 00:40:46,840
All right, then well, uh

559
00:40:47,560 --> 00:40:49,560
I'll I will be a combination

560
00:40:50,280 --> 00:40:53,800
prompt engineer. I hate that term a prompt engineer and a

561
00:40:54,360 --> 00:40:57,400
Talent coach in my future life not not

562
00:40:58,120 --> 00:41:02,760
But I think there's there's something there and I yeah, I think mostly

563
00:41:03,560 --> 00:41:04,600
like

564
00:41:04,600 --> 00:41:09,960
Rather than look at ai as magic. I look at what I see are big problems and one big problem as I see is that

565
00:41:10,920 --> 00:41:14,280
Oh god, I could tell stories probably get fired for them, but I see

566
00:41:15,400 --> 00:41:16,780
uh

567
00:41:16,780 --> 00:41:19,500
There's just a lot of flaws across the industry here

568
00:41:19,580 --> 00:41:25,260
There's some other folks as well in how people lead teams and they fail to see how important it is

569
00:41:25,820 --> 00:41:26,860
to

570
00:41:26,860 --> 00:41:32,300
Focus on those things as a leader the critical thing. So I wish they would get the coaching they need

571
00:41:32,380 --> 00:41:36,300
They don't know they need it. They're never going to see it. They think they're fine. Whatever whatever whatever like

572
00:41:36,540 --> 00:41:37,500
Let me give you one more example

573
00:41:37,500 --> 00:41:38,300
We can close to the day

574
00:41:38,300 --> 00:41:43,500
But you and I'll go back to microsoft in the old days when it was toxic microsoft and the way to get to the top

575
00:41:43,500 --> 00:41:45,500
Was to leave bodies in the wake

576
00:41:45,500 --> 00:41:51,900
You know you you and I both saw people who got promoted for being complete assholes and tearing people apart

577
00:41:52,460 --> 00:41:53,660
and

578
00:41:53,660 --> 00:41:58,620
What happens that happened? Did they get whatever whatever level it is at a higher level? Um

579
00:42:00,030 --> 00:42:04,670
What it does is it reinforces that behavior so they do more of it so they continue to grow

580
00:42:05,150 --> 00:42:10,830
Yeah, and what it does is it tears apart teams and makes good people leave to go find different things

581
00:42:10,830 --> 00:42:12,990
The great thing about microsoft if there's one

582
00:42:13,550 --> 00:42:20,190
Is just teasing is that uh, like I survived there because I as you know, I changed jobs every 18 months

583
00:42:21,080 --> 00:42:25,320
Because if if I got bored on the job or I didn't like the manager there was another place I could go work

584
00:42:25,400 --> 00:42:27,440
That would be better for a while

585
00:42:27,440 --> 00:42:29,440
Yes completely agree

586
00:42:29,520 --> 00:42:32,400
That the no it and there's unknown

587
00:42:34,220 --> 00:42:36,220
you get more of

588
00:42:36,540 --> 00:42:41,340
For any organization that you probably know this better. You can probably cite this better than I can

589
00:42:41,980 --> 00:42:46,060
but in any organization you get more of the

590
00:42:48,090 --> 00:42:51,230
Personality as derived by their actions

591
00:42:52,400 --> 00:42:54,280
Of the leader

592
00:42:54,280 --> 00:42:58,200
Right first and foremost like leaders will say oh we need to do this

593
00:42:58,680 --> 00:43:00,680
But if their actions say differently

594
00:43:00,760 --> 00:43:05,800
It's their actions that you're going to get more of whatever is the principle that drives those actions

595
00:43:06,600 --> 00:43:11,720
Right if it's charge the hill you're going to get charged the hill if it's going to be steamroll your

596
00:43:12,600 --> 00:43:13,480
your

597
00:43:13,480 --> 00:43:16,440
Your office neighbor. That's what you're going to get

598
00:43:17,160 --> 00:43:18,330
now

599
00:43:18,330 --> 00:43:21,770
Those are bad. But the other extremes in my view are bad as well

600
00:43:22,840 --> 00:43:24,360
right like

601
00:43:24,360 --> 00:43:28,680
No one ever talks about what is collaboration taken to the nth degree

602
00:43:30,080 --> 00:43:33,460
Right. That's that's the consensus committee bureaucracy

603
00:43:34,400 --> 00:43:40,560
Model where you get in the meeting and you can't move forward because everyone has to agree and no one does

604
00:43:42,080 --> 00:43:44,270
So there's

605
00:43:44,570 --> 00:43:46,570
Balance

606
00:43:48,110 --> 00:43:50,990
Balance is what's key it is it is all right

607
00:43:51,070 --> 00:43:54,590
Well with a few breaks for audio and technical issues as happens

608
00:43:55,790 --> 00:43:57,310
The cat's fine

609
00:43:57,310 --> 00:44:02,990
Brent you got you got to leave the ponytail man. No, I love it. Okay. All right. I'm gonna grow my hair out even further

610
00:44:04,510 --> 00:44:08,910
All right, um a quick story then we're gonna go I should do this at the beginning but uh,

611
00:44:09,630 --> 00:44:10,670
I

612
00:44:10,670 --> 00:44:12,110
Had a week

613
00:44:12,110 --> 00:44:13,550
but on monday

614
00:44:13,550 --> 00:44:18,990
Um, you know, I haven't uh as some longtime listeners will know at one point. I was somewhat of a musician

615
00:44:19,390 --> 00:44:24,190
I auditioned for a band on monday night. Did you really I did so I that is awesome

616
00:44:24,670 --> 00:44:28,350
And uh, so I got a note back there auditioning a few more people, but i'm definitely in the running

617
00:44:28,670 --> 00:44:31,950
I will know more I won't know more for it till the like the end of the year

618
00:44:32,190 --> 00:44:36,350
Because they have some other gigs coming up with subs and things. But anyway a lot of fun. It was fun

619
00:44:36,910 --> 00:44:42,670
Playing with other people for the first time that was great. I also I forgot my neck strap for my zaxophone

620
00:44:43,550 --> 00:44:48,430
And so I had to use like a string like a referee string and it pulled on my neck so hard

621
00:44:48,910 --> 00:44:53,630
But I was having so much fun. I didn't notice like it completely wrecked my shoulders and back like

622
00:44:54,270 --> 00:44:58,030
Uh, I could finally get my arms above my head. I couldn't get my arms with my head for a couple days

623
00:44:58,110 --> 00:45:01,790
so well, I was actually more worried that because you said

624
00:45:02,350 --> 00:45:05,310
Because the surface area is much smaller and i'm like, okay

625
00:45:05,390 --> 00:45:09,710
What's he so into it that he was actually starting to slice into his no

626
00:45:09,710 --> 00:45:14,190
No, there was a line in my neck for about 12 hours, but it went away. I'm good. I'm recovered. I'm recovered

627
00:45:14,590 --> 00:45:17,870
Okay, and I will never ever forget my neck strap again. What kind of band?

628
00:45:18,430 --> 00:45:21,580
A ska band. Oh awesome. Awesome

629
00:45:21,980 --> 00:45:24,780
All right. I will let you know if I get into the band we'll know by the end of the year

630
00:45:25,180 --> 00:45:26,060
uh

631
00:45:26,060 --> 00:45:27,020
All right, man

632
00:45:27,020 --> 00:45:33,900
I think i'm we'll figure out the next recording. We got probably one maybe two more before the end of the year i've got our uh,

633
00:45:34,940 --> 00:45:38,460
Predictions episode in reflection on the last year all that stuff

634
00:45:39,100 --> 00:45:44,400
All right. Well, uh, i'm alan i'm brit. We'll see you next time

