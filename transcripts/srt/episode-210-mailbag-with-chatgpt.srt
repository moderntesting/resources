1
00:00:07,500 --> 00:00:13,380
Welcome to AVE testing podcast your modern testing podcast. Your hosts Alan

2
00:00:13,380 --> 00:00:18,500
and Brent will be here to guide you through topics on testing, leadership,

3
00:00:18,500 --> 00:00:24,100
Agile, and anything else that comes to mind. Now on with the show. Hello Brent, I

4
00:00:24,100 --> 00:00:30,440
am the human that used to be Alan Page. How are you? I am also the human that

5
00:00:30,440 --> 00:00:37,240
used to be Alan. Well I want to know your name. My name

6
00:00:37,360 --> 00:00:46,290
According to the session here, the label is Weasel, but I'm Alan, but I like all

7
00:00:46,290 --> 00:00:53,310
good sheep. After a couple years of pandemic and stired non grooming, I cut

8
00:00:53,310 --> 00:00:59,980
off six inches of my hair and six inches of my beard and I look, you know, normal.

9
00:00:59,980 --> 00:01:05,270
No, no, I'm gonna go with that. You just keep on going. So Brent is Satya Nadella

10
00:01:05,270 --> 00:01:11,230
is calling Brent right now and it's a question about whether or not Microsoft

11
00:01:11,230 --> 00:01:18,590
should do anything with LLMs and Gen AI. And the answer for Satya is maybe, maybe,

12
00:01:18,590 --> 00:01:24,950
but it should probably make sure he's doing things to stop people from being

13
00:01:24,950 --> 00:01:30,510
dumb, etc, etc. Okay, so once he gets done with Satya, I have a quick phone

14
00:01:30,510 --> 00:01:34,590
call I have with Kamala Harris talked to her a little bit about technical

15
00:01:34,590 --> 00:01:39,110
advisory for upcoming White House stint. And then after that, we're gonna do the

16
00:01:39,110 --> 00:01:45,610
podcast. So we're on the podcast here and I am joined by Brent Jensen. Hey, Brent,

17
00:01:45,610 --> 00:01:50,950
say hi. Hello. It's really cool because unless you listen to the podcast, you'll

18
00:01:50,950 --> 00:01:57,590
never know what I said. So I hope you told Satya I said hi. We had, you know,

19
00:01:57,590 --> 00:02:02,750
we've talked about this before. Well, before we get into the topic, how are

20
00:02:02,750 --> 00:02:09,710
things squeaky chair Brent? How's life? What's new? Do anything cool lately?

21
00:02:09,710 --> 00:02:24,150
Sounds like a no. No. So drama's coming up this weekend. My eldest who is about to

22
00:02:24,150 --> 00:02:29,950
turn, my eldest is about to turn 26. So we're gonna be celebrating his birthday.

23
00:02:29,950 --> 00:02:40,370
He's home with us. But what makes it dramatic is at 26. Do you know the

24
00:02:40,370 --> 00:02:44,850
magic thing that happens? You're not in your parents insurance anymore. Yeah.

25
00:02:44,850 --> 00:02:54,900
Yeah. He is just about to finish his second week at his new job. He was

26
00:02:54,900 --> 00:03:00,620
looking for something different and I'm like, dude, you're about to turn 26.

27
00:03:01,220 --> 00:03:05,540
Go for something different, but you need something different with insurance.

28
00:03:05,540 --> 00:03:16,100
Yep. Yep. America for healthcare is a privilege, not a right. He did go with,

29
00:03:16,100 --> 00:03:20,660
he did get something with insurance, but it's not something different.

30
00:03:20,660 --> 00:03:25,980
Oh, well, that's insurance. So, you know, at one point, don't listen if you work

31
00:03:25,980 --> 00:03:28,860
with you. I thought, you know what, I want to, what's the easiest decision

32
00:03:28,860 --> 00:03:34,180
for my retirement? Don't worry. What's the easiest job I can do that as health

33
00:03:34,180 --> 00:03:38,860
benefits? And of course the barista came up. That's a lot of weird customers. One

34
00:03:38,860 --> 00:03:45,540
in their double half calf frappe whip Oreo mint pumpkin scream green thing.

35
00:03:45,540 --> 00:03:54,410
And so, but my retire, the job I'm moving toward my career goal is assistant

36
00:03:54,410 --> 00:04:05,060
librarian. Interesting. It's quiet. I don't have to talk a lot. It's a, yeah,

37
00:04:05,060 --> 00:04:10,900
don't look at him. That's my thing. Now. So the thing is actually, I'm wondering,

38
00:04:10,900 --> 00:04:16,580
is that one of the few jobs that AI won't take away? Maybe not. Maybe not.

39
00:04:16,580 --> 00:04:20,660
Because well, you know what? AI has not taken away any jobs because you know what?

40
00:04:20,660 --> 00:04:26,340
Well, I shouldn't say that AI today, as we know it is far too stupid to take away a job.

41
00:04:26,340 --> 00:04:32,100
It doesn't know what it's doing. And I can't even go to LinkedIn anymore because people

42
00:04:32,100 --> 00:04:36,100
keep on telling me AI won't do this. I say, thanks for you. Thank you for the

43
00:04:36,100 --> 00:04:41,480
straw man argument. Of course it won't, you stupid freaking idiot. All right. So

44
00:04:42,810 --> 00:04:48,250
topics today, speaking of AI, there was a time earlier in the life of this podcast.

45
00:04:48,250 --> 00:04:55,050
Are we episode two 10 this week? We are. It's welcome to episode 210 of the

46
00:04:55,050 --> 00:04:59,930
AB testing podcast. We bring you valuable and relevant, well thought out,

47
00:05:00,490 --> 00:05:06,250
practically scripted information. I'm kidding. We used to make a Kanban, like little lists of

48
00:05:06,250 --> 00:05:10,490
things to go through. It was great. We had topics to talk about and they rolled through them.

49
00:05:10,490 --> 00:05:14,490
And maybe you liked that and maybe you didn't, but now you get this version of the AB podcast

50
00:05:14,490 --> 00:05:22,090
testing podcast where Brent types while I'm talking, which I may or may not go through

51
00:05:22,090 --> 00:05:28,410
and, uh, and, and remove later. It's horrible because we both have mechanical keyboards

52
00:05:28,410 --> 00:05:35,210
and Brent is redoing an exercise that we've already done, but he's doing it now in order to get me to

53
00:05:35,210 --> 00:05:42,490
continue to share just nothing, nothing at all while he does his thing. So what we've done

54
00:05:42,490 --> 00:05:46,970
for today's topic on the fly, like sometimes I'll come in and go, Hey, I don't want to talk about

55
00:05:46,970 --> 00:05:50,490
this and Brent won't know what it is, but we'll get there and we'll roll with something.

56
00:05:50,490 --> 00:05:56,570
And occasionally, occasionally a little teeny nugget of something good comes out and someone

57
00:05:56,570 --> 00:06:04,010
says great podcast. And I say, man, drugs are good where you live. But today I asked Brent for a topic

58
00:06:04,010 --> 00:06:12,650
and Brent said, oh no. And I looked through my like five for five. I got a topic on her about

59
00:06:12,650 --> 00:06:18,490
tomorrow, my blog post, and that's not a topic for today. It's a rare situation where Alan too

60
00:06:18,570 --> 00:06:24,810
like, I don't know. It's been a day. Usually, usually I will take, because as the producer

61
00:06:24,810 --> 00:06:29,370
of this podcast, Brent just shows up. I spend time thinking about topics ahead of time.

62
00:06:29,370 --> 00:06:33,690
I do some Googling, please bring your screen back up. Cause I need to see it and I will

63
00:06:33,690 --> 00:06:37,610
be prepared. I go, here's what we're going to talk about. I don't worry about the details.

64
00:06:37,610 --> 00:06:45,050
No scripts, no special effects. But what we did today is you may have heard on this podcast or

65
00:06:45,050 --> 00:06:51,690
maybe, maybe even on the internet about a thing called chat GPT and my chat for those of you that

66
00:06:51,690 --> 00:06:59,850
don't know chat GPT is a tool based on generative AI. There's indexed on a whole basically the

67
00:06:59,850 --> 00:07:06,170
internet. We have a special one that's, uh, I don't know. Did you use, you did use a special one

68
00:07:06,730 --> 00:07:12,090
that knows more about us and what we do a little bit more, which could be a good thing or a bad

69
00:07:12,090 --> 00:07:19,000
thing. And then Brent, if you can scroll up for a second, Brent asked this thing, uh, as advocate,

70
00:07:19,000 --> 00:07:24,520
the way that let's go up. Hey, Hey, he starts off with, Hey, I left out Brent is you can tell

71
00:07:24,520 --> 00:07:28,920
something about someone's personality by the way they interact with the chat bot. It's like,

72
00:07:28,920 --> 00:07:34,520
good morning, dear sir. He says, Hey, give Alan Page and Brent Jensen a good mailbag question

73
00:07:34,520 --> 00:07:39,080
that talked about on their podcast. So we have three ideas, but before we start on any of them,

74
00:07:39,080 --> 00:07:46,790
let's do this. Okay. And Brent, I'm going to let you pick. We're going to, but we're going to,

75
00:07:46,790 --> 00:07:50,070
but no, I'm not going to let you pick. We're going to kind of vote here. Would you stop

76
00:07:50,710 --> 00:07:57,110
moving the page? I'm trying to read. Oh, this is the new one. No, wait. Now I got two questions.

77
00:07:57,110 --> 00:08:04,870
Oh my God. All of this crap is it just defined why we would care. Imagine giving a presentation

78
00:08:04,950 --> 00:08:10,710
and someone keeps on changing the slide randomly on you. So, um, one of it, one of the questions is,

79
00:08:11,980 --> 00:08:16,060
Hey, Brent, no says, Hey, no, it doesn't say that either. It says as advocates of modern

80
00:08:16,060 --> 00:08:20,460
testing principles, how do you see the role of testers evolving in AI heavy environments?

81
00:08:20,460 --> 00:08:24,700
That depends on what that means with AI systems handling more decision-making.

82
00:08:24,700 --> 00:08:29,180
Don't want that to happen, including testing. How do you ensure teams are still responsible

83
00:08:29,180 --> 00:08:33,500
for quality, especially when AI outputs can be unpredictable or biased? I am happy to talk

84
00:08:33,500 --> 00:08:39,260
about this one. This is actually, cause there's a lot of kind of hints at straw men here. And it

85
00:08:39,260 --> 00:08:43,980
goes back to my comment earlier. People, they're, they're straw men arguments on LinkedIn. Like

86
00:08:43,980 --> 00:08:48,060
they would see us and they would go off on some tangent around how test cases aren't testing. And

87
00:08:48,060 --> 00:08:53,340
I would blow my brains out and life would move on. So is there another question we want to look

88
00:08:53,340 --> 00:08:57,980
at? Another one. The other question is you've discussed the importance of shifting quality

89
00:08:57,980 --> 00:09:02,620
responsibilities across the team and modern testing. However, with the increase in AI driven

90
00:09:02,620 --> 00:09:06,860
development automation tools, how do you see the role of traditional testers evolving further?

91
00:09:06,860 --> 00:09:12,220
Should they focus on new skills like understanding AI biases or models? Yes. Or is there still value

92
00:09:12,220 --> 00:09:16,780
in deep exploratory testing for human centered validation? The interesting thing here is we're

93
00:09:16,780 --> 00:09:23,020
going to answer that question. Because no, the question, no, no, the question, because no matter

94
00:09:23,020 --> 00:09:30,090
which one we pick, the answer is going to be kind of the same. There's a little bit of new

95
00:09:30,090 --> 00:09:34,090
ones. Well, if you're going to give a three second answer, they'd be different. If we give

96
00:09:34,170 --> 00:09:40,330
the answer of any depth or substance, it'll be kind of the same answer. So I want to leave this one.

97
00:09:40,330 --> 00:09:43,050
Let's go back to the other one. We're going to come back and touch on this one to

98
00:09:44,570 --> 00:09:51,770
make sure we, my prediction here. Oh, what if I'm Gen AI and I just have a bunch of words that

99
00:09:51,770 --> 00:10:00,420
I'm trying to throw out in an order that makes sense based on context? Are we all living in a

100
00:10:00,420 --> 00:10:08,100
simulation? That is 100% the definition of Gen AI. I know. Fuck. Oh, wait. Sorry, kids.

101
00:10:08,100 --> 00:10:14,740
That I didn't, I'll forget that at that, but my brain is spinning. Okay. Let's talk about

102
00:10:15,860 --> 00:10:20,660
role of testers in evolving AI. And you're going to have to, okay, the question's gone now,

103
00:10:20,660 --> 00:10:24,500
because now Brant's asking new questions because Brant is off script as usual.

104
00:10:25,660 --> 00:10:33,100
Wait, is there a script now? When did that happen? The script? No, dude, dude, it's in here.

105
00:10:33,100 --> 00:10:40,700
You just haven't prompted it out of me yet. Ah, okay. Got it. Holy cow. Maybe, maybe Gen AI.

106
00:10:40,700 --> 00:10:46,860
I got tired of your, of your beautiful voice preamble. So I just went ahead and asked.

107
00:10:46,860 --> 00:10:55,740
To answer it. Okay. I asked open chat GPT to answer the question.

108
00:10:55,740 --> 00:11:03,030
Oh my God. No, because it won't have the, it's probably right. Honestly. And I'm going to read

109
00:11:03,030 --> 00:11:07,350
one thing on here because first off is from the second paragraph. Stop scrolling. First off,

110
00:11:07,350 --> 00:11:11,350
testers need to stop thinking about testing is finding bugs and more about accelerating teams

111
00:11:11,430 --> 00:11:16,790
learning and decision making. Let's talk about how AI is helping teams accelerate the achievement

112
00:11:16,790 --> 00:11:25,420
of simple quality. Yeah. No, it's actually, this is not bad in terms of what you would say.

113
00:11:25,420 --> 00:11:33,050
I'm sorry. I would, I would add flair. So I think it's a question that gets asked all

114
00:11:33,050 --> 00:11:36,330
over the place. And I feel like maybe we've talked about it before, but we can dive in

115
00:11:37,130 --> 00:11:44,340
is one to repeat the thing we've said 10 million times. And now I find

116
00:11:44,340 --> 00:11:50,500
a thousand other people are saying the same thing. Gen AI AI is not taking your job away.

117
00:11:52,490 --> 00:12:00,010
People who are adept at understanding how LLMs and Gen AI work may. And

118
00:12:02,920 --> 00:12:08,540
okay. I'm gonna, I'm gonna end up repeating myself and I've been talking for a long time.

119
00:12:08,540 --> 00:12:14,810
I'm gonna let you start with this and I'll roll in in a bit. Okay. Because it's interesting

120
00:12:14,810 --> 00:12:20,920
because it's not taking your job away yet, but it does absolutely having an impact.

121
00:12:22,740 --> 00:12:29,910
There was a report I saw this morning from a professor at University of Berkeley. And

122
00:12:31,030 --> 00:12:39,270
he's already observing that a lot of graduate students are finding it difficult in the CS

123
00:12:39,270 --> 00:12:51,670
degree, finding it difficult to land new jobs. So yeah, it's not gonna take your, my job away

124
00:12:52,470 --> 00:13:00,600
yet, but it's gonna start, it's already starting to not take anyone's jobs away, but

125
00:13:01,320 --> 00:13:06,840
not open them either. Speaking of which, and here's a different take on this question that

126
00:13:08,230 --> 00:13:14,070
I'm gonna, this is where chat GPT can't find the patterns. There was an article this week,

127
00:13:14,070 --> 00:13:21,670
last week at the latest, no surprise at all, talking about the rise in bug rates due to in,

128
00:13:22,310 --> 00:13:26,470
I don't remember the details where they got the applications from, but due to people more

129
00:13:26,470 --> 00:13:33,030
or less blindly accepting co-pilot suggestions. Yep. Not surprising at all. We knew that it's

130
00:13:33,110 --> 00:13:40,840
gonna happen. So if we're putting more bugs in the code, Brent, does that mean we need testers

131
00:13:40,840 --> 00:13:53,880
even more now? No, no, we have a different problem. Yeah. Right. Because the, and it's in alignment

132
00:13:53,880 --> 00:14:02,300
with actually GPT's fake response for you. Right. If you read this last one, in short, testers in

133
00:14:02,300 --> 00:14:07,660
the AI driven world aren't trying to catch everything before it happens. They're accelerating

134
00:14:07,660 --> 00:14:13,350
learning. Okay. And, and, and here it italicized and I'm not going to read the rest of it.

135
00:14:13,350 --> 00:14:20,620
It's all about, so we can respond faster. But here is the thing that's problematic with AI

136
00:14:20,620 --> 00:14:26,780
systems and it's in alignment with what you just said. These AI systems, are they accelerating

137
00:14:26,780 --> 00:14:37,450
learning or are they accelerating laziness? Because if they accelerate laziness in ways that

138
00:14:37,450 --> 00:14:44,300
are important, then it's going to be problematic. I don't think, I don't think bringing back,

139
00:14:44,300 --> 00:14:51,180
you know, dedicated testers are going to be the solution. Right. It's going to be, how do we

140
00:14:51,180 --> 00:14:58,060
battle laziness? And in quite honestly, if it comes down to that, we're going to have a big

141
00:14:58,060 --> 00:15:08,550
problem, a big problem. Cause laziness is like, um, uh, a key principle of the software development.

142
00:15:11,000 --> 00:15:16,950
You know, it's, uh, it's interesting. Sorry. I have a bunch of different threads going on at once,

143
00:15:16,950 --> 00:15:23,020
but where I think AI can, so let me go, let me back up a step. There's a lot of testers out

144
00:15:23,020 --> 00:15:27,100
there who feel their job is to find, I mean, if you don't want to go back 30 years is to find books.

145
00:15:27,100 --> 00:15:32,060
Right. And we know, we know most, I think most testers who have paid attention realize that's

146
00:15:32,060 --> 00:15:39,020
not their job. It's a byproduct of doing their job at best. Uh, there is a school out there

147
00:15:39,020 --> 00:15:44,140
that says to all testers do is provide information to stakeholders, which I, which we've talked about

148
00:15:44,140 --> 00:15:50,220
before. I don't want to go deeply into that. Um, sure, but that's not going to really help

149
00:15:50,220 --> 00:15:56,460
here. So again, fast feedback loops. When we talk about, when we've talked about teams not having

150
00:15:56,460 --> 00:16:01,180
dedicated testers, it isn't because we don't like testers isn't because we don't think they're

151
00:16:01,180 --> 00:16:06,940
valuable, but what's more valuable is getting fast feedback loops on the work produced by

152
00:16:06,940 --> 00:16:12,620
the teams or trying to accelerate the team. So if we're trying to accelerate the team and the

153
00:16:12,620 --> 00:16:21,190
team has potentially more bugs, more functional correctness bugs, because they're being lazy with,

154
00:16:21,190 --> 00:16:27,210
uh, with, uh, code prompts, uh, what can testers do to help?

155
00:16:27,930 --> 00:16:31,130
Because that's going to slow you down because now you have bugs and you have rework

156
00:16:31,130 --> 00:16:36,650
and that, and your, and your cycles are slower while you get stuff fixed. Potentially. Uh,

157
00:16:37,210 --> 00:16:41,050
what does, and I have an answer for this, but I'm curious on yours first.

158
00:16:42,040 --> 00:16:46,920
What do, what do testers do in this environment? Do they just report the bugs and perform

159
00:16:46,920 --> 00:16:50,120
information, report information on the bugs to the stakeholders and call it good?

160
00:16:50,760 --> 00:17:01,820
No, no, it's right. It's, it's this, it's the same pattern, right? If we go, if we go to

161
00:17:02,860 --> 00:17:09,500
what's the definition of quality and what's the goal of a test, the definition, uh, as we call

162
00:17:09,500 --> 00:17:14,380
out, we don't know the definition of quality. It's based on the customer. But customer is the one

163
00:17:14,380 --> 00:17:24,280
that judges it. Um, and the testers job is, is to, to understand and help drive towards

164
00:17:24,280 --> 00:17:31,000
business impact, having them go back to sort of a traditional model, uh, finding bugs and all of

165
00:17:31,000 --> 00:17:39,180
that. No, that all still needs to fall into the role of, um, the developer in this. Yes.

166
00:17:39,820 --> 00:17:52,900
But that, that does not change ever. Correct. But the definition of quality does to some degree,

167
00:17:52,900 --> 00:18:03,720
because now we have the system in between, um, essentially making shit up and sometimes it's

168
00:18:03,720 --> 00:18:08,840
going to make shit up in a good way. And sometimes it's going to do it in a way we can't expect.

169
00:18:10,010 --> 00:18:22,900
Um, and so I think testers need to start training around how to identify the patterns around these

170
00:18:22,900 --> 00:18:30,260
problems. And the way, the way I see it is still on the quality coach, the quality coach angle,

171
00:18:30,260 --> 00:18:35,060
but now they're going to have to be a quality coach in a space that a lot of these folks

172
00:18:35,700 --> 00:18:43,130
may not have learned before. Right. And so that's where they need to be, uh, aggressively learning.

173
00:18:43,130 --> 00:18:50,810
Or as, as the, the fake Allen page bot said accelerate that learning, but then tie it to back

174
00:18:50,810 --> 00:18:59,050
to quality and then tie it back to the developer in a way that, that adds friction to the, the

175
00:18:59,050 --> 00:19:05,130
laziness concern. Let me build on that because you took my slow pitch and you, you get a nice,

176
00:19:05,850 --> 00:19:13,610
soft over the wall home run with it. So nice work. Uh, when we talk about a lot of the testers we see

177
00:19:13,610 --> 00:19:20,330
on LinkedIn, again, we have folks living in a world we're not in that much anymore. And they are a,

178
00:19:21,370 --> 00:19:27,930
in a role that's specialized to doing part of the development role. And that's, you're absolutely

179
00:19:27,930 --> 00:19:33,450
right to say that doesn't work. That doesn't help. Doesn't change. We need folks who, and again,

180
00:19:33,450 --> 00:19:38,810
it may not be considered a test role anymore is the issue. I think testers are exceptionally good

181
00:19:38,810 --> 00:19:45,770
at this quality coach testers are exceptionally good at in general at systems thinking and critical

182
00:19:45,770 --> 00:19:50,090
thinking. Although sometimes if I'll sign LinkedIn, maybe question that thought, but I'm going to,

183
00:19:50,090 --> 00:19:55,130
I'm going to stick on optimist Allen optimus, optimus Allen optimist, not optimus prime.

184
00:19:55,690 --> 00:19:59,850
Wow. I wonder if optimus prime was an optimist, but it doesn't seem like it,

185
00:19:59,930 --> 00:20:05,720
but that's a, that's a thread we don't have to go into. So here's where I think they can help.

186
00:20:05,720 --> 00:20:12,580
So again, going back to the AI angle, cause things are changing. So if I said they're good

187
00:20:12,580 --> 00:20:17,860
at systems thinking to a tester who is good at system thinking is going to be even better

188
00:20:17,860 --> 00:20:27,270
when assisted by jet AI, let me feed our entire code base into an LLM and ask and

189
00:20:28,150 --> 00:20:35,110
take some time asking the, I'm going to call it chat GPT, the, the, the LLM via whatever

190
00:20:35,110 --> 00:20:41,030
interface you want some questions about the code and how it works and, and areas of concern or

191
00:20:41,030 --> 00:20:45,030
impact. It's actually pretty good at code reviews, even if it can get some things wrong

192
00:20:45,030 --> 00:20:49,590
because it's just, it's doing some copying and pasting. If you will recall, and this is going

193
00:20:49,590 --> 00:20:54,230
to come up again at the very least in our end of year show, which is not that far away.

194
00:20:54,790 --> 00:20:59,990
Uh, but last year in my prediction episode, I predicted, and maybe I'm a year or two off on

195
00:20:59,990 --> 00:21:05,910
this. It hasn't quite made that turn yet that the ability to read code would be more important

196
00:21:05,910 --> 00:21:10,870
than the ability to write code. It's probably not going to happen this year, but you can see

197
00:21:11,860 --> 00:21:19,140
with what's happening with code pilot and co-pilot and pro and even asking chest GPT to write code,

198
00:21:19,140 --> 00:21:23,620
the ability to read it, understand it and critique it is more important than the ability

199
00:21:23,620 --> 00:21:28,840
to write it in the first place. Won't be true in every case. There's some things that, that

200
00:21:28,840 --> 00:21:34,440
the LLMs won't be able to help you with for now, but the ability to read that, understand,

201
00:21:34,440 --> 00:21:40,680
fit it into a system is great. I may have told the story before, but I'm going to tell it again.

202
00:21:40,680 --> 00:21:47,000
There is actually, this is, uh, I can mention it here. Uh, I think a lot of folks know I am on

203
00:21:47,000 --> 00:21:54,330
the board for a, uh, uh, not the board board, just an advisory board for a testing tool called

204
00:21:54,330 --> 00:22:01,290
autify. Autify started off as just another, yet another, uh, machine learning assisted you

205
00:22:01,290 --> 00:22:05,450
animation tool. Uh, we had some folks off from another company a while back. There's a bunch of

206
00:22:05,450 --> 00:22:10,810
these. They're all pretty good. And I think they're, if I was a developer today, I would

207
00:22:10,810 --> 00:22:16,250
10 times out of 10 use one of these tools over selenium. If I had to, if I had to have UI tests,

208
00:22:16,250 --> 00:22:22,490
I will, I will fight you on that and I'll win every time. Now, what a cool thing that

209
00:22:22,490 --> 00:22:27,290
autify showed me, and maybe it's not announced yet. Maybe I can't mess it. I'm going to say it anyway

210
00:22:27,290 --> 00:22:37,020
is the demo demo, demo where the demo where right now, but they took a design doc, a spec, fed it

211
00:22:37,020 --> 00:22:44,810
to an LLM, the LLM gave with an eye LLM. It gave them a list of test cases, which were editable

212
00:22:44,810 --> 00:22:48,730
case. They were wrong. And you know, for a model based testing, a lot of times we found

213
00:22:48,730 --> 00:22:54,810
if we created test cases based on the spec, which we did, uh, it was because the spec was wrong. So

214
00:22:55,450 --> 00:23:02,250
it's and specs are always wrong to some extent. Um, but all editable, so you could fix it. And

215
00:23:02,250 --> 00:23:08,730
from there it would could generate the playwright code for those tests. Super cool. I think it

216
00:23:08,730 --> 00:23:14,470
works backward and forward. It's the nugget of something cool, but God, why would you spend a

217
00:23:14,550 --> 00:23:20,890
bunch of time reviewing a big in test cases? Aren't testing. I agree with that part,

218
00:23:20,890 --> 00:23:26,360
but why would you spend a bunch of time, uh, looking at a spec, reading his back,

219
00:23:26,360 --> 00:23:29,480
vetting his bed, ask questions about a spec, writing some tests, you know,

220
00:23:29,480 --> 00:23:35,640
figuring out what tests are going to write automated or not. Um, and it just seems slow.

221
00:23:36,440 --> 00:23:42,280
So yeah. Uh, I think what it does to what tools like this will do, looping it back to

222
00:23:42,280 --> 00:23:47,320
the quality coach person and the role that like a lot of, you know, today's testers should be in

223
00:23:47,320 --> 00:23:55,060
in the future is figuring out how the team can use and not just cause their AI tools,

224
00:23:55,060 --> 00:24:01,060
but help the team use tools that help speed up their feedback loops. If I can write code

225
00:24:01,060 --> 00:24:05,780
and get the test for that code super fast and run those and get the results from those tests all in

226
00:24:06,420 --> 00:24:13,620
seconds for brand new code, that's pretty good. And as, as a quality coach, and that's how I'm

227
00:24:13,620 --> 00:24:17,860
going to help the people use these tools, understand when they should and shouldn't use them.

228
00:24:17,860 --> 00:24:23,940
I may even like, if it was me today, 30 years of software programming and, you know,

229
00:24:23,940 --> 00:24:31,060
a half a minute of working with LLMs, I would pair program with someone who was taking

230
00:24:31,060 --> 00:24:36,980
co-pilot prompts to help get a second set of eyes and code review on those things are blindly

231
00:24:36,980 --> 00:24:42,820
accepting because, you know, with, with, uh, uh, sorry for, I forgot to work for a second

232
00:24:42,820 --> 00:24:47,940
with pair programming, uh, one person at that 10,000 foot leo or one, that one person is deep

233
00:24:47,940 --> 00:24:51,780
into it. If that person deep into it was like, yep, looks good. But if I'm out there going,

234
00:24:51,780 --> 00:24:57,860
um, that's not going to work because of a B and C that's kind of cool. And that's going to help

235
00:24:57,860 --> 00:25:02,740
solve this problem. So why aren't we, my question to the survey that I don't have a link for

236
00:25:02,740 --> 00:25:07,700
this, this story I read about bugs coming. People are blindly accepting their co-pilot,

237
00:25:08,340 --> 00:25:15,260
uh, suggestions is why aren't they pair programming to that with that? Huh? Why not? Why not?

238
00:25:15,820 --> 00:25:20,980
Brent Brent's too busy asking you the scenario. Um,

239
00:25:26,710 --> 00:25:29,590
you know, we're working hard when you hear the keys clicking.

240
00:25:29,590 --> 00:25:35,180
Right. I didn't see, I didn't. All right. The.

241
00:25:40,360 --> 00:25:53,020
Yeah, that's a good idea. Yeah, I know. The, the thing that I was, I was listening to you for.

242
00:25:53,820 --> 00:26:06,600
Okay. And here's where I see, I, I'm trying to see it not as a sort of like, don't do that.

243
00:26:07,450 --> 00:26:11,530
Sorry. I was disciplining my cat who was trying to eat my mule near.

244
00:26:11,610 --> 00:26:14,010
Yep. Just punched him in the face. Her.

245
00:26:19,240 --> 00:26:25,240
So in your story, right? Hey, if you can do the code and you can do the tests and,

246
00:26:25,960 --> 00:26:35,750
and read them really quickly and, um, get code out in production in seconds, why wouldn't you do that?

247
00:26:36,460 --> 00:26:41,740
And the short answer is you would, you absolutely would. But here's the thing.

248
00:26:44,710 --> 00:26:51,770
How does that accelerate learning? To me, I'm like, that is not a, that's not accelerating learning.

249
00:26:52,490 --> 00:26:57,960
That's, that's you being a monkey now. It accelerates laziness.

250
00:26:57,960 --> 00:27:01,880
I forgot that in my gen AI, uh, generated response, but yeah,

251
00:27:02,440 --> 00:27:07,480
we should accelerate learning pair program and gets to that, but that's, um, I'm, I'm,

252
00:27:07,480 --> 00:27:12,420
I'm retro answering now. Right. But the pair programming. So here's the thing that I'm

253
00:27:12,420 --> 00:27:18,300
seeing it right now is that pair programming, why is that valuable? Right. Well, it's not valuable

254
00:27:18,300 --> 00:27:24,220
right now because you and me are old geezers and we know stuff and we know common ways that the

255
00:27:24,220 --> 00:27:33,160
GPT could be screwing it up right now. Right. But the thing is that's temporary. I don't know

256
00:27:33,160 --> 00:27:37,960
if you've seen that. It was, that was what I was just trying to type out the new version, um,

257
00:27:38,520 --> 00:27:45,960
uh, GPT, one of the new models. Um, you know, the, the, the chain of thought model.

258
00:27:46,810 --> 00:27:52,170
I do not. Okay. I mean, I know, actually, I know it completely, but it would be good to explain

259
00:27:52,170 --> 00:27:58,170
it to our listener. Chain of thought is essentially when you, when you do a chain of thought prompt,

260
00:27:58,170 --> 00:28:06,340
what you're doing is you're, you're giving it a clue around how to work the problem,

261
00:28:06,340 --> 00:28:13,460
how to break it, how to decompose it into smaller parts. Okay. It does that now. It doesn't

262
00:28:13,460 --> 00:28:23,430
automatically. I'll see if I can find, um, why is it not letting me scroll? Where the hell is this

263
00:28:23,430 --> 00:28:36,390
roll bar? Okay. It's cause you're using edge and edge sucks. Okay. You see right here. Yeah.

264
00:28:36,470 --> 00:28:46,660
Okay. So on the chat GPT, oh one dash mini model. Okay. I asked it a prompt, uh, to create, uh, three

265
00:28:46,660 --> 00:28:54,740
pieces of PowerShell code, a code that will reproduce the problem test code that validates it.

266
00:28:54,740 --> 00:29:02,360
And then code that fixes it. Okay. Interesting. Interesting. Okay. Um, but right here, what I'm

267
00:29:02,360 --> 00:29:08,680
showing Alan right now is a new prompt version that says thought for four seconds. That should,

268
00:29:08,680 --> 00:29:15,910
that thought should be an air quotes, but go on. Yeah. And now I expanded it and showed Alan what

269
00:29:15,910 --> 00:29:25,500
it's doing. Okay. And what it did is this thinking for four seconds is it generating its

270
00:29:25,500 --> 00:29:31,100
own chain of thought prompt. I see that. Yeah. It's, it's solving it. It's taking a problem.

271
00:29:31,100 --> 00:29:35,980
Like here's what I tell my team to do all the time. Take the big problem, break it into solvable

272
00:29:35,980 --> 00:29:45,140
steps. And Chad GPT is showing that's exactly what it did. But it's, it's, it's not what it did.

273
00:29:46,180 --> 00:29:52,740
It basically created that plan and then it executed that plan. Yeah. Okay. And I'm going

274
00:29:52,740 --> 00:30:01,610
to tell you in terms of what I said in terms of what I asked it to do, um, it did it really

275
00:30:01,610 --> 00:30:07,500
goddamn well. Okay. And here's the thing, cause I've been in this AI business now for 10 years.

276
00:30:08,460 --> 00:30:15,340
The first thing you do is you make your AI transparent because everyone is suspicious and

277
00:30:15,340 --> 00:30:20,700
they learn about, right. And then eventually people are like, yeah, it's good enough. Like,

278
00:30:20,700 --> 00:30:26,380
yeah, that's some bugs, but no one complained or we worked through them when they did complain.

279
00:30:27,100 --> 00:30:38,420
And then it's just, it's just this, right? My coding then becomes me writing, what do you think?

280
00:30:39,140 --> 00:30:45,540
50 word instruction. Yeah. Yeah. So what, what's, I'm going to interrupt for a second. I do want to

281
00:30:45,540 --> 00:30:53,380
go in and see the answer, but going back to the answer, our original question is it's interesting

282
00:30:53,380 --> 00:30:58,860
because what I am, and this is something you've talked about a lot on the podcast. This is where

283
00:30:58,860 --> 00:31:05,900
I think we can help accelerate learning on dev teams is one of my big gripes with gen AI is like

284
00:31:06,540 --> 00:31:10,780
all the people who they just don't understand how it can help them solve the problem.

285
00:31:11,510 --> 00:31:17,350
Like, like you, like the credit to you is you inherently knew this is probably a question

286
00:31:17,910 --> 00:31:22,470
that gen AI can help me solve. And it did it, it didn't know where that delighted you,

287
00:31:22,470 --> 00:31:29,530
which is great. I think a lot of folks, to be clear, delighted part of me scared the crap out

288
00:31:29,530 --> 00:31:34,810
of the other side. All right. Fair enough. Fair enough. But what I see from the internet,

289
00:31:34,810 --> 00:31:39,930
you saw the thread two weeks ago, three weeks ago on the people all freaked out how bad LLMs were

290
00:31:39,930 --> 00:31:45,850
because they couldn't count the letter number of hours and strawberry, uh, super dumb. Yeah. But

291
00:31:45,850 --> 00:31:58,460
one of the key, like the key to knowledge worker success in the future is understanding when, and

292
00:31:58,460 --> 00:32:08,020
when not a LLM gen AI can help you solve the problem and then giving it the right prompt

293
00:32:08,020 --> 00:32:12,340
to solve that problem for you. I don't want, I don't even want to talk about prompt engineering,

294
00:32:12,340 --> 00:32:20,570
but it's like I get praised a lot for my Google Fu, uh, because I can, I, my wife or somebody,

295
00:32:20,570 --> 00:32:23,210
she's actually pretty good at it too, but someone will search something in the internet and say,

296
00:32:23,210 --> 00:32:29,050
I can't find anything. I can find the right words in duck duck go or Google to, to find what I'm

297
00:32:29,050 --> 00:32:36,170
looking for via search. Uh, it's a skill and it kind of, uh, yeah, Brad's showing it this broken

298
00:32:36,170 --> 00:32:41,320
too. Um, we're going to fix this in a second. Uh, people don't know what we're talking about,

299
00:32:41,400 --> 00:32:47,240
but the ability to understand, oh, this is a problem that LLM can solve, or this is a problem

300
00:32:47,240 --> 00:32:53,880
that LLM can't solve, uh, is critical. And then the coming up with the prompt is almost,

301
00:32:53,880 --> 00:32:59,000
I think, I think actually, um, all I'm just going to get more forgiving on that. So maybe

302
00:32:59,000 --> 00:33:01,960
it's just the first part, maybe figuring out this is a problem that can be solved. This is

303
00:33:01,960 --> 00:33:07,560
a problem that can't, so Brent has asked, um, he has done the question in chat video, many,

304
00:33:07,640 --> 00:33:14,600
and he's asked how many Rs are in strawberry and it says two, which is incorrect. And it's

305
00:33:14,600 --> 00:33:18,040
even confident as you can see the letter R appears twice in strawberry. Now try this, Brent,

306
00:33:18,040 --> 00:33:24,360
try this, ask it to write Python code to count the number of Rs in the, in the word strawberry.

307
00:33:24,360 --> 00:33:28,760
Okay. Uh, I'll do that. You talk.

308
00:33:29,320 --> 00:33:34,680
Okay. And what should happen here again, because again, this is people just don't take

309
00:33:34,680 --> 00:33:40,600
the time to understand the LLMs look for, they have looked at such a wide body of text.

310
00:33:41,530 --> 00:33:45,770
They don't know what they're saying. They're putting the words together in a way that makes

311
00:33:45,770 --> 00:33:52,660
sense based on the gazillions of words they've looked at. Now there's no story books. There's

312
00:33:52,660 --> 00:33:59,700
no research papers written about, uh, written about how many Rs are in the word strawberry,

313
00:33:59,780 --> 00:34:06,020
but they are really good at writing code. So Brent wrote exactly what asked him to write

314
00:34:06,020 --> 00:34:11,380
Python code to count the number of Rs in strawberry. It says thought for four seconds,

315
00:34:12,300 --> 00:34:17,800
examining the count. He says, let me see. I'm identifying three Rs in strawberry positions,

316
00:34:17,800 --> 00:34:22,360
three, seven, and eight. Contrast it didn't even know. We didn't even say you're wrong.

317
00:34:23,080 --> 00:34:27,800
So just write some code for this. And it automatically got the right answer because

318
00:34:27,800 --> 00:34:32,600
it knows code and it can fit and it, and it can get the context right. It goes,

319
00:34:32,600 --> 00:34:38,520
Oh, the question I get it. And it's just understanding how they work. You can make

320
00:34:38,520 --> 00:34:42,440
them behave in the right way. And there was a five or Friday post like two months ago,

321
00:34:43,000 --> 00:34:49,450
where there's an actual, someone wrote a nice little tutorial where you had to get an LLM

322
00:34:49,450 --> 00:34:53,450
to give some answers. And the goal was to give it the right prompt, understand enough,

323
00:34:53,450 --> 00:34:59,210
what was going on. It was a good little, almost a capture the flag on prompt engineering. Again,

324
00:34:59,210 --> 00:35:03,530
I hate that word. But anyway, the code doesn't matter. The fact that the write the code makes

325
00:35:03,530 --> 00:35:07,850
it understand what it did wrong, which I think is fantastic. It's a little scary that it learns

326
00:35:07,850 --> 00:35:12,970
like that air quote learns like that. But so that's, that's that. Okay. We went on a tangent

327
00:35:12,970 --> 00:35:21,770
and we export it deeply. Let's pop the stack. I am saying that the ability to understand when

328
00:35:21,770 --> 00:35:26,570
and when not to use an LLM is one of the key skills of knowledge workers in the future. Fight me.

329
00:35:34,090 --> 00:35:38,250
I do think that's going. So yeah, in terms of accelerating learning,

330
00:35:38,890 --> 00:35:44,170
that's what you need to accelerate learning in for sure. Right. Because

331
00:35:46,780 --> 00:35:54,060
like I look at this thing, like Alan, Alan neglected to point out that yes, indeed,

332
00:35:55,020 --> 00:36:00,220
it wrote Python code that would 100% generate the correct answer.

333
00:36:02,540 --> 00:36:11,420
Yeah, of course. Yeah, that was. I've while we do see bugs in in in LLM generated code,

334
00:36:11,420 --> 00:36:15,030
not usually on simple things. Let me just double check.

335
00:36:17,350 --> 00:36:19,670
Python string class.

336
00:36:20,460 --> 00:36:26,870
Oh, see, there's a better way to do it.

337
00:36:27,770 --> 00:36:33,290
Now. So the only thing I have seen it do, particularly with with Python,

338
00:36:34,230 --> 00:36:39,110
is that it will sometimes invent interfaces that don't exist.

339
00:36:39,110 --> 00:36:46,870
Oh, interesting methods. In this case, right string is going to be a common class that is used.

340
00:36:47,670 --> 00:36:53,910
And so yeah, it does indeed have a count function. So yeah, it generated it.

341
00:36:54,550 --> 00:36:57,510
Every method exists if you have the right libraries installed.

342
00:36:58,310 --> 00:37:09,640
Right. No, but some of the problems like Azure Data Explorer or what used to be known as

343
00:37:09,640 --> 00:37:13,480
Cousteau. I don't know if you ever had experience. I do remember. Oh my god,

344
00:37:13,560 --> 00:37:18,520
the blast from the past. Yeah, no, it's it's alive and well, and it's awesome.

345
00:37:20,740 --> 00:37:30,230
You can get this to like, I'll do it now. Right? It is. And what I do is I give commentary while

346
00:37:30,230 --> 00:37:35,430
Brent's typing in a chat GPT because this this is the podcast you pay for.

347
00:37:36,310 --> 00:37:44,120
It worth every penny of your subscription saying, write Cousteau code to count the number of

348
00:37:44,120 --> 00:37:50,870
ours in strawberry. Is it going to know what Cousteau is? Yeah. Was Cousteau, was that ever

349
00:37:50,870 --> 00:37:56,860
external? Multiple things released with that. I mean, its formal name is is.

350
00:37:57,900 --> 00:38:05,000
Oh, I can't wait for this. Data Explorer. Right. So yeah. Oh my god, I recognize that

351
00:38:05,080 --> 00:38:12,280
code. Oh my god. Let's see. Yeah. So what it did. So walking through it first created a variable

352
00:38:13,240 --> 00:38:18,440
and you didn't call out, but I completely murdered my spelling of strawberry.

353
00:38:19,400 --> 00:38:23,000
No, you just added a backslash at the end. Extra Y and a backslash. And no,

354
00:38:23,000 --> 00:38:30,360
and I actually two Rs. Yeah. Okay. All right. But it wrote the code, did the right spelling of

355
00:38:30,360 --> 00:38:36,920
strawberry, converted it to lowercase, then figured out the string length of it,

356
00:38:38,040 --> 00:38:46,120
then removed all Rs from that string and then counted that string length and then did the delta.

357
00:38:46,980 --> 00:38:53,110
Well, that's an interesting way to do it. But if you don't have like a counting function or a way

358
00:38:53,110 --> 00:38:58,870
to index it, this is an old. You're not going to, you're not going to loop in. So Cousteau,

359
00:38:58,870 --> 00:39:04,620
by the way, if you ever use Cousteau, used to use it at Microsoft, it seems like a million

360
00:39:04,620 --> 00:39:10,860
years ago, probably, you know, eight years ago, just a query language for looking at usually

361
00:39:10,860 --> 00:39:18,620
analytics data. Okay. But now here is why I brought in Cousteau. Okay. So god, I hope there was a reason.

362
00:39:18,620 --> 00:39:27,540
There is. So this function, do you recognize that function? I can't see your pointer. Oh,

363
00:39:27,540 --> 00:39:33,700
Stirlin. Yes, I do. Okay. Where does it come from? C. It comes from C. What about this one?

364
00:39:35,780 --> 00:39:40,820
That's not a C function. I don't know what it is. That's a Python function. Replace. Yeah. Okay.

365
00:39:40,820 --> 00:39:45,540
Okay. So the problem with the Cousteau language is that the developers of it

366
00:39:46,660 --> 00:39:54,580
pick and chose things, names for things that already existed. Okay. So when you do,

367
00:39:54,580 --> 00:39:59,940
there's certain ways you can ask it to do something in Cousteau. You go and say, okay,

368
00:40:00,660 --> 00:40:06,500
create me a thing that does this in Cousteau. It will often invent things that don't exist

369
00:40:07,690 --> 00:40:15,540
because they do exist in other languages. And because Gen AI is nothing more than a probabilistic

370
00:40:15,540 --> 00:40:22,820
thing, it knows, hey, this, this fake, or this function that I want to use here, I know what

371
00:40:22,820 --> 00:40:29,220
comes next. And it has, it has lost the fact that it doesn't work in Cousteau. I get it. So

372
00:40:29,220 --> 00:40:33,940
you picked a more obscure language with, with attributes like this, because it's more apt to

373
00:40:33,940 --> 00:40:42,820
make errors. Right. Right. Now, as it, Oh, and by the way, I just scroll down. It not only gave

374
00:40:42,820 --> 00:40:53,660
me one way of doing it. It gave me two, three, four. I would have honestly, I would have done four.

375
00:40:54,060 --> 00:41:09,510
A regex. The last one. Yeah. Yeah. Um, yeah. This one says count if lower word matches regex.

376
00:41:11,020 --> 00:41:19,220
Okay. So this fourth one would have been broken. Um, because this is a single Boolean condition

377
00:41:21,770 --> 00:41:27,000
and there's only one word. So it would have returned. This one would return a one. Um,

378
00:41:28,280 --> 00:41:33,640
but there is a way in Cousteau, they picked the wrong function. There is a way in Cousteau where

379
00:41:33,640 --> 00:41:38,120
you could use regex and then you would count the number of groups. That one would not work.

380
00:41:38,920 --> 00:41:46,100
Okay. So what's the, what's the main point here? Well, so I like, like you connecting the dots.

381
00:41:46,980 --> 00:41:56,580
It's like, yeah, you need to accelerate learning, but your, your counter, it may, you may not need,

382
00:41:56,660 --> 00:42:03,420
you may not need to be accelerating learning in what you think you need to be accelerating learning.

383
00:42:03,420 --> 00:42:09,100
Yes. Yeah. And it goes back to the other question on, which is based around what AI tools should

384
00:42:09,100 --> 00:42:17,220
do use. And the answer is not until you have to, I mean, they're not magic and you're there,

385
00:42:17,220 --> 00:42:21,060
and there are some that are going to help you. Of course, Gen AI for first, it's all the

386
00:42:22,780 --> 00:42:27,260
air quote AI power tools. It's the new dot net that I'm a little afraid of.

387
00:42:27,900 --> 00:42:36,980
I think actually in the top concept of AI, I think you're now world famous technology

388
00:42:36,980 --> 00:42:41,980
might need to be updated, which is what automation should be right.

389
00:42:41,980 --> 00:42:45,900
Alan. We should automate all the tests that should be automated.

390
00:42:45,900 --> 00:42:50,820
Okay. So what AI should we be using? All the AI that we should.

391
00:42:51,380 --> 00:42:54,260
Right. And no more, no less.

392
00:42:54,260 --> 00:42:56,540
Yeah. Oh God.

393
00:42:57,100 --> 00:43:04,380
Have I ever, so I'm okay on, we've got a few minutes here, but that reminds me,

394
00:43:04,380 --> 00:43:07,180
I never shared these, but I do have a list of the weasel laws.

395
00:43:08,090 --> 00:43:13,110
Okay. Just in case I ever need to refer, am I old and I forget things. So I'm just going to

396
00:43:13,110 --> 00:43:17,670
share these as a bonus for our listeners. And I think you've heard all of these. These are

397
00:43:17,670 --> 00:43:21,670
all things I say a lot of times that you're, you've said, and I've stolen them.

398
00:43:22,380 --> 00:43:23,820
Oh, I would love to have that.

399
00:43:24,540 --> 00:43:29,980
Um, you should automate 100% of the tests that should be automated. Weasel law number one,

400
00:43:30,540 --> 00:43:35,740
weasel law number two, the answer to any reasonably complex question is it depends.

401
00:43:37,610 --> 00:43:41,690
Number three, code coverage is a wonderful tool that a horrible metric.

402
00:43:41,690 --> 00:43:42,570
Yep.

403
00:43:42,570 --> 00:43:47,290
Number four, the more widespread a term is the less it holds to its original purpose.

404
00:43:48,310 --> 00:43:49,430
Case in point agile.

405
00:43:50,540 --> 00:43:51,040
Right.

406
00:43:51,800 --> 00:43:56,440
Weasel law number five, you can change your manager or you can change your manager or the

407
00:43:56,440 --> 00:44:00,280
version we also use, you can change your org or you can change your org.

408
00:44:00,280 --> 00:44:01,160
Yes.

409
00:44:01,160 --> 00:44:05,480
And weasel law number six, which, um, this is the newest one. They've come in order of

410
00:44:05,480 --> 00:44:09,720
having using them. You are not nearly as much of a snowflake as you think you are.

411
00:44:13,790 --> 00:44:19,150
Is that a law or is that, is that officially Alan entering into geezer hood?

412
00:44:19,790 --> 00:44:20,270
No, but

413
00:44:20,990 --> 00:44:21,950
I'm stupid.

414
00:44:22,670 --> 00:44:24,510
No, no, it's not that it's like, well,

415
00:44:25,790 --> 00:44:26,670
Brent, Jenny, I,

416
00:44:27,870 --> 00:44:31,790
well, it goes, it goes right to the software testers. You know,

417
00:44:31,790 --> 00:44:37,390
is it Jenny? I, it seems like a really cool advanced technology, but for the kind of testing

418
00:44:37,390 --> 00:44:38,990
I do, it's not really going to help.

419
00:44:40,730 --> 00:44:42,250
Right. No, that.

420
00:44:42,810 --> 00:44:47,210
And like, uh, you are not as much of a snowflake as you think you are.

421
00:44:47,210 --> 00:44:57,760
Actually. Yeah, no. Yeah. Your scenario. It reminds me of, of, of the three principles

422
00:44:57,760 --> 00:45:04,480
from, uh, how to measure anything, which of which I'm forgetting. He has three key principles.

423
00:45:04,480 --> 00:45:10,830
I'm nearly and the book is usually right here. I don't know what I did with it.

424
00:45:10,830 --> 00:45:12,990
It's one of the books I keep an arm distance.

425
00:45:13,630 --> 00:45:15,870
Mine I do as well.

426
00:45:15,870 --> 00:45:17,870
Oh, I know what you're talking. Yeah. Yeah.

427
00:45:17,950 --> 00:45:21,870
I know what you're talking about and I have them written down somewhere.

428
00:45:22,780 --> 00:45:24,940
God, where is the book? Find yours.

429
00:45:26,970 --> 00:45:30,090
This is what we do here. This is what we do.

430
00:45:32,170 --> 00:45:36,730
Uh, God, it's, um, it's like you, it's, there's three things around data.

431
00:45:37,560 --> 00:45:39,240
You probably already have enough data.

432
00:45:40,430 --> 00:45:44,620
You probably, or something that you, that's not it. Those aren't it.

433
00:45:47,320 --> 00:45:48,680
Those aren't the right ones.

434
00:45:48,680 --> 00:45:52,440
I know. I'm, let me, let me ask the better search engine.

435
00:45:52,440 --> 00:45:56,920
You have just a second. I'll see if I can find it for you to this.

436
00:45:56,920 --> 00:45:58,920
Now this is compelling podcasting.

437
00:45:59,560 --> 00:46:00,280
Yeah.

438
00:46:00,280 --> 00:46:03,640
This is really, and I'm gonna, I have a different way of searching for it.

439
00:46:03,640 --> 00:46:04,600
I'm gonna see if it works.

440
00:46:05,480 --> 00:46:08,040
Um, I got it. I win.

441
00:46:09,310 --> 00:46:16,890
No, no, that's it. You have it. I was going to accept, put this in my blog once, uh,

442
00:46:16,890 --> 00:46:21,370
the three, you have more data than you think you need less data than you think.

443
00:46:21,450 --> 00:46:25,610
And adequate, adequate amount of new data is more accessible than you think.

444
00:46:25,610 --> 00:46:30,490
And I remember even in a metrics course, uh, I did a taught at Microsoft a long time ago.

445
00:46:30,490 --> 00:46:35,720
We talked about these because yeah, people want to measure everything and,

446
00:46:35,720 --> 00:46:40,570
and see if any magic comes out, which anyway, you have more data than you think,

447
00:46:40,570 --> 00:46:44,970
which is true. You need less data than you think. Also very true.

448
00:46:45,850 --> 00:46:50,570
Adequate amount of new data is like getting, getting the new data you need to answer it

449
00:46:50,570 --> 00:46:53,530
like some nuance of a question. That's probably pretty easy too.

450
00:46:55,080 --> 00:47:02,640
The, the, and I, I'm realizing I may need to go back and reread this,

451
00:47:02,640 --> 00:47:07,520
his book to reenergize it and tie the context to, to current.

452
00:47:07,520 --> 00:47:08,480
Yeah.

453
00:47:08,480 --> 00:47:11,040
Because I look at this and I'm like, yeah, he's right on.

454
00:47:11,600 --> 00:47:17,200
Right. The, the, um, you need less data than you think.

455
00:47:18,320 --> 00:47:26,780
Right here, he's inspiring. Um, Hey, spend a little extra time thinking about

456
00:47:27,580 --> 00:47:33,340
what's the decision you're trying to make and do you really need to, to do this?

457
00:47:33,340 --> 00:47:39,980
And I'll, I'll tell you. So for example, I do a lot of A B testing, um, things for people.

458
00:47:41,550 --> 00:47:43,710
That's what we do here. A B testing.

459
00:47:43,790 --> 00:47:48,910
And sometimes the scenario that they're trying to validate is so infrequent.

460
00:47:49,630 --> 00:47:54,830
Like, Hey, if we do this, it'll stop this bug that happens. And the bug only happens at Wednesday at

461
00:47:54,830 --> 00:48:00,510
midnight and only in a random region. Right. It's a rare bug, right? How do you get the,

462
00:48:00,510 --> 00:48:09,920
the sample set enough to, to, to do an A B test, get statistical significance

463
00:48:09,920 --> 00:48:17,360
to a degree that you can use the rules of data science and bless it. Right. And the answer is,

464
00:48:17,360 --> 00:48:23,040
is you let it run for F and ever because of the smaller number of sample size, the more you have

465
00:48:23,040 --> 00:48:28,240
to let it run. But if you don't have the time to let it run, when you look at it, you measure it a

466
00:48:28,240 --> 00:48:35,360
cup, two, three times you go, okay, is it trending? Do we, do we see any evidence of having,

467
00:48:35,360 --> 00:48:41,200
right? You go, all right. You put what I often end up doing is I tell them what it says.

468
00:48:42,160 --> 00:48:50,320
However, I also say we can measure sort of the probabilistic angle around how we're seeing these

469
00:48:50,320 --> 00:48:58,250
results land. And we can then kind of accelerate, um, what will probably be our decision. If we let

470
00:48:58,250 --> 00:49:05,370
this run, it is a risk. It'll be wrong, but I can now measure the distribution and from that

471
00:49:05,370 --> 00:49:10,650
distribution, I can run through simulation and go, yeah, this is probably heading in the direction

472
00:49:10,650 --> 00:49:17,160
where it will not pass significance or it will pass. Yeah. Well, you've, you've applied critical

473
00:49:17,160 --> 00:49:24,080
thinking to data analysis. Right. I mean, no, I mean, I mean, it's, this is knowledge work and I,

474
00:49:25,040 --> 00:49:30,720
and it goes back to the, you know, testers, blah, blah, blah. It's, uh, it's, it's all knowledge

475
00:49:30,720 --> 00:49:39,390
work and not to minimize what testing development, lawyering, doctoring is we do our job based on

476
00:49:39,390 --> 00:49:44,430
the knowledge and context we have. And we focus on continuous improvements. So we need to adapt

477
00:49:44,430 --> 00:49:51,950
and do that work better. Uh, AI can help accelerate the learning. We need to do that.

478
00:49:51,950 --> 00:50:00,440
End of story. Agreed. All right. Let's, um, let's call that good, man. That wasn't a bad question.

479
00:50:00,440 --> 00:50:05,320
We're not going to do this every week. Sometimes we'll think of our own stuff, but in a pinch,

480
00:50:06,200 --> 00:50:13,310
not too bad, not too bad. And yeah, we need to make sure. Um, one of the things we need to talk

481
00:50:13,310 --> 00:50:20,270
to Jason about is how he gets, how he updates this thing with the transcript or podcasts.

482
00:50:20,670 --> 00:50:24,270
I don't know if our transcripts make it in. Let's figure that out. But, uh,

483
00:50:24,270 --> 00:50:29,150
no, no, no, we, we just need to, you know, tell them to make that happen. Yeah. Right. Because

484
00:50:29,150 --> 00:50:33,150
in a couple of months when we asked, I think we're doing that right now. Cause Jason,

485
00:50:33,150 --> 00:50:39,230
listen, Jason make your completely free, completely free to us all effort on your

486
00:50:39,230 --> 00:50:43,630
thing. Could you please put a whole bunch of effort into it to make your free thing more

487
00:50:43,630 --> 00:50:49,070
valuable to Brent and I. Right. So that next week, next week, we can ask a new question

488
00:50:49,070 --> 00:50:55,630
and it'll be a new one that a repeat. And can you sample our voices and just get like,

489
00:50:55,630 --> 00:51:05,290
do we have to be here for this? Uh, that would maybe not, maybe not. That's can I just go to

490
00:51:05,290 --> 00:51:11,770
chat cheapy to say, Hey, please post a new AB testing podcast episode two, two 11 on this date.

491
00:51:11,770 --> 00:51:19,020
And it just shows up and like, why not? Why not? We're not that far away. Please.

492
00:51:25,180 --> 00:51:29,340
All right. No, we're not there yet. It's not going to work. It's going to,

493
00:51:29,340 --> 00:51:33,260
but I can't wait to see what sort of confidently incorrect answer it gives.

494
00:51:35,120 --> 00:51:37,600
This is going to be great. Fred is asking it. Please post episode two,

495
00:51:37,600 --> 00:51:43,040
let the AB testing podcast by Sunday, October 6th. Um, it's now searching for that term.

496
00:51:43,040 --> 00:51:48,400
It's browed. It's browsing podcast addict. Unfortunately, that episode hasn't been released

497
00:51:48,400 --> 00:51:55,660
yet. So it, it gave you an answer that was correct. It's like a political debate when it didn't answer

498
00:51:55,660 --> 00:52:02,730
the question you asked. Right. We're done. Brent, Brent, no, Brent's going to be insistent.

499
00:52:02,730 --> 00:52:08,570
I want you to generate it and post it. Oh my God. This is the worst podcast, but you know what?

500
00:52:08,570 --> 00:52:14,330
It's probably better than AI for now. It does not have the capability to generate or post content.

501
00:52:15,360 --> 00:52:21,500
So, uh, oh, great. Now we have a sample script.

502
00:52:25,020 --> 00:52:27,420
Scroll down a little bit. We can act out a little bit. Then we gotta go.

503
00:52:28,540 --> 00:52:31,340
So wait, go back, go up, go up, go up. We're going to talk. We're just going to do a little

504
00:52:31,340 --> 00:52:36,620
bit. We got over our lines here. So Brent, let's kick things off with AI again, but this time

505
00:52:36,700 --> 00:52:41,500
let's take a closer look at how it's truly changing the way we approach quality beyond the hype.

506
00:52:42,460 --> 00:52:47,980
Right. We've talked about AI plenty, but there's something critical here. The role of data

507
00:52:47,980 --> 00:52:55,420
quality and bias in AI driven testing tools. AI is not the magic bullet. It's a magnifier

508
00:52:55,420 --> 00:53:01,180
of your data's quality. Then we have a discussion. And then I say exactly. And Jason Arbin has been

509
00:53:01,180 --> 00:53:07,180
talking about how biases creep into AI testing systems. He emphasizes that we can't eliminate

510
00:53:07,180 --> 00:53:14,060
bias entirely, but can minimize harmful biases in our data and models. This is huge for testers.

511
00:53:14,060 --> 00:53:20,060
All right. There's your preview of episode two 11. I'm Alan. I'm Brad. We're out of here.

512
00:53:20,060 --> 00:53:36,700
Did we just change our podcast to ABC testing?

