1
00:00:11,060 --> 00:00:15,780
Howdy, I'm Alan. I'm Brent. And we're here for another episode of EBTesting. Which number

2
00:00:15,780 --> 00:00:23,500
are we Brent? 67. 67? Awesome. I'm very happy about that. I don't know why. I'm just happy.

3
00:00:23,500 --> 00:00:28,700
The numbers get bigger and I'm still alive. Yeah. So, as I mentioned to you while I was

4
00:00:28,700 --> 00:00:34,180
plugging in the microphones, I went to PAX, the Penny Arcade Expo, this weekend. It was

5
00:00:34,180 --> 00:00:40,100
actually really awesome. Lots of great cosplay. Some fantastic, like six foot, seven inch

6
00:00:40,100 --> 00:00:47,180
tall orcs. Did you cosplay up? No, I did not. I played as... That's too bad. I went as the

7
00:00:47,180 --> 00:00:53,650
51 year old gamer. You know what? I don't think that's abnormal anymore. I borrowed my

8
00:00:53,650 --> 00:00:59,330
kids' Rocket League shirt. The great thing is, you are almost of the age of me and they

9
00:00:59,330 --> 00:01:05,530
have... I am nowhere close to the age of you. And in the hall upstairs... I gotta live

10
00:01:05,530 --> 00:01:13,090
10% more of my life. In the hall on the top floor, they had some retro arcade games.

11
00:01:13,090 --> 00:01:20,050
Nice. And I... This old guy still got it. I set the top... The fourth highest score.

12
00:01:20,050 --> 00:01:26,650
This is on Monday, mind you. Fourth highest score on Tempest. Nice. The reflexes came

13
00:01:26,650 --> 00:01:34,780
back, had people watching over my shoulder. I felt pretty good, just like at the arcade.

14
00:01:34,780 --> 00:01:40,860
Just like it was a long, long time ago. You had the cassette player with Rush playing.

15
00:01:40,860 --> 00:01:52,010
Yeah. I went to the Infinity Mirrors exhibition at Seattle Art Museum afterwards.

16
00:01:52,010 --> 00:01:58,560
Before we do the podcast, anything new and exciting with you? We went to the

17
00:01:58,560 --> 00:02:04,240
Evergreen State Fair yesterday. Alright. That's in Monroe, Washington. All the

18
00:02:04,240 --> 00:02:12,440
way up in Monroe. My daughter had a blast. My 15 year old boy, he just wanted to do the

19
00:02:12,440 --> 00:02:27,480
rides and the carnival games. Dude, 10 tickets, 10 bucks. One game of throwing six balls at the

20
00:02:27,480 --> 00:02:35,470
iron milk bottles, 10 bucks. I'm like, whoa. When you look at the thing and you go,

21
00:02:35,470 --> 00:02:40,830
10 tickets. Oh, 10 tickets. That's what? $2? No.

22
00:02:40,830 --> 00:02:45,280
Evergreen State Fair funding Monroe for the next 12 months.

23
00:02:45,280 --> 00:02:51,680
Yeah. The one thing that we did discover though, we were shopping for a new kitchen table,

24
00:02:53,220 --> 00:03:00,820
our new dining room table. Oh, yeah. And in one of the exhibition rooms was a full-length pool

25
00:03:00,820 --> 00:03:07,220
table that had leaves that you can cover up the pool table and turn it into a kitchen table. And

26
00:03:07,220 --> 00:03:13,660
it was really nice. That's a smart move. And I'm like, I got to look at that. And I also,

27
00:03:13,660 --> 00:03:18,060
the one thing though is it had four leaves that you put on it. And I'm like, okay,

28
00:03:18,060 --> 00:03:23,340
if someone spills on this, then I, then as it go through the cracks and destroy my felt,

29
00:03:24,450 --> 00:03:31,380
don't know. Anyway, it was nice. Cool. Totally. We're here in the world to discover these things

30
00:03:31,380 --> 00:03:37,470
and share them with our three listeners. Further proving why we have three listeners.

31
00:03:38,610 --> 00:03:43,250
Exactly. Shall we start the podcast? Sure. All right. We had something special today.

32
00:03:44,030 --> 00:03:47,410
Do you want to describe it? No.

33
00:03:48,930 --> 00:03:53,970
Do you want me to try? Go ahead. So we have talked on this show very much about what we're

34
00:03:53,970 --> 00:04:00,210
calling modern testing, testing in the agile world. And as it's funny, as I write more about

35
00:04:00,210 --> 00:04:04,130
it and tweet more about it, I see it's not just me. It's not just Brett. This is a sort of testing

36
00:04:04,130 --> 00:04:08,610
and changes in testing that are happening everywhere, yet everywhere it's happening.

37
00:04:08,610 --> 00:04:16,320
There is a counter example of somewhere where the 1990s mentality test manager is apparent.

38
00:04:16,960 --> 00:04:24,880
So I was thinking, we were thinking that me as a manager of a modern testing team here in 2017

39
00:04:24,880 --> 00:04:31,520
and Brent, who happened to actually manage a 1990s test team. Yep. We thought it would be

40
00:04:31,520 --> 00:04:37,040
interesting to have a time travel discussion between a, what should I call you besides a

41
00:04:37,040 --> 00:04:44,140
90s test manager? I prefer to be called a traditional test manager. Okay. So we're going

42
00:04:44,140 --> 00:04:49,580
to have a conversation here through the magic of AB testing podcast and the technology we have put

43
00:04:49,580 --> 00:04:55,180
in place to have a conversation between a traditional test manager and a modern test

44
00:04:55,180 --> 00:05:01,660
manager. And I'm going to play the role of the modern test manager. And Brent, if you remember

45
00:05:01,660 --> 00:05:09,020
him, uh, we'll play the role of the traditional test manager. This should be interesting. It should

46
00:05:09,020 --> 00:05:14,990
be. It's something that would traditionally require rehearsal or preparation, but no, welcome. Yeah.

47
00:05:14,990 --> 00:05:25,470
AB testing. That's why we only have three listeners. So Brent, can you give me a summary,

48
00:05:25,470 --> 00:05:33,010
like maybe the elevator pitch of what you do as a test manager? As a test manager, it's my job

49
00:05:33,890 --> 00:05:42,930
to make sure that the product goes out bug free. So I manage a team of STEs and a manager,

50
00:05:42,930 --> 00:05:49,700
as well as a team of S, S debts whose job it is is once, once devs done with their code,

51
00:05:50,820 --> 00:05:54,820
we test the crap out of it and make sure that it's good enough to ship.

52
00:05:56,110 --> 00:06:05,070
So actually my job is to sign off. My role as a test manager is I manage a community of testers.

53
00:06:05,070 --> 00:06:10,110
So all of my test team, STEs and Estes, they're really just test specialists and test and

54
00:06:10,110 --> 00:06:14,350
quality specialists who are embedded in the dev teams from the very beginning. And they work

55
00:06:14,350 --> 00:06:20,270
together to ship a high quality product. My, my role is really much more of a test coach

56
00:06:20,910 --> 00:06:25,870
than of a day to day test manager. I'm in charge of their career growth and I'm in charge of

57
00:06:25,870 --> 00:06:29,950
vision and philosophy for the team. But most of the time I just give them a framework and

58
00:06:29,950 --> 00:06:37,440
get out of their way until I need. So how do you manage the growth of, of a test tester

59
00:06:37,440 --> 00:06:43,040
community? So I have regular one-on-ones with my team. I find out what's growth mean.

60
00:06:44,020 --> 00:06:48,980
So I want to make sure that they become more of a quality and testing expert. So I will give them,

61
00:06:49,700 --> 00:06:52,900
point them towards tools they may need, connect them with each other. Again, I said,

62
00:06:52,900 --> 00:06:58,100
I manage a community. So as I find areas where I think even though they work in completely

63
00:06:58,100 --> 00:07:01,060
separate teams where they can help each other out, I will help make that connection.

64
00:07:01,940 --> 00:07:05,950
That seems stupid. It seems like your job is to just make sure,

65
00:07:07,470 --> 00:07:13,710
your job to make sure you said that they're experts, right? Who's actually finding the

66
00:07:13,710 --> 00:07:18,860
bugs? The whole team is. In fact, one of the great things about my team is the whole

67
00:07:18,860 --> 00:07:24,780
team owns quality. The developers actually write most of the testing. The testers help them. They

68
00:07:24,780 --> 00:07:32,860
write some more complex tests. They'll write some diagnostic tools, but the dev team finds

69
00:07:32,860 --> 00:07:36,380
most of the bugs themselves. Then how do you grow your testers?

70
00:07:37,710 --> 00:07:40,430
I make sure they have. If you have a bunch of testers whose,

71
00:07:43,790 --> 00:07:50,430
testers whose job it is, is to find bugs and you have all of your dev team finding the bugs,

72
00:07:50,430 --> 00:07:53,630
then what do you grow your dev team? That's an interesting statement because I

73
00:07:53,630 --> 00:07:58,270
don't believe it's the at all, it's the responsibility of my test team to find bugs.

74
00:07:58,270 --> 00:08:02,350
The roles I put it on my team is they're there to accelerate the achievement of shippable quality.

75
00:08:03,490 --> 00:08:08,050
And they do that sometimes they're finding bugs and really complex, interesting bugs.

76
00:08:09,010 --> 00:08:12,610
But most of the bugs they find are actually through data analysis and looking at how

77
00:08:12,610 --> 00:08:18,370
customers using the product versus doing a writing typical tests or test automation.

78
00:08:19,010 --> 00:08:23,090
The dev as far as functional test automation goes, our development team writes all of that stuff.

79
00:08:26,100 --> 00:08:29,540
So you have testers, that's what you call them, right?

80
00:08:29,540 --> 00:08:31,540
That's, I could call them anything I want.

81
00:08:31,540 --> 00:08:37,060
Okay. So you have testers who don't find bugs and you just got through saying they don't even

82
00:08:37,060 --> 00:08:38,020
write automation?

83
00:08:38,020 --> 00:08:43,460
No, they don't because the dev team writes that. It's redundant. It's inefficient for the dev team

84
00:08:44,020 --> 00:08:45,860
to not test their own code.

85
00:08:46,420 --> 00:08:51,860
So how do you stack rank these guys at review? What I heard you just say is that you have a whole

86
00:08:51,860 --> 00:08:54,180
team whose job it is, is to occupy space.

87
00:08:54,740 --> 00:09:02,100
No, not at all. In fact, well, one, we don't stack rank. That's just a process we don't use trying

88
00:09:02,100 --> 00:09:07,060
to reward people, everyone based on their impact and value to the team.

89
00:09:08,300 --> 00:09:10,860
But what was the other part of your question? How do I, what do they do?

90
00:09:10,860 --> 00:09:11,580
Oh, what do they do?

91
00:09:11,580 --> 00:09:14,940
You say you have testers who don't file bugs and they don't do automation.

92
00:09:14,940 --> 00:09:19,340
This is all moved. You said it's all moved to the dev team and we should talk about how

93
00:09:19,340 --> 00:09:20,220
stupid that is.

94
00:09:20,780 --> 00:09:31,490
But what I want to understand is how do you grow a community, a community of people whose

95
00:09:31,490 --> 00:09:38,450
job it is, it sounds like to occupy space. Like you, you schedule, you know, fatty lunches

96
00:09:39,010 --> 00:09:42,370
so that they grow how much space they occupy. What are you talking about?

97
00:09:43,840 --> 00:09:46,080
Sounds like you got a bunch of people who do nothing.

98
00:09:46,080 --> 00:09:51,440
It's actually quite the opposite. And in fact, and almost every single one of my teams,

99
00:09:51,440 --> 00:09:59,280
the test person, the quality person and some teams is called productivity engineers.

100
00:09:59,280 --> 00:10:05,360
I know some teams call them quality assistants. They are viewed across the team in most cases

101
00:10:05,360 --> 00:10:12,000
as the most valuable person on that team because their job is to help see the big picture.

102
00:10:12,000 --> 00:10:17,280
If you look at testing top to bottom, including everything from unit test to

103
00:10:17,280 --> 00:10:22,240
Explorer, short testing and Illates and code analysis and code coverage,

104
00:10:22,240 --> 00:10:27,360
it's their job really to make sure all of that happens versus do all of that.

105
00:10:28,080 --> 00:10:32,560
So while the devs on our team may be focused on, again, they're writing,

106
00:10:32,560 --> 00:10:35,200
they're implementing code and they're writing tests for that.

107
00:10:36,640 --> 00:10:43,120
Often even the writing more tests, they may not see the big picture. And what I see across my

108
00:10:43,120 --> 00:10:48,480
team is everyone on the team helps the entire feature team see the big picture and see what's

109
00:10:48,480 --> 00:10:55,950
missing in order to ship a quality product. It seems terribly inefficient. We have that same

110
00:10:55,950 --> 00:11:04,910
thing in our space. We call it a release manager and a program manager. That's their job to figure

111
00:11:04,910 --> 00:11:12,700
it out. So it sounds like you got a whole army of people who do what we do with a...

112
00:11:12,940 --> 00:11:17,340
I wouldn't even say it's an army. We have one tester per feature team. So the ratio,

113
00:11:17,340 --> 00:11:22,860
if you're counting ratios, it's going to be somewhere between one to five and one to nine

114
00:11:24,020 --> 00:11:29,150
tester for every developer, for everyone focused on implementing code primarily.

115
00:11:29,710 --> 00:11:34,110
In fact, one thing maybe worth to mention is our whole team is made up of

116
00:11:35,010 --> 00:11:39,090
of what we call generalizing specialists or specializing generalists.

117
00:11:39,330 --> 00:11:45,330
Everybody can do a lot of different things. And what my team does is provides their expertise,

118
00:11:45,330 --> 00:11:50,530
their specialization is in the areas of quality and testing. But most of the testers on my team

119
00:11:50,530 --> 00:11:59,660
have also written shipping product code as well. I don't worry about one group of people

120
00:11:59,660 --> 00:12:05,020
having one responsibility. Everybody can do everything. Of course, they have specialties

121
00:12:05,020 --> 00:12:07,660
and they're better at some things and that's the way they work.

122
00:12:08,220 --> 00:12:14,910
Actually, I want to point out one more thing. You said talk about inefficiency. It sounds like

123
00:12:14,910 --> 00:12:21,070
from your summary, with doing all the testing at the end, when you find those bugs, what happens?

124
00:12:21,070 --> 00:12:23,310
We ship them back to dev.

125
00:12:23,310 --> 00:12:28,910
And don't you find that inefficient? If they find these bugs, they have to go back and

126
00:12:28,910 --> 00:12:33,870
relearn the code because they may have written them in it weeks or months or longer ago and

127
00:12:33,870 --> 00:12:38,270
they have to go relearn that code. Remember where that bug is and try and fix it. And then

128
00:12:39,230 --> 00:12:44,510
they fix it and you test it again and it's not fixed. And there's this horrible game of ping pong

129
00:12:44,510 --> 00:12:49,870
going back and forth where if you just fix the bugs as you found them, wouldn't that be more efficient?

130
00:12:53,280 --> 00:12:58,160
I'm here representing the test discipline and that's a dev discipline problem.

131
00:13:00,580 --> 00:13:03,460
I'm really happy with my partner.

132
00:13:03,460 --> 00:13:07,380
But you said your job was to sign off. And how can you sign off ever?

133
00:13:08,340 --> 00:13:12,580
It's like if you keep on moving half the distance to something over and over and over,

134
00:13:12,580 --> 00:13:14,980
you never actually get there. So how do you know when you can ship?

135
00:13:15,780 --> 00:13:20,020
I just do another test pass and then I won't sign off until it's clean.

136
00:13:22,510 --> 00:13:33,390
My job is to test. I've been doing this job for a very long time. My peers have been doing

137
00:13:33,390 --> 00:13:40,350
their job for a very long time. There's no reason for me to distrust that that dev hasn't

138
00:13:40,350 --> 00:13:45,630
figured out how to do their job optimal. Well, I've been doing this for a long time also.

139
00:13:45,630 --> 00:13:51,630
One thing I've noticed is the way we make software and the type of software we make

140
00:13:51,630 --> 00:13:57,710
has changed quite a bit. So we're shipping our software. Most of what we ship ships every day.

141
00:13:59,180 --> 00:14:03,660
How can I do a full test pass and ship every day? How can you do a full test pass,

142
00:14:03,660 --> 00:14:10,740
ship every day? We don't ship every day. That's too risky. I got a huge full test pass that I

143
00:14:10,740 --> 00:14:14,660
got to get done in order to make sure it's clean enough to go out in the customer's eyes.

144
00:14:15,740 --> 00:14:25,200
Like, look, here's the reality today. We got a bunch of new kids coming from college.

145
00:14:26,080 --> 00:14:37,360
They're trained in what you call agile, I call cowboy. You just yourself said there's no controls

146
00:14:37,360 --> 00:14:43,360
that your company has over your dev. They fix whatever they want. They ship whenever they want.

147
00:14:45,120 --> 00:14:50,880
Who's to say without these controls in place, you don't know what risks are going to ship out

148
00:14:50,880 --> 00:15:01,020
to the customer. At the end of the day, my job is on the line. I'm the one accountable to make sure

149
00:15:01,900 --> 00:15:08,460
that when I sign off, this thing's good. So many things you said I want to comment on.

150
00:15:08,460 --> 00:15:16,060
I understand, I guess, why you might consider a process that moves so much more quickly to be

151
00:15:16,620 --> 00:15:25,740
air quote cowboy. But what I've actually found on my team is we, in a very brief meeting every

152
00:15:25,740 --> 00:15:30,860
morning, we know exactly which bugs and which feature work we're doing every day, what's going

153
00:15:30,860 --> 00:15:37,100
to ship that day. And we can make confident risk, even all even yours, yours, we can make

154
00:15:37,100 --> 00:15:46,620
confident risk based decisions on getting those things out. One thing is we count on customer

155
00:15:46,620 --> 00:15:53,550
data a lot for determining which bugs to fix. So if we find a bug, but we know through the data we

156
00:15:53,550 --> 00:15:59,070
get from our customer analysis that customers aren't hitting it, we may not prioritize or we

157
00:15:59,070 --> 00:16:05,790
may not even fix that bug. When we roll out, when we, I said we ship every day, but when we ship,

158
00:16:06,430 --> 00:16:13,230
we ship to a subset of customers and we use the data to find out whether that's working for them

159
00:16:13,230 --> 00:16:17,550
and ensure that it's working for that subset before we go to larger subsets and eventually

160
00:16:17,550 --> 00:16:21,470
all of our customers. And that's one of the things that allows us to ship every day.

161
00:16:23,550 --> 00:16:28,110
You mentioned you have a full test pass you have to run. When I should mention, we also have many,

162
00:16:28,110 --> 00:16:34,190
many tests that run, but they run in production as tests against our production data and they

163
00:16:34,190 --> 00:16:40,030
fire alerts that we can react to when any, any anomaly, I wouldn't call it a failure.

164
00:16:40,670 --> 00:16:45,230
Any of those happen. That allows us to react quickly and fix. And when we make a mistake,

165
00:16:45,230 --> 00:16:50,030
we can, when a bug happens, it's not the end of the world. Cause we're able to roll back

166
00:16:50,030 --> 00:16:56,110
quickly to a previous version that doesn't have that bug fix and then roll forward again.

167
00:16:56,110 --> 00:17:02,750
It's a different world for you because I know you want to, you want to have control

168
00:17:03,630 --> 00:17:08,990
and you want to save your job by signing off on a high quality product where I live in a world where

169
00:17:09,950 --> 00:17:14,510
we want to get better every day. And one of the things to get better every day and to improve

170
00:17:14,510 --> 00:17:21,300
every day is we're going to make mistakes and those are okay. They're not job ending. And we're

171
00:17:21,300 --> 00:17:25,540
going to re we're going to react appropriately when those happen and learn from those and not

172
00:17:25,540 --> 00:17:39,900
make those same mistakes again. All it takes is one regression that is rarely hit, but has such

173
00:17:39,900 --> 00:17:48,580
impact that causes your customers to reject your product. All it takes is just one of those in

174
00:17:48,580 --> 00:17:58,910
your company's gone. So one thing that your one regression that doesn't get tested in your daily

175
00:17:58,910 --> 00:18:04,350
test pass because it's, it's super rare. All it takes is one customer to go through that.

176
00:18:05,440 --> 00:18:12,690
They lose all of their data and company's gone. So a couple of things. One, we still have monitoring

177
00:18:12,690 --> 00:18:18,050
for those things. Find out if they're failing. Two, one of the things, because we're not,

178
00:18:18,050 --> 00:18:22,930
because we're not focused on writing a test pass or running a test pass every day and writing

179
00:18:23,650 --> 00:18:28,930
a tons of functional automation. One of the things my testing quality experts do

180
00:18:29,570 --> 00:18:37,010
is write a lot of reliability tests to hopefully catch some of those things. But again, rely on

181
00:18:37,010 --> 00:18:40,770
monitoring to figure out if those things are going to happen. I would argue and say,

182
00:18:42,370 --> 00:18:46,930
I'll agree and say you will losing customer trust is an awful thing to do.

183
00:18:47,650 --> 00:18:55,410
But if you're only shipping every year or two, you're not able to get new features or new bug

184
00:18:55,410 --> 00:19:01,250
fixes your customers need. I would say that also loses a lot of trust and they'll ship to a

185
00:19:01,250 --> 00:19:07,890
competitor who can give them what they need quicker. And I would argue that psychologically,

186
00:19:07,890 --> 00:19:13,410
and again, you, you described a sort of doomsday scenario bug losing data, which again,

187
00:19:13,410 --> 00:19:21,250
I agree is horrible. But if we ship a product and has a little inconsistency, that's a bug,

188
00:19:21,250 --> 00:19:26,370
and you may never ship with that when we're able to fix that again, the next day, plus give them

189
00:19:27,330 --> 00:19:33,490
new functionality or fix something else, that product satisfaction goes up even higher.

190
00:19:37,420 --> 00:19:43,040
I'd like to see your, your numbers for that. From where I sit, I mean, it's,

191
00:19:43,120 --> 00:19:52,160
it again sounds like you got a bunch of Cowboys producing lead code for, for a disorganized

192
00:19:53,520 --> 00:20:04,160
product. Like we have armies of PMs that go through talk to customers and upfront understand

193
00:20:04,160 --> 00:20:11,360
exactly the scenario we need. So I don't believe that customers, you can, we talk to customers a lot,

194
00:20:12,000 --> 00:20:18,160
but what I've found is it's, they really don't know what they want. And it's very important to

195
00:20:18,160 --> 00:20:26,780
give them what they need versus what they want. We have some PMs and they do connect with customers

196
00:20:26,780 --> 00:20:36,240
extensively, but we experiment a lot. Meaning that if we think we have, when we, as soon as

197
00:20:36,240 --> 00:20:40,160
we think we have an idea of what a solution for it to solve the customer problem, a real

198
00:20:40,160 --> 00:20:45,840
problem that we've discovered, we will run an experiment. Meaning we'll deploy the solution to

199
00:20:45,840 --> 00:20:51,040
that, the feature that fixes that, the new application to a small subset of our users.

200
00:20:51,760 --> 00:20:58,400
Look at the data. We format, have specific business objectives in mind that we want to

201
00:20:58,400 --> 00:21:03,760
measure as part of that rollout. We give them that information. We, we, we roll that out to that

202
00:21:03,760 --> 00:21:10,080
subset of customers. We collect that information and we'll only roll forward if the metrics

203
00:21:10,720 --> 00:21:19,280
show that is a good business decision to move forward versus, and that works much better for

204
00:21:19,280 --> 00:21:24,480
us. And we can run multiple experiments a day, try many, many different solutions versus the,

205
00:21:25,040 --> 00:21:29,440
let's build, let's build something for a long time and give it to you and tell you you like it.

206
00:21:30,000 --> 00:21:37,120
That seems cowboy to me. We're very adaptive. Your model is a predictive model where you know

207
00:21:37,120 --> 00:21:42,320
that this is coding time. This is testing time. This is what first, this is PM talking to customer

208
00:21:42,320 --> 00:21:48,320
time. Then we code and then way down here we test. We are constantly adapting and learning.

209
00:21:48,320 --> 00:21:54,640
I have a customer. I have a very far from cowboy that you it's, it's ridiculous. You would use

210
00:21:54,640 --> 00:22:01,900
that term. I have a, an easy heuristic. Let's look at the Microsoft stock price.

211
00:22:03,950 --> 00:22:13,620
When Microsoft was full on waterfall, the stock price was unstoppable. Once we started shifting to

212
00:22:13,620 --> 00:22:21,390
agile, stayed flat and kind of has stayed there since I think there's an obvious conclusion.

213
00:22:21,390 --> 00:22:24,590
I think you throw in the baby out with the bathwater a lot there because,

214
00:22:25,390 --> 00:22:34,420
I'm not throwing no baby or, I don't think that if, and again, one thing that my team does

215
00:22:34,420 --> 00:22:39,460
is we look, we understand a lot more about data and how data and statistics work.

216
00:22:40,020 --> 00:22:47,780
And what you have described here is a classic example of correlation without causation.

217
00:22:47,780 --> 00:22:52,340
If the only thing that had changed at Microsoft was, the only thing that had changed in the

218
00:22:52,340 --> 00:22:56,100
industry, if you look at this as a big, as a system, if the only thing that had changed

219
00:22:56,180 --> 00:23:04,580
was Microsoft air quote moving to agile. And I can tell you that one, Microsoft didn't and has not

220
00:23:04,580 --> 00:23:11,460
made a wholehearted across the board move to agile across the teams. But even if that were the case,

221
00:23:12,340 --> 00:23:19,220
you also have seen a huge shift in moving from shrink wrap products you buy in a store

222
00:23:19,220 --> 00:23:25,300
to services and subscription models, which end up bringing in money over the long-term,

223
00:23:25,300 --> 00:23:31,380
much better than the short-term. But you have to look at the entire system versus one anecdote of

224
00:23:31,380 --> 00:23:37,780
change to be able to make any reasonable correlation that can be believed. So there's

225
00:23:37,780 --> 00:23:47,740
just a lot more going on there. Brent is doing a really good job making the face that of a

226
00:23:47,740 --> 00:23:53,340
traditional test manager to these comments. This should be a video cast because he's selling the

227
00:23:53,340 --> 00:23:59,420
roll well. So let me ask you a question. Yeah, go ahead. Talk about you want to sign off on the

228
00:23:59,420 --> 00:24:06,540
product and you run these test passes and they pass. And so do you fix all the bugs you find?

229
00:24:07,500 --> 00:24:11,500
How many bugs do you have in your products right now? Just throw a number out.

230
00:24:12,820 --> 00:24:17,300
We have a pretty big product. So we have a couple thousand that still need to be fixed.

231
00:24:18,500 --> 00:24:22,100
And when are you going to fix these? But that's okay. We have, I mean,

232
00:24:23,460 --> 00:24:32,500
CodeComplete should close in a month or so. And then we have already in our schedule another

233
00:24:32,500 --> 00:24:38,020
four months just for bug fixing. So I'm not worried about it. We have a big dev team. They'll

234
00:24:38,020 --> 00:24:42,500
knock those down. And how many new bugs will they create in fixing those couple thousand?

235
00:24:43,140 --> 00:24:52,860
Well, you talk about stats. I've done the stats here. And in general, the rule of thumb is for

236
00:24:52,860 --> 00:25:00,560
every 10 bug fixes, they'll regress or create another one. So it's predictable. I'm not worried

237
00:25:00,560 --> 00:25:07,490
about it. So what happens if you, do you have a ship date planned? Yeah, we have to hit November

238
00:25:07,490 --> 00:25:13,810
so that we can get our product out by Christmas. Okay, great. So what happens if bug fixing goes

239
00:25:13,810 --> 00:25:22,210
slower than expected? It doesn't seem very predictable. Well, so with that amount of time,

240
00:25:23,460 --> 00:25:30,930
we have about six months. Oh, I just changed the calendar in our stick.

241
00:25:31,810 --> 00:25:39,360
Yeah. We're in a time machine. It's all right. We're in a time machine. So I guess it's May.

242
00:25:40,560 --> 00:25:47,820
For our father, dads and grads. We have about six months. Our listeners get the point. It's okay.

243
00:25:49,490 --> 00:25:59,170
So we have about six months. Like the scorecard is automated. The exit criteria is published.

244
00:25:59,890 --> 00:26:09,570
Again, I have in the dev team, I have what I view as five star developers will will

245
00:26:11,570 --> 00:26:19,710
continue to track progress. My dev manager even has has created a new principle around how many

246
00:26:19,710 --> 00:26:26,910
bugs they got a fixed fix to get it down. And they're constantly re triaging the bugs.

247
00:26:27,900 --> 00:26:33,980
The dev team and the PMs are constantly re triaging the bugs. And towards the end,

248
00:26:34,620 --> 00:26:40,060
like whichever ones they haven't fixed, they'll be prepared to to cut. You mentioned, and I'm

249
00:26:40,060 --> 00:26:44,060
going to bring this up because you brought up the word first, but you mentioned inefficiency.

250
00:26:44,060 --> 00:26:49,420
And I talked about the inefficiency of bugs going back and forth from test to dev. But if

251
00:26:49,420 --> 00:26:56,460
you're re triaging constantly, you're looking at the same bugs over and over, determining which

252
00:26:56,460 --> 00:27:02,060
ones are going to fix. We talked about regression rates for bugs. Also, given that code complete

253
00:27:02,060 --> 00:27:08,220
isn't done yet, you will find a lot more bugs after that given your model. So let me talk about,

254
00:27:08,220 --> 00:27:13,600
and I don't know if I mentioned this already, but there's quite a bit of difference in between

255
00:27:13,600 --> 00:27:22,750
the way you and I approach bugs. When our team finds a bug, or if a customer reports a bug,

256
00:27:23,790 --> 00:27:29,070
we look at it immediately and choose to either fix it immediately or to not fix it at all.

257
00:27:29,870 --> 00:27:33,550
Our bugs, and again, Brent is doing the great traditional test manager phase,

258
00:27:34,270 --> 00:27:41,870
our bugs are fixed or punted daily. What this means is across our org, we have zero bugs on a

259
00:27:41,870 --> 00:27:46,830
daily basis. What you have zero bugs in your database, you just got through saying that you're

260
00:27:47,470 --> 00:27:53,280
punting things proactively. What's going to happen if a customer comes back later

261
00:27:54,080 --> 00:27:58,720
and that thing that you punted, we already know about the bug.

262
00:27:58,720 --> 00:28:02,240
Usually when we decide to punt a bug, and there's another point I want to make in a second here,

263
00:28:02,240 --> 00:28:07,920
usually when we decide to punt a bug, at the very least we'll add some extra data,

264
00:28:08,640 --> 00:28:12,960
some analysis, some instrumentation in the code around that. So if it happens again,

265
00:28:13,680 --> 00:28:19,840
if it or when it happens again, we have enough data about that issue and when it happens to make a

266
00:28:19,840 --> 00:28:25,920
better choice on whether to fix it immediately or not. But it's purely a business decision. We look at

267
00:28:25,920 --> 00:28:30,240
the work we have to do for new features, for new functionality, along with incoming

268
00:28:30,240 --> 00:28:34,960
bugs, which are risk, and we make a business decision on what is this important to fix right

269
00:28:34,960 --> 00:28:41,040
away or not. And it's funny you mentioned you have the bugs in the bug database. This is

270
00:28:41,040 --> 00:28:44,960
giving your reaction so far. This may really freak you out. It may cause you to use the

271
00:28:44,960 --> 00:28:50,670
Calaboy word yet again, but we don't really use a bug database. Since we're fixing bugs as we find

272
00:28:50,670 --> 00:28:57,230
them, bugs go into our work item tracking and they're fixed right along with feature implementation

273
00:28:57,230 --> 00:29:00,190
and there's no reason to track them in a bug database.

274
00:29:01,630 --> 00:29:09,550
Again, traditional TMs head just exploded. Again, if I don't have a bug database,

275
00:29:09,630 --> 00:29:14,590
then I have no way of measuring my testers at review time.

276
00:29:14,590 --> 00:29:17,470
So the only value your testers bring to the team is bugs?

277
00:29:17,470 --> 00:29:23,070
Yeah, that's their job. His job is to find bugs to make sure that we build a quality product.

278
00:29:24,670 --> 00:29:32,750
Again, I'm still a little unclear as to, you got through telling me your testers don't find bugs.

279
00:29:32,750 --> 00:29:43,730
You don't even have a bug database. They don't automate their features. To me, it seems like

280
00:29:44,370 --> 00:29:52,530
your dev's a bunch of cowboys and your test team is a bunch of scam artists sitting around getting

281
00:29:52,530 --> 00:29:59,970
paid to do nothing. And then you, you as a community leader, you've also broadened this scam

282
00:30:00,610 --> 00:30:04,050
in a way that such that when the product ships and it's crap,

283
00:30:05,330 --> 00:30:08,930
no one's going to come after you because your job is to build a community.

284
00:30:09,730 --> 00:30:19,250
Yet we ship dozens of high quality products that customers love every day, despite the scam.

285
00:30:20,210 --> 00:30:28,210
And when I look at the value my team or my community provides, it's one of the things

286
00:30:28,210 --> 00:30:34,100
I look for, one mantra is better quality, better product every day. And my better product

287
00:30:34,100 --> 00:30:39,940
is more value to the customers, which is it could be more functionality. It could be higher quality

288
00:30:39,940 --> 00:30:44,740
and measured in a bunch of different ways. Their role again in achieving, sorry,

289
00:30:44,740 --> 00:30:49,220
accelerating the achievement, higher quality. Higher quality you said.

290
00:30:49,220 --> 00:30:55,380
I did say higher quality. Right. So higher quality, by definition,

291
00:30:56,020 --> 00:31:01,060
the way you've set up the year, how you guys do things, if you have zero bugs every day,

292
00:31:02,100 --> 00:31:06,820
right, you already have perfect quality every day. Well, that's, and that's because you're

293
00:31:06,820 --> 00:31:11,460
only if you have the moronic assumption that lack of bugs equals quality.

294
00:31:11,460 --> 00:31:18,580
We have dozens, if not hundreds of health metrics we track across our products that

295
00:31:18,580 --> 00:31:25,620
everything from reliability to performance latency to every single error that can happen

296
00:31:25,620 --> 00:31:31,220
in in our code pass all monitored to make sure they're improving constantly over time.

297
00:31:31,940 --> 00:31:39,580
So bugs are one measure of quality, but all bugs are not created equal. In fact, I wouldn't,

298
00:31:39,580 --> 00:31:44,380
I would even argue that bugs maybe aren't a measure of quality of bugs.

299
00:31:44,380 --> 00:31:50,060
Could be a measure of engineering quality. You can't just as part of this debate,

300
00:31:50,060 --> 00:31:55,660
you can't just redefine the definition of quality. What is quality then, Mr. Test Manager?

301
00:31:55,660 --> 00:32:02,850
Quality is the bug count. It has been the bug count for forever in the test community.

302
00:32:02,850 --> 00:32:09,410
In my world, it is not. Quality is the value of our product to our customers.

303
00:32:10,270 --> 00:32:17,790
And you, I think are losing your left nut over this because you cannot fathom the idea of it

304
00:32:17,790 --> 00:32:24,510
being anything but bugs or anything you can't distinctly measure in one way while using

305
00:32:25,230 --> 00:32:35,010
bug metrics or code coverage metrics or test pass rates. Those things all have their place,

306
00:32:35,010 --> 00:32:41,010
but I don't even, we run tests all the time. I do not have a single dashboard in place that

307
00:32:41,010 --> 00:32:46,290
shows me the pass rate of those tests because that information isn't useful. If there are

308
00:32:46,290 --> 00:32:51,410
failures, we fix the bugs. Of course it's useful. Useful for what? What's your test

309
00:32:51,410 --> 00:32:56,370
pass rate good for? What do you use your test pass rate for? It's primarily to understand which

310
00:32:56,370 --> 00:33:02,930
dev teams need to consider a change in their practices to avoid regressions. So if you

311
00:33:02,930 --> 00:33:09,230
have a dev team that has any failures in their tests, what do you do to them? How do you go fix

312
00:33:09,230 --> 00:33:16,590
their practices? Well, we file the bugs and then again we have dashboards that will display which

313
00:33:16,590 --> 00:33:23,070
dev teams are excelling at preventing regressions and which teams are sucking. So sort of shame

314
00:33:23,070 --> 00:33:26,830
dashboards. Yeah, it's very effective. Have you ever heard of Hawthorne's principle?

315
00:33:26,830 --> 00:33:32,350
No, I have. I have the Hawthorne effect, which means that you get what you measure.

316
00:33:33,500 --> 00:33:40,780
Right. And I want fewer regressions. So do you think there's any chance at all of the dev teams

317
00:33:40,780 --> 00:33:46,300
that are, air quote, excelling are gaming that system? Well, certainly not in your system.

318
00:33:47,330 --> 00:33:53,970
Since you hide all bugs away from everybody. But we don't do that. We, you know this for

319
00:33:53,970 --> 00:34:00,130
a fact that there are bugs you find that you go, Oh my God, you got to fix this bug that no customer

320
00:34:00,130 --> 00:34:05,010
has ever or will ever hit. Yet you ship your product and customers find bugs. Then you go

321
00:34:05,010 --> 00:34:10,060
back and add a test for that because you forgot it. And then three months later, six months later,

322
00:34:10,060 --> 00:34:17,340
a year later, they get a fix for that bug. In my cowboy world, what we do is we constantly

323
00:34:17,340 --> 00:34:23,660
monitor and react. And if there's something that a customer finds that or customer is into

324
00:34:23,660 --> 00:34:28,220
something that triggers an error, we will look at that and fix that right away.

325
00:34:29,890 --> 00:34:36,100
The biggest flaw I see in the, you're just talking about your team, right?

326
00:34:37,920 --> 00:34:48,270
I'm in a big organization. Like if everyone, with several thousand developers contributing,

327
00:34:49,260 --> 00:34:57,660
if everyone were to do this, then we would end up with one big piece of crap product that didn't

328
00:34:57,660 --> 00:35:05,970
integrate. And you could see the seams in the product left and right. Like I don't see that.

329
00:35:05,970 --> 00:35:13,730
Number one, yeah, I still stand by you've got this beautiful cowboy scam going on.

330
00:35:14,450 --> 00:35:19,810
But number two, the practical reality is I don't see how this scales.

331
00:35:20,530 --> 00:35:24,530
I think scaling to large teams of a thousand is certainly a challenge.

332
00:35:26,190 --> 00:35:32,590
Scaling up to about 300 has proven to be very effective, especially when that team is focused

333
00:35:32,590 --> 00:35:38,190
on a, what we call a large vertical or a vertical slice. Maybe not a vertical slice,

334
00:35:38,190 --> 00:35:43,230
that's something different. A large vertical. It's a challenge. We do write many, many

335
00:35:43,230 --> 00:35:49,390
integration tests and we work, we find many ways to connect those teams together. But

336
00:35:49,950 --> 00:35:56,000
I'll admit that can be a challenge, but I think the fact that we can, because we're

337
00:35:56,000 --> 00:36:00,160
shipping so frequently and the fact that we can react when those integration problems come up,

338
00:36:01,120 --> 00:36:07,040
I think it works for us. Scaling is always going to be a challenge. And there are very

339
00:36:07,040 --> 00:36:11,280
few products in the world. And again, you work at a very large traditional company,

340
00:36:11,280 --> 00:36:19,390
but there are very few products in the world that have development teams of 500 or a thousand

341
00:36:19,470 --> 00:36:26,530
or more. Those are unique to some specific companies. Let me close the door on the time

342
00:36:26,530 --> 00:36:35,740
machine now and bring back real Brent. Oh my God, it was horrible.

343
00:36:38,060 --> 00:36:43,500
So the frightening thing here was if you couldn't see Brent's face, but he actually

344
00:36:43,500 --> 00:36:50,380
really was in that persona. And I could feel 90s Brent coming out and it was, it was,

345
00:36:50,380 --> 00:36:58,510
I spent, I think I've shared this before. I've spent the majority of my time at, in my career

346
00:36:58,510 --> 00:37:08,990
with the title test manager in, in the waterfall world, right? What, what I fell back on on my

347
00:37:08,990 --> 00:37:18,190
inspiration is like, I remember being a test manager when Agile was rolling out and how

348
00:37:18,910 --> 00:37:28,500
horrific it was. Everything you said I've heard, I've heard myself. Everything I said of a great

349
00:37:28,500 --> 00:37:36,140
deal part I used as winning arguments back in the day. I believe it. I believe it. And I had,

350
00:37:36,140 --> 00:37:40,620
I've mentioned on the podcast before, I had a dev director, I was, it was a dev director on Xbox

351
00:37:41,570 --> 00:37:47,730
who said he wanted to have this conversation with me about test and the role and, and

352
00:37:48,450 --> 00:37:54,770
didn't really in the end care what I had to say. But what he said was the role of test is to sign

353
00:37:54,770 --> 00:38:05,540
off. And I thought, you, well, you know, I mean, as we all know, the role of test is to inform.

354
00:38:06,930 --> 00:38:11,730
It's not it either. Right. And I don't think I did in my traditional test manager. One thing

355
00:38:11,730 --> 00:38:17,970
I want to work on my elevator pitch on is that answers like, and you could say very abstractly,

356
00:38:17,970 --> 00:38:22,370
we accelerate the achievement of simple quality, but the points I brought up were absolutely true.

357
00:38:22,370 --> 00:38:27,410
I think if you have someone who is looking at that big picture and finding bottlenecks and

358
00:38:27,410 --> 00:38:32,850
mitigating them and enabling your feature team to move quicker with higher quality,

359
00:38:33,490 --> 00:38:40,370
people see that it's, it may not be measurable in the, to the exactness of bug counts,

360
00:38:41,250 --> 00:38:47,010
but it is highly appreciated and valuable. If you rely a little bit more on user,

361
00:38:47,810 --> 00:38:53,890
on peer feedback, I think the model works. I don't know if in your world, in our world,

362
00:38:53,890 --> 00:39:00,130
if there's a better way to measure that, but I don't really like measurements metrics applied to people.

363
00:39:01,820 --> 00:39:10,460
Metrics applied to people. I will do it today only in the cases where I'm trying to

364
00:39:11,420 --> 00:39:19,100
hot thorns effect can be used for good and for evil. But one of the things that I do think

365
00:39:20,130 --> 00:39:27,250
for those listening, and I think, I think the number one thing. So first off, Alan did a very

366
00:39:27,250 --> 00:39:38,210
good job, in my view, defending the modern role and incorporating tasks. I think the one thing

367
00:39:39,170 --> 00:39:47,330
that's probably important to hit hard is the thing of, yeah, we are redefining quality,

368
00:39:48,260 --> 00:39:56,180
right? Quality is from a customer point of view. And I think, I think hitting that from a business

369
00:39:56,180 --> 00:40:01,220
sense. Yeah. One thing I've said before to interrupt is the traditional approach was a lot

370
00:40:01,220 --> 00:40:07,620
about measuring engineering quality. Yeah, I call it correctness. But like we wanted to test

371
00:40:07,620 --> 00:40:11,860
the quality of the engineering system we were making versus the quality of the product that

372
00:40:11,860 --> 00:40:19,220
customers use. Right. A lot of our scorecards back in the day, a lot of our scorecards,

373
00:40:19,220 --> 00:40:27,780
their whole purpose in life. And I will say this now, I spent a lot of my time, I used to have this

374
00:40:27,780 --> 00:40:35,810
speech about, again, as a test manager, how I rated subjectively different test leads,

375
00:40:36,290 --> 00:40:44,130
not really rated is the wrong way of saying it, but your skill on being a test lead heavily correlates

376
00:40:44,930 --> 00:40:52,340
to how well you can answer the following question. Why was this bug not found before?

377
00:40:53,940 --> 00:41:05,520
That just hurts me. And so a lot of the, a lot of my infrastructure was built around not only

378
00:41:06,400 --> 00:41:17,680
building the best product I could, based upon a greed upon exit criteria in a predictive fashion,

379
00:41:18,320 --> 00:41:27,360
but it was also sort of playing a chess game in assuring that I was super transparent all along

380
00:41:28,240 --> 00:41:35,040
that when the inevitable dev doesn't have enough time to fix bugs, PM has decided that we need to

381
00:41:35,040 --> 00:41:40,080
get these other additional DCRs in like we didn't talk about that in their discussion.

382
00:41:40,800 --> 00:41:44,480
DCR is design change request for those asking at home.

383
00:41:45,200 --> 00:41:52,800
How do I assure that when I come back and say, Hey, you've now compressed me, and we have to slip

384
00:41:54,060 --> 00:41:59,760
that, that I'm on the moral high ground. So Hawthorne's effect in that particular case was

385
00:41:59,760 --> 00:42:06,160
all about proactively or part of it was about proactively identifying the teams that are,

386
00:42:06,160 --> 00:42:13,620
that are actually causing the product to slip. And it's not just because I come last.

387
00:42:14,340 --> 00:42:20,400
Correct. So there's a lot of proactive finger pointing in that model.

388
00:42:20,400 --> 00:42:28,690
And to be fair, because we were shipping very infrequently at that time, the measuring

389
00:42:28,690 --> 00:42:35,090
engineering quality was the best proxy we had, but we just have better, we have better ways to

390
00:42:35,090 --> 00:42:41,380
measure customer quality now. And we need to take advantage of those. It is a shift. And I see why

391
00:42:41,380 --> 00:42:48,480
you're stuck where you are, Mr. BTM, Brent, traditional manager. But it's just a different

392
00:42:48,480 --> 00:42:55,310
world we live in both in the end. And it'd be funny, maybe 10 years from now, I'll be dead,

393
00:42:55,310 --> 00:43:02,670
but you'll be having a podcast with, and maybe it's shifted yet again. But probably not going

394
00:43:02,670 --> 00:43:11,950
backward. I think we've talked before, drink, that my role doesn't exist. In fact, I've started to

395
00:43:11,950 --> 00:43:20,260
float that idea more up than down that if I do my job correctly, I work myself out of a job

396
00:43:20,260 --> 00:43:26,610
because the community I've built is self-sustaining. Yeah. I completely.

397
00:43:27,490 --> 00:43:30,610
And I'm okay with that. Maybe I just take out, I don't know what I'd do at that point,

398
00:43:30,610 --> 00:43:37,570
take out a larger community or, I don't know, I haven't thought about it. I'm not going to fix it

399
00:43:37,570 --> 00:43:44,500
this year. I'm not going to build that in. But it is not just my community. I mean,

400
00:43:44,500 --> 00:43:50,100
my community needs to become all the engineers for the features I'm responsible for.

401
00:43:50,100 --> 00:43:57,940
What I realized so several years ago, and when I came to that sort of conclusion,

402
00:43:57,940 --> 00:44:03,380
and this isn't the first time you've done this, it just so happened. This isn't the new learning for

403
00:44:03,380 --> 00:44:11,620
you. But when I came to that same conclusion, I realized that I needed to begin to invest my

404
00:44:11,620 --> 00:44:19,220
time in a different direction. Right? And a lot of kind of, there is a lot of alignment between

405
00:44:21,600 --> 00:44:28,960
my philosophy as a test manager and my philosophy now as a data scientist. I'm still just as

406
00:44:28,960 --> 00:44:35,760
passionate about customer quality. My inner tester is still there. Yeah, for sure. It just doesn't

407
00:44:35,760 --> 00:44:49,300
care about testing. He cares about how do we win in providing the most customer quality against

408
00:44:49,300 --> 00:44:53,380
our competitors. I think one thing that has changed, if you look at it from a little bit

409
00:44:53,380 --> 00:44:59,780
of a meta point of view, is our approach to testing and quality in a traditional model is

410
00:44:59,780 --> 00:45:04,500
very formulaic. We knew we had bug metrics, and we used the same bug metrics from product

411
00:45:04,500 --> 00:45:08,100
to product and code coverage. We measured a bunch of things the same way because it's

412
00:45:08,100 --> 00:45:14,180
the formulas you had to use. And like software, our approach to quality has become much more

413
00:45:14,180 --> 00:45:20,980
adaptive and relied on our own and our team's constant learning. And I think that will continue.

414
00:45:20,980 --> 00:45:26,740
That's where the growth and the wins will happen. There'll be other things that will happen

415
00:45:26,740 --> 00:45:34,020
around applying AI and machine learning and much, much more of what you're doing in the data

416
00:45:34,020 --> 00:45:41,170
science world. The reason why I brought that up is, to me, I still think it's an obvious thing.

417
00:45:44,420 --> 00:45:54,910
As you are working yourself out of a job, begin to stepwise incorporate data analysis and data

418
00:45:54,910 --> 00:46:03,630
analytics into your team's duties. Hey, now Dev's doing all feature automation themselves. Great.

419
00:46:03,630 --> 00:46:08,910
That's fantastic because I need these resources to spend 25% of their time

420
00:46:10,290 --> 00:46:17,410
building statistical models that help us precisely. And actually, in real world, that's what the two

421
00:46:17,410 --> 00:46:21,970
big principles I have on my team is working. Some teams are closest to others to this actually

422
00:46:21,970 --> 00:46:29,330
working from zero bugs. Most teams are very close or there. The two principles are zero bugs and

423
00:46:29,330 --> 00:46:35,650
let's use data. And we'll start off with, we'll start off a little bit. It's heading in the right

424
00:46:35,650 --> 00:46:39,010
direction where I've been a little bit of a tipping point. I'm really excited about it. So, it's fun

425
00:46:39,010 --> 00:46:46,560
to be a modern test manager. Okay, everybody. That's it for this episode of AB testing. I'm

426
00:46:46,560 --> 00:46:50,160
Alan. I'm Brent. We'll see you next time.

